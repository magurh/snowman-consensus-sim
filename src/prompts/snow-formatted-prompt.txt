README.md:
```
# go-flare

go-flare is a modified version of [avalanchego@v1.9.0](https://github.com/ava-labs/avalanchego/releases/tag/v1.9.0) and [coreth@v0.11.0](https://github.com/ava-labs/coreth/releases/tag/v0.11.0), incorporating specific features for Flare and Songbird networks. These features include prioritized contract handling and the invocation of the daemon contract.

All nodes should upgrade to the version 1.9.1 **before the following dates**:
- Coston2 network: November 26, 2024 at 12:00:00 UTC
- Flare network: December 17, 2024 at 12:00:00 UTC
- Coston network: January 7, 2025 at 12:00:00 UTC
- Songbird network: January 28, 2025 at 12:00:00 UTC

## System Requirements
- go version 1.21.8
- gcc, g++ and jq
- CPU: Equivalent of 8 AWS vCPU
- RAM: 16 GiB
- Storage: 1TB Flare / 3.5TB Songbird
- OS: Ubuntu 20.04/22.04

## Compilation

After cloning this repository, run:

```sh
cd go-flare/avalanchego && ./scripts/build.sh
```

## Deploy a Validation Node

These servers fulfill a critical role in securing the network:

- They check that all received transactions are valid.
- They run a consensus algorithm so that all validators in the network agree on the transactions to add to the blockchain.
- Finally, they add the agreed-upon transactions to their copy of the ledger.

This guide explains how to deploy your own validator node so you can participate in the consensus and collect the rewards that the network provides to those who help secure it: https://docs.flare.network/infra/validation/deploying/

## Deploy an Observation Node

Observation nodes enable anyone to observe the network and submit transactions. Unlike validator nodes, which provide state consensus and add blocks, observation nodes remain outside the network and have no effect on consensus or blocks.

This guide explains how to deploy your own observation node: https://docs.flare.network/infra/observation/deploying/

## Tests

See `tests/README.md` for testing details

## Container image

Public container images are hosted on [Docker HUB](https://hub.docker.com/r/flarefoundation/go-flare) and [Github Packages](https://github.com/orgs/flare-foundation/packages?repo_name=go-flare);
```
docker.io/flarefoundation/go-flare
hgcr.io/flare-foundation/go-flare
```

### Container builds in CI

CI builds on each:
- push on `main` branch, pushes image tagged as "dev"
- creation of a tag, pushes images tagged as the tag itself

Builds: \
two images, `go-flare:<TAG>` one with `leveldb`

```

Directory Structure:
[avalanchego/snow/]
avalanchego/snow/README.md
avalanchego/snow/acceptor.go
[avalanchego/snow/choices/]
avalanchego/snow/choices/decidable.go
avalanchego/snow/choices/status.go
avalanchego/snow/choices/status_test.go
avalanchego/snow/choices/test_decidable.go
[avalanchego/snow/consensus/]
[avalanchego/snow/consensus/avalanche/]
avalanchego/snow/consensus/avalanche/consensus.go
avalanchego/snow/consensus/avalanche/consensus_test.go
avalanchego/snow/consensus/avalanche/factory.go
avalanchego/snow/consensus/avalanche/parameters.go
avalanchego/snow/consensus/avalanche/parameters_test.go
[avalanchego/snow/consensus/avalanche/poll/]
avalanchego/snow/consensus/avalanche/poll/early_term_no_traversal.go
avalanchego/snow/consensus/avalanche/poll/early_term_no_traversal_test.go
avalanchego/snow/consensus/avalanche/poll/interfaces.go
avalanchego/snow/consensus/avalanche/poll/no_early_term.go
avalanchego/snow/consensus/avalanche/poll/no_early_term_test.go
avalanchego/snow/consensus/avalanche/poll/set.go
avalanchego/snow/consensus/avalanche/poll/set_test.go
avalanchego/snow/consensus/avalanche/test_vertex.go
avalanchego/snow/consensus/avalanche/topological.go
avalanchego/snow/consensus/avalanche/topological_test.go
avalanchego/snow/consensus/avalanche/transaction_vertex.go
avalanchego/snow/consensus/avalanche/vertex.go
[avalanchego/snow/consensus/metrics/]
avalanchego/snow/consensus/metrics/height.go
avalanchego/snow/consensus/metrics/latency.go
avalanchego/snow/consensus/metrics/polls.go
[avalanchego/snow/consensus/snowball/]
avalanchego/snow/consensus/snowball/binary_slush.go
avalanchego/snow/consensus/snowball/binary_snowball.go
avalanchego/snow/consensus/snowball/binary_snowball_test.go
avalanchego/snow/consensus/snowball/binary_snowflake.go
avalanchego/snow/consensus/snowball/binary_snowflake_test.go
avalanchego/snow/consensus/snowball/consensus.go
avalanchego/snow/consensus/snowball/consensus_performance_test.go
avalanchego/snow/consensus/snowball/consensus_reversibility_test.go
avalanchego/snow/consensus/snowball/consensus_test.go
avalanchego/snow/consensus/snowball/factory.go
avalanchego/snow/consensus/snowball/flat.go
avalanchego/snow/consensus/snowball/flat_test.go
avalanchego/snow/consensus/snowball/network_test.go
avalanchego/snow/consensus/snowball/nnary_slush.go
avalanchego/snow/consensus/snowball/nnary_snowball.go
avalanchego/snow/consensus/snowball/nnary_snowball_test.go
avalanchego/snow/consensus/snowball/nnary_snowflake.go
avalanchego/snow/consensus/snowball/nnary_snowflake_test.go
avalanchego/snow/consensus/snowball/parameters.go
avalanchego/snow/consensus/snowball/parameters_test.go
avalanchego/snow/consensus/snowball/tree.go
avalanchego/snow/consensus/snowball/tree_test.go
avalanchego/snow/consensus/snowball/unary_snowball.go
avalanchego/snow/consensus/snowball/unary_snowball_test.go
avalanchego/snow/consensus/snowball/unary_snowflake.go
avalanchego/snow/consensus/snowball/unary_snowflake_test.go
[avalanchego/snow/consensus/snowman/]
avalanchego/snow/consensus/snowman/block.go
avalanchego/snow/consensus/snowman/consensus.go
avalanchego/snow/consensus/snowman/consensus_test.go
avalanchego/snow/consensus/snowman/factory.go
avalanchego/snow/consensus/snowman/mock_block.go
avalanchego/snow/consensus/snowman/network_test.go
avalanchego/snow/consensus/snowman/oracle_block.go
[avalanchego/snow/consensus/snowman/poll/]
avalanchego/snow/consensus/snowman/poll/early_term_no_traversal.go
avalanchego/snow/consensus/snowman/poll/early_term_no_traversal_test.go
avalanchego/snow/consensus/snowman/poll/interfaces.go
avalanchego/snow/consensus/snowman/poll/no_early_term.go
avalanchego/snow/consensus/snowman/poll/no_early_term_test.go
avalanchego/snow/consensus/snowman/poll/set.go
avalanchego/snow/consensus/snowman/poll/set_test.go
avalanchego/snow/consensus/snowman/snowman_block.go
avalanchego/snow/consensus/snowman/test_block.go
avalanchego/snow/consensus/snowman/topological.go
avalanchego/snow/consensus/snowman/topological_test.go
[avalanchego/snow/consensus/snowstorm/]
avalanchego/snow/consensus/snowstorm/acceptor.go
avalanchego/snow/consensus/snowstorm/benchmark_test.go
avalanchego/snow/consensus/snowstorm/consensus.go
avalanchego/snow/consensus/snowstorm/consensus_test.go
avalanchego/snow/consensus/snowstorm/directed.go
avalanchego/snow/consensus/snowstorm/directed_test.go
avalanchego/snow/consensus/snowstorm/factory.go
avalanchego/snow/consensus/snowstorm/network_test.go
avalanchego/snow/consensus/snowstorm/rejector.go
avalanchego/snow/consensus/snowstorm/snowball.go
avalanchego/snow/consensus/snowstorm/stringer.go
avalanchego/snow/consensus/snowstorm/test_tx.go
avalanchego/snow/consensus/snowstorm/tx.go
avalanchego/snow/context.go
[avalanchego/snow/engine/]
[avalanchego/snow/engine/avalanche/]
[avalanchego/snow/engine/avalanche/bootstrap/]
avalanchego/snow/engine/avalanche/bootstrap/bootstrapper.go
avalanchego/snow/engine/avalanche/bootstrap/bootstrapper_test.go
avalanchego/snow/engine/avalanche/bootstrap/config.go
avalanchego/snow/engine/avalanche/bootstrap/metrics.go
avalanchego/snow/engine/avalanche/bootstrap/tx_job.go
avalanchego/snow/engine/avalanche/bootstrap/vertex_job.go
avalanchego/snow/engine/avalanche/config.go
avalanchego/snow/engine/avalanche/config_test.go
avalanchego/snow/engine/avalanche/engine.go
[avalanchego/snow/engine/avalanche/getter/]
avalanchego/snow/engine/avalanche/getter/getter.go
avalanchego/snow/engine/avalanche/getter/getter_test.go
avalanchego/snow/engine/avalanche/issuer.go
avalanchego/snow/engine/avalanche/metrics.go
[avalanchego/snow/engine/avalanche/mocks/]
avalanchego/snow/engine/avalanche/mocks/engine.go
[avalanchego/snow/engine/avalanche/state/]
avalanchego/snow/engine/avalanche/state/prefixed_state.go
avalanchego/snow/engine/avalanche/state/serializer.go
avalanchego/snow/engine/avalanche/state/state.go
avalanchego/snow/engine/avalanche/state/unique_vertex.go
avalanchego/snow/engine/avalanche/state/unique_vertex_test.go
avalanchego/snow/engine/avalanche/test_avalanche_engine.go
avalanchego/snow/engine/avalanche/transitive.go
avalanchego/snow/engine/avalanche/transitive_test.go
[avalanchego/snow/engine/avalanche/vertex/]
avalanchego/snow/engine/avalanche/vertex/builder.go
avalanchego/snow/engine/avalanche/vertex/builder_test.go
avalanchego/snow/engine/avalanche/vertex/codec.go
avalanchego/snow/engine/avalanche/vertex/heap.go
avalanchego/snow/engine/avalanche/vertex/heap_test.go
avalanchego/snow/engine/avalanche/vertex/manager.go
[avalanchego/snow/engine/avalanche/vertex/mocks/]
avalanchego/snow/engine/avalanche/vertex/mocks/dag_vm.go
avalanchego/snow/engine/avalanche/vertex/parser.go
avalanchego/snow/engine/avalanche/vertex/parser_test.go
avalanchego/snow/engine/avalanche/vertex/sorting.go
avalanchego/snow/engine/avalanche/vertex/stateless_vertex.go
avalanchego/snow/engine/avalanche/vertex/stateless_vertex_test.go
avalanchego/snow/engine/avalanche/vertex/storage.go
avalanchego/snow/engine/avalanche/vertex/test_builder.go
avalanchego/snow/engine/avalanche/vertex/test_manager.go
avalanchego/snow/engine/avalanche/vertex/test_parser.go
avalanchego/snow/engine/avalanche/vertex/test_storage.go
avalanchego/snow/engine/avalanche/vertex/test_vm.go
avalanchego/snow/engine/avalanche/vertex/vm.go
avalanchego/snow/engine/avalanche/voter.go
avalanchego/snow/engine/avalanche/voter_test.go
[avalanchego/snow/engine/common/]
[avalanchego/snow/engine/common/appsender/]
avalanchego/snow/engine/common/appsender/appsender_client.go
avalanchego/snow/engine/common/appsender/appsender_server.go
avalanchego/snow/engine/common/bootstrapable.go
avalanchego/snow/engine/common/bootstrapper.go
avalanchego/snow/engine/common/config.go
avalanchego/snow/engine/common/engine.go
avalanchego/snow/engine/common/fetcher.go
avalanchego/snow/engine/common/fx.go
avalanchego/snow/engine/common/halter.go
avalanchego/snow/engine/common/http_handler.go
avalanchego/snow/engine/common/message.go
avalanchego/snow/engine/common/mixed_query.go
avalanchego/snow/engine/common/mixed_query_test.go
avalanchego/snow/engine/common/mock_sender.go
avalanchego/snow/engine/common/no_ops_handlers.go
[avalanchego/snow/engine/common/queue/]
avalanchego/snow/engine/common/queue/job.go
avalanchego/snow/engine/common/queue/jobs.go
avalanchego/snow/engine/common/queue/jobs_test.go
avalanchego/snow/engine/common/queue/parser.go
avalanchego/snow/engine/common/queue/state.go
avalanchego/snow/engine/common/queue/test_job.go
avalanchego/snow/engine/common/queue/test_parser.go
avalanchego/snow/engine/common/requests.go
avalanchego/snow/engine/common/requests_test.go
avalanchego/snow/engine/common/sender.go
avalanchego/snow/engine/common/state_syncer.go
avalanchego/snow/engine/common/subnet.go
avalanchego/snow/engine/common/subnet_tracker.go
avalanchego/snow/engine/common/test_bootstrapable.go
avalanchego/snow/engine/common/test_bootstrapper.go
avalanchego/snow/engine/common/test_config.go
avalanchego/snow/engine/common/test_engine.go
avalanchego/snow/engine/common/test_sender.go
avalanchego/snow/engine/common/test_subnet.go
avalanchego/snow/engine/common/test_timer.go
avalanchego/snow/engine/common/test_vm.go
avalanchego/snow/engine/common/timer.go
[avalanchego/snow/engine/common/tracker/]
avalanchego/snow/engine/common/tracker/peers.go
avalanchego/snow/engine/common/tracker/peers_test.go
avalanchego/snow/engine/common/tracker/startup.go
avalanchego/snow/engine/common/vm.go
[avalanchego/snow/engine/snowman/]
avalanchego/snow/engine/snowman/ancestor_tree.go
avalanchego/snow/engine/snowman/ancestor_tree_test.go
[avalanchego/snow/engine/snowman/block/]
avalanchego/snow/engine/snowman/block/README.md
avalanchego/snow/engine/snowman/block/batched_vm.go
avalanchego/snow/engine/snowman/block/batched_vm_test.go
avalanchego/snow/engine/snowman/block/height_indexed_vm.go
[avalanchego/snow/engine/snowman/block/mocks/]
avalanchego/snow/engine/snowman/block/mocks/chain_vm.go
avalanchego/snow/engine/snowman/block/mocks/state_syncable_vm.go
avalanchego/snow/engine/snowman/block/state_summary.go
avalanchego/snow/engine/snowman/block/state_syncable_vm.go
avalanchego/snow/engine/snowman/block/test_batched_vm.go
avalanchego/snow/engine/snowman/block/test_height_indexed_vm.go
avalanchego/snow/engine/snowman/block/test_state_summary.go
avalanchego/snow/engine/snowman/block/test_state_syncable_vm.go
avalanchego/snow/engine/snowman/block/test_vm.go
avalanchego/snow/engine/snowman/block/vm.go
[avalanchego/snow/engine/snowman/bootstrap/]
avalanchego/snow/engine/snowman/bootstrap/block_job.go
avalanchego/snow/engine/snowman/bootstrap/bootstrapper.go
avalanchego/snow/engine/snowman/bootstrap/bootstrapper_test.go
avalanchego/snow/engine/snowman/bootstrap/config.go
avalanchego/snow/engine/snowman/bootstrap/metrics.go
avalanchego/snow/engine/snowman/config.go
avalanchego/snow/engine/snowman/config_test.go
avalanchego/snow/engine/snowman/engine.go
[avalanchego/snow/engine/snowman/getter/]
avalanchego/snow/engine/snowman/getter/getter.go
avalanchego/snow/engine/snowman/getter/getter_test.go
avalanchego/snow/engine/snowman/issuer.go
avalanchego/snow/engine/snowman/memory_block.go
avalanchego/snow/engine/snowman/metrics.go
[avalanchego/snow/engine/snowman/mocks/]
avalanchego/snow/engine/snowman/mocks/engine.go
[avalanchego/snow/engine/snowman/syncer/]
avalanchego/snow/engine/snowman/syncer/config.go
avalanchego/snow/engine/snowman/syncer/state_syncer.go
avalanchego/snow/engine/snowman/syncer/state_syncer_test.go
avalanchego/snow/engine/snowman/syncer/utils_test.go
avalanchego/snow/engine/snowman/test_snowman_engine.go
avalanchego/snow/engine/snowman/transitive.go
avalanchego/snow/engine/snowman/transitive_test.go
avalanchego/snow/engine/snowman/voter.go
[avalanchego/snow/events/]
avalanchego/snow/events/blockable.go
avalanchego/snow/events/blocker.go
avalanchego/snow/events/blocker_test.go
[avalanchego/snow/networking/]
[avalanchego/snow/networking/benchlist/]
avalanchego/snow/networking/benchlist/benchable.go
avalanchego/snow/networking/benchlist/benchlist.go
avalanchego/snow/networking/benchlist/benchlist_test.go
avalanchego/snow/networking/benchlist/manager.go
avalanchego/snow/networking/benchlist/metrics.go
avalanchego/snow/networking/benchlist/test_benchable.go
[avalanchego/snow/networking/handler/]
avalanchego/snow/networking/handler/handler.go
avalanchego/snow/networking/handler/handler_test.go
avalanchego/snow/networking/handler/message_queue.go
avalanchego/snow/networking/handler/message_queue_metrics.go
avalanchego/snow/networking/handler/message_queue_test.go
avalanchego/snow/networking/handler/metrics.go
avalanchego/snow/networking/handler/parser.go
[avalanchego/snow/networking/router/]
avalanchego/snow/networking/router/chain_router.go
avalanchego/snow/networking/router/chain_router_metrics.go
avalanchego/snow/networking/router/chain_router_test.go
avalanchego/snow/networking/router/health.go
avalanchego/snow/networking/router/inbound_handler.go
avalanchego/snow/networking/router/router.go
[avalanchego/snow/networking/sender/]
avalanchego/snow/networking/sender/external_sender.go
avalanchego/snow/networking/sender/sender.go
avalanchego/snow/networking/sender/sender_test.go
avalanchego/snow/networking/sender/test_external_sender.go
[avalanchego/snow/networking/timeout/]
avalanchego/snow/networking/timeout/manager.go
avalanchego/snow/networking/timeout/manager_test.go
avalanchego/snow/networking/timeout/metrics.go
[avalanchego/snow/networking/tracker/]
avalanchego/snow/networking/tracker/mock_resource_tracker.go
avalanchego/snow/networking/tracker/mock_targeter.go
avalanchego/snow/networking/tracker/resource_tracker.go
avalanchego/snow/networking/tracker/resource_tracker_test.go
avalanchego/snow/networking/tracker/targeter.go
avalanchego/snow/networking/tracker/targeter_test.go
[avalanchego/snow/networking/worker/]
avalanchego/snow/networking/worker/mock_pool.go
avalanchego/snow/networking/worker/pool.go
avalanchego/snow/state.go
[avalanchego/snow/uptime/]
avalanchego/snow/uptime/locked_calculator.go
avalanchego/snow/uptime/locked_calculator_test.go
avalanchego/snow/uptime/manager.go
avalanchego/snow/uptime/manager_test.go
[avalanchego/snow/uptime/mocks/]
avalanchego/snow/uptime/mocks/calculator.go
avalanchego/snow/uptime/state.go
avalanchego/snow/uptime/test_state.go
[avalanchego/snow/validators/]
avalanchego/snow/validators/connector.go
avalanchego/snow/validators/custom.go
avalanchego/snow/validators/custom_test.go
avalanchego/snow/validators/manager.go
avalanchego/snow/validators/set.go
avalanchego/snow/validators/set_test.go
avalanchego/snow/validators/state.go
avalanchego/snow/validators/test_state.go
avalanchego/snow/validators/validator.go

avalanchego/snow/README.md:
```
# Flow of a Single Blockchain

```mermaid
graph LR
    A[P2P] --> B[Chain Router]
    B --> C[Handler]
    C --> D[Consensus Engine]
    D --> E[Consensus]
    D --> F[VM]
    D --> G[DB]
    D --> I[Sender]
    F --> G
    I --> A
    I --> B 
```

## Intro

The Avalanche network consists of 3 built-in blockchains: X-Chain, C-Chain, and P-Chain. The X-Chain is used to manage assets and uses the Avalanche consensus protocol. The C-Chain is used to create and interact with smart contracts and uses the Snowman consensus protocol. The P-Chain is used to coordinate validators and stake and also uses the Snowman consensus protocol. At the time of writing, the Avalanche network has ~1200 validators. A set of validators makes up a subnet. Subnets can validate 1 or more chains. It is a common misconception that 1 subnet = 1 chain and this is shown by the primary subnet of Avalanche which is made up of the X-Chain, C-Chain, and P-Chain.

A node in the Avalanche network can either be a validator or a non-validator. A validator stakes AVAX tokens and participates in consensus to earn rewards. A non-validator does not participate in consensus or have any AVAX staked but is used as a public API. Both validators and non-validator need to have their own copy of the chain and to know the current state of the mempool. At the time of writing, there are ~1200 validators and ~1800 non-validator.

Each blockchain on Avalanche has several components: the virtual machine, database, consensus engine, sender, and handler. These components help the chain run smoothly. Blockchains also interact with the P2P layer and the chain router to send and receive messages.

## P2P

### [Outbound Messages](https://github.com/ava-labs/avalanchego/blob/master/message/outbound_msg_builder.go)

The `OutboundMsgBuilder` interface specifies methods that build messages of type `OutboundMessage`. Nodes communicate to other nodes by sending `OutboundMessage` messages.

All messaging functions in `OutboundMsgBuilder` can be categorized as follows:

- **Handshake**
  - Nodes need to be on a certain version before they can be accepted into the network.
- **State Sync**
  - A new node can ask other nodes for the current state of the network. It only syncs the required state for a specific block.
- **Bootstrapping**
  - Nodes can ask other nodes for blocks to build their own copy of the chain. A node can fetch all blocks from the locally last accepted block to the current last accepted block in the network.
- **Consensus**
  - Once a node is up to tip they can participate in consensus! During consensus, a node conducts a poll to several different small random samples of the validator set. They can communicate decisons on whether or not they have accepted/rejected a block.
- **App**
  - VMs communicate application-specific messages to other nodes through app messages. A common example is mempool gossiping.

Currently, Avalanchego implements its own message serialization to communicate. In the future, Avalanchego will use protocol buffers to communicate.

### [Network](https://github.com/ava-labs/avalanchego/blob/master/network/network.go)

The networking interface is shared across all chains. It implements functions from the `ExternalSender` interface. The two functions it implements are `Send` and `Gossip`. `Send` sends a message of type `OutboundMessage` to a specific set of nodes (specified by an array of `NodeIDs`). `Gossip` sends a message of type `OutboundMessage` to a random group of nodes in a subnet (can be a validator or a non-validator). Gossipping is used to push transactions across the network. The networking protocol uses TLS to pass messages between peers.

Along with sending and gossiping, the networking library is also responsible for making connections and maintaining connections. Any node whether they are a validator or non-validator will attempt to connect to the primary network.

## [Router](https://github.com/ava-labs/avalanchego/blob/master/snow/networking/router/chain_router.go)

The `ChainRouter` routes all incoming messages to its respective blockchain using `ChainID`. It does this by pushing all the messages onto the respective Chain handler’s queue. The `ChainRouter` references all existing chains on the network such as the X-chain, C-chain, P-chain and possibly any other chain. The `ChainRouter` handles timeouts as well. When sending messages on the P2P layer, timeouts are registered on the sender and cleared on the `ChainRouter` side when a response is received. If no response is received, then we trigger a timeout. Because we handle timeouts on the `ChainRouter` side, the handler is reliable. Peers not responding means timeouts trigger and the `ChainRouter` will still notify the handler of failure cases. The timeout manager within `ChainRouter` is also adaptive. If the network is experiencing long latencies, timeouts will then be adjusted as well.

## [Handler](https://github.com/ava-labs/avalanchego/blob/master/snow/networking/handler/handler.go)

The main function of the `Handler` is to pass messages from the network to the consensus engine. It receives these messages from the `ChainRouter`. It passes messages by pushing them onto a sync or async queue (depends on message type). Messages are then popped from the queue, parsed, and routed to the correct function in consensus engine. This can be one of the following.

- **State sync message (sync queue)**
- **Bootstrapping message (sync queue)**
- **Consensus message (sync queue)**
- **App message (async queue)**

## [Sender](https://github.com/ava-labs/avalanchego/blob/master/snow/networking/sender/sender.go)

The main role of the `sender` is to build and send outbound messages. It is actually a very thin wrapper around the normal networking code. The main difference here is that sender registers timeouts and tells the [router] to expect a response message. The timer starts on the sender side. If there is no response, sender will send a failed response to the [router]. If a node is repeatedly unresponsive, that node will get benched and sender will immediately start marking those messages as failed. If a sufficient amount of network deems the node benched, it might not get rewards (as a validator).

## [Consensus Engine](https://github.com/ava-labs/avalanchego/blob/master/snow/consensus/snowman/consensus.go)

Consensus is defined as getting a group of distributed systems to agree on an outcome. In the case of the Avalanche network, consensus is achieved when validators are in agreement with the state of the blockchain. The novel consensus algorithm is documented in the [white paper](https://assets.website-files.com/5d80307810123f5ffbb34d6e/6009805681b416f34dcae012_Avalanche%20Consensus%20Whitepaper.pdf). There are two main consensus algorithms: Avalanche and Snowman. The engine is responsible for adding proposing a new block to consensus, repeatedly polling the network for decisions (accept/reject), and communicating that decision to the `Sender`.

## [Blockchain Creation](https://github.com/ava-labs/avalanchego/blob/master/chains/manager.go)

The `Manager` is what kickstarts everything in regards to blockchain creation, starting with the P-Chain. Once the P-Chain finishes bootstrapping, it will kickstart C-Chain and X-Chain and any other chain. The `Manager`’s job is not done yet, if a create chain transaction is seen by a validator, a whole new process to create a chain will be started by the `Manager`. This can happen dynamically, long after the original 3 chains are created and bootstrapped.

```

avalanchego/snow/acceptor.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snow

import (
	"fmt"
	"sync"

	"go.uber.org/zap"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/utils/logging"
)

var (
	_ Acceptor = noOpAcceptor{}
	_ Acceptor = &AcceptorTracker{}
	_ Acceptor = acceptorWrapper{}

	_ AcceptorGroup = &acceptorGroup{}
)

// Acceptor is implemented when a struct is monitoring if a message is accepted
type Acceptor interface {
	// Accept must be called before [containerID] is committed to the VM as
	// accepted.
	//
	// If the returned error is non-nil, the chain associated with [ctx] should
	// shut down and not commit [container] or any other container to its
	// database as accepted.
	Accept(ctx *ConsensusContext, containerID ids.ID, container []byte) error
}

type noOpAcceptor struct{}

func (noOpAcceptor) Accept(*ConsensusContext, ids.ID, []byte) error { return nil }

// AcceptorTracker tracks the dispatched accept events by its ID and counts.
// Useful for testing.
type AcceptorTracker struct {
	lock     sync.RWMutex
	accepted map[ids.ID]int
}

func NewAcceptorTracker() *AcceptorTracker {
	return &AcceptorTracker{
		accepted: make(map[ids.ID]int),
	}
}

func (a *AcceptorTracker) Accept(ctx *ConsensusContext, containerID ids.ID, container []byte) error {
	a.lock.Lock()
	a.accepted[containerID]++
	a.lock.Unlock()
	return nil
}

func (a *AcceptorTracker) IsAccepted(containerID ids.ID) (int, bool) {
	a.lock.RLock()
	count, ok := a.accepted[containerID]
	a.lock.RUnlock()
	return count, ok
}

type acceptorWrapper struct {
	Acceptor

	// If true and Accept returns an error, the chain this callback corresponds
	// to will stop.
	dieOnError bool
}

type AcceptorGroup interface {
	// Calling Accept() calls all of the registered acceptors for the relevant
	// chain.
	Acceptor

	// RegisterAcceptor causes [acceptor] to be called every time an operation
	// is accepted on chain [chainID].
	// If [dieOnError], chain [chainID] stops if Accept returns a non-nil error.
	RegisterAcceptor(chainID ids.ID, acceptorName string, acceptor Acceptor, dieOnError bool) error

	// DeregisterAcceptor removes an acceptor from the group.
	DeregisterAcceptor(chainID ids.ID, acceptorName string) error
}

type acceptorGroup struct {
	log logging.Logger

	lock sync.RWMutex
	// Chain ID --> Acceptor Name --> Acceptor
	acceptors map[ids.ID]map[string]acceptorWrapper
}

func NewAcceptorGroup(log logging.Logger) AcceptorGroup {
	return &acceptorGroup{
		log:       log,
		acceptors: make(map[ids.ID]map[string]acceptorWrapper),
	}
}

func (a *acceptorGroup) Accept(ctx *ConsensusContext, containerID ids.ID, container []byte) error {
	a.lock.RLock()
	defer a.lock.RUnlock()

	for acceptorName, acceptor := range a.acceptors[ctx.ChainID] {
		if err := acceptor.Accept(ctx, containerID, container); err != nil {
			a.log.Error("failed accepting container",
				zap.String("acceptorName", acceptorName),
				zap.Stringer("chainID", ctx.ChainID),
				zap.Stringer("containerID", containerID),
				zap.Error(err),
			)
			if acceptor.dieOnError {
				return fmt.Errorf("acceptor %s on chain %s erred while accepting %s: %w", acceptorName, ctx.ChainID, containerID, err)
			}
		}
	}
	return nil
}

func (a *acceptorGroup) RegisterAcceptor(chainID ids.ID, acceptorName string, acceptor Acceptor, dieOnError bool) error {
	a.lock.Lock()
	defer a.lock.Unlock()

	acceptors, exist := a.acceptors[chainID]
	if !exist {
		acceptors = make(map[string]acceptorWrapper)
		a.acceptors[chainID] = acceptors
	}

	if _, ok := acceptors[acceptorName]; ok {
		return fmt.Errorf("callback %s already exists on chain %s", acceptorName, chainID)
	}

	acceptors[acceptorName] = acceptorWrapper{
		Acceptor:   acceptor,
		dieOnError: dieOnError,
	}
	return nil
}

func (a *acceptorGroup) DeregisterAcceptor(chainID ids.ID, acceptorName string) error {
	a.lock.Lock()
	defer a.lock.Unlock()

	acceptors, exist := a.acceptors[chainID]
	if !exist {
		return fmt.Errorf("chain %s has no callbacks", chainID)
	}

	if _, ok := acceptors[acceptorName]; !ok {
		return fmt.Errorf("callback %s does not exist on chain %s", acceptorName, chainID)
	}

	if len(acceptors) == 1 {
		delete(a.acceptors, chainID)
	} else {
		delete(acceptors, acceptorName)
	}
	return nil
}

```

avalanchego/snow/choices/decidable.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package choices

import (
	"github.com/ava-labs/avalanchego/ids"
)

// Decidable represents element that can be decided.
//
// Decidable objects are typically thought of as either transactions, blocks, or
// vertices.
type Decidable interface {
	// ID returns a unique ID for this element.
	//
	// Typically, this is implemented by using a cryptographic hash of a
	// binary representation of this element. An element should return the same
	// IDs upon repeated calls.
	ID() ids.ID

	// Accept this element.
	//
	// This element will be accepted by every correct node in the network.
	Accept() error

	// Reject this element.
	//
	// This element will not be accepted by any correct node in the network.
	Reject() error

	// Status returns this element's current status.
	//
	// If Accept has been called on an element with this ID, Accepted should be
	// returned. Similarly, if Reject has been called on an element with this
	// ID, Rejected should be returned. If the contents of this element are
	// unknown, then Unknown should be returned. Otherwise, Processing should be
	// returned.
	//
	// TODO: Consider allowing Status to return an error.
	Status() Status
}

```

avalanchego/snow/choices/status.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package choices

import (
	"errors"

	"github.com/ava-labs/avalanchego/utils/wrappers"
)

var errUnknownStatus = errors.New("unknown status")

type Status uint32

// List of possible status values
// [Unknown] Zero value, means the operation is not known
// [Processing] means the operation is known, but hasn't been decided yet
// [Rejected] means the operation will never be accepted
// [Accepted] means the operation was accepted
const (
	Unknown Status = iota
	Processing
	Rejected
	Accepted
)

func (s Status) MarshalJSON() ([]byte, error) {
	if err := s.Valid(); err != nil {
		return nil, err
	}
	return []byte("\"" + s.String() + "\""), nil
}

func (s *Status) UnmarshalJSON(b []byte) error {
	str := string(b)
	if str == "null" {
		return nil
	}
	switch str {
	case "\"Unknown\"":
		*s = Unknown
	case "\"Processing\"":
		*s = Processing
	case "\"Rejected\"":
		*s = Rejected
	case "\"Accepted\"":
		*s = Accepted
	default:
		return errUnknownStatus
	}
	return nil
}

// Fetched returns true if the status has been set.
func (s Status) Fetched() bool {
	switch s {
	case Processing:
		return true
	default:
		return s.Decided()
	}
}

// Decided returns true if the status is Rejected or Accepted.
func (s Status) Decided() bool {
	switch s {
	case Rejected, Accepted:
		return true
	default:
		return false
	}
}

// Valid returns nil if the status is a valid status.
func (s Status) Valid() error {
	switch s {
	case Unknown, Processing, Rejected, Accepted:
		return nil
	default:
		return errUnknownStatus
	}
}

func (s Status) String() string {
	switch s {
	case Unknown:
		return "Unknown"
	case Processing:
		return "Processing"
	case Rejected:
		return "Rejected"
	case Accepted:
		return "Accepted"
	default:
		return "Invalid status"
	}
}

// Bytes returns the byte repr. of this status
func (s Status) Bytes() []byte {
	p := wrappers.Packer{Bytes: make([]byte, 4)}
	p.PackInt(uint32(s))
	return p.Bytes
}

```

avalanchego/snow/choices/status_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package choices

import (
	"math"
	"testing"
)

func TestStatusValid(t *testing.T) {
	if err := Accepted.Valid(); err != nil {
		t.Fatalf("%s failed verification", Accepted)
	} else if err := Rejected.Valid(); err != nil {
		t.Fatalf("%s failed verification", Rejected)
	} else if err := Processing.Valid(); err != nil {
		t.Fatalf("%s failed verification", Processing)
	} else if err := Unknown.Valid(); err != nil {
		t.Fatalf("%s failed verification", Unknown)
	} else if badStatus := Status(math.MaxInt32); badStatus.Valid() == nil {
		t.Fatalf("%s passed verification", badStatus)
	}
}

func TestStatusDecided(t *testing.T) {
	if !Accepted.Decided() {
		t.Fatalf("%s failed decision", Accepted)
	} else if !Rejected.Decided() {
		t.Fatalf("%s failed decision", Rejected)
	} else if Processing.Decided() {
		t.Fatalf("%s failed decision", Processing)
	} else if Unknown.Decided() {
		t.Fatalf("%s failed decision", Unknown)
	} else if badStatus := Status(math.MaxInt32); badStatus.Decided() {
		t.Fatalf("%s failed decision", badStatus)
	}
}

func TestStatusFetched(t *testing.T) {
	if !Accepted.Fetched() {
		t.Fatalf("%s failed issue", Accepted)
	} else if !Rejected.Fetched() {
		t.Fatalf("%s failed issue", Rejected)
	} else if !Processing.Fetched() {
		t.Fatalf("%s failed issue", Processing)
	} else if Unknown.Fetched() {
		t.Fatalf("%s failed issue", Unknown)
	} else if badStatus := Status(math.MaxInt32); badStatus.Fetched() {
		t.Fatalf("%s failed issue", badStatus)
	}
}

func TestStatusString(t *testing.T) {
	if Accepted.String() != "Accepted" {
		t.Fatalf("%s failed printing", Accepted)
	} else if Rejected.String() != "Rejected" {
		t.Fatalf("%s failed printing", Rejected)
	} else if Processing.String() != "Processing" {
		t.Fatalf("%s failed printing", Processing)
	} else if Unknown.String() != "Unknown" {
		t.Fatalf("%s failed printing", Unknown)
	} else if badStatus := Status(math.MaxInt32); badStatus.String() != "Invalid status" {
		t.Fatalf("%s failed printing", badStatus)
	}
}

```

avalanchego/snow/choices/test_decidable.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package choices

import (
	"fmt"

	"github.com/ava-labs/avalanchego/ids"
)

var _ Decidable = &TestDecidable{}

// TestDecidable is a test Decidable
type TestDecidable struct {
	IDV              ids.ID
	AcceptV, RejectV error
	StatusV          Status
}

func (d *TestDecidable) ID() ids.ID { return d.IDV }

func (d *TestDecidable) Accept() error {
	switch d.StatusV {
	case Unknown, Rejected:
		return fmt.Errorf("invalid state transaition from %s to %s",
			d.StatusV, Accepted)
	default:
		d.StatusV = Accepted
		return d.AcceptV
	}
}

func (d *TestDecidable) Reject() error {
	switch d.StatusV {
	case Unknown, Accepted:
		return fmt.Errorf("invalid state transaition from %s to %s",
			d.StatusV, Rejected)
	default:
		d.StatusV = Rejected
		return d.RejectV
	}
}

func (d *TestDecidable) Status() Status { return d.StatusV }

```

avalanchego/snow/consensus/avalanche/consensus.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package avalanche

import (
	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow"
	"github.com/ava-labs/avalanchego/snow/consensus/snowstorm"
)

// TODO: Implement pruning of accepted decisions.
// To perfectly preserve the protocol, this implementation will need to store
// the hashes of all accepted decisions. It is possible to add a heuristic that
// removes sufficiently old decisions. However, that will need to be analyzed to
// ensure safety. It is doable with a weak syncrony assumption.

// Consensus represents a general avalanche instance that can be used directly
// to process a series of partially ordered elements.
type Consensus interface {
	// Takes in alpha, beta1, beta2, the accepted frontier, the join statuses,
	// the mutation statuses, and the consumer statuses. If accept or reject is
	// called, the status maps should be immediately updated accordingly.
	// Assumes each element in the accepted frontier will return accepted from
	// the join status map.
	Initialize(*snow.ConsensusContext, Parameters, []Vertex) error

	// Returns the parameters that describe this avalanche instance
	Parameters() Parameters

	// Returns the number of vertices processing
	NumProcessing() int

	// Returns true if the transaction is virtuous.
	// That is, no transaction has been added that conflicts with it
	IsVirtuous(snowstorm.Tx) bool

	// Adds a new decision. Assumes the dependencies have already been added.
	// Assumes that mutations don't conflict with themselves. Returns if a
	// critical error has occurred.
	Add(Vertex) error

	// VertexIssued returns true iff Vertex has been added
	VertexIssued(Vertex) bool

	// TxIssued returns true if a vertex containing this transaction has been added
	TxIssued(snowstorm.Tx) bool

	// Returns the set of transaction IDs that are virtuous but not contained in
	// any preferred vertices.
	Orphans() ids.Set

	// Returns a set of vertex IDs that were virtuous at the last update.
	Virtuous() ids.Set

	// Returns a set of vertex IDs that are preferred
	Preferences() ids.Set

	// RecordPoll collects the results of a network poll. If a result has not
	// been added, the result is dropped. Returns if a critical error has
	// occurred.
	RecordPoll(ids.UniqueBag) error

	// Quiesce is guaranteed to return true if the instance is finalized. It
	// may, but doesn't need to, return true if all processing vertices are
	// rogue. It must return false if there is a virtuous vertex that is still
	// processing.
	Quiesce() bool

	// Finalized returns true if all transactions that have been added have been
	// finalized. Note, it is possible that after returning finalized, a new
	// decision may be added such that this instance is no longer finalized.
	Finalized() bool

	// HealthCheck returns information about the consensus health.
	HealthCheck() (interface{}, error)
}

```

avalanchego/snow/consensus/avalanche/consensus_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package avalanche

import (
	"errors"
	"math"
	"path"
	"reflect"
	"runtime"
	"strings"
	"testing"

	"github.com/prometheus/client_golang/prometheus"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow"
	"github.com/ava-labs/avalanchego/snow/choices"
	"github.com/ava-labs/avalanchego/snow/consensus/snowball"
	"github.com/ava-labs/avalanchego/snow/consensus/snowstorm"
)

type testFunc func(*testing.T, Factory)

var testFuncs = []testFunc{
	MetricsTest,
	ParamsTest,
	NumProcessingTest,
	AddTest,
	VertexIssuedTest,
	TxIssuedTest,
	VirtuousTest,
	VirtuousSkippedUpdateTest,
	VotingTest,
	IgnoreInvalidVotingTest,
	IgnoreInvalidTransactionVertexVotingTest,
	TransitiveVotingTest,
	SplitVotingTest,
	TransitiveRejectionTest,
	IsVirtuousTest,
	QuiesceTest,
	QuiesceAfterVotingTest,
	TransactionVertexTest,
	OrphansTest,
	OrphansUpdateTest,
	ErrorOnVacuousAcceptTest,
	ErrorOnTxAcceptTest,
	ErrorOnVtxAcceptTest,
	ErrorOnVtxRejectTest,
	ErrorOnParentVtxRejectTest,
	ErrorOnTransitiveVtxRejectTest,
	SilenceTransactionVertexEventsTest,
}

func runConsensusTests(t *testing.T, factory Factory) {
	for _, test := range testFuncs {
		t.Run(getTestName(test), func(tt *testing.T) {
			test(tt, factory)
		})
	}
}

func getTestName(i interface{}) string {
	return strings.Split(path.Base(runtime.FuncForPC(reflect.ValueOf(i).Pointer()).Name()), ".")[1]
}

func MetricsTest(t *testing.T, factory Factory) {
	ctx := snow.DefaultConsensusContextTest()

	{
		avl := factory.New()
		params := Parameters{
			Parameters: snowball.Parameters{
				K:                 2,
				Alpha:             2,
				BetaVirtuous:      1,
				BetaRogue:         2,
				ConcurrentRepolls: 1,
				OptimalProcessing: 1,
			},
			Parents:   2,
			BatchSize: 1,
		}
		err := ctx.Registerer.Register(prometheus.NewGauge(prometheus.GaugeOpts{
			Name: "vtx_processing",
		}))
		if err != nil {
			t.Fatal(err)
		}
		if err := avl.Initialize(ctx, params, nil); err == nil {
			t.Fatalf("should have failed due to registering a duplicated statistic")
		}
	}
	{
		avl := factory.New()
		params := Parameters{
			Parameters: snowball.Parameters{
				K:                 2,
				Alpha:             2,
				BetaVirtuous:      1,
				BetaRogue:         2,
				ConcurrentRepolls: 1,
				OptimalProcessing: 1,
			},
			Parents:   2,
			BatchSize: 1,
		}
		err := ctx.Registerer.Register(prometheus.NewGauge(prometheus.GaugeOpts{
			Name: "vtx_accepted",
		}))
		if err != nil {
			t.Fatal(err)
		}
		if err := avl.Initialize(ctx, params, nil); err == nil {
			t.Fatalf("should have failed due to registering a duplicated statistic")
		}
	}
	{
		avl := factory.New()
		params := Parameters{
			Parameters: snowball.Parameters{
				K:                 2,
				Alpha:             2,
				BetaVirtuous:      1,
				BetaRogue:         2,
				ConcurrentRepolls: 1,
				OptimalProcessing: 1,
			},
			Parents:   2,
			BatchSize: 1,
		}
		err := ctx.Registerer.Register(prometheus.NewGauge(prometheus.GaugeOpts{
			Name: "vtx_rejected",
		}))
		if err != nil {
			t.Fatal(err)
		}
		if err := avl.Initialize(ctx, params, nil); err == nil {
			t.Fatalf("should have failed due to registering a duplicated statistic")
		}
	}
}

func ParamsTest(t *testing.T, factory Factory) {
	avl := factory.New()

	ctx := snow.DefaultConsensusContextTest()
	params := Parameters{
		Parameters: snowball.Parameters{
			K:                     2,
			Alpha:                 2,
			BetaVirtuous:          1,
			BetaRogue:             2,
			ConcurrentRepolls:     1,
			OptimalProcessing:     1,
			MaxOutstandingItems:   1,
			MaxItemProcessingTime: 1,
		},
		Parents:   2,
		BatchSize: 1,
	}

	if err := avl.Initialize(ctx, params, nil); err != nil {
		t.Fatal(err)
	}

	p := avl.Parameters()
	switch {
	case p.K != params.K:
		t.Fatalf("Wrong K parameter")
	case p.Alpha != params.Alpha:
		t.Fatalf("Wrong Alpha parameter")
	case p.BetaVirtuous != params.BetaVirtuous:
		t.Fatalf("Wrong Beta1 parameter")
	case p.BetaRogue != params.BetaRogue:
		t.Fatalf("Wrong Beta2 parameter")
	case p.Parents != params.Parents:
		t.Fatalf("Wrong Parents parameter")
	}
}

func NumProcessingTest(t *testing.T, factory Factory) {
	avl := factory.New()

	params := Parameters{
		Parameters: snowball.Parameters{
			K:                     1,
			Alpha:                 1,
			BetaVirtuous:          1,
			BetaRogue:             1,
			ConcurrentRepolls:     1,
			OptimalProcessing:     1,
			MaxOutstandingItems:   1,
			MaxItemProcessingTime: 1,
		},
		Parents:   2,
		BatchSize: 1,
	}
	vts := []Vertex{
		&TestVertex{TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Accepted,
		}},
		&TestVertex{TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Accepted,
		}},
	}
	utxos := []ids.ID{ids.GenerateTestID()}

	if err := avl.Initialize(snow.DefaultConsensusContextTest(), params, vts); err != nil {
		t.Fatal(err)
	}

	if numProcessing := avl.NumProcessing(); numProcessing != 0 {
		t.Fatalf("expected %d vertices processing but returned %d", 0, numProcessing)
	}

	tx0 := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Processing,
	}}
	tx0.InputIDsV = append(tx0.InputIDsV, utxos[0])

	vtx0 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx0},
	}

	if err := avl.Add(vtx0); err != nil {
		t.Fatal(err)
	}

	if numProcessing := avl.NumProcessing(); numProcessing != 1 {
		t.Fatalf("expected %d vertices processing but returned %d", 1, numProcessing)
	}

	tx1 := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Processing,
	}}
	tx1.InputIDsV = append(tx1.InputIDsV, utxos[0])

	vtx1 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx1},
	}

	if err := avl.Add(vtx1); err != nil {
		t.Fatal(err)
	}

	if numProcessing := avl.NumProcessing(); numProcessing != 2 {
		t.Fatalf("expected %d vertices processing but returned %d", 2, numProcessing)
	}

	if err := avl.Add(vtx1); err != nil {
		t.Fatal(err)
	}

	if numProcessing := avl.NumProcessing(); numProcessing != 2 {
		t.Fatalf("expected %d vertices processing but returned %d", 2, numProcessing)
	}

	if err := avl.Add(vts[0]); err != nil {
		t.Fatal(err)
	}

	if numProcessing := avl.NumProcessing(); numProcessing != 2 {
		t.Fatalf("expected %d vertices processing but returned %d", 2, numProcessing)
	}

	votes := ids.UniqueBag{}
	votes.Add(0, vtx0.ID())
	if err := avl.RecordPoll(votes); err != nil {
		t.Fatal(err)
	}

	if numProcessing := avl.NumProcessing(); numProcessing != 0 {
		t.Fatalf("expected %d vertices processing but returned %d", 0, numProcessing)
	}
}

func AddTest(t *testing.T, factory Factory) {
	avl := factory.New()

	params := Parameters{
		Parameters: snowball.Parameters{
			K:                     2,
			Alpha:                 2,
			BetaVirtuous:          1,
			BetaRogue:             2,
			ConcurrentRepolls:     1,
			OptimalProcessing:     1,
			MaxOutstandingItems:   1,
			MaxItemProcessingTime: 1,
		},
		Parents:   2,
		BatchSize: 1,
	}

	seedVertices := []Vertex{
		&TestVertex{
			TestDecidable: choices.TestDecidable{
				IDV:     ids.GenerateTestID(),
				StatusV: choices.Accepted,
			},
		},
		&TestVertex{
			TestDecidable: choices.TestDecidable{
				IDV:     ids.GenerateTestID(),
				StatusV: choices.Accepted,
			},
		},
	}

	ctx := snow.DefaultConsensusContextTest()
	// track consensus events to ensure idempotency in case of redundant vertex adds
	consensusEvents := snow.NewAcceptorTracker()
	ctx.ConsensusAcceptor = consensusEvents

	if err := avl.Initialize(ctx, params, seedVertices); err != nil {
		t.Fatal(err)
	}

	if !avl.Finalized() {
		t.Fatal("An empty avalanche instance is not finalized")
	}
	if !ids.UnsortedEquals([]ids.ID{seedVertices[0].ID(), seedVertices[1].ID()}, avl.Preferences().List()) {
		t.Fatal("Initial frontier failed to be set")
	}

	utxos := []ids.ID{ids.GenerateTestID()}
	vtx0 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: seedVertices,
		HeightV:  1,
		TxsV: []snowstorm.Tx{
			&snowstorm.TestTx{
				TestDecidable: choices.TestDecidable{
					IDV:     ids.GenerateTestID(),
					StatusV: choices.Processing,
				},
				InputIDsV: utxos,
			},
		},
	}
	vtx1 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: seedVertices,
		HeightV:  1,
		TxsV: []snowstorm.Tx{
			&snowstorm.TestTx{
				TestDecidable: choices.TestDecidable{
					IDV:     ids.GenerateTestID(),
					StatusV: choices.Processing,
				},
				InputIDsV: utxos,
			},
		},
	}

	tt := []struct {
		toAdd         Vertex
		err           error
		finalized     bool
		preferenceSet []ids.ID
		issued        int
		accepted      int
	}{
		{
			toAdd:         vtx0,
			err:           nil,
			finalized:     false,
			preferenceSet: []ids.ID{vtx0.IDV},
			issued:        1, // on "add", it should be issued
			accepted:      0,
		},
		{
			toAdd:         vtx1,
			err:           nil,
			finalized:     false,
			preferenceSet: []ids.ID{vtx0.IDV},
			issued:        1, // on "add", it should be issued
			accepted:      0,
		},
		{
			toAdd:         seedVertices[0],
			err:           nil,
			finalized:     false,
			preferenceSet: []ids.ID{vtx0.IDV},
			issued:        0, // initialized vertex should not belong to in-processing nodes
			accepted:      0,
		},
	}
	for i, tv := range tt {
		for _, j := range []int{1, 2} { // duplicate vertex add should be skipped
			err := avl.Add(tv.toAdd)
			if err != tv.err {
				t.Fatalf("#%d-%d: expected error %v, got %v", i, j, tv.err, err)
			}
			finalized := avl.Finalized()
			if finalized != tv.finalized {
				t.Fatalf("#%d-%d: expected finalized %v, got %v", i, j, finalized, tv.finalized)
			}
			preferenceSet := avl.Preferences().List()
			if !ids.UnsortedEquals(tv.preferenceSet, preferenceSet) {
				t.Fatalf("#%d-%d: expected preferenceSet %v, got %v", i, j, preferenceSet, tv.preferenceSet)
			}
			if accepted, _ := consensusEvents.IsAccepted(tv.toAdd.ID()); accepted != tv.accepted {
				t.Fatalf("#%d-%d: expected accepted %d, got %d", i, j, tv.accepted, accepted)
			}
		}
	}
}

func VertexIssuedTest(t *testing.T, factory Factory) {
	avl := factory.New()

	params := Parameters{
		Parameters: snowball.Parameters{
			K:                     2,
			Alpha:                 2,
			BetaVirtuous:          1,
			BetaRogue:             2,
			ConcurrentRepolls:     1,
			OptimalProcessing:     1,
			MaxOutstandingItems:   1,
			MaxItemProcessingTime: 1,
		},
		Parents:   2,
		BatchSize: 1,
	}
	vts := []Vertex{
		&TestVertex{TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Accepted,
		}},
		&TestVertex{TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Accepted,
		}},
	}
	utxos := []ids.ID{ids.GenerateTestID()}

	if err := avl.Initialize(snow.DefaultConsensusContextTest(), params, vts); err != nil {
		t.Fatal(err)
	}

	if !avl.VertexIssued(vts[0]) {
		t.Fatalf("Genesis Vertex not reported as issued")
	}

	tx := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Processing,
	}}
	tx.InputIDsV = append(tx.InputIDsV, utxos[0])

	vtx := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx},
	}

	if avl.VertexIssued(vtx) {
		t.Fatalf("Vertex reported as issued")
	} else if err := avl.Add(vtx); err != nil {
		t.Fatal(err)
	} else if !avl.VertexIssued(vtx) {
		t.Fatalf("Vertex reported as not issued")
	}
}

func TxIssuedTest(t *testing.T, factory Factory) {
	avl := factory.New()

	params := Parameters{
		Parameters: snowball.Parameters{
			K:                     2,
			Alpha:                 2,
			BetaVirtuous:          1,
			BetaRogue:             2,
			ConcurrentRepolls:     1,
			OptimalProcessing:     1,
			MaxOutstandingItems:   1,
			MaxItemProcessingTime: 1,
		},
		Parents:   2,
		BatchSize: 1,
	}

	tx0 := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}
	vts := []Vertex{&TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Accepted,
		},
		TxsV: []snowstorm.Tx{tx0},
	}}
	utxos := []ids.ID{ids.GenerateTestID()}

	tx1 := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Processing,
	}}
	tx1.InputIDsV = append(tx1.InputIDsV, utxos[0])

	if err := avl.Initialize(snow.DefaultConsensusContextTest(), params, vts); err != nil {
		t.Fatal(err)
	}

	if !avl.TxIssued(tx0) {
		t.Fatalf("Genesis Tx not reported as issued")
	} else if avl.TxIssued(tx1) {
		t.Fatalf("Tx reported as issued")
	}

	vtx := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		HeightV: 1,
		TxsV:    []snowstorm.Tx{tx1},
	}

	if err := avl.Add(vtx); err != nil {
		t.Fatal(err)
	} else if !avl.TxIssued(tx1) {
		t.Fatalf("Tx reported as not issued")
	}
}

func VirtuousTest(t *testing.T, factory Factory) {
	avl := factory.New()

	params := Parameters{
		Parameters: snowball.Parameters{
			K:                     2,
			Alpha:                 2,
			BetaVirtuous:          10,
			BetaRogue:             20,
			ConcurrentRepolls:     1,
			OptimalProcessing:     1,
			MaxOutstandingItems:   1,
			MaxItemProcessingTime: 1,
		},
		Parents:   2,
		BatchSize: 1,
	}
	vts := []Vertex{
		&TestVertex{TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Accepted,
		}},
		&TestVertex{TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Accepted,
		}},
	}
	utxos := []ids.ID{ids.GenerateTestID(), ids.GenerateTestID()}

	err := avl.Initialize(snow.DefaultConsensusContextTest(), params, vts)
	if err != nil {
		t.Fatal(err)
	}

	virtuous := avl.Virtuous()
	switch {
	case virtuous.Len() != 2:
		t.Fatalf("Wrong number of virtuous.")
	case !virtuous.Contains(vts[0].ID()):
		t.Fatalf("Wrong virtuous")
	case !virtuous.Contains(vts[1].ID()):
		t.Fatalf("Wrong virtuous")
	}

	tx0 := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Processing,
	}}
	tx0.InputIDsV = append(tx0.InputIDsV, utxos[0])

	vtx0 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx0},
	}

	tx1 := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Processing,
	}}
	tx1.InputIDsV = append(tx1.InputIDsV, utxos[0])

	vtx1 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx1},
	}

	tx2 := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Processing,
	}}
	tx2.InputIDsV = append(tx2.InputIDsV, utxos[1])

	vtx2 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: []Vertex{vtx0},
		HeightV:  2,
		TxsV:     []snowstorm.Tx{tx2},
	}

	if err := avl.Add(vtx0); err != nil {
		t.Fatal(err)
	} else if virtuous := avl.Virtuous(); virtuous.Len() != 1 {
		t.Fatalf("Wrong number of virtuous.")
	} else if !virtuous.Contains(vtx0.IDV) {
		t.Fatalf("Wrong virtuous")
	} else if err := avl.Add(vtx1); err != nil {
		t.Fatal(err)
	} else if virtuous := avl.Virtuous(); virtuous.Len() != 1 {
		t.Fatalf("Wrong number of virtuous.")
	} else if !virtuous.Contains(vtx0.IDV) {
		t.Fatalf("Wrong virtuous")
	}

	votes := ids.UniqueBag{}
	votes.Add(0, vtx1.ID())
	votes.Add(1, vtx1.ID())

	if err := avl.RecordPoll(votes); err != nil {
		t.Fatal(err)
	}

	virtuous = avl.Virtuous()
	switch {
	case virtuous.Len() != 2:
		t.Fatalf("Wrong number of virtuous.")
	case !virtuous.Contains(vts[0].ID()):
		t.Fatalf("Wrong virtuous")
	case !virtuous.Contains(vts[1].ID()):
		t.Fatalf("Wrong virtuous")
	}

	if err := avl.Add(vtx2); err != nil {
		t.Fatal(err)
	}

	virtuous = avl.Virtuous()
	switch {
	case virtuous.Len() != 2:
		t.Fatalf("Wrong number of virtuous.")
	case !virtuous.Contains(vts[0].ID()):
		t.Fatalf("Wrong virtuous")
	case !virtuous.Contains(vts[1].ID()):
		t.Fatalf("Wrong virtuous")
	}

	if err := avl.RecordPoll(votes); err != nil {
		t.Fatal(err)
	}

	virtuous = avl.Virtuous()
	switch {
	case virtuous.Len() != 2:
		t.Fatalf("Wrong number of virtuous.")
	case !virtuous.Contains(vts[0].ID()):
		t.Fatalf("Wrong virtuous")
	case !virtuous.Contains(vts[1].ID()):
		t.Fatalf("Wrong virtuous")
	}
}

func VirtuousSkippedUpdateTest(t *testing.T, factory Factory) {
	avl := factory.New()

	params := Parameters{
		Parameters: snowball.Parameters{
			K:                     2,
			Alpha:                 2,
			BetaVirtuous:          10,
			BetaRogue:             20,
			ConcurrentRepolls:     1,
			OptimalProcessing:     1,
			MaxOutstandingItems:   1,
			MaxItemProcessingTime: 1,
		},
		Parents:   2,
		BatchSize: 1,
	}
	vts := []Vertex{
		&TestVertex{TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Accepted,
		}},
		&TestVertex{TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Accepted,
		}},
	}
	utxos := []ids.ID{
		ids.GenerateTestID(),
		ids.GenerateTestID(),
	}

	err := avl.Initialize(snow.DefaultConsensusContextTest(), params, vts)
	if err != nil {
		t.Fatal(err)
	}

	virtuous := avl.Virtuous()
	switch {
	case virtuous.Len() != 2:
		t.Fatalf("Wrong number of virtuous.")
	case !virtuous.Contains(vts[0].ID()):
		t.Fatalf("Wrong virtuous")
	case !virtuous.Contains(vts[1].ID()):
		t.Fatalf("Wrong virtuous")
	}

	tx0 := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Processing,
	}}
	tx0.InputIDsV = append(tx0.InputIDsV, utxos[0])

	vtx0 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx0},
	}

	tx1 := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Processing,
	}}
	tx1.InputIDsV = append(tx1.InputIDsV, utxos[0])

	vtx1 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx1},
	}

	if err := avl.Add(vtx0); err != nil {
		t.Fatal(err)
	} else if virtuous := avl.Virtuous(); virtuous.Len() != 1 {
		t.Fatalf("Wrong number of virtuous.")
	} else if !virtuous.Contains(vtx0.IDV) {
		t.Fatalf("Wrong virtuous")
	} else if err := avl.Add(vtx1); err != nil {
		t.Fatal(err)
	} else if virtuous := avl.Virtuous(); virtuous.Len() != 1 {
		t.Fatalf("Wrong number of virtuous.")
	} else if !virtuous.Contains(vtx0.IDV) {
		t.Fatalf("Wrong virtuous")
	} else if err := avl.RecordPoll(ids.UniqueBag{}); err != nil {
		t.Fatal(err)
	} else if virtuous := avl.Virtuous(); virtuous.Len() != 1 {
		t.Fatalf("Wrong number of virtuous.")
	} else if !virtuous.Contains(vtx0.IDV) {
		t.Fatalf("Wrong virtuous")
	}
}

// Creates two conflicting transactions in different vertices
// and make sure only one is accepted
func VotingTest(t *testing.T, factory Factory) {
	avl := factory.New()

	params := Parameters{
		Parameters: snowball.Parameters{
			K:                     2,
			Alpha:                 2,
			BetaVirtuous:          1,
			BetaRogue:             2,
			ConcurrentRepolls:     1,
			OptimalProcessing:     1,
			MaxOutstandingItems:   1,
			MaxItemProcessingTime: 1,
		},
		Parents:   2,
		BatchSize: 1,
	}
	vts := []Vertex{
		&TestVertex{TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Accepted,
		}},
		&TestVertex{TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Accepted,
		}},
	}
	utxos := []ids.ID{ids.GenerateTestID()}

	err := avl.Initialize(snow.DefaultConsensusContextTest(), params, vts)
	if err != nil {
		t.Fatal(err)
	}

	// create two different transactions with the same input UTXO (double-spend)
	tx0 := &snowstorm.TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		InputIDsV: []ids.ID{utxos[0]},
	}
	tx1 := &snowstorm.TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		InputIDsV: []ids.ID{utxos[0]},
	}

	// put them in different vertices
	vtx0 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx0},
	}
	vtx1 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx1},
	}

	// issue two vertices with conflicting transaction to the consensus instance
	if err := avl.Add(vtx0); err != nil {
		t.Fatal(err)
	}
	if err := avl.Add(vtx1); err != nil {
		t.Fatal(err)
	}

	// create poll results, all vote for vtx1, not for vtx0
	sm := ids.UniqueBag{}
	sm.Add(0, vtx1.IDV)
	sm.Add(1, vtx1.IDV)

	// "BetaRogue" is 2, thus consensus should not be finalized yet
	err = avl.RecordPoll(sm)
	switch {
	case err != nil:
		t.Fatal(err)
	case avl.Finalized():
		t.Fatalf("An avalanche instance finalized too early")
	case !ids.UnsortedEquals([]ids.ID{vtx1.IDV}, avl.Preferences().List()):
		t.Fatalf("Initial frontier failed to be set")
	case tx0.Status() != choices.Processing:
		t.Fatalf("Tx should have been Processing")
	case tx1.Status() != choices.Processing:
		t.Fatalf("Tx should have been Processing")
	}

	// second poll should reach consensus,
	// and the other vertex of conflict transaction should be rejected
	err = avl.RecordPoll(sm)
	switch {
	case err != nil:
		t.Fatal(err)
	case !avl.Finalized():
		t.Fatalf("An avalanche instance finalized too late")
	case !ids.UnsortedEquals([]ids.ID{vtx1.IDV}, avl.Preferences().List()):
		// rejected vertex ID (vtx0) must have been removed from the preferred set
		t.Fatalf("Initial frontier failed to be set")
	case tx0.Status() != choices.Rejected:
		t.Fatalf("Tx should have been rejected")
	case tx1.Status() != choices.Accepted:
		t.Fatalf("Tx should have been accepted")
	}
}

func IgnoreInvalidVotingTest(t *testing.T, factory Factory) {
	avl := factory.New()

	params := Parameters{
		Parameters: snowball.Parameters{
			K:                     3,
			Alpha:                 2,
			BetaVirtuous:          1,
			BetaRogue:             1,
			ConcurrentRepolls:     1,
			OptimalProcessing:     1,
			MaxOutstandingItems:   1,
			MaxItemProcessingTime: 1,
		},
		Parents:   2,
		BatchSize: 1,
	}

	vts := []Vertex{
		&TestVertex{TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Accepted,
		}},
		&TestVertex{TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Accepted,
		}},
	}
	utxos := []ids.ID{ids.GenerateTestID()}

	if err := avl.Initialize(snow.DefaultConsensusContextTest(), params, vts); err != nil {
		t.Fatal(err)
	}

	tx0 := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Processing,
	}}
	tx0.InputIDsV = append(tx0.InputIDsV, utxos[0])

	vtx0 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx0},
	}

	tx1 := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Processing,
	}}
	tx1.InputIDsV = append(tx1.InputIDsV, utxos[0])

	vtx1 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx1},
	}

	if err := avl.Add(vtx0); err != nil {
		t.Fatal(err)
	} else if err := avl.Add(vtx1); err != nil {
		t.Fatal(err)
	}

	sm := ids.UniqueBag{}
	sm.Add(0, vtx0.IDV)
	sm.Add(1, vtx1.IDV)

	// Add Illegal Vote cast by Response 2
	sm.Add(2, vtx0.IDV)
	sm.Add(2, vtx1.IDV)

	if err := avl.RecordPoll(sm); err != nil {
		t.Fatal(err)
	} else if avl.Finalized() {
		t.Fatalf("An avalanche instance finalized too early")
	}
}

func IgnoreInvalidTransactionVertexVotingTest(t *testing.T, factory Factory) {
	avl := factory.New()

	params := Parameters{
		Parameters: snowball.Parameters{
			K:                     3,
			Alpha:                 2,
			BetaVirtuous:          1,
			BetaRogue:             1,
			ConcurrentRepolls:     1,
			OptimalProcessing:     1,
			MaxOutstandingItems:   1,
			MaxItemProcessingTime: 1,
		},
		Parents:   2,
		BatchSize: 1,
	}

	vts := []Vertex{
		&TestVertex{TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Accepted,
		}},
		&TestVertex{TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Accepted,
		}},
	}

	if err := avl.Initialize(snow.DefaultConsensusContextTest(), params, vts); err != nil {
		t.Fatal(err)
	}

	vtx0 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
	}

	tx1 := &snowstorm.TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		InputIDsV: []ids.ID{vtx0.ID()},
	}

	vtx1 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx1},
	}

	if err := avl.Add(vtx0); err != nil {
		t.Fatal(err)
	} else if err := avl.Add(vtx1); err != nil {
		t.Fatal(err)
	}

	sm := ids.UniqueBag{}
	sm.Add(0, vtx0.IDV)
	sm.Add(1, vtx1.IDV)

	// Add Illegal Vote cast by Response 2
	sm.Add(2, vtx0.IDV)
	sm.Add(2, vtx1.IDV)

	if err := avl.RecordPoll(sm); err != nil {
		t.Fatal(err)
	} else if avl.Finalized() {
		t.Fatalf("An avalanche instance finalized too early")
	}
}

func TransitiveVotingTest(t *testing.T, factory Factory) {
	avl := factory.New()

	params := Parameters{
		Parameters: snowball.Parameters{
			K:                     2,
			Alpha:                 2,
			BetaVirtuous:          1,
			BetaRogue:             2,
			ConcurrentRepolls:     1,
			OptimalProcessing:     1,
			MaxOutstandingItems:   1,
			MaxItemProcessingTime: 1,
		},
		Parents:   2,
		BatchSize: 1,
	}
	vts := []Vertex{
		&TestVertex{TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Accepted,
		}},
		&TestVertex{TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Accepted,
		}},
	}
	utxos := []ids.ID{ids.GenerateTestID(), ids.GenerateTestID()}

	err := avl.Initialize(snow.DefaultConsensusContextTest(), params, vts)
	if err != nil {
		t.Fatal(err)
	}

	tx0 := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Processing,
	}}
	tx0.InputIDsV = append(tx0.InputIDsV, utxos[0])

	vtx0 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx0},
	}

	tx1 := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Processing,
	}}
	tx1.InputIDsV = append(tx1.InputIDsV, utxos[1])

	vtx1 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: []Vertex{vtx0},
		HeightV:  2,
		TxsV:     []snowstorm.Tx{tx1},
	}

	vtx2 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: []Vertex{vtx1},
		HeightV:  3,
		TxsV:     []snowstorm.Tx{tx1},
	}

	if err := avl.Add(vtx0); err != nil {
		t.Fatal(err)
	} else if err := avl.Add(vtx1); err != nil {
		t.Fatal(err)
	} else if err := avl.Add(vtx2); err != nil {
		t.Fatal(err)
	}

	sm1 := ids.UniqueBag{}
	sm1.Add(0, vtx0.IDV)
	sm1.Add(1, vtx2.IDV)

	err = avl.RecordPoll(sm1)
	switch {
	case err != nil:
		t.Fatal(err)
	case avl.Finalized():
		t.Fatalf("An avalanche instance finalized too early")
	case !ids.UnsortedEquals([]ids.ID{vtx2.IDV}, avl.Preferences().List()):
		t.Fatalf("Initial frontier failed to be set")
	case tx0.Status() != choices.Accepted:
		t.Fatalf("Tx should have been accepted")
	}

	sm2 := ids.UniqueBag{}
	sm2.Add(0, vtx2.IDV)
	sm2.Add(1, vtx2.IDV)

	err = avl.RecordPoll(sm2)
	switch {
	case err != nil:
		t.Fatal(err)
	case !avl.Finalized():
		t.Fatalf("An avalanche instance finalized too late")
	case !ids.UnsortedEquals([]ids.ID{vtx2.IDV}, avl.Preferences().List()):
		t.Fatalf("Initial frontier failed to be set")
	case tx0.Status() != choices.Accepted:
		t.Fatalf("Tx should have been accepted")
	case tx1.Status() != choices.Accepted:
		t.Fatalf("Tx should have been accepted")
	}
}

func SplitVotingTest(t *testing.T, factory Factory) {
	avl := factory.New()

	params := Parameters{
		Parameters: snowball.Parameters{
			K:                     2,
			Alpha:                 2,
			BetaVirtuous:          1,
			BetaRogue:             2,
			ConcurrentRepolls:     1,
			OptimalProcessing:     1,
			MaxOutstandingItems:   1,
			MaxItemProcessingTime: 1,
		},
		Parents:   2,
		BatchSize: 1,
	}
	vts := []Vertex{
		&TestVertex{TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Accepted,
		}},
		&TestVertex{TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Accepted,
		}},
	}
	utxos := []ids.ID{ids.GenerateTestID()}

	err := avl.Initialize(snow.DefaultConsensusContextTest(), params, vts)
	if err != nil {
		t.Fatal(err)
	}

	tx0 := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Processing,
	}}
	tx0.InputIDsV = append(tx0.InputIDsV, utxos[0])

	vtx0 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx0},
	}

	vtx1 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx0},
	}

	if err := avl.Add(vtx0); err != nil {
		t.Fatal(err)
	} else if err := avl.Add(vtx1); err != nil {
		t.Fatal(err)
	}

	sm1 := ids.UniqueBag{}
	sm1.Add(0, vtx0.IDV) // peer 0 votes for the tx though vtx0
	sm1.Add(1, vtx1.IDV) // peer 1 votes for the tx though vtx1

	err = avl.RecordPoll(sm1)
	switch {
	case err != nil:
		t.Fatal(err)
	case avl.Finalized(): // avalanche shouldn't be finalized because the vertex transactions are still processing
		t.Fatalf("An avalanche instance finalized too late")
	case !ids.UnsortedEquals([]ids.ID{vtx0.IDV, vtx1.IDV}, avl.Preferences().List()):
		t.Fatalf("Initial frontier failed to be set")
	case tx0.Status() != choices.Accepted:
		t.Fatalf("Tx should have been accepted")
	}

	// Give alpha votes for both tranaction vertices
	sm2 := ids.UniqueBag{}
	sm2.Add(0, vtx0.IDV, vtx1.IDV) // peer 0 votes for vtx0 and vtx1
	sm2.Add(1, vtx0.IDV, vtx1.IDV) // peer 1 votes for vtx0 and vtx1

	err = avl.RecordPoll(sm2)
	switch {
	case err != nil:
		t.Fatal(err)
	case !avl.Finalized():
		t.Fatalf("An avalanche instance finalized too late")
	case !ids.UnsortedEquals([]ids.ID{vtx0.IDV, vtx1.IDV}, avl.Preferences().List()):
		t.Fatalf("Initial frontier failed to be set")
	case tx0.Status() != choices.Accepted:
		t.Fatalf("Tx should have been accepted")
	}
}

func TransitiveRejectionTest(t *testing.T, factory Factory) {
	avl := factory.New()

	params := Parameters{
		Parameters: snowball.Parameters{
			K:                     2,
			Alpha:                 2,
			BetaVirtuous:          1,
			BetaRogue:             2,
			ConcurrentRepolls:     1,
			OptimalProcessing:     1,
			MaxOutstandingItems:   1,
			MaxItemProcessingTime: 1,
		},
		Parents:   2,
		BatchSize: 1,
	}
	vts := []Vertex{
		&TestVertex{TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Accepted,
		}},
		&TestVertex{TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Accepted,
		}},
	}
	utxos := []ids.ID{ids.GenerateTestID(), ids.GenerateTestID()}

	err := avl.Initialize(snow.DefaultConsensusContextTest(), params, vts)
	if err != nil {
		t.Fatal(err)
	}

	tx0 := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Processing,
	}}
	tx0.InputIDsV = append(tx0.InputIDsV, utxos[0])

	vtx0 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx0},
	}

	tx1 := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Processing,
	}}
	tx1.InputIDsV = append(tx1.InputIDsV, utxos[0])

	vtx1 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx1},
	}

	tx2 := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Processing,
	}}
	tx2.InputIDsV = append(tx2.InputIDsV, utxos[1])

	vtx2 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: []Vertex{vtx0},
		HeightV:  2,
		TxsV:     []snowstorm.Tx{tx2},
	}

	if err := avl.Add(vtx0); err != nil {
		t.Fatal(err)
	} else if err := avl.Add(vtx1); err != nil {
		t.Fatal(err)
	} else if err := avl.Add(vtx2); err != nil {
		t.Fatal(err)
	}

	sm := ids.UniqueBag{}
	sm.Add(0, vtx1.IDV)
	sm.Add(1, vtx1.IDV)

	err = avl.RecordPoll(sm)
	switch {
	case err != nil:
		t.Fatal(err)
	case avl.Finalized():
		t.Fatalf("An avalanche instance finalized too early")
	case !ids.UnsortedEquals([]ids.ID{vtx1.IDV}, avl.Preferences().List()):
		t.Fatalf("Initial frontier failed to be set")
	}

	err = avl.RecordPoll(sm)
	switch {
	case err != nil:
		t.Fatal(err)
	case avl.Finalized():
		t.Fatalf("An avalanche instance finalized too early")
	case !ids.UnsortedEquals([]ids.ID{vtx1.IDV}, avl.Preferences().List()):
		t.Fatalf("Initial frontier failed to be set")
	case tx0.Status() != choices.Rejected:
		t.Fatalf("Tx should have been rejected")
	case tx1.Status() != choices.Accepted:
		t.Fatalf("Tx should have been accepted")
	case tx2.Status() != choices.Processing:
		t.Fatalf("Tx should not have been decided")
	}

	err = avl.RecordPoll(sm)
	switch {
	case err != nil:
		t.Fatal(err)
	case avl.Finalized():
		t.Fatalf("An avalanche instance finalized too early")
	case !ids.UnsortedEquals([]ids.ID{vtx1.IDV}, avl.Preferences().List()):
		t.Fatalf("Initial frontier failed to be set")
	case tx0.Status() != choices.Rejected:
		t.Fatalf("Tx should have been rejected")
	case tx1.Status() != choices.Accepted:
		t.Fatalf("Tx should have been accepted")
	case tx2.Status() != choices.Processing:
		t.Fatalf("Tx should not have been decided")
	}
}

func IsVirtuousTest(t *testing.T, factory Factory) {
	avl := factory.New()

	params := Parameters{
		Parameters: snowball.Parameters{
			K:                     2,
			Alpha:                 2,
			BetaVirtuous:          1,
			BetaRogue:             2,
			ConcurrentRepolls:     1,
			OptimalProcessing:     1,
			MaxOutstandingItems:   1,
			MaxItemProcessingTime: 1,
		},
		Parents:   2,
		BatchSize: 1,
	}
	vts := []Vertex{
		&TestVertex{TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Accepted,
		}},
		&TestVertex{TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Accepted,
		}},
	}
	utxos := []ids.ID{ids.GenerateTestID(), ids.GenerateTestID()}

	err := avl.Initialize(snow.DefaultConsensusContextTest(), params, vts)
	if err != nil {
		t.Fatal(err)
	}

	virtuous := avl.Virtuous()
	switch {
	case virtuous.Len() != 2:
		t.Fatalf("Wrong number of virtuous.")
	case !virtuous.Contains(vts[0].ID()):
		t.Fatalf("Wrong virtuous")
	case !virtuous.Contains(vts[1].ID()):
		t.Fatalf("Wrong virtuous")
	}

	tx0 := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Processing,
	}}
	tx0.InputIDsV = append(tx0.InputIDsV, utxos[0])

	vtx0 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx0},
	}

	tx1 := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Processing,
	}}
	tx1.InputIDsV = append(tx1.InputIDsV, utxos[0])

	vtx1 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx1},
	}

	if !avl.IsVirtuous(tx0) {
		t.Fatalf("Should be virtuous.")
	} else if !avl.IsVirtuous(tx1) {
		t.Fatalf("Should be virtuous.")
	}

	err = avl.Add(vtx0)
	switch {
	case err != nil:
		t.Fatal(err)
	case !avl.IsVirtuous(tx0):
		t.Fatalf("Should be virtuous.")
	case avl.IsVirtuous(tx1):
		t.Fatalf("Should not be virtuous.")
	}

	err = avl.Add(vtx1)
	switch {
	case err != nil:
		t.Fatal(err)
	case avl.IsVirtuous(tx0):
		t.Fatalf("Should not be virtuous.")
	case avl.IsVirtuous(tx1):
		t.Fatalf("Should not be virtuous.")
	}
}

func QuiesceTest(t *testing.T, factory Factory) {
	avl := factory.New()

	params := Parameters{
		Parameters: snowball.Parameters{
			K:                     1,
			Alpha:                 1,
			BetaVirtuous:          1,
			BetaRogue:             2,
			ConcurrentRepolls:     1,
			OptimalProcessing:     1,
			MaxOutstandingItems:   1,
			MaxItemProcessingTime: 1,
		},
		Parents:   2,
		BatchSize: 1,
	}
	vts := []Vertex{
		&TestVertex{TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Accepted,
		}},
		&TestVertex{TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Accepted,
		}},
	}
	utxos := []ids.ID{ids.GenerateTestID(), ids.GenerateTestID()}

	err := avl.Initialize(snow.DefaultConsensusContextTest(), params, vts)
	if err != nil {
		t.Fatal(err)
	}

	tx0 := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Processing,
	}}
	tx0.InputIDsV = append(tx0.InputIDsV, utxos[0])

	vtx0 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx0},
	}

	tx1 := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Processing,
	}}
	tx1.InputIDsV = append(tx1.InputIDsV, utxos[0])

	vtx1 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx1},
	}

	tx2 := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Processing,
	}}
	tx2.InputIDsV = append(tx2.InputIDsV, utxos[1])

	vtx2 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx2},
	}

	// Add [vtx0] containing [tx0]. Because [tx0] is virtuous, the instance
	// shouldn't quiesce.
	if err := avl.Add(vtx0); err != nil {
		t.Fatal(err)
	}
	if avl.Quiesce() {
		t.Fatalf("Shouldn't quiesce")
	}

	// Add [vtx1] containing [tx1]. Because [tx1] conflicts with [tx0], neither
	// [tx0] nor [tx1] are now virtuous. This means there are no virtuous
	// transaction left in the consensus instance and it can quiesce.
	if err := avl.Add(vtx1); err != nil {
		t.Fatal(err)
	}

	// The virtuous frontier is only updated sometimes, so force the frontier to
	// be re-calculated by changing the preference of tx1.
	sm1 := ids.UniqueBag{}
	sm1.Add(0, vtx1.IDV)
	if err := avl.RecordPoll(sm1); err != nil {
		t.Fatal(err)
	}

	if !avl.Quiesce() {
		t.Fatalf("Should quiesce")
	}

	// Add [vtx2] containing [tx2]. Because [tx2] is virtuous, the instance
	// shouldn't quiesce, even though [tx0] and [tx1] conflict.
	if err := avl.Add(vtx2); err != nil {
		t.Fatal(err)
	}
	if avl.Quiesce() {
		t.Fatalf("Shouldn't quiesce")
	}

	sm2 := ids.UniqueBag{}
	sm2.Add(0, vtx2.IDV)
	if err := avl.RecordPoll(sm2); err != nil {
		t.Fatal(err)
	}

	// Because [tx2] was accepted, there is again no remaining virtuous
	// transactions left in consensus.
	if !avl.Quiesce() {
		t.Fatalf("Should quiesce")
	}
}

func QuiesceAfterVotingTest(t *testing.T, factory Factory) {
	avl := factory.New()

	params := Parameters{
		Parameters: snowball.Parameters{
			K:                     1,
			Alpha:                 1,
			BetaVirtuous:          2,
			BetaRogue:             2,
			ConcurrentRepolls:     1,
			OptimalProcessing:     1,
			MaxOutstandingItems:   1,
			MaxItemProcessingTime: 1,
		},
		Parents:   2,
		BatchSize: 1,
	}
	vts := []Vertex{
		&TestVertex{TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Accepted,
		}},
		&TestVertex{TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Accepted,
		}},
	}
	utxos := []ids.ID{
		ids.GenerateTestID(),
		ids.GenerateTestID(),
	}

	err := avl.Initialize(snow.DefaultConsensusContextTest(), params, vts)
	if err != nil {
		t.Fatal(err)
	}

	vtx0 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV: []snowstorm.Tx{
			&snowstorm.TestTx{
				TestDecidable: choices.TestDecidable{
					IDV:     ids.GenerateTestID(),
					StatusV: choices.Processing,
				},
				InputIDsV: utxos[:1],
			},
		},
	}
	vtx1 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV: []snowstorm.Tx{
			&snowstorm.TestTx{
				TestDecidable: choices.TestDecidable{
					IDV:     ids.GenerateTestID(),
					StatusV: choices.Processing,
				},
				InputIDsV: utxos[:1],
			},
		},
	}
	vtx2 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV: []snowstorm.Tx{
			&snowstorm.TestTx{
				TestDecidable: choices.TestDecidable{
					IDV:     ids.GenerateTestID(),
					StatusV: choices.Processing,
				},
				InputIDsV: utxos[1:],
			},
		},
	}
	vtx3 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{},
	}

	if err := avl.Add(vtx0); err != nil {
		t.Fatal(err)
	}
	if err := avl.Add(vtx1); err != nil {
		t.Fatal(err)
	}
	if err := avl.Add(vtx2); err != nil {
		t.Fatal(err)
	}
	if err := avl.Add(vtx3); err != nil {
		t.Fatal(err)
	}

	// Because [vtx2] and [vtx3] are virtuous, the instance shouldn't quiesce.
	if avl.Quiesce() {
		t.Fatalf("Shouldn't quiesce")
	}

	sm12 := ids.UniqueBag{}
	sm12.Add(0, vtx1.IDV, vtx2.IDV)
	if err := avl.RecordPoll(sm12); err != nil {
		t.Fatal(err)
	}

	// Because [vtx2] and [vtx3] are still processing, the instance shouldn't
	// quiesce.
	if avl.Quiesce() {
		t.Fatalf("Shouldn't quiesce")
	}

	sm023 := ids.UniqueBag{}
	sm023.Add(0, vtx0.IDV, vtx2.IDV, vtx3.IDV)
	if err := avl.RecordPoll(sm023); err != nil {
		t.Fatal(err)
	}

	// Because [vtx3] is still processing, the instance shouldn't quiesce.
	if avl.Quiesce() {
		t.Fatalf("Shouldn't quiesce")
	}

	sm3 := ids.UniqueBag{}
	sm3.Add(0, vtx3.IDV)
	if err := avl.RecordPoll(sm3); err != nil {
		t.Fatal(err)
	}

	// Because [vtx0] and [vtx1] are conflicting and [vtx2] and [vtx3] are
	// accepted, the instance can quiesce.
	if !avl.Quiesce() {
		t.Fatalf("Should quiesce")
	}
}

func TransactionVertexTest(t *testing.T, factory Factory) {
	avl := factory.New()

	params := Parameters{
		Parameters: snowball.Parameters{
			K:                     1,
			Alpha:                 1,
			BetaVirtuous:          1,
			BetaRogue:             1,
			ConcurrentRepolls:     1,
			OptimalProcessing:     1,
			MaxOutstandingItems:   1,
			MaxItemProcessingTime: 1,
		},
		Parents:   2,
		BatchSize: 1,
	}
	seedVertices := []Vertex{
		&TestVertex{TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Accepted,
		}},
		&TestVertex{TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Accepted,
		}},
	}

	err := avl.Initialize(snow.DefaultConsensusContextTest(), params, seedVertices)
	if err != nil {
		t.Fatal(err)
	}

	// Add a vertex with no transactions to test that the transaction vertex is
	// required to be accepted before the vertex is.
	vtx0 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: seedVertices,
		HeightV:  1,
	}
	if err := avl.Add(vtx0); err != nil {
		t.Fatal(err)
	}
	if !avl.VertexIssued(vtx0) {
		t.Fatal("vertex with no transaction must have been issued")
	}

	// Because the transaction vertex should be processing, the vertex should
	// still be processing.
	if vtx0.Status() != choices.Processing {
		t.Fatalf("vertex with no transaction should still be processing, got %v", vtx0.Status())
	}

	// After voting for the transaction vertex beta times, the vertex should
	// also be accepted.
	bags := ids.UniqueBag{}
	bags.Add(0, vtx0.IDV)
	bags.Add(1, vtx0.IDV)
	if err := avl.RecordPoll(bags); err != nil {
		t.Fatalf("unexpected RecordPoll error %v", err)
	}

	switch {
	case vtx0.Status() != choices.Accepted:
		t.Fatalf("vertex with no transaction should have been accepted after polling, got %v", vtx0.Status())
	case !avl.Finalized():
		t.Fatal("expected finalized avalanche instance")
	case !ids.UnsortedEquals([]ids.ID{vtx0.IDV}, avl.Preferences().List()):
		t.Fatalf("unexpected frontier %v", avl.Preferences().List())
	}
}

func OrphansTest(t *testing.T, factory Factory) {
	avl := factory.New()

	params := Parameters{
		Parameters: snowball.Parameters{
			K:                     1,
			Alpha:                 1,
			BetaVirtuous:          math.MaxInt32,
			BetaRogue:             math.MaxInt32,
			ConcurrentRepolls:     1,
			OptimalProcessing:     1,
			MaxOutstandingItems:   1,
			MaxItemProcessingTime: 1,
		},
		Parents:   2,
		BatchSize: 1,
	}
	vts := []Vertex{
		&TestVertex{TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Accepted,
		}},
		&TestVertex{TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Accepted,
		}},
	}
	utxos := []ids.ID{ids.GenerateTestID(), ids.GenerateTestID()}

	err := avl.Initialize(snow.DefaultConsensusContextTest(), params, vts)
	if err != nil {
		t.Fatal(err)
	}

	tx0 := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Processing,
	}}
	tx0.InputIDsV = append(tx0.InputIDsV, utxos[0])

	vtx0 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx0},
	}

	tx1 := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Processing,
	}}
	tx1.InputIDsV = append(tx1.InputIDsV, utxos[0])

	vtx1 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx1},
	}

	tx2 := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Processing,
	}}
	tx2.InputIDsV = append(tx2.InputIDsV, utxos[1])

	vtx2 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: []Vertex{vtx0},
		HeightV:  2,
		TxsV:     []snowstorm.Tx{tx2},
	}

	// [vtx0] contains [tx0], both of which will be preferred, so [tx0] is not
	// an orphan.
	if err := avl.Add(vtx0); err != nil {
		t.Fatal(err)
	}
	if orphans := avl.Orphans(); orphans.Len() != 0 {
		t.Fatalf("Wrong number of orphans")
	}

	// [vtx1] contains [tx1], which conflicts with [tx0]. [tx0] is contained in
	// a preferred vertex, and neither [tx0] nor [tx1] are virtuous.
	if err := avl.Add(vtx1); err != nil {
		t.Fatal(err)
	}
	if orphans := avl.Orphans(); orphans.Len() != 0 {
		t.Fatalf("Wrong number of orphans")
	}

	// [vtx2] contains [tx2], both of which will be preferred, so [tx2] is not
	// an orphan.
	if err := avl.Add(vtx2); err != nil {
		t.Fatal(err)
	}
	if orphans := avl.Orphans(); orphans.Len() != 0 {
		t.Fatalf("Wrong number of orphans")
	}

	sm := ids.UniqueBag{}
	sm.Add(0, vtx1.IDV)
	if err := avl.RecordPoll(sm); err != nil {
		t.Fatal(err)
	}

	// By voting for [vtx1], [vtx2] is no longer preferred because it's parent
	// [vtx0] contains [tx0] that is not preferred. Because [tx2] is virtuous,
	// but no longer contained in a preferred vertex, it should now be
	// considered an orphan.
	orphans := avl.Orphans()
	if orphans.Len() != 1 {
		t.Fatalf("Wrong number of orphans")
	}
	if !orphans.Contains(tx2.ID()) {
		t.Fatalf("Wrong orphan")
	}
}

func OrphansUpdateTest(t *testing.T, factory Factory) {
	avl := factory.New()

	params := Parameters{
		Parameters: snowball.Parameters{
			K:                     1,
			Alpha:                 1,
			BetaVirtuous:          math.MaxInt32,
			BetaRogue:             math.MaxInt32,
			ConcurrentRepolls:     1,
			OptimalProcessing:     1,
			MaxOutstandingItems:   1,
			MaxItemProcessingTime: 1,
		},
		Parents:   2,
		BatchSize: 1,
	}
	seedVertices := []Vertex{
		&TestVertex{TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Accepted,
		}},
		&TestVertex{TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Accepted,
		}},
	}
	err := avl.Initialize(snow.DefaultConsensusContextTest(), params, seedVertices)
	if err != nil {
		t.Fatal(err)
	}

	utxos := []ids.ID{ids.GenerateTestID(), ids.GenerateTestID()}

	// tx0 is a virtuous transaction.
	tx0 := &snowstorm.TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		InputIDsV: utxos[:1],
	}
	vtx0 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: seedVertices,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx0},
	}

	// tx1 conflicts with tx2.
	tx1 := &snowstorm.TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		InputIDsV: utxos[1:],
	}
	vtx1 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: seedVertices,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx1},
	}

	tx2 := &snowstorm.TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		InputIDsV: utxos[1:],
	}
	vtx2 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: seedVertices,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx2},
	}

	if err := avl.Add(vtx0); err != nil {
		t.Fatal(err)
	}
	if err := avl.Add(vtx1); err != nil {
		t.Fatal(err)
	}
	if err := avl.Add(vtx2); err != nil {
		t.Fatal(err)
	}

	// vtx0 is virtuous, so it should be preferred. vtx1 and vtx2 conflict, but
	// vtx1 was issued before vtx2, so vtx1 should be preferred and vtx2 should
	// not be preferred.
	expectedPreferredSet := ids.Set{
		vtx0.ID(): struct{}{},
		vtx1.ID(): struct{}{},
	}
	preferenceSet := avl.Preferences().List()
	if !ids.UnsortedEquals(expectedPreferredSet.List(), preferenceSet) {
		t.Fatalf("expected preferenceSet %v, got %v", expectedPreferredSet, preferenceSet)
	}

	// Record a successful poll to change the preference from vtx1 to vtx2 and
	// update the orphan set.
	votes := ids.UniqueBag{}
	votes.Add(0, vtx2.IDV)
	if err := avl.RecordPoll(votes); err != nil {
		t.Fatal(err)
	}

	// Because vtx2 was voted for over vtx1, they should be swapped in the
	// preferred set.
	expectedPreferredSet = ids.Set{
		vtx0.ID(): struct{}{},
		vtx2.ID(): struct{}{},
	}
	preferenceSet = avl.Preferences().List()
	if !ids.UnsortedEquals(expectedPreferredSet.List(), preferenceSet) {
		t.Fatalf("expected preferenceSet %v, got %v", expectedPreferredSet, preferenceSet)
	}

	// Because there are no virtuous transactions that are not in a preferred
	// vertex, there should be no orphans.
	expectedOrphanSet := ids.Set{}
	orphanSet := avl.Orphans()
	if !ids.UnsortedEquals(expectedOrphanSet.List(), orphanSet.List()) {
		t.Fatalf("expected orphanSet %v, got %v", expectedOrphanSet, orphanSet)
	}
}

func ErrorOnVacuousAcceptTest(t *testing.T, factory Factory) {
	avl := factory.New()

	params := Parameters{
		Parameters: snowball.Parameters{
			K:                     1,
			Alpha:                 1,
			BetaVirtuous:          math.MaxInt32,
			BetaRogue:             math.MaxInt32,
			ConcurrentRepolls:     1,
			OptimalProcessing:     1,
			MaxOutstandingItems:   1,
			MaxItemProcessingTime: 1,
		},
		Parents:   2,
		BatchSize: 1,
	}
	vts := []Vertex{&TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}}

	err := avl.Initialize(snow.DefaultConsensusContextTest(), params, vts)
	if err != nil {
		t.Fatal(err)
	}

	tx0 := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		AcceptV: errors.New(""),
		StatusV: choices.Processing,
	}}

	vtx0 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx0},
	}

	if err := avl.Add(vtx0); err == nil {
		t.Fatalf("Should have errored on vertex issuance")
	}
}

func ErrorOnTxAcceptTest(t *testing.T, factory Factory) {
	avl := factory.New()

	params := Parameters{
		Parameters: snowball.Parameters{
			K:                     1,
			Alpha:                 1,
			BetaVirtuous:          1,
			BetaRogue:             1,
			ConcurrentRepolls:     1,
			OptimalProcessing:     1,
			MaxOutstandingItems:   1,
			MaxItemProcessingTime: 1,
		},
		Parents:   2,
		BatchSize: 1,
	}
	vts := []Vertex{&TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}}
	utxos := []ids.ID{ids.GenerateTestID()}

	err := avl.Initialize(snow.DefaultConsensusContextTest(), params, vts)
	if err != nil {
		t.Fatal(err)
	}

	tx0 := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		AcceptV: errors.New(""),
		StatusV: choices.Processing,
	}}
	tx0.InputIDsV = append(tx0.InputIDsV, utxos[0])

	vtx0 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx0},
	}

	if err := avl.Add(vtx0); err != nil {
		t.Fatal(err)
	}

	votes := ids.UniqueBag{}
	votes.Add(0, vtx0.IDV)
	if err := avl.RecordPoll(votes); err == nil {
		t.Fatalf("Should have errored on vertex acceptance")
	}
}

func ErrorOnVtxAcceptTest(t *testing.T, factory Factory) {
	avl := factory.New()

	params := Parameters{
		Parameters: snowball.Parameters{
			K:                     1,
			Alpha:                 1,
			BetaVirtuous:          1,
			BetaRogue:             1,
			ConcurrentRepolls:     1,
			OptimalProcessing:     1,
			MaxOutstandingItems:   1,
			MaxItemProcessingTime: 1,
		},
		Parents:   2,
		BatchSize: 1,
	}
	vts := []Vertex{&TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}}
	utxos := []ids.ID{ids.GenerateTestID()}

	err := avl.Initialize(snow.DefaultConsensusContextTest(), params, vts)
	if err != nil {
		t.Fatal(err)
	}

	tx0 := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Processing,
	}}
	tx0.InputIDsV = append(tx0.InputIDsV, utxos[0])

	vtx0 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			AcceptV: errors.New(""),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx0},
	}

	if err := avl.Add(vtx0); err != nil {
		t.Fatal(err)
	}

	votes := ids.UniqueBag{}
	votes.Add(0, vtx0.IDV)
	if err := avl.RecordPoll(votes); err == nil {
		t.Fatalf("Should have errored on vertex acceptance")
	}
}

func ErrorOnVtxRejectTest(t *testing.T, factory Factory) {
	avl := factory.New()

	params := Parameters{
		Parameters: snowball.Parameters{
			K:                     1,
			Alpha:                 1,
			BetaVirtuous:          1,
			BetaRogue:             1,
			ConcurrentRepolls:     1,
			OptimalProcessing:     1,
			MaxOutstandingItems:   1,
			MaxItemProcessingTime: 1,
		},
		Parents:   2,
		BatchSize: 1,
	}
	vts := []Vertex{&TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}}
	utxos := []ids.ID{ids.GenerateTestID()}

	err := avl.Initialize(snow.DefaultConsensusContextTest(), params, vts)
	if err != nil {
		t.Fatal(err)
	}

	tx0 := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Processing,
	}}
	tx0.InputIDsV = append(tx0.InputIDsV, utxos[0])

	vtx0 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx0},
	}

	tx1 := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Processing,
	}}
	tx1.InputIDsV = append(tx1.InputIDsV, utxos[0])

	vtx1 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			RejectV: errors.New(""),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx1},
	}

	if err := avl.Add(vtx0); err != nil {
		t.Fatal(err)
	} else if err := avl.Add(vtx1); err != nil {
		t.Fatal(err)
	}

	votes := ids.UniqueBag{}
	votes.Add(0, vtx0.IDV)
	if err := avl.RecordPoll(votes); err == nil {
		t.Fatalf("Should have errored on vertex rejection")
	}
}

func ErrorOnParentVtxRejectTest(t *testing.T, factory Factory) {
	avl := factory.New()

	params := Parameters{
		Parameters: snowball.Parameters{
			K:                     1,
			Alpha:                 1,
			BetaVirtuous:          1,
			BetaRogue:             1,
			ConcurrentRepolls:     1,
			OptimalProcessing:     1,
			MaxOutstandingItems:   1,
			MaxItemProcessingTime: 1,
		},
		Parents:   2,
		BatchSize: 1,
	}
	vts := []Vertex{&TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}}
	utxos := []ids.ID{ids.GenerateTestID()}

	err := avl.Initialize(snow.DefaultConsensusContextTest(), params, vts)
	if err != nil {
		t.Fatal(err)
	}

	tx0 := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Processing,
	}}
	tx0.InputIDsV = append(tx0.InputIDsV, utxos[0])

	vtx0 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx0},
	}

	tx1 := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Processing,
	}}
	tx1.InputIDsV = append(tx1.InputIDsV, utxos[0])

	vtx1 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			RejectV: errors.New(""),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx1},
	}

	vtx2 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: []Vertex{vtx1},
		HeightV:  2,
		TxsV:     []snowstorm.Tx{tx1},
	}

	if err := avl.Add(vtx0); err != nil {
		t.Fatal(err)
	} else if err := avl.Add(vtx1); err != nil {
		t.Fatal(err)
	} else if err := avl.Add(vtx2); err != nil {
		t.Fatal(err)
	}

	votes := ids.UniqueBag{}
	votes.Add(0, vtx0.IDV)
	if err := avl.RecordPoll(votes); err == nil {
		t.Fatalf("Should have errored on vertex rejection")
	}
}

func ErrorOnTransitiveVtxRejectTest(t *testing.T, factory Factory) {
	avl := factory.New()

	params := Parameters{
		Parameters: snowball.Parameters{
			K:                     1,
			Alpha:                 1,
			BetaVirtuous:          1,
			BetaRogue:             1,
			ConcurrentRepolls:     1,
			OptimalProcessing:     1,
			MaxOutstandingItems:   1,
			MaxItemProcessingTime: 1,
		},
		Parents:   2,
		BatchSize: 1,
	}
	vts := []Vertex{&TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}}
	utxos := []ids.ID{ids.GenerateTestID()}

	err := avl.Initialize(snow.DefaultConsensusContextTest(), params, vts)
	if err != nil {
		t.Fatal(err)
	}

	tx0 := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Processing,
	}}
	tx0.InputIDsV = append(tx0.InputIDsV, utxos[0])

	vtx0 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx0},
	}

	tx1 := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Processing,
	}}
	tx1.InputIDsV = append(tx1.InputIDsV, utxos[0])

	vtx1 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx1},
	}

	vtx2 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			RejectV: errors.New(""),
			StatusV: choices.Processing,
		},
		ParentsV: []Vertex{vtx1},
		HeightV:  1,
	}

	if err := avl.Add(vtx0); err != nil {
		t.Fatal(err)
	} else if err := avl.Add(vtx1); err != nil {
		t.Fatal(err)
	} else if err := avl.Add(vtx2); err != nil {
		t.Fatal(err)
	}

	votes := ids.UniqueBag{}
	votes.Add(0, vtx0.IDV)
	if err := avl.RecordPoll(votes); err == nil {
		t.Fatalf("Should have errored on vertex rejection")
	}
}

func SilenceTransactionVertexEventsTest(t *testing.T, factory Factory) {
	avl := factory.New()

	params := Parameters{
		Parameters: snowball.Parameters{
			K:                     1,
			Alpha:                 1,
			BetaVirtuous:          1,
			BetaRogue:             1,
			ConcurrentRepolls:     1,
			OptimalProcessing:     1,
			MaxOutstandingItems:   1,
			MaxItemProcessingTime: 1,
		},
		Parents:   2,
		BatchSize: 1,
	}
	vts := []Vertex{&TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}}

	ctx := snow.DefaultConsensusContextTest()
	tracker := snow.NewAcceptorTracker()
	ctx.DecisionAcceptor = tracker

	err := avl.Initialize(ctx, params, vts)
	if err != nil {
		t.Fatal(err)
	}

	vtx0 := &TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
	}

	if err := avl.Add(vtx0); err != nil {
		t.Fatal(err)
	}

	votes := ids.UniqueBag{}
	votes.Add(0, vtx0.IDV)
	if err := avl.RecordPoll(votes); err != nil {
		t.Fatal(err)
	}

	if _, accepted := tracker.IsAccepted(vtx0.ID()); accepted {
		t.Fatalf("Shouldn't have reported the transaction vertex as accepted")
	}
}

```

avalanchego/snow/consensus/avalanche/factory.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package avalanche

// Factory returns new instances of Consensus
type Factory interface {
	New() Consensus
}

```

avalanchego/snow/consensus/avalanche/parameters.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package avalanche

import (
	"fmt"

	"github.com/ava-labs/avalanchego/snow/consensus/snowball"
)

// Parameters the avalanche parameters include the snowball parameters and the
// optimal number of parents
type Parameters struct {
	snowball.Parameters
	Parents   int `json:"parents" yaml:"parents"`
	BatchSize int `json:"batchSize" yaml:"batchSize"`
}

// Valid returns nil if the parameters describe a valid initialization.
func (p Parameters) Valid() error {
	switch {
	case p.Parents <= 1:
		return fmt.Errorf("parents = %d: Fails the condition that: 1 < Parents", p.Parents)
	case p.BatchSize <= 0:
		return fmt.Errorf("batchSize = %d: Fails the condition that: 0 < BatchSize", p.BatchSize)
	default:
		return p.Parameters.Verify()
	}
}

```

avalanchego/snow/consensus/avalanche/parameters_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package avalanche

import (
	"testing"

	"github.com/ava-labs/avalanchego/snow/consensus/snowball"
)

func TestParametersValid(t *testing.T) {
	p := Parameters{
		Parameters: snowball.Parameters{
			K:                     1,
			Alpha:                 1,
			BetaVirtuous:          1,
			BetaRogue:             1,
			ConcurrentRepolls:     1,
			OptimalProcessing:     1,
			MaxOutstandingItems:   1,
			MaxItemProcessingTime: 1,
		},
		Parents:   2,
		BatchSize: 1,
	}

	if err := p.Valid(); err != nil {
		t.Fatal(err)
	}
}

func TestParametersInvalidParents(t *testing.T) {
	p := Parameters{
		Parameters: snowball.Parameters{
			K:                     1,
			Alpha:                 1,
			BetaVirtuous:          1,
			BetaRogue:             1,
			ConcurrentRepolls:     1,
			OptimalProcessing:     1,
			MaxOutstandingItems:   1,
			MaxItemProcessingTime: 1,
		},
		Parents:   1,
		BatchSize: 1,
	}

	if err := p.Valid(); err == nil {
		t.Fatalf("Should have failed due to invalid parents")
	}
}

func TestParametersInvalidBatchSize(t *testing.T) {
	p := Parameters{
		Parameters: snowball.Parameters{
			K:                     1,
			Alpha:                 1,
			BetaVirtuous:          1,
			BetaRogue:             1,
			ConcurrentRepolls:     1,
			OptimalProcessing:     1,
			MaxOutstandingItems:   1,
			MaxItemProcessingTime: 1,
		},
		Parents:   2,
		BatchSize: 0,
	}

	if err := p.Valid(); err == nil {
		t.Fatalf("Should have failed due to invalid batch size")
	}
}

```

avalanchego/snow/consensus/avalanche/poll/early_term_no_traversal.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package poll

import (
	"fmt"

	"github.com/ava-labs/avalanchego/ids"
)

var (
	_ Factory = &earlyTermNoTraversalFactory{}
	_ Poll    = &earlyTermNoTraversalPoll{}
)

type earlyTermNoTraversalFactory struct {
	alpha int
}

// NewEarlyTermNoTraversalFactory returns a factory that returns polls with
// early termination, without doing DAG traversals
func NewEarlyTermNoTraversalFactory(alpha int) Factory {
	return &earlyTermNoTraversalFactory{alpha: alpha}
}

func (f *earlyTermNoTraversalFactory) New(vdrs ids.NodeIDBag) Poll {
	return &earlyTermNoTraversalPoll{
		polled: vdrs,
		alpha:  f.alpha,
	}
}

// earlyTermNoTraversalPoll finishes when any remaining validators can't change
// the result of the poll. However, does not terminate tightly with this bound.
// It terminates as quickly as it can without performing any DAG traversals.
type earlyTermNoTraversalPoll struct {
	votes  ids.UniqueBag
	polled ids.NodeIDBag
	alpha  int
}

// Vote registers a response for this poll
func (p *earlyTermNoTraversalPoll) Vote(vdr ids.NodeID, votes []ids.ID) {
	count := p.polled.Count(vdr)
	// make sure that a validator can't respond multiple times
	p.polled.Remove(vdr)

	// track the votes the validator responded with
	for i := 0; i < count; i++ {
		p.votes.Add(uint(p.polled.Len()+i), votes...)
	}
}

// Finished returns true when all validators have voted
func (p *earlyTermNoTraversalPoll) Finished() bool {
	// If there are no outstanding queries, the poll is finished
	numPending := p.polled.Len()
	if numPending == 0 {
		return true
	}
	// If there are still enough pending responses to include another vertex,
	// then the poll must wait for more responses
	if numPending > p.alpha {
		return false
	}

	// Ignore any vertex that has already received alpha votes. To safely skip
	// DAG traversal, assume that all votes for vertices with less than alpha
	// votes will be applied to a single shared ancestor. In this case, the poll
	// can terminate early, iff there are not enough pending votes for this
	// ancestor to receive alpha votes.
	partialVotes := ids.BitSet64(0)
	for _, vote := range p.votes.List() {
		if voters := p.votes.GetSet(vote); voters.Len() < p.alpha {
			partialVotes.Union(voters)
		}
	}
	return partialVotes.Len()+numPending < p.alpha
}

// Result returns the result of this poll
func (p *earlyTermNoTraversalPoll) Result() ids.UniqueBag { return p.votes }

func (p *earlyTermNoTraversalPoll) PrefixedString(prefix string) string {
	return fmt.Sprintf(
		"waiting on %s\n%sreceived %s",
		p.polled.PrefixedString(prefix),
		prefix,
		p.votes.PrefixedString(prefix),
	)
}

func (p *earlyTermNoTraversalPoll) String() string { return p.PrefixedString("") }

```

avalanchego/snow/consensus/avalanche/poll/early_term_no_traversal_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package poll

import (
	"testing"

	"github.com/ava-labs/avalanchego/ids"
)

func TestEarlyTermNoTraversalResults(t *testing.T) {
	alpha := 1

	vtxID := ids.ID{1}
	votes := []ids.ID{vtxID}

	vdr1 := ids.NodeID{1} // k = 1

	vdrs := ids.NodeIDBag{}
	vdrs.Add(vdr1)

	factory := NewEarlyTermNoTraversalFactory(alpha)
	poll := factory.New(vdrs)

	poll.Vote(vdr1, votes)
	if !poll.Finished() {
		t.Fatalf("Poll did not terminate after receiving k votes")
	}

	result := poll.Result()
	if list := result.List(); len(list) != 1 {
		t.Fatalf("Wrong number of vertices returned")
	} else if retVtxID := list[0]; retVtxID != vtxID {
		t.Fatalf("Wrong vertex returned")
	} else if set := result.GetSet(vtxID); set.Len() != 1 {
		t.Fatalf("Wrong number of votes returned")
	}
}

func TestEarlyTermNoTraversalString(t *testing.T) {
	alpha := 2

	vtxID := ids.ID{1}
	votes := []ids.ID{vtxID}

	vdr1 := ids.NodeID{1}
	vdr2 := ids.NodeID{2} // k = 2

	vdrs := ids.NodeIDBag{}
	vdrs.Add(
		vdr1,
		vdr2,
	)

	factory := NewEarlyTermNoTraversalFactory(alpha)
	poll := factory.New(vdrs)

	poll.Vote(vdr1, votes)

	expected := `waiting on Bag: (Size = 1)
    ID[NodeID-BaMPFdqMUQ46BV8iRcwbVfsam55kMqcp]: Count = 1
received UniqueBag: (Size = 1)
    ID[SYXsAycDPUu4z2ZksJD5fh5nTDcH3vCFHnpcVye5XuJ2jArg]: Members = 0000000000000002`
	if result := poll.String(); expected != result {
		t.Fatalf("Poll should have returned:\n%s\nbut returned\n%s", expected, result)
	}
}

func TestEarlyTermNoTraversalDropsDuplicatedVotes(t *testing.T) {
	alpha := 2

	vtxID := ids.ID{1}
	votes := []ids.ID{vtxID}

	vdr1 := ids.NodeID{1}
	vdr2 := ids.NodeID{2} // k = 2

	vdrs := ids.NodeIDBag{}
	vdrs.Add(
		vdr1,
		vdr2,
	)

	factory := NewEarlyTermNoTraversalFactory(alpha)
	poll := factory.New(vdrs)

	poll.Vote(vdr1, votes)
	if poll.Finished() {
		t.Fatalf("Poll finished after less than alpha votes")
	}
	poll.Vote(vdr1, votes)
	if poll.Finished() {
		t.Fatalf("Poll finished after getting a duplicated vote")
	}
	poll.Vote(vdr2, votes)
	if !poll.Finished() {
		t.Fatalf("Poll did not terminate after receiving k votes")
	}
}

func TestEarlyTermNoTraversalTerminatesEarly(t *testing.T) {
	alpha := 3

	vtxID := ids.ID{1}
	votes := []ids.ID{vtxID}

	vdr1 := ids.NodeID{1}
	vdr2 := ids.NodeID{2}
	vdr3 := ids.NodeID{3}
	vdr4 := ids.NodeID{4}
	vdr5 := ids.NodeID{5} // k = 5

	vdrs := ids.NodeIDBag{}
	vdrs.Add(
		vdr1,
		vdr2,
		vdr3,
		vdr4,
		vdr5,
	)

	factory := NewEarlyTermNoTraversalFactory(alpha)
	poll := factory.New(vdrs)

	poll.Vote(vdr1, votes)
	if poll.Finished() {
		t.Fatalf("Poll finished after less than alpha votes")
	}
	poll.Vote(vdr2, votes)
	if poll.Finished() {
		t.Fatalf("Poll finished after less than alpha votes")
	}
	poll.Vote(vdr3, votes)
	if !poll.Finished() {
		t.Fatalf("Poll did not terminate early after receiving alpha votes for one vertex and none for other vertices")
	}
}

func TestEarlyTermNoTraversalForSharedAncestor(t *testing.T) {
	alpha := 4

	vtxA := ids.ID{1}
	vtxB := ids.ID{2}
	vtxC := ids.ID{3}
	vtxD := ids.ID{4}

	// If validators 1-3 vote for frontier vertices
	// B, C, and D respectively, which all share the common ancestor
	// A, then we cannot terminate early with alpha = k = 4
	// If the final vote is cast for any of A, B, C, or D, then
	// vertex A will have transitively received alpha = 4 votes
	vdr1 := ids.NodeID{1}
	vdr2 := ids.NodeID{2}
	vdr3 := ids.NodeID{3}
	vdr4 := ids.NodeID{4}

	vdrs := ids.NodeIDBag{}
	vdrs.Add(vdr1)
	vdrs.Add(vdr2)
	vdrs.Add(vdr3)
	vdrs.Add(vdr4)

	factory := NewEarlyTermNoTraversalFactory(alpha)
	poll := factory.New(vdrs)

	poll.Vote(vdr1, []ids.ID{vtxB})
	if poll.Finished() {
		t.Fatalf("Poll finished early after receiving one vote")
	}
	poll.Vote(vdr2, []ids.ID{vtxC})
	if poll.Finished() {
		t.Fatalf("Poll finished early after receiving two votes")
	}
	poll.Vote(vdr3, []ids.ID{vtxD})
	if poll.Finished() {
		t.Fatalf("Poll terminated early, when a shared ancestor could have received alpha votes")
	}
	poll.Vote(vdr4, []ids.ID{vtxA})
	if !poll.Finished() {
		t.Fatalf("Poll did not terminate after receiving all outstanding votes")
	}
}

func TestEarlyTermNoTraversalWithFastDrops(t *testing.T) {
	alpha := 2

	vdr1 := ids.NodeID{1}
	vdr2 := ids.NodeID{2}
	vdr3 := ids.NodeID{3} // k = 3

	vdrs := ids.NodeIDBag{}
	vdrs.Add(
		vdr1,
		vdr2,
		vdr3,
	)

	factory := NewEarlyTermNoTraversalFactory(alpha)
	poll := factory.New(vdrs)

	poll.Vote(vdr1, nil)
	if poll.Finished() {
		t.Fatalf("Poll finished early after dropping one vote")
	}
	poll.Vote(vdr2, nil)
	if !poll.Finished() {
		t.Fatalf("Poll did not terminate after dropping two votes")
	}
}

```

avalanchego/snow/consensus/avalanche/poll/interfaces.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package poll

import (
	"fmt"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/utils/formatting"
)

// Set is a collection of polls
type Set interface {
	fmt.Stringer

	Add(requestID uint32, vdrs ids.NodeIDBag) bool
	Vote(requestID uint32, vdr ids.NodeID, votes []ids.ID) []ids.UniqueBag
	Len() int
}

// Poll is an outstanding poll
type Poll interface {
	formatting.PrefixedStringer

	Vote(vdr ids.NodeID, votes []ids.ID)
	Finished() bool
	Result() ids.UniqueBag
}

// Factory creates a new Poll
type Factory interface {
	New(vdrs ids.NodeIDBag) Poll
}

```

avalanchego/snow/consensus/avalanche/poll/no_early_term.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package poll

import (
	"fmt"

	"github.com/ava-labs/avalanchego/ids"
)

var (
	_ Factory = &noEarlyTermFactory{}
	_ Poll    = &noEarlyTermPoll{}
)

type noEarlyTermFactory struct{}

// NewNoEarlyTermFactory returns a factory that returns polls with no early
// termination
func NewNoEarlyTermFactory() Factory { return noEarlyTermFactory{} }

func (noEarlyTermFactory) New(vdrs ids.NodeIDBag) Poll {
	return &noEarlyTermPoll{polled: vdrs}
}

// noEarlyTermPoll finishes when all polled validators either respond to the
// query or a timeout occurs
type noEarlyTermPoll struct {
	votes  ids.UniqueBag
	polled ids.NodeIDBag
}

// Vote registers a response for this poll
func (p *noEarlyTermPoll) Vote(vdr ids.NodeID, votes []ids.ID) {
	count := p.polled.Count(vdr)
	// make sure that a validator can't respond multiple times
	p.polled.Remove(vdr)

	for i := 0; i < count; i++ {
		// track the votes the validator responded with
		p.votes.Add(uint(p.polled.Len()+i), votes...)
	}
}

// Finished returns true when all validators have voted
func (p *noEarlyTermPoll) Finished() bool { return p.polled.Len() == 0 }

// Result returns the result of this poll
func (p *noEarlyTermPoll) Result() ids.UniqueBag { return p.votes }

func (p *noEarlyTermPoll) PrefixedString(prefix string) string {
	return fmt.Sprintf(
		"waiting on %s\n%sreceived %s",
		p.polled.PrefixedString(prefix),
		prefix,
		p.votes.PrefixedString(prefix),
	)
}

func (p *noEarlyTermPoll) String() string { return p.PrefixedString("") }

```

avalanchego/snow/consensus/avalanche/poll/no_early_term_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package poll

import (
	"testing"

	"github.com/ava-labs/avalanchego/ids"
)

func TestNoEarlyTermResults(t *testing.T) {
	vtxID := ids.ID{1}
	votes := []ids.ID{vtxID}

	vdr1 := ids.NodeID{1} // k = 1

	vdrs := ids.NodeIDBag{}
	vdrs.Add(vdr1)

	factory := NewNoEarlyTermFactory()
	poll := factory.New(vdrs)

	poll.Vote(vdr1, votes)
	if !poll.Finished() {
		t.Fatalf("Poll did not terminate after receiving k votes")
	}

	result := poll.Result()
	if list := result.List(); len(list) != 1 {
		t.Fatalf("Wrong number of vertices returned")
	} else if retVtxID := list[0]; retVtxID != vtxID {
		t.Fatalf("Wrong vertex returned")
	} else if set := result.GetSet(vtxID); set.Len() != 1 {
		t.Fatalf("Wrong number of votes returned")
	}
}

func TestNoEarlyTermString(t *testing.T) {
	vtxID := ids.ID{1}
	votes := []ids.ID{vtxID}

	vdr1 := ids.NodeID{1}
	vdr2 := ids.NodeID{2} // k = 2

	vdrs := ids.NodeIDBag{}
	vdrs.Add(
		vdr1,
		vdr2,
	)

	factory := NewNoEarlyTermFactory()
	poll := factory.New(vdrs)

	poll.Vote(vdr1, votes)

	expected := `waiting on Bag: (Size = 1)
    ID[NodeID-BaMPFdqMUQ46BV8iRcwbVfsam55kMqcp]: Count = 1
received UniqueBag: (Size = 1)
    ID[SYXsAycDPUu4z2ZksJD5fh5nTDcH3vCFHnpcVye5XuJ2jArg]: Members = 0000000000000002`
	if result := poll.String(); expected != result {
		t.Fatalf("Poll should have returned %s but returned %s", expected, result)
	}
}

func TestNoEarlyTermDropsDuplicatedVotes(t *testing.T) {
	vtxID := ids.ID{1}
	votes := []ids.ID{vtxID}

	vdr1 := ids.NodeID{1}
	vdr2 := ids.NodeID{2} // k = 2

	vdrs := ids.NodeIDBag{}
	vdrs.Add(
		vdr1,
		vdr2,
	)

	factory := NewNoEarlyTermFactory()
	poll := factory.New(vdrs)

	poll.Vote(vdr1, votes)
	if poll.Finished() {
		t.Fatalf("Poll finished after less than alpha votes")
	}
	poll.Vote(vdr1, votes)
	if poll.Finished() {
		t.Fatalf("Poll finished after getting a duplicated vote")
	}
	poll.Vote(vdr2, votes)
	if !poll.Finished() {
		t.Fatalf("Poll did not terminate after receiving k votes")
	}
}

```

avalanchego/snow/consensus/avalanche/poll/set.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package poll

import (
	"fmt"
	"strings"
	"time"

	"go.uber.org/zap"

	"github.com/prometheus/client_golang/prometheus"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/utils/linkedhashmap"
	"github.com/ava-labs/avalanchego/utils/logging"
	"github.com/ava-labs/avalanchego/utils/metric"
)

var (
	_ Set  = &set{}
	_ Poll = &poll{}
)

type pollHolder interface {
	GetPoll() Poll
	StartTime() time.Time
}

type poll struct {
	Poll
	start time.Time
}

func (p poll) GetPoll() Poll {
	return p
}

func (p poll) StartTime() time.Time {
	return p.start
}

type set struct {
	log      logging.Logger
	numPolls prometheus.Gauge
	durPolls metric.Averager
	factory  Factory
	// maps requestID -> poll
	polls linkedhashmap.LinkedHashmap[uint32, pollHolder]
}

// NewSet returns a new empty set of polls
func NewSet(
	factory Factory,
	log logging.Logger,
	namespace string,
	reg prometheus.Registerer,
) Set {
	numPolls := prometheus.NewGauge(prometheus.GaugeOpts{
		Namespace: namespace,
		Name:      "polls",
		Help:      "Number of pending network polls",
	})
	if err := reg.Register(numPolls); err != nil {
		log.Error("failed to register polls statistics",
			zap.Error(err),
		)
	}

	durPolls, err := metric.NewAverager(
		namespace,
		"poll_duration",
		"time (in ns) this poll took to complete",
		reg,
	)
	if err != nil {
		log.Error("failed to register poll_duration statistics",
			zap.Error(err),
		)
	}

	return &set{
		log:      log,
		numPolls: numPolls,
		durPolls: durPolls,
		factory:  factory,
		polls:    linkedhashmap.New[uint32, pollHolder](),
	}
}

// Add to the current set of polls
// Returns true if the poll was registered correctly and the network sample
//         should be made.
func (s *set) Add(requestID uint32, vdrs ids.NodeIDBag) bool {
	if _, exists := s.polls.Get(requestID); exists {
		s.log.Debug("dropping poll",
			zap.String("reason", "duplicated request"),
			zap.Uint32("requestID", requestID),
		)
		return false
	}

	s.log.Verbo("creating poll",
		zap.Uint32("requestID", requestID),
		zap.Stringer("validators", &vdrs),
	)

	s.polls.Put(requestID, poll{
		Poll:  s.factory.New(vdrs), // create the new poll
		start: time.Now(),
	})
	s.numPolls.Inc() // increase the metrics
	return true
}

// Vote registers the connections response to a query for [id]. If there was no
// query, or the response has already be registered, nothing is performed.
func (s *set) Vote(requestID uint32, vdr ids.NodeID, votes []ids.ID) []ids.UniqueBag {
	holder, exists := s.polls.Get(requestID)
	if !exists {
		s.log.Verbo("dropping vote",
			zap.String("reason", "unknown poll"),
			zap.Stringer("validator", vdr),
			zap.Uint32("requestID", requestID),
		)
		return nil
	}

	p := holder.GetPoll()

	s.log.Verbo("processing votes",
		zap.Stringer("validator", vdr),
		zap.Uint32("requestID", requestID),
		zap.Stringer("votes", ids.SliceStringer(votes)),
	)

	p.Vote(vdr, votes)
	if !p.Finished() {
		return nil
	}

	var results []ids.UniqueBag

	// iterate from oldest to newest
	iter := s.polls.NewIterator()
	for iter.Next() {
		holder := iter.Value()
		p := holder.GetPoll()
		if !p.Finished() {
			// since we're iterating from oldest to newest, if the next poll has not finished,
			// we can break and return what we have so far
			break
		}

		s.log.Verbo("poll finished",
			zap.Uint32("requestID", requestID),
			zap.Stringer("poll", p),
		)
		s.durPolls.Observe(float64(time.Since(holder.StartTime())))
		s.numPolls.Dec() // decrease the metrics

		results = append(results, p.Result())
		s.polls.Delete(iter.Key()) // remove the poll from the current set
	}

	// only gets here if the poll has finished
	// results will have values if this and other newer polls have finished
	return results
}

// Len returns the number of outstanding polls
func (s *set) Len() int { return s.polls.Len() }

func (s *set) String() string {
	sb := strings.Builder{}
	sb.WriteString(fmt.Sprintf("current polls: (Size = %d)", s.polls.Len()))
	iter := s.polls.NewIterator()
	for iter.Next() {
		requestID := iter.Key()
		poll := iter.Value().(Poll)
		sb.WriteString(fmt.Sprintf("\n    RequestID %d:\n        %s", requestID, poll.PrefixedString("        ")))
	}
	return sb.String()
}

```

avalanchego/snow/consensus/avalanche/poll/set_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package poll

import (
	"testing"

	"github.com/stretchr/testify/require"

	"github.com/prometheus/client_golang/prometheus"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/utils/logging"
	"github.com/ava-labs/avalanchego/utils/wrappers"
)

func TestNewSetErrorOnMetrics(t *testing.T) {
	factory := NewNoEarlyTermFactory()
	log := logging.NoLog{}
	namespace := ""
	registerer := prometheus.NewRegistry()

	errs := wrappers.Errs{}
	errs.Add(
		registerer.Register(prometheus.NewCounter(prometheus.CounterOpts{
			Name: "polls",
		})),
		registerer.Register(prometheus.NewCounter(prometheus.CounterOpts{
			Name: "poll_duration",
		})),
	)
	if errs.Errored() {
		t.Fatal(errs.Err)
	}

	if s := NewSet(factory, log, namespace, registerer); s == nil {
		t.Fatalf("shouldn't have errored due to metrics failures")
	}
}

func TestCreateAndFinishPoll(t *testing.T) {
	factory := NewNoEarlyTermFactory()
	log := logging.NoLog{}
	namespace := ""
	registerer := prometheus.NewRegistry()
	s := NewSet(factory, log, namespace, registerer)

	vtxID := ids.ID{1}
	votes := []ids.ID{vtxID}

	vdr1 := ids.NodeID{1}
	vdr2 := ids.NodeID{2} // k = 2

	vdrs := ids.NodeIDBag{}
	vdrs.Add(
		vdr1,
		vdr2,
	)

	if s.Len() != 0 {
		t.Fatalf("Shouldn't have any active polls yet")
	} else if !s.Add(0, vdrs) {
		t.Fatalf("Should have been able to add a new poll")
	} else if s.Len() != 1 {
		t.Fatalf("Should only have one active poll")
	} else if s.Add(0, vdrs) {
		t.Fatalf("Shouldn't have been able to add a duplicated poll")
	} else if s.Len() != 1 {
		t.Fatalf("Should only have one active poll")
	} else if results := s.Vote(1, vdr1, votes); len(results) > 0 {
		t.Fatalf("Shouldn't have been able to finish a non-existent poll")
	} else if results = s.Vote(0, vdr1, votes); len(results) > 0 {
		t.Fatalf("Shouldn't have been able to finish an ongoing poll")
	} else if results = s.Vote(0, vdr1, votes); len(results) > 0 {
		t.Fatalf("Should have dropped a duplicated poll")
	} else if results = s.Vote(0, vdr2, votes); len(results) == 0 {
		t.Fatalf("Should have finished the poll")
	} else if len(results) != 1 {
		t.Fatalf("Wrong number of results returned")
	} else if list := results[0].List(); len(list) != 1 {
		t.Fatalf("Wrong number of vertices returned")
	} else if retVtxID := list[0]; retVtxID != vtxID {
		t.Fatalf("Wrong vertex returned")
	} else if set := results[0].GetSet(vtxID); set.Len() != 2 {
		t.Fatalf("Wrong number of votes returned")
	}
}

func TestCreateAndFinishPollOutOfOrder_OlderFinishesFirst(t *testing.T) {
	factory := NewNoEarlyTermFactory()
	log := logging.NoLog{}
	namespace := ""
	registerer := prometheus.NewRegistry()
	s := NewSet(factory, log, namespace, registerer)

	// create validators
	vdr1 := ids.NodeID{1}
	vdr2 := ids.NodeID{2}
	vdr3 := ids.NodeID{3}

	vdrs := []ids.NodeID{vdr1, vdr2, vdr3}

	// create two polls for the two vtxs
	vdrBag := ids.NodeIDBag{}
	vdrBag.Add(vdrs...)
	added := s.Add(1, vdrBag)
	require.True(t, added)

	vdrBag = ids.NodeIDBag{}
	vdrBag.Add(vdrs...)
	added = s.Add(2, vdrBag)
	require.True(t, added)
	require.Equal(t, s.Len(), 2)

	// vote vtx1 for poll 1
	// vote vtx2 for poll 2
	vtx1 := ids.ID{1}
	vtx2 := ids.ID{2}

	var results []ids.UniqueBag

	// vote out of order
	results = s.Vote(1, vdr1, []ids.ID{vtx1})
	require.Len(t, results, 0)
	results = s.Vote(2, vdr2, []ids.ID{vtx2})
	require.Len(t, results, 0)
	results = s.Vote(2, vdr3, []ids.ID{vtx2})
	require.Len(t, results, 0)

	results = s.Vote(1, vdr2, []ids.ID{vtx1})
	require.Len(t, results, 0)

	results = s.Vote(1, vdr3, []ids.ID{vtx1}) // poll 1 finished, poll 2 still remaining
	require.Len(t, results, 1)                // because 1 is the oldest
	require.Equal(t, vtx1, results[0].List()[0])

	results = s.Vote(2, vdr1, []ids.ID{vtx2}) // poll 2 finished
	require.Len(t, results, 1)                // because 2 is the oldest now
	require.Equal(t, vtx2, results[0].List()[0])
}

func TestCreateAndFinishPollOutOfOrder_UnfinishedPollsGaps(t *testing.T) {
	factory := NewNoEarlyTermFactory()
	log := logging.NoLog{}
	namespace := ""
	registerer := prometheus.NewRegistry()
	s := NewSet(factory, log, namespace, registerer)

	// create validators
	vdr1 := ids.NodeID{1}
	vdr2 := ids.NodeID{2}
	vdr3 := ids.NodeID{3}

	vdrs := []ids.NodeID{vdr1, vdr2, vdr3}

	// create three polls for the two vtxs
	vdrBag := ids.NodeIDBag{}
	vdrBag.Add(vdrs...)
	added := s.Add(1, vdrBag)
	require.True(t, added)

	vdrBag = ids.NodeIDBag{}
	vdrBag.Add(vdrs...)
	added = s.Add(2, vdrBag)
	require.True(t, added)

	vdrBag = ids.NodeIDBag{}
	vdrBag.Add(vdrs...)
	added = s.Add(3, vdrBag)
	require.True(t, added)
	require.Equal(t, s.Len(), 3)

	// vote vtx1 for poll 1
	// vote vtx2 for poll 2
	// vote vtx3 for poll 3
	vtx1 := ids.ID{1}
	vtx2 := ids.ID{2}
	vtx3 := ids.ID{3}

	var results []ids.UniqueBag

	// vote out of order
	// 2 finishes first to create a gap of finished poll between two unfinished polls 1 and 3
	results = s.Vote(2, vdr3, []ids.ID{vtx2})
	require.Len(t, results, 0)
	results = s.Vote(2, vdr2, []ids.ID{vtx2})
	require.Len(t, results, 0)
	results = s.Vote(2, vdr1, []ids.ID{vtx2})
	require.Len(t, results, 0)

	// 3 finishes now, 2 has already finished but 1 is not finished so we expect to receive no results still
	results = s.Vote(3, vdr2, []ids.ID{vtx3})
	require.Len(t, results, 0)
	results = s.Vote(3, vdr3, []ids.ID{vtx3})
	require.Len(t, results, 0)
	results = s.Vote(3, vdr1, []ids.ID{vtx3})
	require.Len(t, results, 0)

	// 1 finishes now, 2 and 3 have already finished so we expect 3 items in results
	results = s.Vote(1, vdr1, []ids.ID{vtx1})
	require.Len(t, results, 0)
	results = s.Vote(1, vdr2, []ids.ID{vtx1})
	require.Len(t, results, 0)
	results = s.Vote(1, vdr3, []ids.ID{vtx1})
	require.Len(t, results, 3)
	require.Equal(t, vtx1.String(), results[0].List()[0].String())
	require.Equal(t, vtx2.String(), results[1].List()[0].String())
	require.Equal(t, vtx3.String(), results[2].List()[0].String())
}

func TestSetString(t *testing.T) {
	factory := NewNoEarlyTermFactory()
	log := logging.NoLog{}
	namespace := ""
	registerer := prometheus.NewRegistry()
	s := NewSet(factory, log, namespace, registerer)

	vdr1 := ids.NodeID{1} // k = 1

	vdrs := ids.NodeIDBag{}
	vdrs.Add(vdr1)

	expected := `current polls: (Size = 1)
    RequestID 0:
        waiting on Bag: (Size = 1)
            ID[NodeID-6HgC8KRBEhXYbF4riJyJFLSHt37UNuRt]: Count = 1
        received UniqueBag: (Size = 0)`
	if !s.Add(0, vdrs) {
		t.Fatalf("Should have been able to add a new poll")
	} else if str := s.String(); expected != str {
		t.Fatalf("Set return wrong string, Expected:\n%s\nReturned:\n%s",
			expected,
			str)
	}
}

```

avalanchego/snow/consensus/avalanche/test_vertex.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package avalanche

import (
	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/choices"
	"github.com/ava-labs/avalanchego/snow/consensus/snowstorm"
)

var _ Vertex = &TestVertex{}

// TestVertex is a useful test vertex
type TestVertex struct {
	choices.TestDecidable

	VerifyErrV    error
	ParentsV      []Vertex
	ParentsErrV   error
	HasWhitelistV bool
	WhitelistV    ids.Set
	WhitelistErrV error
	HeightV       uint64
	HeightErrV    error
	TxsV          []snowstorm.Tx
	TxsErrV       error
	BytesV        []byte
}

func (v *TestVertex) Verify() error                { return v.VerifyErrV }
func (v *TestVertex) Parents() ([]Vertex, error)   { return v.ParentsV, v.ParentsErrV }
func (v *TestVertex) HasWhitelist() bool           { return v.HasWhitelistV }
func (v *TestVertex) Whitelist() (ids.Set, error)  { return v.WhitelistV, v.WhitelistErrV }
func (v *TestVertex) Height() (uint64, error)      { return v.HeightV, v.HeightErrV }
func (v *TestVertex) Txs() ([]snowstorm.Tx, error) { return v.TxsV, v.TxsErrV }
func (v *TestVertex) Bytes() []byte                { return v.BytesV }

```

avalanchego/snow/consensus/avalanche/topological.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package avalanche

import (
	"errors"
	"fmt"
	"strings"

	"go.uber.org/zap"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow"
	"github.com/ava-labs/avalanchego/snow/choices"
	"github.com/ava-labs/avalanchego/snow/consensus/metrics"
	"github.com/ava-labs/avalanchego/snow/consensus/snowstorm"
)

const minMapSize = 16

var (
	errNoLeaves = errors.New("couldn't pop a leaf from leaf set")

	_ Factory   = &TopologicalFactory{}
	_ Consensus = &Topological{}
)

// TopologicalFactory implements Factory by returning a topological struct
type TopologicalFactory struct{}

func (TopologicalFactory) New() Consensus { return &Topological{} }

// TODO: Implement pruning of decisions.
// To perfectly preserve the protocol, this implementation will need to store
// the hashes of all accepted decisions. It is possible to add a heuristic that
// removes sufficiently old decisions. However, that will need to be analyzed to
// ensure safety. It is doable when adding in a weak synchrony assumption.

// Topological performs the avalanche algorithm by utilizing a topological sort
// of the voting results. Assumes that vertices are inserted in topological
// order.
type Topological struct {
	metrics.Latency

	// pollNumber is the number of times RecordPolls has been called
	pollNumber uint64

	// Context used for logging
	ctx *snow.ConsensusContext
	// Threshold for confidence increases
	params Parameters

	// Maps vtxID -> transactionVertex wrapping the provided vertex as a
	// transaction
	nodes map[ids.ID]*transactionVertex

	// Tracks the conflict relations
	cg snowstorm.Consensus

	// preferred is the frontier of vtxIDs that are strongly preferred
	preferred ids.Set

	// virtuous is the frontier of vtxIDs that are strongly virtuous
	virtuous ids.Set

	// orphans are the txIDs that are virtuous, but not preferred
	orphans ids.Set

	// virtuousVoting are the txIDs that are virtuous and still awaiting
	// additional votes before acceptance. transactionVertices whose vertices
	// are not considered virtuous are removed from this set.
	virtuousVoting ids.Set

	// frontier is the set of vts that have no descendents
	frontier map[ids.ID]Vertex
	// preferenceCache is the cache for strongly preferred checks
	// virtuousCache is the cache for strongly virtuous checks
	preferenceCache, virtuousCache map[ids.ID]bool

	// Used in [calculateInDegree] and [markAncestorInDegrees].
	// Should only be accessed in those methods.
	// We use this one instance of ids.Set instead of creating a
	// new ids.Set during each call to [calculateInDegree].
	leaves ids.Set

	// Kahn nodes used in [calculateInDegree] and [markAncestorInDegrees].
	// Should only be accessed in those methods.
	// We use this one map instead of creating a new map
	// during each call to [calculateInDegree].
	kahnNodes map[ids.ID]kahnNode

	// Used in [pushVotes]. Should only be accessed in that method.
	// We use this one instance instead of creating a new ids.UniqueBag
	// during each call to [pushVotes].
	votes ids.UniqueBag
}

type kahnNode struct {
	inDegree int
	votes    ids.BitSet64
}

func (ta *Topological) Initialize(
	ctx *snow.ConsensusContext,
	params Parameters,
	frontier []Vertex,
) error {
	if err := params.Valid(); err != nil {
		return err
	}

	ta.ctx = ctx
	ta.params = params
	ta.leaves = ids.Set{}
	ta.votes = ids.UniqueBag{}
	ta.kahnNodes = make(map[ids.ID]kahnNode)

	latencyMetrics, err := metrics.NewLatency("vtx", "vertex/vertices", ctx.Log, "", ctx.Registerer)
	if err != nil {
		return err
	}
	ta.Latency = latencyMetrics

	ta.nodes = make(map[ids.ID]*transactionVertex, minMapSize)

	ta.cg = &snowstorm.Directed{}
	if err := ta.cg.Initialize(ctx, params.Parameters); err != nil {
		return err
	}

	ta.frontier = make(map[ids.ID]Vertex, minMapSize)
	for _, vtx := range frontier {
		ta.frontier[vtx.ID()] = vtx
	}
	return ta.updateFrontiers()
}

func (ta *Topological) NumProcessing() int { return len(ta.nodes) }

func (ta *Topological) Parameters() Parameters { return ta.params }

func (ta *Topological) IsVirtuous(tx snowstorm.Tx) bool { return ta.cg.IsVirtuous(tx) }

func (ta *Topological) Add(vtx Vertex) error {
	if vtx.Status().Decided() {
		return nil // Already decided this vertex
	}

	vtxID := vtx.ID()
	if _, exists := ta.nodes[vtxID]; exists {
		return nil // Already inserted this vertex
	}

	txs, err := vtx.Txs()
	if err != nil {
		return err
	}
	for _, tx := range txs {
		if !tx.Status().Decided() {
			// Add the consumers to the conflict graph.
			if err := ta.cg.Add(tx); err != nil {
				return err
			}

			// If the added transaction is virtuous, add it to the set of
			// virtuous transactions that are still being voted on.
			if vs := ta.cg.VirtuousVoting(); vs.Contains(tx.ID()) {
				ta.virtuousVoting.Add(tx.ID())
			}
		}
	}

	txv := newTransactionVertex(vtx, ta.nodes)

	// Add the transaction vertex to the set of processing nodes.
	ta.nodes[vtxID] = txv

	// Also add the transaction vertex to the conflict graph to track conflicts.
	if err := ta.cg.Add(txv); err != nil {
		return err
	}

	// If the added transaction vertex is virtuous, add it to the set of
	// virtuous transactions that are still being voted on. If the vertex isn't
	// virtuous, then the ID will be removed on the subsequent call to update.
	if vs := ta.cg.VirtuousVoting(); vs.Contains(vtxID) {
		ta.virtuousVoting.Add(vtxID)
	}

	ta.Latency.Issued(vtxID, ta.pollNumber)

	// Because we don't call [updateFrontiers], previous vertices that were
	// marked as virtuous will not be updated to no longer being virtuous. Even
	// if this newly added vertex conflicts with them. This is an optimization
	// to avoid a retraversal of the DAG in the issuance path. Their virtuous
	// status will be updated during a future poll. This is safe because the
	// virtuous frontier is only used optimistically to control when it is valid
	// to quiesce.
	return ta.update(vtx) // Update the vertices preference and virtuous status
}

func (ta *Topological) VertexIssued(vtx Vertex) bool {
	if vtx.Status().Decided() {
		return true
	}
	_, ok := ta.nodes[vtx.ID()]
	return ok
}

func (ta *Topological) TxIssued(tx snowstorm.Tx) bool { return ta.cg.Issued(tx) }

func (ta *Topological) Orphans() ids.Set { return ta.orphans }

func (ta *Topological) Virtuous() ids.Set { return ta.virtuous }

func (ta *Topological) Preferences() ids.Set { return ta.preferred }

func (ta *Topological) RecordPoll(responses ids.UniqueBag) error {
	// Register a new poll call
	ta.pollNumber++

	// If it isn't possible to have alpha votes for any transaction, then we can
	// just reset the confidence values in the conflict graph and not perform
	// any traversals.
	partialVotes := ids.BitSet64(0)
	for vote := range responses {
		votes := responses.GetSet(vote)
		partialVotes.Union(votes)
		if partialVotes.Len() >= ta.params.Alpha {
			break
		}
	}
	if partialVotes.Len() < ta.params.Alpha {
		// Because there were less than alpha total returned votes, we can skip
		// the traversals and fail the poll.
		_, err := ta.cg.RecordPoll(ids.Bag{})
		return err
	}

	// Set up the topological sort: O(|Live Set|)
	if err := ta.calculateInDegree(responses); err != nil {
		return err
	}

	// Collect the votes for each transaction: O(|Live Set|)
	votes, err := ta.pushVotes()
	if err != nil {
		return err
	}

	// Update the conflict graph: O(|Transactions|)
	if updated, err := ta.cg.RecordPoll(votes); !updated || err != nil {
		// If the transaction statuses weren't changed, there is no need to
		// perform a traversal.
		return err
	}

	// Update the dag: O(|Live Set|)
	return ta.updateFrontiers()
}

func (ta *Topological) Quiesce() bool { return ta.virtuousVoting.Len() == 0 }

func (ta *Topological) Finalized() bool { return ta.cg.Finalized() }

// HealthCheck returns information about the consensus health.
func (ta *Topological) HealthCheck() (interface{}, error) {
	numOutstandingVtx := ta.Latency.NumProcessing()
	isOutstandingVtx := numOutstandingVtx <= ta.params.MaxOutstandingItems
	healthy := isOutstandingVtx
	details := map[string]interface{}{
		"outstandingVertices": numOutstandingVtx,
	}

	snowstormReport, err := ta.cg.HealthCheck()
	healthy = healthy && err == nil
	details["snowstorm"] = snowstormReport

	if !healthy {
		var errorReasons []string
		if isOutstandingVtx {
			errorReasons = append(errorReasons, fmt.Sprintf("number outstanding vertexes %d > %d", numOutstandingVtx, ta.params.MaxOutstandingItems))
		}
		if err != nil {
			errorReasons = append(errorReasons, err.Error())
		}
		return details, fmt.Errorf("avalanche consensus is not healthy reason: %s", strings.Join(errorReasons, ", "))
	}
	return details, nil
}

// Takes in a list of votes and sets up the topological ordering. Returns the
// reachable section of the graph annotated with the number of inbound edges and
// the non-transitively applied votes. Also returns the list of leaf nodes.
func (ta *Topological) calculateInDegree(responses ids.UniqueBag) error {
	// Clear the kahn node set
	for k := range ta.kahnNodes {
		delete(ta.kahnNodes, k)
	}
	// Clear the leaf set
	ta.leaves.Clear()

	for vote := range responses {
		// If it is not found, then the vote is either for something decided,
		// or something we haven't heard of yet.
		if tv := ta.nodes[vote]; tv != nil {
			vtx := tv.vtx

			kahn, previouslySeen := ta.kahnNodes[vote]
			// Add this new vote to the current bag of votes
			kahn.votes.Union(responses.GetSet(vote))
			ta.kahnNodes[vote] = kahn

			if !previouslySeen {
				// If I've never seen this node before, it is currently a leaf.
				ta.leaves.Add(vote)
				parents, err := vtx.Parents()
				if err != nil {
					return err
				}
				ta.kahnNodes, err = ta.markAncestorInDegrees(ta.kahnNodes, parents)
				if err != nil {
					return err
				}
			}
		}
	}
	return nil
}

// adds a new in-degree reference for all nodes.
// should only be called from [calculateInDegree]
func (ta *Topological) markAncestorInDegrees(
	kahns map[ids.ID]kahnNode,
	deps []Vertex,
) (map[ids.ID]kahnNode, error) {
	frontier := make([]Vertex, 0, len(deps))
	for _, vtx := range deps {
		// The vertex may have been decided, no need to vote in that case
		if !vtx.Status().Decided() {
			frontier = append(frontier, vtx)
		}
	}

	for len(frontier) > 0 {
		newLen := len(frontier) - 1
		current := frontier[newLen]
		frontier = frontier[:newLen]

		currentID := current.ID()
		kahn, alreadySeen := kahns[currentID]
		// I got here through a transitive edge, so increase the in-degree
		kahn.inDegree++
		kahns[currentID] = kahn

		if kahn.inDegree == 1 {
			// If I am transitively seeing this node for the first
			// time, it is no longer a leaf.
			ta.leaves.Remove(currentID)
		}

		if !alreadySeen {
			// If I am seeing this node for the first time, I need to check its
			// parents
			parents, err := current.Parents()
			if err != nil {
				return nil, err
			}
			for _, depVtx := range parents {
				// No need to traverse to a decided vertex
				if !depVtx.Status().Decided() {
					frontier = append(frontier, depVtx)
				}
			}
		}
	}
	return kahns, nil
}

// Count the number of votes for each operation by pushing votes upwards through
// vertex ancestors.
func (ta *Topological) pushVotes() (ids.Bag, error) {
	ta.votes.Clear()
	txConflicts := make(map[ids.ID]ids.Set, minMapSize)

	// A leaf is a node with no inbound edges. This removes each leaf and pushes
	// the votes upwards, potentially creating new leaves, until there are no
	// more leaves.
	for ta.leaves.Len() > 0 {
		// Pop one node from [leaves]
		leaf, ok := ta.leaves.Pop()
		if !ok {
			// Should never happen because we just checked that [ta.leaves] is
			// not empty.
			return ids.Bag{}, errNoLeaves
		}

		kahn := ta.kahnNodes[leaf]

		if tv := ta.nodes[leaf]; tv != nil {
			vtx := tv.vtx
			txs, err := vtx.Txs()
			if err != nil {
				return ids.Bag{}, err
			}
			for _, tx := range txs {
				// Give the votes to the consumer
				txID := tx.ID()
				ta.votes.UnionSet(txID, kahn.votes)

				// Map txID to set of Conflicts
				if _, exists := txConflicts[txID]; !exists {
					txConflicts[txID] = ta.cg.Conflicts(tx)
				}
			}

			// The leaf is the ID of the transaction vertex that was issued to
			// the conflict graph for this vertex. Adding this vote is required
			// to make progress toward accepting this transaction.
			ta.votes.UnionSet(leaf, kahn.votes)

			// Map the vertexID to the set of conflicts from the transaction
			// vertex.
			if _, exists := txConflicts[leaf]; !exists {
				txConflicts[leaf] = ta.cg.Conflicts(tv)
			}

			parents, err := vtx.Parents()
			if err != nil {
				return ids.Bag{}, err
			}
			for _, dep := range parents {
				depID := dep.ID()
				if depNode, notPruned := ta.kahnNodes[depID]; notPruned {
					depNode.inDegree--
					// Give the votes to my parents
					depNode.votes.Union(kahn.votes)
					ta.kahnNodes[depID] = depNode

					if depNode.inDegree == 0 {
						// Only traverse into the leaves
						ta.leaves.Add(depID)
					}
				}
			}
		}
	}

	// Create bag of votes for conflicting transactions
	conflictingVotes := make(ids.UniqueBag)
	for txID, conflicts := range txConflicts {
		for conflictTxID := range conflicts {
			conflictingVotes.UnionSet(txID, ta.votes.GetSet(conflictTxID))
		}
	}

	ta.votes.Difference(&conflictingVotes)
	return ta.votes.Bag(ta.params.Alpha), nil
}

// If I've already checked, do nothing
// If I'm decided, cache the preference and return
// At this point, I must be live
// I now try to accept all my consumers
// I now update all my ancestors
// If any of my parents are rejected, reject myself
// If I'm preferred, remove all my ancestors from the preferred frontier, add
//     myself to the preferred frontier
// If all my parents are accepted and I'm acceptable, accept myself
func (ta *Topological) update(vtx Vertex) error {
	vtxID := vtx.ID()
	if _, cached := ta.preferenceCache[vtxID]; cached {
		return nil // This vertex has already been updated
	}

	// Drop all transaction vertices from the orphan set, as they can never be
	// reissued.
	ta.orphans.Remove(vtxID)

	switch vtx.Status() {
	case choices.Accepted:
		ta.preferred.Add(vtxID) // I'm preferred
		ta.virtuous.Add(vtxID)  // Accepted is defined as virtuous

		ta.frontier[vtxID] = vtx // I have no descendents yet

		ta.preferenceCache[vtxID] = true
		ta.virtuousCache[vtxID] = true
		return nil
	case choices.Rejected:
		// I'm rejected
		ta.preferenceCache[vtxID] = false
		ta.virtuousCache[vtxID] = false
		return nil
	}

	txs, err := vtx.Txs()
	if err != nil {
		return err
	}
	preferences := ta.cg.Preferences()
	virtuousTxs := ta.cg.Virtuous()

	txv, ok := ta.nodes[vtxID]
	if !ok {
		return fmt.Errorf("transaction vertex %s not found in processing nodes set", vtxID)
	}

	initialTxVStatus := txv.Status()

	// acceptable tracks if all transactions included in the vertex have been
	// accepted and if all the parent vertices have been accepted. The
	// transactions include the transaction vertex.
	acceptable := initialTxVStatus == choices.Accepted

	// rejectable tracks if any of the transactions included in the vertex have
	// been rejected or if any of the parent vertices have been rejected. The
	// transactions include the transaction vertex.
	rejectable := initialTxVStatus == choices.Rejected

	preferred := acceptable || preferences.Contains(vtxID)
	virtuous := acceptable || virtuousTxs.Contains(vtxID)

	for _, tx := range txs {
		txID := tx.ID()
		s := tx.Status()
		if s == choices.Rejected {
			// If I contain a rejected consumer, I am rejectable
			rejectable = true
			preferred = false
			virtuous = false
		}
		if s != choices.Accepted {
			// If I contain a non-accepted consumer, I am not acceptable
			acceptable = false
			preferred = preferred && preferences.Contains(txID)
			virtuous = virtuous && virtuousTxs.Contains(txID)
		}
	}

	deps, err := vtx.Parents()
	if err != nil {
		return err
	}
	// Update all of my dependencies
	for _, dep := range deps {
		if err := ta.update(dep); err != nil {
			return err
		}

		depID := dep.ID()
		preferred = preferred && ta.preferenceCache[depID]
		virtuous = virtuous && ta.virtuousCache[depID]
	}

	// Check my parent statuses
	for _, dep := range deps {
		switch status := dep.Status(); status {
		case choices.Rejected:
			// My parent is rejected, so I should be rejected
			ta.ctx.Log.Trace("rejecting vertex",
				zap.String("reason", "rejected parent"),
				zap.Stringer("vtxID", vtxID),
				zap.Stringer("parentID", dep.ID()),
			)
			if !txv.Status().Decided() {
				if err := ta.cg.Remove(vtxID); err != nil {
					return fmt.Errorf("failed to remove transaction vertex %s from snowstorm before rejecting vertex itself", vtxID)
				}
				ta.virtuousVoting.Remove(vtxID)
			}
			if err := vtx.Reject(); err != nil {
				return err
			}
			delete(ta.nodes, vtxID)
			ta.Latency.Rejected(vtxID, ta.pollNumber)

			ta.preferenceCache[vtxID] = false
			ta.virtuousCache[vtxID] = false
			return nil
		case choices.Accepted:
			// If the dependency is accepted, then the vertex's acceptability
			// doesn't change.
		default:
			acceptable = false // A parent isn't accepted, so I can't be
		}
	}

	// Technically, we could also check to see if there are direct conflicts
	// between this vertex and a vertex in it's ancestry. If there does exist
	// such a conflict, this vertex could also be rejected. However, this would
	// require a traversal. Therefore, this memory optimization is ignored.
	// Also, this will only happen from a byzantine node issuing the vertex.
	// Therefore, this is very unlikely to actually be triggered in practice.

	// Remove all my parents from the frontier
	for _, dep := range deps {
		delete(ta.frontier, dep.ID())
	}
	ta.frontier[vtxID] = vtx // I have no descendents yet

	ta.preferenceCache[vtxID] = preferred
	ta.virtuousCache[vtxID] = virtuous

	if preferred {
		ta.preferred.Add(vtxID) // I'm preferred
		for _, dep := range deps {
			ta.preferred.Remove(dep.ID()) // My parents aren't part of the frontier
		}

		// Transactions are marked as orphans if they are virtuous, but not
		// contained in a preferred vertex. Since this vertex is preferred,
		// remove all the internal transactions from the orphan set.
		//
		// As an optimization, we only iterate over the transactions if the set
		// of orphans isn't empty. As if the set is empty, nothing will be able
		// to be removed anyway.
		if ta.orphans.Len() > 0 {
			for _, tx := range txs {
				if tx.Status() != choices.Accepted {
					ta.orphans.Remove(tx.ID())
				}
			}
		}
	}

	if virtuous {
		ta.virtuous.Add(vtxID) // I'm virtuous
		for _, dep := range deps {
			ta.virtuous.Remove(dep.ID()) // My parents aren't part of the frontier
		}
	} else {
		// If the vertex isn't virtuous, then we can remove the transaction
		// vertex from the voting set to pessemistically quiesce early.
		ta.virtuousVoting.Remove(vtxID)
	}

	switch {
	case acceptable:
		// I'm acceptable, why not accept?
		// Note that ConsensusAcceptor.Accept must be called before vtx.Accept
		// to honor Acceptor.Accept's invariant.
		if err := ta.ctx.ConsensusAcceptor.Accept(ta.ctx, vtxID, vtx.Bytes()); err != nil {
			return err
		}

		if err := vtx.Accept(); err != nil {
			return err
		}
		delete(ta.nodes, vtxID)
		ta.Latency.Accepted(vtxID, ta.pollNumber)
	case rejectable:
		// I'm rejectable, why not reject?
		ta.ctx.Log.Trace("rejecting vertex",
			zap.String("reason", "conflicting acceptance"),
			zap.Stringer("vtxID", vtxID),
		)
		if !txv.Status().Decided() {
			if err := ta.cg.Remove(vtxID); err != nil {
				return fmt.Errorf("failed to remove transaction vertex %s from snowstorm before rejecting vertex itself", vtxID)
			}
			ta.virtuousVoting.Remove(vtxID)
		}
		if err := vtx.Reject(); err != nil {
			return err
		}
		delete(ta.nodes, vtxID)
		ta.Latency.Rejected(vtxID, ta.pollNumber)
	}
	return nil
}

// Update the frontier sets
func (ta *Topological) updateFrontiers() error {
	vts := ta.frontier

	ta.preferred.Clear()
	ta.virtuous.Clear()
	ta.virtuousVoting.Clear()

	ta.orphans.Clear()
	ta.frontier = make(map[ids.ID]Vertex, minMapSize)
	ta.preferenceCache = make(map[ids.ID]bool, minMapSize)
	ta.virtuousCache = make(map[ids.ID]bool, minMapSize)

	ta.virtuousVoting.Union(ta.cg.VirtuousVoting())
	ta.orphans.Union(ta.cg.Virtuous()) // Initially, nothing is preferred

	for _, vtx := range vts {
		// Update all the vertices that were in my previous frontier
		if err := ta.update(vtx); err != nil {
			return err
		}
	}
	return nil
}

```

avalanchego/snow/consensus/avalanche/topological_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package avalanche

import (
	"testing"
)

func TestTopological(t *testing.T) { runConsensusTests(t, TopologicalFactory{}) }

```

avalanchego/snow/consensus/avalanche/transaction_vertex.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package avalanche

import (
	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/choices"
	"github.com/ava-labs/avalanchego/snow/consensus/snowstorm"
)

var _ snowstorm.Tx = &transactionVertex{}

// newTransactionVertex returns a new transactionVertex initialized with a
// processing status.
func newTransactionVertex(vtx Vertex, nodes map[ids.ID]*transactionVertex) *transactionVertex {
	return &transactionVertex{
		vtx:    vtx,
		nodes:  nodes,
		status: choices.Processing,
	}
}

type transactionVertex struct {
	// vtx is the vertex that this transaction is attempting to confirm.
	vtx Vertex

	// nodes is used to look up other transaction vertices that are currently
	// processing. This is used to get parent vertices of this transaction.
	nodes map[ids.ID]*transactionVertex

	// status reports the status of this transaction vertex in snowstorm which
	// is then used by avalanche to determine the accaptability of the vertex.
	status choices.Status
}

func (tv *transactionVertex) Bytes() []byte {
	// Snowstorm uses the bytes of the transaction to broadcast through the
	// decision dispatcher. Because this is an internal transaction type, we
	// don't want to have this transaction broadcast. So, we return nil here.
	return nil
}

func (tv *transactionVertex) ID() ids.ID {
	return tv.vtx.ID()
}

func (tv *transactionVertex) Accept() error {
	tv.status = choices.Accepted
	return nil
}

func (tv *transactionVertex) Reject() error {
	tv.status = choices.Rejected
	return nil
}

func (tv *transactionVertex) Status() choices.Status { return tv.status }

// Verify isn't called in the consensus code. So this implementation doesn't
// really matter. However it's used to implement the tx interface.
func (tv *transactionVertex) Verify() error { return nil }

// Dependencies returns the currently processing transaction vertices of this
// vertex's parents.
func (tv *transactionVertex) Dependencies() ([]snowstorm.Tx, error) {
	parents, err := tv.vtx.Parents()
	if err != nil {
		return nil, err
	}
	txs := make([]snowstorm.Tx, 0, len(parents))
	for _, parent := range parents {
		if parentTx, ok := tv.nodes[parent.ID()]; ok {
			txs = append(txs, parentTx)
		}
	}
	return txs, nil
}

// InputIDs must return a non-empty slice to avoid having the snowstorm engine
// vaciously accept it. A slice is returned containing just the vertexID in
// order to produce no conflicts based on the consumed input.
func (tv *transactionVertex) InputIDs() []ids.ID { return []ids.ID{tv.vtx.ID()} }

func (tv *transactionVertex) HasWhitelist() bool { return tv.vtx.HasWhitelist() }

func (tv *transactionVertex) Whitelist() (ids.Set, error) { return tv.vtx.Whitelist() }

```

avalanchego/snow/consensus/avalanche/vertex.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package avalanche

import (
	"github.com/ava-labs/avalanchego/snow/choices"
	"github.com/ava-labs/avalanchego/snow/consensus/snowstorm"
	"github.com/ava-labs/avalanchego/vms/components/verify"
)

// Vertex is a collection of multiple transactions tied to other vertices
type Vertex interface {
	choices.Decidable
	// Vertex verification should be performed before issuance.
	verify.Verifiable
	snowstorm.Whitelister

	// Returns the vertices this vertex depends on
	Parents() ([]Vertex, error)

	// Returns the height of this vertex. A vertex's height is defined by one
	// greater than the maximum height of the parents.
	Height() (uint64, error)

	// Returns a series of state transitions to be performed on acceptance
	Txs() ([]snowstorm.Tx, error)

	// Returns the binary representation of this vertex
	Bytes() []byte
}

```

avalanchego/snow/consensus/metrics/height.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package metrics

import (
	"github.com/prometheus/client_golang/prometheus"
)

var _ Height = &height{}

// Height reports the last accepted height
type Height interface {
	Accepted(height uint64)
}

type height struct {
	// lastAcceptedHeight keeps track of the last accepted height
	lastAcceptedHeight prometheus.Gauge
}

func NewHeight(namespace string, reg prometheus.Registerer) (Height, error) {
	h := &height{
		lastAcceptedHeight: prometheus.NewGauge(prometheus.GaugeOpts{
			Namespace: namespace,
			Name:      "last_accepted_height",
			Help:      "Last height accepted",
		}),
	}
	return h, reg.Register(h.lastAcceptedHeight)
}

func (h *height) Accepted(height uint64) {
	h.lastAcceptedHeight.Set(float64(height))
}

```

avalanchego/snow/consensus/metrics/latency.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package metrics

import (
	"fmt"
	"time"

	"github.com/prometheus/client_golang/prometheus"

	"go.uber.org/zap"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/choices"
	"github.com/ava-labs/avalanchego/utils/linkedhashmap"
	"github.com/ava-labs/avalanchego/utils/logging"
	"github.com/ava-labs/avalanchego/utils/metric"
	"github.com/ava-labs/avalanchego/utils/wrappers"
)

var _ Latency = &latency{}

type Latency interface {
	// Issued marks the item as having been issued.
	Issued(id ids.ID, pollNumber uint64)

	// Accepted marks the item as having been accepted.
	Accepted(id ids.ID, pollNumber uint64)

	// Rejected marks the item as having been rejected.
	Rejected(id ids.ID, pollNumber uint64)

	// MeasureAndGetOldestDuration returns the amount of time the oldest item
	// has been processing.
	MeasureAndGetOldestDuration() time.Duration

	// NumProcessing returns the number of currently processing items.
	NumProcessing() int
}

type opStart struct {
	time       time.Time
	pollNumber uint64
}

// Latency reports commonly used consensus latency metrics.
type latency struct {
	// ProcessingEntries keeps track of the [opStart] that each item was issued
	// into the consensus instance. This is used to calculate the amount of time
	// to accept or reject the item.
	processingEntries linkedhashmap.LinkedHashmap[ids.ID, opStart]

	// log reports anomalous events.
	log logging.Logger

	// numProcessing keeps track of the number of items processing
	numProcessing prometheus.Gauge

	// pollsAccepted tracks the number of polls that an item was in processing
	// for before being accepted
	pollsAccepted metric.Averager

	// pollsRejected tracks the number of polls that an item was in processing
	// for before being rejected
	pollsRejected metric.Averager

	// latAccepted tracks the number of nanoseconds that an item was processing
	// before being accepted
	latAccepted metric.Averager

	// rejected tracks the number of nanoseconds that an item was processing
	// before being rejected
	latRejected metric.Averager
}

// Initialize the metrics with the provided names.
func NewLatency(metricName, descriptionName string, log logging.Logger, namespace string, reg prometheus.Registerer) (Latency, error) {
	errs := wrappers.Errs{}
	l := &latency{
		processingEntries: linkedhashmap.New[ids.ID, opStart](),
		log:               log,
		numProcessing: prometheus.NewGauge(prometheus.GaugeOpts{
			Namespace: namespace,
			Name:      fmt.Sprintf("%s_processing", metricName),
			Help:      fmt.Sprintf("Number of currently processing %s", metricName),
		}),
		pollsAccepted: metric.NewAveragerWithErrs(
			namespace,
			fmt.Sprintf("%s_polls_accepted", metricName),
			fmt.Sprintf("number of polls from issuance of a %s to its acceptance", descriptionName),
			reg,
			&errs,
		),
		pollsRejected: metric.NewAveragerWithErrs(
			namespace,
			fmt.Sprintf("%s_polls_rejected", metricName),
			fmt.Sprintf("number of polls from issuance of a %s to its rejection", descriptionName),
			reg,
			&errs,
		),
		latAccepted: metric.NewAveragerWithErrs(
			namespace,
			fmt.Sprintf("%s_accepted", metricName),
			fmt.Sprintf("time (in ns) from issuance of a %s to its acceptance", descriptionName),
			reg,
			&errs,
		),
		latRejected: metric.NewAveragerWithErrs(
			namespace,
			fmt.Sprintf("%s_rejected", metricName),
			fmt.Sprintf("time (in ns) from issuance of a %s to its rejection", descriptionName),
			reg,
			&errs,
		),
	}
	errs.Add(reg.Register(l.numProcessing))
	return l, errs.Err
}

func (l *latency) Issued(id ids.ID, pollNumber uint64) {
	l.processingEntries.Put(id, opStart{
		time:       time.Now(),
		pollNumber: pollNumber,
	})
	l.numProcessing.Inc()
}

func (l *latency) Accepted(id ids.ID, pollNumber uint64) {
	start, ok := l.processingEntries.Get(id)
	if !ok {
		l.log.Debug("unable to measure tx latency",
			zap.Stringer("status", choices.Accepted),
			zap.Stringer("txID", id),
		)
		return
	}
	l.processingEntries.Delete(id)

	l.pollsAccepted.Observe(float64(pollNumber - start.pollNumber))

	duration := time.Since(start.time)
	l.latAccepted.Observe(float64(duration))
	l.numProcessing.Dec()
}

func (l *latency) Rejected(id ids.ID, pollNumber uint64) {
	start, ok := l.processingEntries.Get(id)
	if !ok {
		l.log.Debug("unable to measure tx latency",
			zap.Stringer("status", choices.Rejected),
			zap.Stringer("txID", id),
		)
		return
	}
	l.processingEntries.Delete(id)

	l.pollsRejected.Observe(float64(pollNumber - start.pollNumber))

	duration := time.Since(start.time)
	l.latRejected.Observe(float64(duration))
	l.numProcessing.Dec()
}

func (l *latency) MeasureAndGetOldestDuration() time.Duration {
	_, oldestOp, exists := l.processingEntries.Oldest()
	if !exists {
		return 0
	}
	return time.Since(oldestOp.time)
}

func (l *latency) NumProcessing() int {
	return l.processingEntries.Len()
}

```

avalanchego/snow/consensus/metrics/polls.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package metrics

import (
	"github.com/prometheus/client_golang/prometheus"

	"github.com/ava-labs/avalanchego/utils/wrappers"
)

var _ Polls = &polls{}

// Polls reports commonly used consensus poll metrics.
type Polls interface {
	Successful()
	Failed()
}

type polls struct {
	// numFailedPolls keeps track of the number of polls that failed
	numFailedPolls prometheus.Counter

	// numSuccessfulPolls keeps track of the number of polls that succeeded
	numSuccessfulPolls prometheus.Counter
}

func NewPolls(namespace string, reg prometheus.Registerer) (Polls, error) {
	p := &polls{
		numSuccessfulPolls: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: namespace,
			Name:      "polls_successful",
			Help:      "Number of successful polls",
		}),
		numFailedPolls: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: namespace,
			Name:      "polls_failed",
			Help:      "Number of failed polls",
		}),
	}
	errs := wrappers.Errs{}
	errs.Add(
		reg.Register(p.numFailedPolls),
		reg.Register(p.numSuccessfulPolls),
	)
	return p, errs.Err
}

func (p *polls) Failed() {
	p.numFailedPolls.Inc()
}

func (p *polls) Successful() {
	p.numSuccessfulPolls.Inc()
}

```

avalanchego/snow/consensus/snowball/binary_slush.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowball

import (
	"fmt"
)

var _ BinarySlush = &binarySlush{}

// binarySlush is the implementation of a binary slush instance
type binarySlush struct {
	// preference is the choice that last had a successful poll. Unless there
	// hasn't been a successful poll, in which case it is the initially provided
	// choice.
	preference int
}

func (sl *binarySlush) Initialize(choice int) { sl.preference = choice }

func (sl *binarySlush) Preference() int { return sl.preference }

func (sl *binarySlush) RecordSuccessfulPoll(choice int) { sl.preference = choice }

func (sl *binarySlush) String() string { return fmt.Sprintf("SL(Preference = %d)", sl.preference) }

```

avalanchego/snow/consensus/snowball/binary_snowball.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowball

import (
	"fmt"
)

var _ BinarySnowball = &binarySnowball{}

// binarySnowball is the implementation of a binary snowball instance
type binarySnowball struct {
	// wrap the binary snowflake logic
	binarySnowflake

	// preference is the choice with the largest number of successful polls.
	// Ties are broken by switching choice lazily
	preference int

	// numSuccessfulPolls tracks the total number of successful network polls of
	// the 0 and 1 choices
	numSuccessfulPolls [2]int
}

func (sb *binarySnowball) Initialize(beta, choice int) {
	sb.binarySnowflake.Initialize(beta, choice)
	sb.preference = choice
}

func (sb *binarySnowball) Preference() int {
	// It is possible, with low probability, that the snowflake preference is
	// not equal to the snowball preference when snowflake finalizes. However,
	// this case is handled for completion. Therefore, if snowflake is
	// finalized, then our finalized snowflake choice should be preferred.
	if sb.Finalized() {
		return sb.binarySnowflake.Preference()
	}
	return sb.preference
}

func (sb *binarySnowball) RecordSuccessfulPoll(choice int) {
	sb.numSuccessfulPolls[choice]++
	if sb.numSuccessfulPolls[choice] > sb.numSuccessfulPolls[1-choice] {
		sb.preference = choice
	}
	sb.binarySnowflake.RecordSuccessfulPoll(choice)
}

func (sb *binarySnowball) String() string {
	return fmt.Sprintf(
		"SB(Preference = %d, NumSuccessfulPolls[0] = %d, NumSuccessfulPolls[1] = %d, %s)",
		sb.preference,
		sb.numSuccessfulPolls[0],
		sb.numSuccessfulPolls[1],
		&sb.binarySnowflake)
}

```

avalanchego/snow/consensus/snowball/binary_snowball_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowball

import (
	"testing"
)

func TestBinarySnowball(t *testing.T) {
	Red := 0
	Blue := 1

	beta := 2

	sb := binarySnowball{}
	sb.Initialize(beta, Red)

	if pref := sb.Preference(); pref != Red {
		t.Fatalf("Wrong preference. Expected %d got %d", Red, pref)
	} else if sb.Finalized() {
		t.Fatalf("Finalized too early")
	}

	sb.RecordSuccessfulPoll(Blue)

	if pref := sb.Preference(); pref != Blue {
		t.Fatalf("Wrong preference. Expected %d got %d", Blue, pref)
	} else if sb.Finalized() {
		t.Fatalf("Finalized too early")
	}

	sb.RecordSuccessfulPoll(Red)

	if pref := sb.Preference(); pref != Blue {
		t.Fatalf("Wrong preference. Expected %d got %d", Blue, pref)
	} else if sb.Finalized() {
		t.Fatalf("Finalized too early")
	}

	sb.RecordSuccessfulPoll(Blue)

	if pref := sb.Preference(); pref != Blue {
		t.Fatalf("Wrong preference. Expected %d got %d", Blue, pref)
	} else if sb.Finalized() {
		t.Fatalf("Finalized too early")
	}

	sb.RecordSuccessfulPoll(Blue)

	if pref := sb.Preference(); pref != Blue {
		t.Fatalf("Wrong preference. Expected %d got %d", Blue, pref)
	} else if !sb.Finalized() {
		t.Fatalf("Didn't finalized correctly")
	}
}

func TestBinarySnowballRecordUnsuccessfulPoll(t *testing.T) {
	Red := 0
	Blue := 1

	beta := 2

	sb := binarySnowball{}
	sb.Initialize(beta, Red)

	if pref := sb.Preference(); pref != Red {
		t.Fatalf("Wrong preference. Expected %d got %d", Red, pref)
	} else if sb.Finalized() {
		t.Fatalf("Finalized too early")
	}

	sb.RecordSuccessfulPoll(Blue)

	if pref := sb.Preference(); pref != Blue {
		t.Fatalf("Wrong preference. Expected %d got %d", Blue, pref)
	} else if sb.Finalized() {
		t.Fatalf("Finalized too early")
	}

	sb.RecordUnsuccessfulPoll()

	sb.RecordSuccessfulPoll(Blue)

	if pref := sb.Preference(); pref != Blue {
		t.Fatalf("Wrong preference. Expected %d got %d", Blue, pref)
	} else if sb.Finalized() {
		t.Fatalf("Finalized too early")
	}

	sb.RecordSuccessfulPoll(Blue)

	if pref := sb.Preference(); pref != Blue {
		t.Fatalf("Wrong preference. Expected %d got %d", Blue, pref)
	} else if !sb.Finalized() {
		t.Fatalf("Finalized too late")
	}

	expected := "SB(Preference = 1, NumSuccessfulPolls[0] = 0, NumSuccessfulPolls[1] = 3, SF(Confidence = 2, Finalized = true, SL(Preference = 1)))"
	if str := sb.String(); str != expected {
		t.Fatalf("Wrong state. Expected:\n%s\nGot:\n%s", expected, str)
	}
}

func TestBinarySnowballAcceptWeirdColor(t *testing.T) {
	Blue := 0
	Red := 1

	beta := 2

	sb := binarySnowball{}
	sb.Initialize(beta, Red)

	if pref := sb.Preference(); pref != Red {
		t.Fatalf("Wrong preference. Expected %d got %d", Red, pref)
	} else if sb.Finalized() {
		t.Fatalf("Finalized too early")
	}

	sb.RecordSuccessfulPoll(Red)
	sb.RecordUnsuccessfulPoll()

	if pref := sb.Preference(); pref != Red {
		t.Fatalf("Wrong preference. Expected %d got %d", Red, pref)
	} else if sb.Finalized() {
		t.Fatalf("Finalized too early")
	}

	sb.RecordSuccessfulPoll(Red)
	sb.RecordUnsuccessfulPoll()

	if pref := sb.Preference(); pref != Red {
		t.Fatalf("Wrong preference. Expected %d got %d", Red, pref)
	} else if sb.Finalized() {
		t.Fatalf("Finalized too early")
	}

	sb.RecordSuccessfulPoll(Blue)

	if pref := sb.Preference(); pref != Red {
		t.Fatalf("Wrong preference. Expected %d got %d", Red, pref)
	} else if sb.Finalized() {
		t.Fatalf("Finalized too early")
	}

	sb.RecordSuccessfulPoll(Blue)

	if pref := sb.Preference(); pref != Blue {
		t.Fatalf("Wrong preference. Expected %d got %d", Blue, pref)
	} else if !sb.Finalized() {
		t.Fatalf("Finalized too late")
	}

	expected := "SB(Preference = 1, NumSuccessfulPolls[0] = 2, NumSuccessfulPolls[1] = 2, SF(Confidence = 2, Finalized = true, SL(Preference = 0)))"
	if str := sb.String(); str != expected {
		t.Fatalf("Wrong state. Expected:\n%s\nGot:\n%s", expected, str)
	}
}

func TestBinarySnowballLockColor(t *testing.T) {
	Red := 0
	Blue := 1

	beta := 1

	sb := binarySnowball{}
	sb.Initialize(beta, Red)

	sb.RecordSuccessfulPoll(Red)

	if pref := sb.Preference(); pref != Red {
		t.Fatalf("Wrong preference. Expected %d got %d", Red, pref)
	} else if !sb.Finalized() {
		t.Fatalf("Finalized too late")
	}

	sb.RecordSuccessfulPoll(Blue)

	if pref := sb.Preference(); pref != Red {
		t.Fatalf("Wrong preference. Expected %d got %d", Red, pref)
	} else if !sb.Finalized() {
		t.Fatalf("Finalized too late")
	}

	sb.RecordSuccessfulPoll(Blue)

	if pref := sb.Preference(); pref != Red {
		t.Fatalf("Wrong preference. Expected %d got %d", Red, pref)
	} else if !sb.Finalized() {
		t.Fatalf("Finalized too late")
	}

	expected := "SB(Preference = 1, NumSuccessfulPolls[0] = 1, NumSuccessfulPolls[1] = 2, SF(Confidence = 1, Finalized = true, SL(Preference = 0)))"
	if str := sb.String(); str != expected {
		t.Fatalf("Wrong state. Expected:\n%s\nGot:\n%s", expected, str)
	}
}

```

avalanchego/snow/consensus/snowball/binary_snowflake.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowball

import (
	"fmt"
)

var _ BinarySnowflake = &binarySnowflake{}

// binarySnowflake is the implementation of a binary snowflake instance
type binarySnowflake struct {
	// wrap the binary slush logic
	binarySlush

	// confidence tracks the number of successful polls in a row that have
	// returned the preference
	confidence int

	// beta is the number of consecutive successful queries required for
	// finalization.
	beta int

	// finalized prevents the state from changing after the required number of
	// consecutive polls has been reached
	finalized bool
}

func (sf *binarySnowflake) Initialize(beta, choice int) {
	sf.binarySlush.Initialize(choice)
	sf.beta = beta
}

func (sf *binarySnowflake) RecordSuccessfulPoll(choice int) {
	if sf.finalized {
		return // This instace is already decided.
	}

	if preference := sf.Preference(); preference == choice {
		sf.confidence++
	} else {
		// confidence is set to 1 because there has already been 1 successful
		// poll, namely this poll.
		sf.confidence = 1
	}

	sf.finalized = sf.confidence >= sf.beta
	sf.binarySlush.RecordSuccessfulPoll(choice)
}

func (sf *binarySnowflake) RecordUnsuccessfulPoll() { sf.confidence = 0 }

func (sf *binarySnowflake) Finalized() bool { return sf.finalized }

func (sf *binarySnowflake) String() string {
	return fmt.Sprintf("SF(Confidence = %d, Finalized = %v, %s)",
		sf.confidence,
		sf.finalized,
		&sf.binarySlush)
}

```

avalanchego/snow/consensus/snowball/binary_snowflake_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowball

import (
	"testing"
)

func TestBinarySnowflake(t *testing.T) {
	Blue := 0
	Red := 1

	beta := 2

	sf := binarySnowflake{}
	sf.Initialize(beta, Red)

	if pref := sf.Preference(); pref != Red {
		t.Fatalf("Wrong preference. Expected %d got %d", Red, pref)
	} else if sf.Finalized() {
		t.Fatalf("Finalized too early")
	}

	sf.RecordSuccessfulPoll(Blue)

	if pref := sf.Preference(); pref != Blue {
		t.Fatalf("Wrong preference. Expected %d got %d", Blue, pref)
	} else if sf.Finalized() {
		t.Fatalf("Finalized too early")
	}

	sf.RecordSuccessfulPoll(Red)

	if pref := sf.Preference(); pref != Red {
		t.Fatalf("Wrong preference. Expected %d got %d", Red, pref)
	} else if sf.Finalized() {
		t.Fatalf("Finalized too early")
	}

	sf.RecordSuccessfulPoll(Blue)

	if pref := sf.Preference(); pref != Blue {
		t.Fatalf("Wrong preference. Expected %d got %d", Blue, pref)
	} else if sf.Finalized() {
		t.Fatalf("Finalized too early")
	}

	sf.RecordSuccessfulPoll(Blue)

	if pref := sf.Preference(); pref != Blue {
		t.Fatalf("Wrong preference. Expected %d got %d", Blue, pref)
	} else if !sf.Finalized() {
		t.Fatalf("Didn't finalized correctly")
	}
}

```

avalanchego/snow/consensus/snowball/consensus.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowball

import (
	"fmt"

	"github.com/ava-labs/avalanchego/ids"
)

// Consensus represents a general snow instance that can be used directly to
// process the results of network queries.
type Consensus interface {
	fmt.Stringer

	// Takes in alpha, beta1, beta2, and the initial choice
	Initialize(params Parameters, initialPreference ids.ID)

	// Returns the parameters that describe this snowball instance
	Parameters() Parameters

	// Adds a new choice to vote on
	Add(newChoice ids.ID)

	// Returns the currently preferred choice to be finalized
	Preference() ids.ID

	// RecordPoll records the results of a network poll. Assumes all choices
	// have been previously added.
	//
	// If the consensus instance was not previously finalized, this function
	// will return true if the poll was successful and false if the poll was
	// unsuccessful.
	//
	// If the consensus instance was previously finalized, the function may
	// return true or false.
	RecordPoll(votes ids.Bag) bool

	// RecordUnsuccessfulPoll resets the snowflake counters of this consensus
	// instance
	RecordUnsuccessfulPoll()

	// Return whether a choice has been finalized
	Finalized() bool
}

// NnarySnowball augments NnarySnowflake with a counter that tracks the total
// number of positive responses from a network sample.
type NnarySnowball interface{ NnarySnowflake }

// NnarySnowflake is a snowflake instance deciding between an unbounded number
// of values. After performing a network sample of k nodes, if you have alpha
// votes for one of the choices, you should vote for that choice. Otherwise, you
// should reset.
type NnarySnowflake interface {
	fmt.Stringer

	// Takes in beta1, beta2, and the initial choice
	Initialize(betaVirtuous, betaRogue int, initialPreference ids.ID)

	// Adds a new possible choice
	Add(newChoice ids.ID)

	// Returns the currently preferred choice to be finalized
	Preference() ids.ID

	// RecordSuccessfulPoll records a successful poll towards finalizing the
	// specified choice. Assumes the choice was previously added.
	RecordSuccessfulPoll(choice ids.ID)

	// RecordUnsuccessfulPoll resets the snowflake counter of this instance
	RecordUnsuccessfulPoll()

	// Return whether a choice has been finalized
	Finalized() bool
}

// NnarySlush is a slush instance deciding between an unbounded number of
// values. After performing a network sample of k nodes, if you have alpha
// votes for one of the choices, you should vote for that choice.
type NnarySlush interface {
	fmt.Stringer

	// Takes in the initial choice
	Initialize(initialPreference ids.ID)

	// Returns the currently preferred choice to be finalized
	Preference() ids.ID

	// RecordSuccessfulPoll records a successful poll towards finalizing the
	// specified choice. Assumes the choice was previously added.
	RecordSuccessfulPoll(choice ids.ID)
}

// BinarySnowball augments BinarySnowflake with a counter that tracks the total
// number of positive responses from a network sample.
type BinarySnowball interface{ BinarySnowflake }

// BinarySnowflake is a snowball instance deciding between two values
// After performing a network sample of k nodes, if you have alpha votes for
// one of the choices, you should vote for that choice. Otherwise, you should
// reset.
type BinarySnowflake interface {
	fmt.Stringer

	// Takes in the beta value, and the initial choice
	Initialize(beta, initialPreference int)

	// Returns the currently preferred choice to be finalized
	Preference() int

	// RecordSuccessfulPoll records a successful poll towards finalizing the
	// specified choice
	RecordSuccessfulPoll(choice int)

	// RecordUnsuccessfulPoll resets the snowflake counter of this instance
	RecordUnsuccessfulPoll()

	// Return whether a choice has been finalized
	Finalized() bool
}

// BinarySlush is a slush instance deciding between two values. After performing
// a network sample of k nodes, if you have alpha votes for one of the choices,
// you should vote for that choice.
type BinarySlush interface {
	fmt.Stringer

	// Takes in the initial choice
	Initialize(initialPreference int)

	// Returns the currently preferred choice to be finalized
	Preference() int

	// RecordSuccessfulPoll records a successful poll towards finalizing the
	// specified choice
	RecordSuccessfulPoll(choice int)
}

// UnarySnowball is a snowball instance deciding on one value. After performing
// a network sample of k nodes, if you have alpha votes for the choice, you
// should vote. Otherwise, you should reset.
type UnarySnowball interface {
	fmt.Stringer

	// Takes in the beta value
	Initialize(beta int)

	// RecordSuccessfulPoll records a successful poll towards finalizing
	RecordSuccessfulPoll()

	// RecordUnsuccessfulPoll resets the snowflake counter of this instance
	RecordUnsuccessfulPoll()

	// Return whether a choice has been finalized
	Finalized() bool

	// Returns a new binary snowball instance with the agreement parameters
	// transferred. Takes in the new beta value and the original choice
	Extend(beta, originalPreference int) BinarySnowball

	// Returns a new unary snowball instance with the same state
	Clone() UnarySnowball
}

// UnarySnowflake is a snowflake instance deciding on one value. After
// performing a network sample of k nodes, if you have alpha votes for the
// choice, you should vote. Otherwise, you should reset.
type UnarySnowflake interface {
	fmt.Stringer

	// Takes in the beta value
	Initialize(beta int)

	// RecordSuccessfulPoll records a successful poll towards finalizing
	RecordSuccessfulPoll()

	// RecordUnsuccessfulPoll resets the snowflake counter of this instance
	RecordUnsuccessfulPoll()

	// Return whether a choice has been finalized
	Finalized() bool

	// Returns a new binary snowball instance with the agreement parameters
	// transferred. Takes in the new beta value and the original choice
	Extend(beta, originalPreference int) BinarySnowflake

	// Returns a new unary snowflake instance with the same state
	Clone() UnarySnowflake
}

```

avalanchego/snow/consensus/snowball/consensus_performance_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowball

import (
	"testing"

	"github.com/ava-labs/avalanchego/utils/sampler"
)

func TestSnowballOptimized(t *testing.T) {
	numColors := 10
	numNodes := 100
	params := Parameters{
		K: 20, Alpha: 15, BetaVirtuous: 20, BetaRogue: 30,
	}
	seed := int64(0)

	nBitwise := Network{}
	nBitwise.Initialize(params, numColors)

	nNaive := nBitwise

	sampler.Seed(seed)
	for i := 0; i < numNodes; i++ {
		nBitwise.AddNode(&Tree{})
	}

	sampler.Seed(seed)
	for i := 0; i < numNodes; i++ {
		nNaive.AddNode(&Flat{})
	}

	numRounds := 0
	for !nBitwise.Finalized() && !nBitwise.Disagreement() && !nNaive.Finalized() && !nNaive.Disagreement() {
		sampler.Seed(int64(numRounds) + seed)
		nBitwise.Round()

		sampler.Seed(int64(numRounds) + seed)
		nNaive.Round()
		numRounds++
	}

	if nBitwise.Disagreement() || nNaive.Disagreement() {
		t.Fatalf("Network agreed on inconsistent values")
	}

	// Although this can theoretically fail with a correct implementation, it
	// shouldn't in practice
	if !nBitwise.Finalized() {
		t.Fatalf("Network agreed on values faster with naive implementation")
	}
	if !nBitwise.Agreement() {
		t.Fatalf("Network agreed on inconsistent values")
	}
}

```

avalanchego/snow/consensus/snowball/consensus_reversibility_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowball

import (
	"testing"

	"github.com/ava-labs/avalanchego/utils/sampler"
)

func TestSnowballGovernance(t *testing.T) {
	numColors := 2
	numNodes := 100
	numByzantine := 10
	numRed := 55
	params := Parameters{
		K: 20, Alpha: 15, BetaVirtuous: 20, BetaRogue: 30,
	}
	seed := int64(0)

	nBitwise := Network{}
	nBitwise.Initialize(params, numColors)

	sampler.Seed(seed)
	for i := 0; i < numRed; i++ {
		nBitwise.AddNodeSpecificColor(&Tree{}, 0, []int{1})
	}

	for _, node := range nBitwise.nodes {
		if node.Preference() != nBitwise.colors[0] {
			t.Fatalf("Wrong preferences")
		}
	}

	for i := 0; i < numNodes-numByzantine-numRed; i++ {
		nBitwise.AddNodeSpecificColor(&Tree{}, 1, []int{0})
	}

	for i := 0; i < numByzantine; i++ {
		nBitwise.AddNodeSpecificColor(&Byzantine{}, 1, []int{0})
	}

	for !nBitwise.Finalized() {
		nBitwise.Round()
	}

	for _, node := range nBitwise.nodes {
		if _, ok := node.(*Byzantine); ok {
			continue
		}
		if node.Preference() != nBitwise.colors[0] {
			t.Fatalf("Wrong preferences")
		}
	}
}

```

avalanchego/snow/consensus/snowball/consensus_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowball

import (
	"testing"

	"github.com/ava-labs/avalanchego/ids"
)

var _ Consensus = &Byzantine{}

// Byzantine is a naive implementation of a multi-choice snowball instance
type Byzantine struct {
	// params contains all the configurations of a snowball instance
	params Parameters

	// Hardcode the preference
	preference ids.ID
}

func (b *Byzantine) Initialize(params Parameters, choice ids.ID) {
	b.params = params
	b.preference = choice
}

func (b *Byzantine) Parameters() Parameters        { return b.params }
func (b *Byzantine) Add(choice ids.ID)             {}
func (b *Byzantine) Preference() ids.ID            { return b.preference }
func (b *Byzantine) RecordPoll(votes ids.Bag) bool { return false }
func (b *Byzantine) RecordUnsuccessfulPoll()       {}
func (b *Byzantine) Finalized() bool               { return true }
func (b *Byzantine) String() string                { return b.preference.String() }

var (
	Red   = ids.Empty.Prefix(0)
	Blue  = ids.Empty.Prefix(1)
	Green = ids.Empty.Prefix(2)
)

func ParamsTest(t *testing.T, factory Factory) {
	sb := factory.New()

	params := Parameters{
		K: 2, Alpha: 2, BetaVirtuous: 1, BetaRogue: 2, ConcurrentRepolls: 1,
	}
	sb.Initialize(params, Red)

	p := sb.Parameters()
	switch {
	case p.K != params.K:
		t.Fatalf("Wrong K parameter")
	case p.Alpha != params.Alpha:
		t.Fatalf("Wrong Alpha parameter")
	case p.BetaVirtuous != params.BetaVirtuous:
		t.Fatalf("Wrong Beta1 parameter")
	case p.BetaRogue != params.BetaRogue:
		t.Fatalf("Wrong Beta2 parameter")
	case p.ConcurrentRepolls != params.ConcurrentRepolls:
		t.Fatalf("Wrong Repoll parameter")
	}
}

```

avalanchego/snow/consensus/snowball/factory.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowball

// Factory returns new instances of Consensus
type Factory interface {
	New() Consensus
}

```

avalanchego/snow/consensus/snowball/flat.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowball

import (
	"github.com/ava-labs/avalanchego/ids"
)

var (
	_ Factory   = &FlatFactory{}
	_ Consensus = &Flat{}
)

// FlatFactory implements Factory by returning a flat struct
type FlatFactory struct{}

func (FlatFactory) New() Consensus { return &Flat{} }

// Flat is a naive implementation of a multi-choice snowball instance
type Flat struct {
	// wraps the n-nary snowball logic
	nnarySnowball

	// params contains all the configurations of a snowball instance
	params Parameters
}

func (f *Flat) Initialize(params Parameters, choice ids.ID) {
	f.nnarySnowball.Initialize(params.BetaVirtuous, params.BetaRogue, choice)
	f.params = params
}

func (f *Flat) Parameters() Parameters { return f.params }

func (f *Flat) RecordPoll(votes ids.Bag) bool {
	if pollMode, numVotes := votes.Mode(); numVotes >= f.params.Alpha {
		f.RecordSuccessfulPoll(pollMode)
		return true
	}

	f.RecordUnsuccessfulPoll()
	return false
}

```

avalanchego/snow/consensus/snowball/flat_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowball

import (
	"testing"

	"github.com/stretchr/testify/require"

	"github.com/ava-labs/avalanchego/ids"
)

func TestFlatParams(t *testing.T) { ParamsTest(t, FlatFactory{}) }

func TestFlat(t *testing.T) {
	require := require.New(t)

	params := Parameters{
		K: 2, Alpha: 2, BetaVirtuous: 1, BetaRogue: 2,
	}
	f := Flat{}
	f.Initialize(params, Red)
	f.Add(Green)
	f.Add(Blue)

	require.Equal(Red, f.Preference())
	require.False(f.Finalized())

	twoBlue := ids.Bag{}
	twoBlue.Add(Blue, Blue)
	require.True(f.RecordPoll(twoBlue))
	require.Equal(Blue, f.Preference())
	require.False(f.Finalized())

	oneRedOneBlue := ids.Bag{}
	oneRedOneBlue.Add(Red, Blue)
	require.False(f.RecordPoll(oneRedOneBlue))
	require.Equal(Blue, f.Preference())
	require.False(f.Finalized())

	require.True(f.RecordPoll(twoBlue))
	require.Equal(Blue, f.Preference())
	require.False(f.Finalized())

	require.True(f.RecordPoll(twoBlue))
	require.Equal(Blue, f.Preference())
	require.True(f.Finalized())

	expected := "SB(Preference = TtF4d2QWbk5vzQGTEPrN48x6vwgAoAmKQ9cbp79inpQmcRKES, NumSuccessfulPolls = 3, SF(Confidence = 2, Finalized = true, SL(Preference = TtF4d2QWbk5vzQGTEPrN48x6vwgAoAmKQ9cbp79inpQmcRKES)))"
	require.Equal(expected, f.String())
}

```

avalanchego/snow/consensus/snowball/network_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowball

import (
	"math/rand"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/utils/sampler"
)

type Network struct {
	params         Parameters
	colors         []ids.ID
	nodes, running []Consensus
}

// Initialize sets the parameters for the network and adds [numColors] different
// possible colors to the network configuration.
func (n *Network) Initialize(params Parameters, numColors int) {
	n.params = params
	for i := 0; i < numColors; i++ {
		n.colors = append(n.colors, ids.Empty.Prefix(uint64(i)))
	}
}

func (n *Network) AddNode(sb Consensus) {
	s := sampler.NewUniform()
	_ = s.Initialize(uint64(len(n.colors)))
	indices, _ := s.Sample(len(n.colors))
	sb.Initialize(n.params, n.colors[int(indices[0])])
	for _, index := range indices[1:] {
		sb.Add(n.colors[int(index)])
	}

	n.nodes = append(n.nodes, sb)
	if !sb.Finalized() {
		n.running = append(n.running, sb)
	}
}

// AddNodeSpecificColor adds [sb] to the network which will initially prefer
// [initialPreference] and additionally adds each of the specified [options] to
// consensus.
func (n *Network) AddNodeSpecificColor(sb Consensus, initialPreference int, options []int) {
	sb.Initialize(n.params, n.colors[initialPreference])
	for _, i := range options {
		sb.Add(n.colors[i])
	}

	n.nodes = append(n.nodes, sb)
	if !sb.Finalized() {
		n.running = append(n.running, sb)
	}
}

// Finalized returns true iff every node added to the network has finished
// running.
func (n *Network) Finalized() bool {
	return len(n.running) == 0
}

// Round simulates a round of consensus by randomly selecting a running node and
// performing an unbiased poll of the nodes in the network for that node.
func (n *Network) Round() {
	if len(n.running) > 0 {
		runningInd := rand.Intn(len(n.running)) // #nosec G404
		running := n.running[runningInd]

		s := sampler.NewUniform()
		_ = s.Initialize(uint64(len(n.nodes)))
		count := len(n.nodes)
		if count > n.params.K {
			count = n.params.K
		}
		indices, _ := s.Sample(count)
		sampledColors := ids.Bag{}
		for _, index := range indices {
			peer := n.nodes[int(index)]
			sampledColors.Add(peer.Preference())
		}

		running.RecordPoll(sampledColors)

		// If this node has been finalized, remove it from the poller
		if running.Finalized() {
			newSize := len(n.running) - 1
			n.running[runningInd] = n.running[newSize]
			n.running = n.running[:newSize]
		}
	}
}

// Disagreement returns true iff there are any two nodes in the network that
// have finalized two different preferences.
func (n *Network) Disagreement() bool {
	// Iterate [i] to the index of the first node that has finalized.
	i := 0
	for ; i < len(n.nodes) && !n.nodes[i].Finalized(); i++ {
	}
	// If none of the nodes have finalized, then there is no disagreement.
	if i >= len(n.nodes) {
		return false
	}

	// Return true if any other finalized node has finalized a different
	// preference.
	pref := n.nodes[i].Preference()
	for ; i < len(n.nodes); i++ {
		if node := n.nodes[i]; node.Finalized() && pref != node.Preference() {
			return true
		}
	}
	return false
}

// Agreement returns true iff every node in the network prefers the same value.
func (n *Network) Agreement() bool {
	if len(n.nodes) == 0 {
		return true
	}
	pref := n.nodes[0].Preference()
	for _, node := range n.nodes {
		if pref != node.Preference() {
			return false
		}
	}
	return true
}

```

avalanchego/snow/consensus/snowball/nnary_slush.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowball

import (
	"fmt"

	"github.com/ava-labs/avalanchego/ids"
)

var _ NnarySlush = &nnarySlush{}

// nnarySlush is the implementation of a slush instance with an unbounded number
// of choices
type nnarySlush struct {
	// preference is the choice that last had a successful poll. Unless there
	// hasn't been a successful poll, in which case it is the initially provided
	// choice.
	preference ids.ID
}

func (sl *nnarySlush) Initialize(choice ids.ID) { sl.preference = choice }

func (sl *nnarySlush) Preference() ids.ID { return sl.preference }

func (sl *nnarySlush) RecordSuccessfulPoll(choice ids.ID) { sl.preference = choice }

func (sl *nnarySlush) String() string { return fmt.Sprintf("SL(Preference = %s)", sl.preference) }

```

avalanchego/snow/consensus/snowball/nnary_snowball.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowball

import (
	"fmt"

	"github.com/ava-labs/avalanchego/ids"
)

var _ NnarySnowball = &nnarySnowball{}

// nnarySnowball is a naive implementation of a multi-color snowball instance
type nnarySnowball struct {
	// wrap the n-nary snowflake logic
	nnarySnowflake

	// preference is the choice with the largest number of successful polls.
	// Ties are broken by switching choice lazily
	preference ids.ID

	// maxSuccessfulPolls maximum number of successful polls this instance has
	// gotten for any choice
	maxSuccessfulPolls int

	// numSuccessfulPolls tracks the total number of successful network polls of
	// the choices
	numSuccessfulPolls map[ids.ID]int
}

func (sb *nnarySnowball) Initialize(betaVirtuous, betaRogue int, choice ids.ID) {
	sb.nnarySnowflake.Initialize(betaVirtuous, betaRogue, choice)
	sb.preference = choice
	sb.numSuccessfulPolls = make(map[ids.ID]int)
}

func (sb *nnarySnowball) Preference() ids.ID {
	// It is possible, with low probability, that the snowflake preference is
	// not equal to the snowball preference when snowflake finalizes. However,
	// this case is handled for completion. Therefore, if snowflake is
	// finalized, then our finalized snowflake choice should be preferred.
	if sb.Finalized() {
		return sb.nnarySnowflake.Preference()
	}
	return sb.preference
}

func (sb *nnarySnowball) RecordSuccessfulPoll(choice ids.ID) {
	numSuccessfulPolls := sb.numSuccessfulPolls[choice] + 1
	sb.numSuccessfulPolls[choice] = numSuccessfulPolls

	if numSuccessfulPolls > sb.maxSuccessfulPolls {
		sb.preference = choice
		sb.maxSuccessfulPolls = numSuccessfulPolls
	}

	sb.nnarySnowflake.RecordSuccessfulPoll(choice)
}

func (sb *nnarySnowball) String() string {
	return fmt.Sprintf("SB(Preference = %s, NumSuccessfulPolls = %d, %s)",
		sb.preference, sb.maxSuccessfulPolls, &sb.nnarySnowflake)
}

```

avalanchego/snow/consensus/snowball/nnary_snowball_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowball

import (
	"testing"
)

func TestNnarySnowball(t *testing.T) {
	betaVirtuous := 2
	betaRogue := 2

	sb := nnarySnowball{}
	sb.Initialize(betaVirtuous, betaRogue, Red)
	sb.Add(Blue)
	sb.Add(Green)

	if pref := sb.Preference(); Red != pref {
		t.Fatalf("Wrong preference. Expected %s got %s", Red, pref)
	} else if sb.Finalized() {
		t.Fatalf("Finalized too early")
	}

	sb.RecordSuccessfulPoll(Blue)

	if pref := sb.Preference(); Blue != pref {
		t.Fatalf("Wrong preference. Expected %s got %s", Blue, pref)
	} else if sb.Finalized() {
		t.Fatalf("Finalized too early")
	}

	sb.RecordSuccessfulPoll(Red)

	if pref := sb.Preference(); Blue != pref {
		t.Fatalf("Wrong preference. Expected %s got %s", Blue, pref)
	} else if sb.Finalized() {
		t.Fatalf("Finalized too early")
	}

	sb.RecordSuccessfulPoll(Blue)

	if pref := sb.Preference(); Blue != pref {
		t.Fatalf("Wrong preference. Expected %s got %s", Blue, pref)
	} else if sb.Finalized() {
		t.Fatalf("Finalized too early")
	}

	sb.RecordSuccessfulPoll(Blue)

	if pref := sb.Preference(); Blue != pref {
		t.Fatalf("Wrong preference. Expected %s got %s", Blue, pref)
	} else if !sb.Finalized() {
		t.Fatalf("Should be finalized")
	}
}

func TestVirtuousNnarySnowball(t *testing.T) {
	betaVirtuous := 1
	betaRogue := 2

	sb := nnarySnowball{}
	sb.Initialize(betaVirtuous, betaRogue, Red)

	if pref := sb.Preference(); Red != pref {
		t.Fatalf("Wrong preference. Expected %s got %s", Red, pref)
	} else if sb.Finalized() {
		t.Fatalf("Finalized too early")
	}

	sb.RecordSuccessfulPoll(Red)

	if pref := sb.Preference(); Red != pref {
		t.Fatalf("Wrong preference. Expected %s got %s", Red, pref)
	} else if !sb.Finalized() {
		t.Fatalf("Should be finalized")
	}
}

func TestNarySnowballRecordUnsuccessfulPoll(t *testing.T) {
	betaVirtuous := 2
	betaRogue := 2

	sb := nnarySnowball{}
	sb.Initialize(betaVirtuous, betaRogue, Red)
	sb.Add(Blue)

	if pref := sb.Preference(); Red != pref {
		t.Fatalf("Wrong preference. Expected %s got %s", Red, pref)
	} else if sb.Finalized() {
		t.Fatalf("Finalized too early")
	}

	sb.RecordSuccessfulPoll(Blue)

	if pref := sb.Preference(); Blue != pref {
		t.Fatalf("Wrong preference. Expected %s got %s", Blue, pref)
	} else if sb.Finalized() {
		t.Fatalf("Finalized too early")
	}

	sb.RecordUnsuccessfulPoll()

	sb.RecordSuccessfulPoll(Blue)

	if pref := sb.Preference(); Blue != pref {
		t.Fatalf("Wrong preference. Expected %s got %s", Blue, pref)
	} else if sb.Finalized() {
		t.Fatalf("Finalized too early")
	}

	sb.RecordSuccessfulPoll(Blue)

	if pref := sb.Preference(); Blue != pref {
		t.Fatalf("Wrong preference. Expected %s got %s", Blue, pref)
	} else if !sb.Finalized() {
		t.Fatalf("Finalized too late")
	}

	expected := "SB(Preference = TtF4d2QWbk5vzQGTEPrN48x6vwgAoAmKQ9cbp79inpQmcRKES, NumSuccessfulPolls = 3, SF(Confidence = 2, Finalized = true, SL(Preference = TtF4d2QWbk5vzQGTEPrN48x6vwgAoAmKQ9cbp79inpQmcRKES)))"
	if str := sb.String(); str != expected {
		t.Fatalf("Wrong state. Expected:\n%s\nGot:\n%s", expected, str)
	}

	for i := 0; i < 4; i++ {
		sb.RecordSuccessfulPoll(Red)

		if pref := sb.Preference(); Blue != pref {
			t.Fatalf("Wrong preference. Expected %s got %s", Blue, pref)
		} else if !sb.Finalized() {
			t.Fatalf("Finalized too late")
		}
	}
}

func TestNarySnowballDifferentSnowflakeColor(t *testing.T) {
	betaVirtuous := 2
	betaRogue := 2

	sb := nnarySnowball{}
	sb.Initialize(betaVirtuous, betaRogue, Red)
	sb.Add(Blue)

	if pref := sb.Preference(); Red != pref {
		t.Fatalf("Wrong preference. Expected %s got %s", Red, pref)
	} else if sb.Finalized() {
		t.Fatalf("Finalized too early")
	}

	sb.RecordSuccessfulPoll(Blue)

	if pref := sb.nnarySnowflake.Preference(); Blue != pref {
		t.Fatalf("Wrong preference. Expected %s got %s", Blue, pref)
	}

	sb.RecordSuccessfulPoll(Red)

	if pref := sb.Preference(); Blue != pref {
		t.Fatalf("Wrong preference. Expected %s got %s", Blue, pref)
	} else if pref := sb.nnarySnowflake.Preference(); Red != pref {
		t.Fatalf("Wrong preference. Expected %s got %s", Blue, pref)
	}
}

```

avalanchego/snow/consensus/snowball/nnary_snowflake.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowball

import (
	"fmt"

	"github.com/ava-labs/avalanchego/ids"
)

var _ NnarySnowflake = &nnarySnowflake{}

// nnarySnowflake is the implementation of a snowflake instance with an
// unbounded number of choices
type nnarySnowflake struct {
	// wrap the n-nary slush logic
	nnarySlush

	// betaVirtuous is the number of consecutive successful queries required for
	// finalization on a virtuous instance.
	betaVirtuous int

	// betaRogue is the number of consecutive successful queries required for
	// finalization on a rogue instance.
	betaRogue int

	// confidence tracks the number of successful polls in a row that have
	// returned the preference
	confidence int

	// rogue tracks if this instance has multiple choices or only one
	rogue bool

	// finalized prevents the state from changing after the required number of
	// consecutive polls has been reached
	finalized bool
}

func (sf *nnarySnowflake) Initialize(betaVirtuous, betaRogue int, choice ids.ID) {
	sf.nnarySlush.Initialize(choice)
	sf.betaVirtuous = betaVirtuous
	sf.betaRogue = betaRogue
}

func (sf *nnarySnowflake) Add(choice ids.ID) { sf.rogue = sf.rogue || choice != sf.preference }

func (sf *nnarySnowflake) RecordSuccessfulPoll(choice ids.ID) {
	if sf.finalized {
		return // This instace is already decided.
	}

	if preference := sf.Preference(); preference == choice {
		sf.confidence++
	} else {
		// confidence is set to 1 because there has already been 1 successful
		// poll, namely this poll.
		sf.confidence = 1
	}

	sf.finalized = (!sf.rogue && sf.confidence >= sf.betaVirtuous) ||
		sf.confidence >= sf.betaRogue
	sf.nnarySlush.RecordSuccessfulPoll(choice)
}

func (sf *nnarySnowflake) RecordUnsuccessfulPoll() { sf.confidence = 0 }

func (sf *nnarySnowflake) Finalized() bool { return sf.finalized }

func (sf *nnarySnowflake) String() string {
	return fmt.Sprintf("SF(Confidence = %d, Finalized = %v, %s)",
		sf.confidence,
		sf.finalized,
		&sf.nnarySlush)
}

```

avalanchego/snow/consensus/snowball/nnary_snowflake_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowball

import (
	"testing"
)

func TestNnarySnowflake(t *testing.T) {
	betaVirtuous := 2
	betaRogue := 2

	sf := nnarySnowflake{}
	sf.Initialize(betaVirtuous, betaRogue, Red)
	sf.Add(Blue)
	sf.Add(Green)

	if pref := sf.Preference(); Red != pref {
		t.Fatalf("Wrong preference. Expected %s got %s", Red, pref)
	} else if sf.Finalized() {
		t.Fatalf("Finalized too early")
	}

	sf.RecordSuccessfulPoll(Blue)

	if pref := sf.Preference(); Blue != pref {
		t.Fatalf("Wrong preference. Expected %s got %s", Blue, pref)
	} else if sf.Finalized() {
		t.Fatalf("Finalized too early")
	}

	sf.RecordSuccessfulPoll(Red)

	if pref := sf.Preference(); Red != pref {
		t.Fatalf("Wrong preference. Expected %s got %s", Red, pref)
	} else if sf.Finalized() {
		t.Fatalf("Finalized too early")
	}

	sf.RecordSuccessfulPoll(Red)

	if pref := sf.Preference(); Red != pref {
		t.Fatalf("Wrong preference. Expected %s got %s", Red, pref)
	} else if !sf.Finalized() {
		t.Fatalf("Should be finalized")
	}

	sf.RecordSuccessfulPoll(Blue)

	if pref := sf.Preference(); Red != pref {
		t.Fatalf("Wrong preference. Expected %s got %s", Red, pref)
	} else if !sf.Finalized() {
		t.Fatalf("Should be finalized")
	}
}

func TestVirtuousNnarySnowflake(t *testing.T) {
	betaVirtuous := 2
	betaRogue := 3

	sb := nnarySnowflake{}
	sb.Initialize(betaVirtuous, betaRogue, Red)

	if pref := sb.Preference(); Red != pref {
		t.Fatalf("Wrong preference. Expected %s got %s", Red, pref)
	} else if sb.Finalized() {
		t.Fatalf("Finalized too early")
	}

	sb.RecordSuccessfulPoll(Red)

	if pref := sb.Preference(); Red != pref {
		t.Fatalf("Wrong preference. Expected %s got %s", Red, pref)
	} else if sb.Finalized() {
		t.Fatalf("Finalized too early")
	}

	sb.RecordSuccessfulPoll(Red)

	if pref := sb.Preference(); Red != pref {
		t.Fatalf("Wrong preference. Expected %s got %s", Red, pref)
	} else if !sb.Finalized() {
		t.Fatalf("Should be finalized")
	}
}

func TestRogueNnarySnowflake(t *testing.T) {
	betaVirtuous := 1
	betaRogue := 2

	sb := nnarySnowflake{}
	sb.Initialize(betaVirtuous, betaRogue, Red)
	if sb.rogue {
		t.Fatalf("Shouldn't be rogue")
	}

	sb.Add(Red)
	if sb.rogue {
		t.Fatalf("Shouldn't be rogue")
	}

	sb.Add(Blue)
	if !sb.rogue {
		t.Fatalf("Should be rogue")
	}

	sb.Add(Red)
	if !sb.rogue {
		t.Fatalf("Should be rogue")
	}

	if pref := sb.Preference(); Red != pref {
		t.Fatalf("Wrong preference. Expected %s got %s", Red, pref)
	} else if sb.Finalized() {
		t.Fatalf("Finalized too early")
	}

	sb.RecordSuccessfulPoll(Red)

	if pref := sb.Preference(); Red != pref {
		t.Fatalf("Wrong preference. Expected %s got %s", Red, pref)
	} else if sb.Finalized() {
		t.Fatalf("Finalized too early")
	}

	sb.RecordSuccessfulPoll(Red)

	if pref := sb.Preference(); Red != pref {
		t.Fatalf("Wrong preference. Expected %s got %s", Red, pref)
	} else if !sb.Finalized() {
		t.Fatalf("Should be finalized")
	}
}

```

avalanchego/snow/consensus/snowball/parameters.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowball

import (
	"fmt"
	"time"
)

const (
	errMsg = "" +
		`__________                    .___` + "\n" +
		`\______   \____________     __| _/__.__.` + "\n" +
		` |    |  _/\_  __ \__  \   / __ <   |  |` + "\n" +
		` |    |   \ |  | \// __ \_/ /_/ |\___  |` + "\n" +
		` |______  / |__|  (____  /\____ |/ ____|` + "\n" +
		`        \/             \/      \/\/` + "\n" +
		"\n" +
		`  🏆    🏆    🏆    🏆    🏆    🏆    🏆` + "\n" +
		`  ________ ________      ________________` + "\n" +
		` /  _____/ \_____  \    /  _  \__    ___/` + "\n" +
		`/   \  ___  /   |   \  /  /_\  \|    |` + "\n" +
		`\    \_\  \/    |    \/    |    \    |` + "\n" +
		` \______  /\_______  /\____|__  /____|` + "\n" +
		`        \/         \/         \/` + "\n"
)

// Parameters required for snowball consensus
type Parameters struct {
	K                 int `json:"k" yaml:"k"`
	Alpha             int `json:"alpha" yaml:"alpha"`
	BetaVirtuous      int `json:"betaVirtuous" yaml:"betaVirtuous"`
	BetaRogue         int `json:"betaRogue" yaml:"betaRogue"`
	ConcurrentRepolls int `json:"concurrentRepolls" yaml:"concurrentRepolls"`
	OptimalProcessing int `json:"optimalProcessing" yaml:"optimalProcessing"`

	// Reports unhealthy if more than this number of items are outstanding.
	MaxOutstandingItems int `json:"maxOutstandingItems" yaml:"maxOutstandingItems"`

	// Reports unhealthy if there is an item processing for longer than this
	// duration.
	MaxItemProcessingTime time.Duration `json:"maxItemProcessingTime" yaml:"maxItemProcessingTime"`

	// If this node is a validator, when a container is inserted into consensus,
	// send a Push Query to this many validators and a Pull Query to the other
	// k - MixedQueryNumPushVdr validators. Must be in [0, K].
	MixedQueryNumPushVdr int `json:"mixedQueryNumPushVdr" yaml:"mixedQueryNumPushVdr"`

	// If this node is not a validator, when a container is inserted into consensus,
	// send a Push Query to this many validators and a Pull Query to the other
	// k - MixedQueryNumPushVdr validators. Must be in [0, K].
	MixedQueryNumPushNonVdr int `json:"mixedQueryNumPushNonVdr" yaml:"mixedQueryNumPushNonVdr"`
}

// Verify returns nil if the parameters describe a valid initialization.
func (p Parameters) Verify() error {
	switch {
	case p.Alpha <= p.K/2:
		return fmt.Errorf("k = %d, alpha = %d: fails the condition that: k/2 < alpha", p.K, p.Alpha)
	case p.K < p.Alpha:
		return fmt.Errorf("k = %d, alpha = %d: fails the condition that: alpha <= k", p.K, p.Alpha)
	case p.BetaVirtuous <= 0:
		return fmt.Errorf("betaVirtuous = %d: fails the condition that: 0 < betaVirtuous", p.BetaVirtuous)
	case p.BetaRogue == 3 && p.BetaVirtuous == 28:
		return fmt.Errorf("betaVirtuous = %d, betaRogue = %d: fails the condition that: betaVirtuous <= betaRogue\n%s", p.BetaVirtuous, p.BetaRogue, errMsg)
	case p.BetaRogue < p.BetaVirtuous:
		return fmt.Errorf("betaVirtuous = %d, betaRogue = %d: fails the condition that: betaVirtuous <= betaRogue", p.BetaVirtuous, p.BetaRogue)
	case p.ConcurrentRepolls <= 0:
		return fmt.Errorf("concurrentRepolls = %d: fails the condition that: 0 < concurrentRepolls", p.ConcurrentRepolls)
	case p.ConcurrentRepolls > p.BetaRogue:
		return fmt.Errorf("concurrentRepolls = %d, betaRogue = %d: fails the condition that: concurrentRepolls <= betaRogue", p.ConcurrentRepolls, p.BetaRogue)
	case p.OptimalProcessing <= 0:
		return fmt.Errorf("optimalProcessing = %d: fails the condition that: 0 < optimalProcessing", p.OptimalProcessing)
	case p.MaxOutstandingItems <= 0:
		return fmt.Errorf("maxOutstandingItems = %d: fails the condition that: 0 < maxOutstandingItems", p.MaxOutstandingItems)
	case p.MaxItemProcessingTime <= 0:
		return fmt.Errorf("maxItemProcessingTime = %d: fails the condition that: 0 < maxItemProcessingTime", p.MaxItemProcessingTime)
	case p.MixedQueryNumPushVdr > p.K:
		return fmt.Errorf("mixedQueryNumPushVdr (%d) > K (%d)", p.MixedQueryNumPushVdr, p.K)
	case p.MixedQueryNumPushNonVdr > p.K:
		return fmt.Errorf("mixedQueryNumPushNonVdr (%d) > K (%d)", p.MixedQueryNumPushNonVdr, p.K)
	default:
		return nil
	}
}

```

avalanchego/snow/consensus/snowball/parameters_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowball

import (
	"fmt"
	"strings"
	"testing"
)

func TestParametersVerify(t *testing.T) {
	p := Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          1,
		BetaRogue:             1,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}

	if err := p.Verify(); err != nil {
		t.Fatal(err)
	}
}

func TestParametersAnotherVerify(t *testing.T) {
	p := Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          28,
		BetaRogue:             30,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}

	if err := p.Verify(); err != nil {
		t.Fatal(err)
	}
}

func TestParametersYetAnotherVerify(t *testing.T) {
	p := Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          3,
		BetaRogue:             3,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}

	if err := p.Verify(); err != nil {
		t.Fatal(err)
	}
}

func TestParametersInvalidK(t *testing.T) {
	p := Parameters{
		K:                     0,
		Alpha:                 1,
		BetaVirtuous:          1,
		BetaRogue:             1,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}

	if err := p.Verify(); err == nil {
		t.Fatalf("Should have failed due to invalid k")
	}
}

func TestParametersInvalidAlpha(t *testing.T) {
	p := Parameters{
		K:                     1,
		Alpha:                 0,
		BetaVirtuous:          1,
		BetaRogue:             1,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}

	if err := p.Verify(); err == nil {
		t.Fatalf("Should have failed due to invalid alpha")
	}
}

func TestParametersInvalidBetaVirtuous(t *testing.T) {
	p := Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          0,
		BetaRogue:             1,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}

	if err := p.Verify(); err == nil {
		t.Fatalf("Should have failed due to invalid beta virtuous")
	}
}

func TestParametersInvalidBetaRogue(t *testing.T) {
	p := Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          1,
		BetaRogue:             0,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}

	if err := p.Verify(); err == nil {
		t.Fatalf("Should have failed due to invalid beta rogue")
	}
}

func TestParametersAnotherInvalidBetaRogue(t *testing.T) {
	p := Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          28,
		BetaRogue:             3,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}

	if err := p.Verify(); err == nil {
		t.Fatalf("Should have failed due to invalid beta rogue")
	} else if !strings.Contains(err.Error(), "\n") {
		t.Fatalf("Should have described the extensive error")
	}
}

func TestParametersInvalidConcurrentRepolls(t *testing.T) {
	tests := []Parameters{
		{
			K:                     1,
			Alpha:                 1,
			BetaVirtuous:          1,
			BetaRogue:             1,
			ConcurrentRepolls:     2,
			OptimalProcessing:     1,
			MaxOutstandingItems:   1,
			MaxItemProcessingTime: 1,
		},
		{
			K:                     1,
			Alpha:                 1,
			BetaVirtuous:          1,
			BetaRogue:             1,
			ConcurrentRepolls:     0,
			OptimalProcessing:     1,
			MaxOutstandingItems:   1,
			MaxItemProcessingTime: 1,
		},
	}
	for _, p := range tests {
		label := fmt.Sprintf("ConcurrentRepolls=%d", p.ConcurrentRepolls)
		t.Run(label, func(t *testing.T) {
			if err := p.Verify(); err == nil {
				t.Error("Should have failed due to invalid concurrent repolls")
			}
		})
	}
}

func TestParametersInvalidOptimalProcessing(t *testing.T) {
	p := Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          1,
		BetaRogue:             1,
		ConcurrentRepolls:     1,
		OptimalProcessing:     0,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}

	if err := p.Verify(); err == nil {
		t.Fatalf("Should have failed due to invalid optimal processing")
	}
}

func TestParametersInvalidMaxOutstandingItems(t *testing.T) {
	p := Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          1,
		BetaRogue:             1,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   0,
		MaxItemProcessingTime: 1,
	}

	if err := p.Verify(); err == nil {
		t.Fatalf("Should have failed due to invalid max outstanding items")
	}
}

func TestParametersInvalidMaxItemProcessingTime(t *testing.T) {
	p := Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          1,
		BetaRogue:             1,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 0,
	}

	if err := p.Verify(); err == nil {
		t.Fatalf("Should have failed due to invalid max item processing time")
	}
}

```

avalanchego/snow/consensus/snowball/tree.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowball

import (
	"fmt"
	"strings"

	"github.com/ava-labs/avalanchego/ids"
)

var (
	_ Factory   = &TreeFactory{}
	_ Consensus = &Tree{}
	_ node      = &unaryNode{}
	_ node      = &binaryNode{}
)

// TreeFactory implements Factory by returning a tree struct
type TreeFactory struct{}

func (TreeFactory) New() Consensus { return &Tree{} }

// Tree implements the snowball interface by using a modified patricia tree.
type Tree struct {
	// node is the root that represents the first snowball instance in the tree,
	// and contains references to all the other snowball instances in the tree.
	node

	// params contains all the configurations of a snowball instance
	params Parameters

	// shouldReset is used as an optimization to prevent needless tree
	// traversals. If a snowball instance does not get an alpha majority, that
	// instance needs to reset by calling RecordUnsuccessfulPoll. Because the
	// tree splits votes based on the branch, when an instance doesn't get an
	// alpha majority none of the children of this instance can get an alpha
	// majority. To avoid calling RecordUnsuccessfulPoll on the full sub-tree of
	// a node that didn't get an alpha majority, shouldReset is used to indicate
	// that any later traversal into this sub-tree should call
	// RecordUnsuccessfulPoll before performing any other action.
	shouldReset bool
}

func (t *Tree) Initialize(params Parameters, choice ids.ID) {
	t.params = params

	snowball := &unarySnowball{}
	snowball.Initialize(params.BetaVirtuous)

	t.node = &unaryNode{
		tree:         t,
		preference:   choice,
		commonPrefix: ids.NumBits, // The initial state has no conflicts
		snowball:     snowball,
	}
}

func (t *Tree) Parameters() Parameters { return t.params }

func (t *Tree) Add(choice ids.ID) {
	prefix := t.node.DecidedPrefix()
	// Make sure that we haven't already decided against this new id
	if ids.EqualSubset(0, prefix, t.Preference(), choice) {
		t.node = t.node.Add(choice)
	}
}

func (t *Tree) RecordPoll(votes ids.Bag) bool {
	// Get the assumed decided prefix of the root node.
	decidedPrefix := t.node.DecidedPrefix()

	// If any of the bits differ from the preference in this prefix, the vote is
	// for a rejected operation. So, we filter out these invalid votes.
	filteredVotes := votes.Filter(0, decidedPrefix, t.Preference())

	// Now that the votes have been restricted to valid votes, pass them into
	// the first snowball instance
	var successful bool
	t.node, successful = t.node.RecordPoll(filteredVotes, t.shouldReset)

	// Because we just passed the reset into the snowball instance, we should no
	// longer reset.
	t.shouldReset = false
	return successful
}

func (t *Tree) RecordUnsuccessfulPoll() { t.shouldReset = true }

func (t *Tree) String() string {
	builder := strings.Builder{}

	prefixes := []string{""}
	nodes := []node{t.node}

	for len(prefixes) > 0 {
		newSize := len(prefixes) - 1

		prefix := prefixes[newSize]
		prefixes = prefixes[:newSize]

		node := nodes[newSize]
		nodes = nodes[:newSize]

		s, newNodes := node.Printable()

		builder.WriteString(prefix)
		builder.WriteString(s)
		builder.WriteString("\n")

		newPrefix := prefix + "    "
		for range newNodes {
			prefixes = append(prefixes, newPrefix)
		}
		nodes = append(nodes, newNodes...)
	}

	return strings.TrimSuffix(builder.String(), "\n")
}

type node interface {
	// Preference returns the preferred choice of this sub-tree
	Preference() ids.ID
	// Return the number of assumed decided bits of this node
	DecidedPrefix() int
	// Adds a new choice to vote on
	// Returns the new node
	Add(newChoice ids.ID) node
	// Apply the votes, reset the model if needed
	// Returns the new node and whether the vote was successful
	RecordPoll(votes ids.Bag, shouldReset bool) (newChild node, successful bool)
	// Returns true if consensus has been reached on this node
	Finalized() bool

	Printable() (string, []node)
}

// unary is a node with either no children, or a single child. It handles the
// voting on a range of identical, virtuous, snowball instances.
type unaryNode struct {
	// tree references the tree that contains this node
	tree *Tree

	// preference is the choice that is preferred at every branch in this
	// sub-tree
	preference ids.ID

	// decidedPrefix is the last bit in the prefix that is assumed to be decided
	decidedPrefix int // Will be in the range [0, 255)

	// commonPrefix is the last bit in the prefix that this node transitively
	// references
	commonPrefix int // Will be in the range (decidedPrefix, 256)

	// snowball wraps the snowball logic
	snowball UnarySnowball

	// shouldReset is used as an optimization to prevent needless tree
	// traversals. It is the continuation of shouldReset in the Tree struct.
	shouldReset bool

	// child is the, possibly nil, node that votes on the next bits in the
	// decision
	child node
}

func (u *unaryNode) Preference() ids.ID { return u.preference }
func (u *unaryNode) DecidedPrefix() int { return u.decidedPrefix }

// This is by far the most complicated function in this algorithm.
// The intuition is that this instance represents a series of consecutive unary
// snowball instances, and this function's purpose is convert one of these unary
// snowball instances into a binary snowball instance.
// There are 5 possible cases.
// 1. None of these instances should be split, we should attempt to split a
//    child
//
//        For example, attempting to insert the value "00001" in this node:
//
//                       +-------------------+ <-- This node will not be split
//                       |                   |
//                       |       0 0 0       |
//                       |                   |
//                       +-------------------+ <-- Pass the add to the child
//                                 ^
//                                 |
//
//        Results in:
//
//                       +-------------------+
//                       |                   |
//                       |       0 0 0       |
//                       |                   |
//                       +-------------------+ <-- With the modified child
//                                 ^
//                                 |
//
// 2. This instance represents a series of only one unary instance and it must
//    be split
//       This will return a binary choice, with one child the same as my child,
//       and another (possibly nil child) representing a new chain to the end of
//       the hash
//
//        For example, attempting to insert the value "1" in this tree:
//
//                       +-------------------+
//                       |                   |
//                       |         0         |
//                       |                   |
//                       +-------------------+
//
//        Results in:
//
//                       +-------------------+
//                       |         |         |
//                       |    0    |    1    |
//                       |         |         |
//                       +-------------------+
//
// 3. This instance must be split on the first bit
//       This will return a binary choice, with one child equal to this instance
//       with decidedPrefix increased by one, and another representing a new
//       chain to the end of the hash
//
//        For example, attempting to insert the value "10" in this tree:
//
//                       +-------------------+
//                       |                   |
//                       |        0 0        |
//                       |                   |
//                       +-------------------+
//
//        Results in:
//
//                       +-------------------+
//                       |         |         |
//                       |    0    |    1    |
//                       |         |         |
//                       +-------------------+
//                            ^         ^
//                           /           \
//            +-------------------+ +-------------------+
//            |                   | |                   |
//            |         0         | |         0         |
//            |                   | |                   |
//            +-------------------+ +-------------------+
//
// 4. This instance must be split on the last bit
//       This will modify this unary choice. The commonPrefix is decreased by
//       one. The child is set to a binary instance that has a child equal to
//       the current child and another child equal to a new unary instance to
//       the end of the hash
//
//        For example, attempting to insert the value "01" in this tree:
//
//                       +-------------------+
//                       |                   |
//                       |        0 0        |
//                       |                   |
//                       +-------------------+
//
//        Results in:
//
//                       +-------------------+
//                       |                   |
//                       |         0         |
//                       |                   |
//                       +-------------------+
//                                 ^
//                                 |
//                       +-------------------+
//                       |         |         |
//                       |    0    |    1    |
//                       |         |         |
//                       +-------------------+
//
// 5. This instance must be split on an interior bit
//       This will modify this unary choice. The commonPrefix is set to the
//       interior bit. The child is set to a binary instance that has a child
//       equal to this unary choice with the decidedPrefix equal to the interior
//       bit and another child equal to a new unary instance to the end of the
//       hash
//
//        For example, attempting to insert the value "010" in this tree:
//
//                       +-------------------+
//                       |                   |
//                       |       0 0 0       |
//                       |                   |
//                       +-------------------+
//
//        Results in:
//
//                       +-------------------+
//                       |                   |
//                       |         0         |
//                       |                   |
//                       +-------------------+
//                                 ^
//                                 |
//                       +-------------------+
//                       |         |         |
//                       |    0    |    1    |
//                       |         |         |
//                       +-------------------+
//                            ^         ^
//                           /           \
//            +-------------------+ +-------------------+
//            |                   | |                   |
//            |         0         | |         0         |
//            |                   | |                   |
//            +-------------------+ +-------------------+
func (u *unaryNode) Add(newChoice ids.ID) node {
	if u.Finalized() {
		return u // Only happens if the tree is finalized, or it's a leaf node
	}

	if index, found := ids.FirstDifferenceSubset(
		u.decidedPrefix, u.commonPrefix, u.preference, newChoice); !found {
		// If the first difference doesn't exist, then this node shouldn't be
		// split
		if u.child != nil {
			// Because this node will finalize before any children could
			// finalize, it must be that the newChoice will match my child's
			// prefix
			u.child = u.child.Add(newChoice)
		}
		// if u.child is nil, then we are attempting to add the same choice into
		// the tree, which should be a noop
	} else {
		// The difference was found, so this node must be split

		bit := u.preference.Bit(uint(index)) // The currently preferred bit
		b := &binaryNode{
			tree:        u.tree,
			bit:         index,
			snowball:    u.snowball.Extend(u.tree.params.BetaRogue, bit),
			shouldReset: [2]bool{u.shouldReset, u.shouldReset},
		}
		b.preferences[bit] = u.preference
		b.preferences[1-bit] = newChoice

		newChildSnowball := &unarySnowball{}
		newChildSnowball.Initialize(u.tree.params.BetaVirtuous)
		newChild := &unaryNode{
			tree:          u.tree,
			preference:    newChoice,
			decidedPrefix: index + 1,   // The new child assumes this branch has decided in it's favor
			commonPrefix:  ids.NumBits, // The new child has no conflicts under this branch
			snowball:      newChildSnowball,
		}

		switch {
		case u.decidedPrefix == u.commonPrefix-1:
			// This node was only voting over one bit. (Case 2. from above)
			b.children[bit] = u.child
			if u.child != nil {
				b.children[1-bit] = newChild
			}
			return b
		case index == u.decidedPrefix:
			// This node was split on the first bit. (Case 3. from above)
			u.decidedPrefix++
			b.children[bit] = u
			b.children[1-bit] = newChild
			return b
		case index == u.commonPrefix-1:
			// This node was split on the last bit. (Case 4. from above)
			u.commonPrefix--
			b.children[bit] = u.child
			if u.child != nil {
				b.children[1-bit] = newChild
			}
			u.child = b
			return u
		default:
			// This node was split on an interior bit. (Case 5. from above)
			originalDecidedPrefix := u.decidedPrefix
			u.decidedPrefix = index + 1
			b.children[bit] = u
			b.children[1-bit] = newChild
			return &unaryNode{
				tree:          u.tree,
				preference:    u.preference,
				decidedPrefix: originalDecidedPrefix,
				commonPrefix:  index,
				snowball:      u.snowball.Clone(),
				child:         b,
			}
		}
	}
	return u // Do nothing, the choice was already rejected
}

func (u *unaryNode) RecordPoll(votes ids.Bag, reset bool) (node, bool) {
	// We are guaranteed that the votes are of IDs that have previously been
	// added. This ensures that the provided votes all have the same bits in the
	// range [u.decidedPrefix, u.commonPrefix) as in u.preference.

	// If my parent didn't get enough votes previously, then neither did I
	if reset {
		u.snowball.RecordUnsuccessfulPoll()
		u.shouldReset = true // Make sure my child is also reset correctly
	}

	if votes.Len() < u.tree.params.Alpha {
		// I didn't get enough votes, I must reset and my child must reset as
		// well
		u.snowball.RecordUnsuccessfulPoll()
		u.shouldReset = true
		return u, false
	}

	// I got enough votes this time
	u.snowball.RecordSuccessfulPoll()

	if u.child != nil {
		// We are guaranteed that u.commonPrefix will equal
		// u.child.DecidedPrefix(). Otherwise, there must have been a
		// decision under this node, which isn't possible because
		// beta1 <= beta2. That means that filtering the votes between
		// u.commonPrefix and u.child.DecidedPrefix() would always result in
		// the same set being returned.

		newChild, _ := u.child.RecordPoll(votes, u.shouldReset)
		if u.Finalized() {
			// If I'm now decided, return my child
			return newChild, true
		}
		u.child = newChild

		// The child's preference may have changed
		u.preference = u.child.Preference()
	}
	// Now that I have passed my votes to my child, I don't need to reset
	// them
	u.shouldReset = false
	return u, true
}

func (u *unaryNode) Finalized() bool { return u.snowball.Finalized() }

func (u *unaryNode) Printable() (string, []node) {
	s := fmt.Sprintf("%s Bits = [%d, %d)",
		u.snowball, u.decidedPrefix, u.commonPrefix)
	if u.child == nil {
		return s, nil
	}
	return s, []node{u.child}
}

// binaryNode is a node with either no children, or two children. It handles the
// voting of a single, rogue, snowball instance.
type binaryNode struct {
	// tree references the tree that contains this node
	tree *Tree

	// preferences are the choices that are preferred at every branch in their
	// sub-tree
	preferences [2]ids.ID

	// bit is the index in the id of the choice this node is deciding on
	bit int // Will be in the range [0, 256)

	// snowball wraps the snowball logic
	snowball BinarySnowball

	// shouldReset is used as an optimization to prevent needless tree
	// traversals. It is the continuation of shouldReset in the Tree struct.
	shouldReset [2]bool

	// children are the, possibly nil, nodes that vote on the next bits in the
	// decision
	children [2]node
}

func (b *binaryNode) Preference() ids.ID { return b.preferences[b.snowball.Preference()] }
func (b *binaryNode) DecidedPrefix() int { return b.bit }

func (b *binaryNode) Add(id ids.ID) node {
	bit := id.Bit(uint(b.bit))
	child := b.children[bit]
	// If child is nil, then we are running an instance on the last bit. Finding
	// two hashes that are equal up to the last bit would be really cool though.
	// Regardless, the case is handled
	if child != nil &&
		// + 1 is used because we already explicitly check the p.bit bit
		ids.EqualSubset(b.bit+1, child.DecidedPrefix(), b.preferences[bit], id) {
		b.children[bit] = child.Add(id)
	}
	// If child is nil, then the id has already been added to the tree, so
	// nothing should be done
	// If the decided prefix isn't matched, then a previous decision has made
	// the id that is being added to have already been rejected
	return b
}

func (b *binaryNode) RecordPoll(votes ids.Bag, reset bool) (node, bool) {
	// The list of votes we are passed is split into votes for bit 0 and votes
	// for bit 1
	splitVotes := votes.Split(uint(b.bit))

	bit := 0
	// We only care about which bit is set if a successful poll can happen
	if splitVotes[1].Len() >= b.tree.params.Alpha {
		bit = 1
	}

	if reset {
		b.snowball.RecordUnsuccessfulPoll()
		b.shouldReset[bit] = true
		// 1-bit isn't set here because it is set below anyway
	}
	b.shouldReset[1-bit] = true // They didn't get the threshold of votes

	prunedVotes := splitVotes[bit]
	if prunedVotes.Len() < b.tree.params.Alpha {
		b.snowball.RecordUnsuccessfulPoll()
		// The winning child didn't get enough votes either
		b.shouldReset[bit] = true
		return b, false
	}

	// This bit got alpha votes, it was a successful poll
	b.snowball.RecordSuccessfulPoll(bit)

	if child := b.children[bit]; child != nil {
		// The votes are filtered to ensure that they are votes that should
		// count for the child
		filteredVotes := prunedVotes.Filter(
			b.bit+1, child.DecidedPrefix(), b.preferences[bit])

		newChild, _ := child.RecordPoll(filteredVotes, b.shouldReset[bit])
		if b.snowball.Finalized() {
			// If we are decided here, that means we must have decided due
			// to this poll. Therefore, we must have decided on bit.
			return newChild, true
		}
		b.children[bit] = newChild
		b.preferences[bit] = newChild.Preference()
	}
	b.shouldReset[bit] = false // We passed the reset down
	return b, true
}

func (b *binaryNode) Finalized() bool { return b.snowball.Finalized() }

func (b *binaryNode) Printable() (string, []node) {
	s := fmt.Sprintf("%s Bit = %d", b.snowball, b.bit)
	if b.children[0] == nil {
		return s, nil
	}
	return s, []node{b.children[1], b.children[0]}
}

```

avalanchego/snow/consensus/snowball/tree_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowball

import (
	"strings"
	"testing"

	"github.com/stretchr/testify/require"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/utils/sampler"
)

const (
	initialUnaryDescription = "SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [0, 256)"
)

func TestTreeParams(t *testing.T) { ParamsTest(t, TreeFactory{}) }

func TestSnowballSingleton(t *testing.T) {
	require := require.New(t)

	params := Parameters{
		K: 1, Alpha: 1, BetaVirtuous: 2, BetaRogue: 5,
	}
	tree := Tree{}
	tree.Initialize(params, Red)

	require.False(tree.Finalized())

	oneRed := ids.Bag{}
	oneRed.Add(Red)
	require.True(tree.RecordPoll(oneRed))
	require.False(tree.Finalized())

	empty := ids.Bag{}
	require.False(tree.RecordPoll(empty))
	require.False(tree.Finalized())

	require.True(tree.RecordPoll(oneRed))
	require.False(tree.Finalized())

	require.True(tree.RecordPoll(oneRed))
	require.Equal(Red, tree.Preference())
	require.True(tree.Finalized())

	tree.Add(Blue)

	require.True(tree.Finalized())

	// Because the tree is already finalized, RecordPoll can return either true
	// or false.
	oneBlue := ids.Bag{}
	oneBlue.Add(Blue)
	tree.RecordPoll(oneBlue)
	require.Equal(Red, tree.Preference())
	require.True(tree.Finalized())
}

func TestSnowballRecordUnsuccessfulPoll(t *testing.T) {
	require := require.New(t)

	params := Parameters{
		K: 1, Alpha: 1, BetaVirtuous: 3, BetaRogue: 5,
	}
	tree := Tree{}
	tree.Initialize(params, Red)

	require.False(tree.Finalized())

	oneRed := ids.Bag{}
	oneRed.Add(Red)
	require.True(tree.RecordPoll(oneRed))

	tree.RecordUnsuccessfulPoll()

	require.True(tree.RecordPoll(oneRed))
	require.False(tree.Finalized())

	require.True(tree.RecordPoll(oneRed))
	require.False(tree.Finalized())

	require.True(tree.RecordPoll(oneRed))
	require.Equal(Red, tree.Preference())
	require.True(tree.Finalized())
}

func TestSnowballBinary(t *testing.T) {
	require := require.New(t)

	params := Parameters{
		K: 1, Alpha: 1, BetaVirtuous: 1, BetaRogue: 2,
	}
	tree := Tree{}
	tree.Initialize(params, Red)
	tree.Add(Blue)

	require.Equal(Red, tree.Preference())
	require.False(tree.Finalized())

	oneBlue := ids.Bag{}
	oneBlue.Add(Blue)
	require.True(tree.RecordPoll(oneBlue))
	require.Equal(Blue, tree.Preference())
	require.False(tree.Finalized())

	oneRed := ids.Bag{}
	oneRed.Add(Red)
	require.True(tree.RecordPoll(oneRed))
	require.Equal(Blue, tree.Preference())
	require.False(tree.Finalized())

	require.True(tree.RecordPoll(oneBlue))
	require.Equal(Blue, tree.Preference())
	require.False(tree.Finalized())

	require.True(tree.RecordPoll(oneBlue))
	require.Equal(Blue, tree.Preference())
	require.True(tree.Finalized())
}

func TestSnowballLastBinary(t *testing.T) {
	require := require.New(t)

	zero := ids.Empty
	one := ids.ID{
		0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
		0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
		0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
		0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x80,
	}

	params := Parameters{
		K: 1, Alpha: 1, BetaVirtuous: 2, BetaRogue: 2,
	}
	tree := Tree{}
	tree.Initialize(params, zero)
	tree.Add(one)

	// Should do nothing
	tree.Add(one)

	expected := "SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [0, 255)\n" +
		"    SB(Preference = 0, NumSuccessfulPolls[0] = 0, NumSuccessfulPolls[1] = 0, SF(Confidence = 0, Finalized = false, SL(Preference = 0))) Bit = 255"
	require.Equal(expected, tree.String())
	require.Equal(zero, tree.Preference())
	require.False(tree.Finalized())

	oneBag := ids.Bag{}
	oneBag.Add(one)
	require.True(tree.RecordPoll(oneBag))
	require.Equal(one, tree.Preference())
	require.False(tree.Finalized())

	expected = "SB(NumSuccessfulPolls = 1, SF(Confidence = 1, Finalized = false)) Bits = [0, 255)\n" +
		"    SB(Preference = 1, NumSuccessfulPolls[0] = 0, NumSuccessfulPolls[1] = 1, SF(Confidence = 1, Finalized = false, SL(Preference = 1))) Bit = 255"
	require.Equal(expected, tree.String())

	require.True(tree.RecordPoll(oneBag))
	require.Equal(one, tree.Preference())
	require.True(tree.Finalized())

	expected = "SB(Preference = 1, NumSuccessfulPolls[0] = 0, NumSuccessfulPolls[1] = 2, SF(Confidence = 2, Finalized = true, SL(Preference = 1))) Bit = 255"
	require.Equal(expected, tree.String())
}

func TestSnowballAddPreviouslyRejected(t *testing.T) {
	require := require.New(t)

	zero := ids.ID{0b00000000}
	one := ids.ID{0b00000001}
	two := ids.ID{0b00000010}
	four := ids.ID{0b00000100}

	params := Parameters{
		K: 1, Alpha: 1, BetaVirtuous: 1, BetaRogue: 2,
	}
	tree := Tree{}
	tree.Initialize(params, zero)
	tree.Add(one)
	tree.Add(four)

	{
		expected := "SB(Preference = 0, NumSuccessfulPolls[0] = 0, NumSuccessfulPolls[1] = 0, SF(Confidence = 0, Finalized = false, SL(Preference = 0))) Bit = 0\n" +
			"    SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [1, 2)\n" +
			"        SB(Preference = 0, NumSuccessfulPolls[0] = 0, NumSuccessfulPolls[1] = 0, SF(Confidence = 0, Finalized = false, SL(Preference = 0))) Bit = 2\n" +
			"            SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [3, 256)\n" +
			"            SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [3, 256)\n" +
			"    SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [1, 256)"
		require.Equal(expected, tree.String())
		require.Equal(zero, tree.Preference())
		require.False(tree.Finalized())
	}

	zeroBag := ids.Bag{}
	zeroBag.Add(zero)
	require.True(tree.RecordPoll(zeroBag))

	{
		expected := "SB(Preference = 0, NumSuccessfulPolls[0] = 1, NumSuccessfulPolls[1] = 0, SF(Confidence = 1, Finalized = false, SL(Preference = 0))) Bit = 0\n" +
			"    SB(Preference = 0, NumSuccessfulPolls[0] = 1, NumSuccessfulPolls[1] = 0, SF(Confidence = 1, Finalized = false, SL(Preference = 0))) Bit = 2\n" +
			"        SB(NumSuccessfulPolls = 1, SF(Confidence = 1, Finalized = true)) Bits = [3, 256)\n" +
			"        SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [3, 256)\n" +
			"    SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [1, 256)"
		require.Equal(expected, tree.String())
		require.Equal(zero, tree.Preference())
		require.False(tree.Finalized())
	}

	tree.Add(two)

	{
		expected := "SB(Preference = 0, NumSuccessfulPolls[0] = 1, NumSuccessfulPolls[1] = 0, SF(Confidence = 1, Finalized = false, SL(Preference = 0))) Bit = 0\n" +
			"    SB(Preference = 0, NumSuccessfulPolls[0] = 1, NumSuccessfulPolls[1] = 0, SF(Confidence = 1, Finalized = false, SL(Preference = 0))) Bit = 2\n" +
			"        SB(NumSuccessfulPolls = 1, SF(Confidence = 1, Finalized = true)) Bits = [3, 256)\n" +
			"        SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [3, 256)\n" +
			"    SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [1, 256)"
		require.Equal(expected, tree.String())
		require.Equal(zero, tree.Preference())
		require.False(tree.Finalized())
	}
}

func TestSnowballNewUnary(t *testing.T) {
	require := require.New(t)

	zero := ids.ID{0b00000000}
	one := ids.ID{0b00000001}

	params := Parameters{
		K: 1, Alpha: 1, BetaVirtuous: 2, BetaRogue: 3,
	}
	tree := Tree{}
	tree.Initialize(params, zero)
	tree.Add(one)

	{
		expected := "SB(Preference = 0, NumSuccessfulPolls[0] = 0, NumSuccessfulPolls[1] = 0, SF(Confidence = 0, Finalized = false, SL(Preference = 0))) Bit = 0\n" +
			"    SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [1, 256)\n" +
			"    SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [1, 256)"
		require.Equal(expected, tree.String())
		require.Equal(zero, tree.Preference())
		require.False(tree.Finalized())
	}

	oneBag := ids.Bag{}
	oneBag.Add(one)
	require.True(tree.RecordPoll(oneBag))

	{
		expected := "SB(Preference = 1, NumSuccessfulPolls[0] = 0, NumSuccessfulPolls[1] = 1, SF(Confidence = 1, Finalized = false, SL(Preference = 1))) Bit = 0\n" +
			"    SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [1, 256)\n" +
			"    SB(NumSuccessfulPolls = 1, SF(Confidence = 1, Finalized = false)) Bits = [1, 256)"
		require.Equal(expected, tree.String())
		require.Equal(one, tree.Preference())
		require.False(tree.Finalized())
	}

	require.True(tree.RecordPoll(oneBag))

	{
		expected := "SB(Preference = 1, NumSuccessfulPolls[0] = 0, NumSuccessfulPolls[1] = 2, SF(Confidence = 2, Finalized = false, SL(Preference = 1))) Bit = 0\n" +
			"    SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [1, 256)\n" +
			"    SB(NumSuccessfulPolls = 2, SF(Confidence = 2, Finalized = true)) Bits = [1, 256)"
		require.Equal(expected, tree.String())
		require.Equal(one, tree.Preference())
		require.False(tree.Finalized())
	}
}

func TestSnowballTransitiveReset(t *testing.T) {
	require := require.New(t)

	zero := ids.ID{0b00000000}
	two := ids.ID{0b00000010}
	eight := ids.ID{0b00001000}

	params := Parameters{
		K: 1, Alpha: 1, BetaVirtuous: 2, BetaRogue: 2,
	}
	tree := Tree{}
	tree.Initialize(params, zero)
	tree.Add(two)
	tree.Add(eight)

	{
		expected := "SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [0, 1)\n" +
			"    SB(Preference = 0, NumSuccessfulPolls[0] = 0, NumSuccessfulPolls[1] = 0, SF(Confidence = 0, Finalized = false, SL(Preference = 0))) Bit = 1\n" +
			"        SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [2, 3)\n" +
			"            SB(Preference = 0, NumSuccessfulPolls[0] = 0, NumSuccessfulPolls[1] = 0, SF(Confidence = 0, Finalized = false, SL(Preference = 0))) Bit = 3\n" +
			"                SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [4, 256)\n" +
			"                SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [4, 256)\n" +
			"        SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [2, 256)"
		require.Equal(expected, tree.String())
		require.Equal(zero, tree.Preference())
		require.False(tree.Finalized())
	}

	zeroBag := ids.Bag{}
	zeroBag.Add(zero)
	require.True(tree.RecordPoll(zeroBag))

	{
		expected := "SB(NumSuccessfulPolls = 1, SF(Confidence = 1, Finalized = false)) Bits = [0, 1)\n" +
			"    SB(Preference = 0, NumSuccessfulPolls[0] = 1, NumSuccessfulPolls[1] = 0, SF(Confidence = 1, Finalized = false, SL(Preference = 0))) Bit = 1\n" +
			"        SB(NumSuccessfulPolls = 1, SF(Confidence = 1, Finalized = false)) Bits = [2, 3)\n" +
			"            SB(Preference = 0, NumSuccessfulPolls[0] = 1, NumSuccessfulPolls[1] = 0, SF(Confidence = 1, Finalized = false, SL(Preference = 0))) Bit = 3\n" +
			"                SB(NumSuccessfulPolls = 1, SF(Confidence = 1, Finalized = false)) Bits = [4, 256)\n" +
			"                SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [4, 256)\n" +
			"        SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [2, 256)"
		require.Equal(expected, tree.String())
		require.Equal(zero, tree.Preference())
		require.False(tree.Finalized())
	}

	emptyBag := ids.Bag{}
	require.False(tree.RecordPoll(emptyBag))

	{
		expected := "SB(NumSuccessfulPolls = 1, SF(Confidence = 0, Finalized = false)) Bits = [0, 1)\n" +
			"    SB(Preference = 0, NumSuccessfulPolls[0] = 1, NumSuccessfulPolls[1] = 0, SF(Confidence = 1, Finalized = false, SL(Preference = 0))) Bit = 1\n" +
			"        SB(NumSuccessfulPolls = 1, SF(Confidence = 1, Finalized = false)) Bits = [2, 3)\n" +
			"            SB(Preference = 0, NumSuccessfulPolls[0] = 1, NumSuccessfulPolls[1] = 0, SF(Confidence = 1, Finalized = false, SL(Preference = 0))) Bit = 3\n" +
			"                SB(NumSuccessfulPolls = 1, SF(Confidence = 1, Finalized = false)) Bits = [4, 256)\n" +
			"                SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [4, 256)\n" +
			"        SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [2, 256)"
		require.Equal(expected, tree.String())
		require.Equal(zero, tree.Preference())
		require.False(tree.Finalized())
	}

	require.True(tree.RecordPoll(zeroBag))

	{
		expected := "SB(NumSuccessfulPolls = 2, SF(Confidence = 1, Finalized = false)) Bits = [0, 1)\n" +
			"    SB(Preference = 0, NumSuccessfulPolls[0] = 2, NumSuccessfulPolls[1] = 0, SF(Confidence = 1, Finalized = false, SL(Preference = 0))) Bit = 1\n" +
			"        SB(NumSuccessfulPolls = 2, SF(Confidence = 1, Finalized = false)) Bits = [2, 3)\n" +
			"            SB(Preference = 0, NumSuccessfulPolls[0] = 2, NumSuccessfulPolls[1] = 0, SF(Confidence = 1, Finalized = false, SL(Preference = 0))) Bit = 3\n" +
			"                SB(NumSuccessfulPolls = 2, SF(Confidence = 1, Finalized = false)) Bits = [4, 256)\n" +
			"                SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [4, 256)\n" +
			"        SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [2, 256)"
		require.Equal(expected, tree.String())
		require.Equal(zero, tree.Preference())
		require.False(tree.Finalized())
	}

	require.True(tree.RecordPoll(zeroBag))

	{
		expected := "SB(NumSuccessfulPolls = 3, SF(Confidence = 2, Finalized = true)) Bits = [4, 256)"
		require.Equal(expected, tree.String())
		require.Equal(zero, tree.Preference())
		require.True(tree.Finalized())
	}
}

func TestSnowballTrinary(t *testing.T) {
	require := require.New(t)

	params := Parameters{
		K: 1, Alpha: 1, BetaVirtuous: 1, BetaRogue: 2,
	}
	tree := Tree{}
	tree.Initialize(params, Green)
	tree.Add(Red)
	tree.Add(Blue)

	//       *
	//      / \
	//     R   *
	//        / \
	//       G   B

	require.Equal(Green, tree.Preference())
	require.False(tree.Finalized())

	redBag := ids.Bag{}
	redBag.Add(Red)
	require.True(tree.RecordPoll(redBag))
	require.Equal(Red, tree.Preference())
	require.False(tree.Finalized())

	blueBag := ids.Bag{}
	blueBag.Add(Blue)
	require.True(tree.RecordPoll(blueBag))
	require.Equal(Red, tree.Preference())
	require.False(tree.Finalized())

	// Here is a case where voting for a color makes a different color become
	// the preferred color. This is intended behavior.
	greenBag := ids.Bag{}
	greenBag.Add(Green)
	require.True(tree.RecordPoll(greenBag))
	require.Equal(Blue, tree.Preference())
	require.False(tree.Finalized())

	// Red has already been rejected here, so this is not a successful poll.
	require.False(tree.RecordPoll(redBag))
	require.Equal(Blue, tree.Preference())
	require.False(tree.Finalized())

	require.True(tree.RecordPoll(greenBag))
	require.Equal(Green, tree.Preference())
	require.False(tree.Finalized())
}

func TestSnowballCloseTrinary(t *testing.T) {
	require := require.New(t)

	yellow := ids.ID{0x01}
	cyan := ids.ID{0x02}
	magenta := ids.ID{0x03}

	params := Parameters{
		K: 1, Alpha: 1, BetaVirtuous: 1, BetaRogue: 2,
	}
	tree := Tree{}
	tree.Initialize(params, yellow)
	tree.Add(cyan)
	tree.Add(magenta)

	//       *
	//      / \
	//     C   *
	//        / \
	//       Y   M

	require.Equal(yellow, tree.Preference())
	require.False(tree.Finalized())

	yellowBag := ids.Bag{}
	yellowBag.Add(yellow)
	require.True(tree.RecordPoll(yellowBag))
	require.Equal(yellow, tree.Preference())
	require.False(tree.Finalized())

	magentaBag := ids.Bag{}
	magentaBag.Add(magenta)
	require.True(tree.RecordPoll(magentaBag))
	require.Equal(yellow, tree.Preference())
	require.False(tree.Finalized())

	// Cyan has already been rejected here, so these are not successful polls.
	cyanBag := ids.Bag{}
	cyanBag.Add(cyan)
	require.False(tree.RecordPoll(cyanBag))
	require.Equal(yellow, tree.Preference())
	require.False(tree.Finalized())

	require.False(tree.RecordPoll(cyanBag))
	require.Equal(yellow, tree.Preference())
	require.False(tree.Finalized())
}

func TestSnowballAddRejected(t *testing.T) {
	require := require.New(t)

	c0000 := ids.ID{0x00} // 0000
	c1000 := ids.ID{0x01} // 1000
	c0101 := ids.ID{0x0a} // 0101
	c0010 := ids.ID{0x04} // 0010

	params := Parameters{
		K: 1, Alpha: 1, BetaVirtuous: 1, BetaRogue: 2,
	}
	tree := Tree{}
	tree.Initialize(params, c0000)
	tree.Add(c1000)
	tree.Add(c0010)

	require.Equal(c0000, tree.Preference())
	require.False(tree.Finalized())

	c0010Bag := ids.Bag{}
	c0010Bag.Add(c0010)
	require.True(tree.RecordPoll(c0010Bag))

	{
		expected := "SB(Preference = 0, NumSuccessfulPolls[0] = 1, NumSuccessfulPolls[1] = 0, SF(Confidence = 1, Finalized = false, SL(Preference = 0))) Bit = 0\n" +
			"    SB(Preference = 1, NumSuccessfulPolls[0] = 0, NumSuccessfulPolls[1] = 1, SF(Confidence = 1, Finalized = false, SL(Preference = 1))) Bit = 2\n" +
			"        SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [3, 256)\n" +
			"        SB(NumSuccessfulPolls = 1, SF(Confidence = 1, Finalized = true)) Bits = [3, 256)\n" +
			"    SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [1, 256)"
		require.Equal(expected, tree.String())
		require.Equal(c0010, tree.Preference())
		require.False(tree.Finalized())
	}

	tree.Add(c0101)

	{
		expected := "SB(Preference = 0, NumSuccessfulPolls[0] = 1, NumSuccessfulPolls[1] = 0, SF(Confidence = 1, Finalized = false, SL(Preference = 0))) Bit = 0\n" +
			"    SB(Preference = 1, NumSuccessfulPolls[0] = 0, NumSuccessfulPolls[1] = 1, SF(Confidence = 1, Finalized = false, SL(Preference = 1))) Bit = 2\n" +
			"        SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [3, 256)\n" +
			"        SB(NumSuccessfulPolls = 1, SF(Confidence = 1, Finalized = true)) Bits = [3, 256)\n" +
			"    SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [1, 256)"
		require.Equal(expected, tree.String())
		require.Equal(c0010, tree.Preference())
		require.False(tree.Finalized())
	}
}

func TestSnowballResetChild(t *testing.T) {
	require := require.New(t)

	c0000 := ids.ID{0x00} // 0000
	c0100 := ids.ID{0x02} // 0100
	c1000 := ids.ID{0x01} // 1000

	params := Parameters{
		K: 1, Alpha: 1, BetaVirtuous: 1, BetaRogue: 2,
	}
	tree := Tree{}
	tree.Initialize(params, c0000)
	tree.Add(c0100)
	tree.Add(c1000)

	require.Equal(c0000, tree.Preference())
	require.False(tree.Finalized())

	c0000Bag := ids.Bag{}
	c0000Bag.Add(c0000)
	require.True(tree.RecordPoll(c0000Bag))

	{
		expected := "SB(Preference = 0, NumSuccessfulPolls[0] = 1, NumSuccessfulPolls[1] = 0, SF(Confidence = 1, Finalized = false, SL(Preference = 0))) Bit = 0\n" +
			"    SB(Preference = 0, NumSuccessfulPolls[0] = 1, NumSuccessfulPolls[1] = 0, SF(Confidence = 1, Finalized = false, SL(Preference = 0))) Bit = 1\n" +
			"        SB(NumSuccessfulPolls = 1, SF(Confidence = 1, Finalized = true)) Bits = [2, 256)\n" +
			"        SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [2, 256)\n" +
			"    SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [1, 256)"
		require.Equal(expected, tree.String())
		require.Equal(c0000, tree.Preference())
		require.False(tree.Finalized())
	}

	emptyBag := ids.Bag{}
	require.False(tree.RecordPoll(emptyBag))

	{
		expected := "SB(Preference = 0, NumSuccessfulPolls[0] = 1, NumSuccessfulPolls[1] = 0, SF(Confidence = 0, Finalized = false, SL(Preference = 0))) Bit = 0\n" +
			"    SB(Preference = 0, NumSuccessfulPolls[0] = 1, NumSuccessfulPolls[1] = 0, SF(Confidence = 1, Finalized = false, SL(Preference = 0))) Bit = 1\n" +
			"        SB(NumSuccessfulPolls = 1, SF(Confidence = 1, Finalized = true)) Bits = [2, 256)\n" +
			"        SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [2, 256)\n" +
			"    SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [1, 256)"
		require.Equal(expected, tree.String())
		require.Equal(c0000, tree.Preference())
		require.False(tree.Finalized())
	}

	require.True(tree.RecordPoll(c0000Bag))

	{
		expected := "SB(Preference = 0, NumSuccessfulPolls[0] = 2, NumSuccessfulPolls[1] = 0, SF(Confidence = 1, Finalized = false, SL(Preference = 0))) Bit = 0\n" +
			"    SB(Preference = 0, NumSuccessfulPolls[0] = 2, NumSuccessfulPolls[1] = 0, SF(Confidence = 1, Finalized = false, SL(Preference = 0))) Bit = 1\n" +
			"        SB(NumSuccessfulPolls = 2, SF(Confidence = 1, Finalized = true)) Bits = [2, 256)\n" +
			"        SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [2, 256)\n" +
			"    SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [1, 256)"
		require.Equal(expected, tree.String())
		require.Equal(c0000, tree.Preference())
		require.False(tree.Finalized())
	}
}

func TestSnowballResetSibling(t *testing.T) {
	require := require.New(t)

	c0000 := ids.ID{0x00} // 0000
	c0100 := ids.ID{0x02} // 0100
	c1000 := ids.ID{0x01} // 1000

	params := Parameters{
		K: 1, Alpha: 1, BetaVirtuous: 1, BetaRogue: 2,
	}
	tree := Tree{}
	tree.Initialize(params, c0000)
	tree.Add(c0100)
	tree.Add(c1000)

	require.Equal(c0000, tree.Preference())
	require.False(tree.Finalized())

	c0100Bag := ids.Bag{}
	c0100Bag.Add(c0100)
	require.True(tree.RecordPoll(c0100Bag))

	{
		expected := "SB(Preference = 0, NumSuccessfulPolls[0] = 1, NumSuccessfulPolls[1] = 0, SF(Confidence = 1, Finalized = false, SL(Preference = 0))) Bit = 0\n" +
			"    SB(Preference = 1, NumSuccessfulPolls[0] = 0, NumSuccessfulPolls[1] = 1, SF(Confidence = 1, Finalized = false, SL(Preference = 1))) Bit = 1\n" +
			"        SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [2, 256)\n" +
			"        SB(NumSuccessfulPolls = 1, SF(Confidence = 1, Finalized = true)) Bits = [2, 256)\n" +
			"    SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [1, 256)"
		require.Equal(expected, tree.String())
		require.Equal(c0100, tree.Preference())
		require.False(tree.Finalized())
	}

	c1000Bag := ids.Bag{}
	c1000Bag.Add(c1000)
	require.True(tree.RecordPoll(c1000Bag))

	{
		expected := "SB(Preference = 0, NumSuccessfulPolls[0] = 1, NumSuccessfulPolls[1] = 1, SF(Confidence = 1, Finalized = false, SL(Preference = 1))) Bit = 0\n" +
			"    SB(Preference = 1, NumSuccessfulPolls[0] = 0, NumSuccessfulPolls[1] = 1, SF(Confidence = 1, Finalized = false, SL(Preference = 1))) Bit = 1\n" +
			"        SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [2, 256)\n" +
			"        SB(NumSuccessfulPolls = 1, SF(Confidence = 1, Finalized = true)) Bits = [2, 256)\n" +
			"    SB(NumSuccessfulPolls = 1, SF(Confidence = 1, Finalized = true)) Bits = [1, 256)"
		require.Equal(expected, tree.String())
		require.Equal(c0100, tree.Preference())
		require.False(tree.Finalized())
	}

	require.True(tree.RecordPoll(c0100Bag))

	{
		expected := "SB(Preference = 0, NumSuccessfulPolls[0] = 2, NumSuccessfulPolls[1] = 1, SF(Confidence = 1, Finalized = false, SL(Preference = 0))) Bit = 0\n" +
			"    SB(Preference = 1, NumSuccessfulPolls[0] = 0, NumSuccessfulPolls[1] = 2, SF(Confidence = 1, Finalized = false, SL(Preference = 1))) Bit = 1\n" +
			"        SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [2, 256)\n" +
			"        SB(NumSuccessfulPolls = 2, SF(Confidence = 1, Finalized = true)) Bits = [2, 256)\n" +
			"    SB(NumSuccessfulPolls = 1, SF(Confidence = 1, Finalized = true)) Bits = [1, 256)"
		require.Equal(expected, tree.String())
		require.Equal(c0100, tree.Preference())
		require.False(tree.Finalized())
	}
}

func TestSnowball5Colors(t *testing.T) {
	require := require.New(t)

	numColors := 5
	params := Parameters{
		K: 5, Alpha: 5, BetaVirtuous: 20, BetaRogue: 30,
	}

	colors := []ids.ID{}
	for i := 0; i < numColors; i++ {
		colors = append(colors, ids.Empty.Prefix(uint64(i)))
	}

	tree0 := Tree{}
	tree0.Initialize(params, colors[4])

	tree0.Add(colors[0])
	tree0.Add(colors[1])
	tree0.Add(colors[2])
	tree0.Add(colors[3])

	tree1 := Tree{}
	tree1.Initialize(params, colors[3])

	tree1.Add(colors[0])
	tree1.Add(colors[1])
	tree1.Add(colors[2])
	tree1.Add(colors[4])

	s1 := tree0.String()
	s2 := tree1.String()
	require.Equal(strings.Count(s1, "    "), strings.Count(s2, "    "))
}

func TestSnowballFineGrained(t *testing.T) {
	require := require.New(t)

	c0000 := ids.ID{0x00}
	c1000 := ids.ID{0x01}
	c1100 := ids.ID{0x03}
	c0010 := ids.ID{0x04}

	params := Parameters{
		K: 1, Alpha: 1, BetaVirtuous: 1, BetaRogue: 2,
	}
	tree := Tree{}
	tree.Initialize(params, c0000)

	require.Equal(initialUnaryDescription, tree.String())
	require.Equal(c0000, tree.Preference())
	require.False(tree.Finalized())

	tree.Add(c1100)

	{
		expected := "SB(Preference = 0, NumSuccessfulPolls[0] = 0, NumSuccessfulPolls[1] = 0, SF(Confidence = 0, Finalized = false, SL(Preference = 0))) Bit = 0\n" +
			"    SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [1, 256)\n" +
			"    SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [1, 256)"
		require.Equal(expected, tree.String())
		require.Equal(c0000, tree.Preference())
		require.False(tree.Finalized())
	}

	tree.Add(c1000)

	{
		expected := "SB(Preference = 0, NumSuccessfulPolls[0] = 0, NumSuccessfulPolls[1] = 0, SF(Confidence = 0, Finalized = false, SL(Preference = 0))) Bit = 0\n" +
			"    SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [1, 256)\n" +
			"    SB(Preference = 1, NumSuccessfulPolls[0] = 0, NumSuccessfulPolls[1] = 0, SF(Confidence = 0, Finalized = false, SL(Preference = 1))) Bit = 1\n" +
			"        SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [2, 256)\n" +
			"        SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [2, 256)"
		require.Equal(expected, tree.String())
		require.Equal(c0000, tree.Preference())
		require.False(tree.Finalized())
	}

	tree.Add(c0010)

	{
		expected := "SB(Preference = 0, NumSuccessfulPolls[0] = 0, NumSuccessfulPolls[1] = 0, SF(Confidence = 0, Finalized = false, SL(Preference = 0))) Bit = 0\n" +
			"    SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [1, 2)\n" +
			"        SB(Preference = 0, NumSuccessfulPolls[0] = 0, NumSuccessfulPolls[1] = 0, SF(Confidence = 0, Finalized = false, SL(Preference = 0))) Bit = 2\n" +
			"            SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [3, 256)\n" +
			"            SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [3, 256)\n" +
			"    SB(Preference = 1, NumSuccessfulPolls[0] = 0, NumSuccessfulPolls[1] = 0, SF(Confidence = 0, Finalized = false, SL(Preference = 1))) Bit = 1\n" +
			"        SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [2, 256)\n" +
			"        SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [2, 256)"
		require.Equal(expected, tree.String())
		require.Equal(c0000, tree.Preference())
		require.False(tree.Finalized())
	}

	c0000Bag := ids.Bag{}
	c0000Bag.Add(c0000)
	require.True(tree.RecordPoll(c0000Bag))

	{
		expected := "SB(Preference = 0, NumSuccessfulPolls[0] = 1, NumSuccessfulPolls[1] = 0, SF(Confidence = 1, Finalized = false, SL(Preference = 0))) Bit = 0\n" +
			"    SB(Preference = 0, NumSuccessfulPolls[0] = 1, NumSuccessfulPolls[1] = 0, SF(Confidence = 1, Finalized = false, SL(Preference = 0))) Bit = 2\n" +
			"        SB(NumSuccessfulPolls = 1, SF(Confidence = 1, Finalized = true)) Bits = [3, 256)\n" +
			"        SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [3, 256)\n" +
			"    SB(Preference = 1, NumSuccessfulPolls[0] = 0, NumSuccessfulPolls[1] = 0, SF(Confidence = 0, Finalized = false, SL(Preference = 1))) Bit = 1\n" +
			"        SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [2, 256)\n" +
			"        SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [2, 256)"
		require.Equal(expected, tree.String())
		require.Equal(c0000, tree.Preference())
		require.False(tree.Finalized())
	}

	c0010Bag := ids.Bag{}
	c0010Bag.Add(c0010)
	require.True(tree.RecordPoll(c0010Bag))

	{
		expected := "SB(Preference = 0, NumSuccessfulPolls[0] = 1, NumSuccessfulPolls[1] = 1, SF(Confidence = 1, Finalized = false, SL(Preference = 1))) Bit = 2\n" +
			"    SB(NumSuccessfulPolls = 1, SF(Confidence = 1, Finalized = true)) Bits = [3, 256)\n" +
			"    SB(NumSuccessfulPolls = 1, SF(Confidence = 1, Finalized = true)) Bits = [3, 256)"
		require.Equal(expected, tree.String())
		require.Equal(c0000, tree.Preference())
		require.False(tree.Finalized())
	}

	require.True(tree.RecordPoll(c0010Bag))

	{
		expected := "SB(NumSuccessfulPolls = 2, SF(Confidence = 2, Finalized = true)) Bits = [3, 256)"
		require.Equal(expected, tree.String())
		require.Equal(c0010, tree.Preference())
		require.True(tree.Finalized())
	}
}

func TestSnowballDoubleAdd(t *testing.T) {
	require := require.New(t)

	params := Parameters{
		K: 1, Alpha: 1, BetaVirtuous: 3, BetaRogue: 5,
	}
	tree := Tree{}
	tree.Initialize(params, Red)
	tree.Add(Red)

	require.Equal(initialUnaryDescription, tree.String())
	require.Equal(Red, tree.Preference())
	require.False(tree.Finalized())
}

func TestSnowballConsistent(t *testing.T) {
	require := require.New(t)

	numColors := 50
	numNodes := 100
	params := Parameters{
		K: 20, Alpha: 15, BetaVirtuous: 20, BetaRogue: 30,
	}
	seed := int64(0)

	sampler.Seed(seed)

	n := Network{}
	n.Initialize(params, numColors)

	for i := 0; i < numNodes; i++ {
		n.AddNode(&Tree{})
	}

	for !n.Finalized() && !n.Disagreement() {
		n.Round()
	}

	require.True(n.Agreement())
}

func TestSnowballFilterBinaryChildren(t *testing.T) {
	require := require.New(t)

	c0000 := ids.ID{0b00000000}
	c1000 := ids.ID{0b00000001}
	c0100 := ids.ID{0b00000010}
	c0010 := ids.ID{0b00000100}

	params := Parameters{
		K: 1, Alpha: 1, BetaVirtuous: 1, BetaRogue: 2,
	}
	tree := Tree{}
	tree.Initialize(params, c0000)

	require.Equal(initialUnaryDescription, tree.String())
	require.Equal(c0000, tree.Preference())
	require.False(tree.Finalized())

	tree.Add(c1000)

	{
		expected := "SB(Preference = 0, NumSuccessfulPolls[0] = 0, NumSuccessfulPolls[1] = 0, SF(Confidence = 0, Finalized = false, SL(Preference = 0))) Bit = 0\n" +
			"    SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [1, 256)\n" +
			"    SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [1, 256)"
		require.Equal(expected, tree.String())
		require.Equal(c0000, tree.Preference())
		require.False(tree.Finalized())
	}

	tree.Add(c0010)

	{
		expected := "SB(Preference = 0, NumSuccessfulPolls[0] = 0, NumSuccessfulPolls[1] = 0, SF(Confidence = 0, Finalized = false, SL(Preference = 0))) Bit = 0\n" +
			"    SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [1, 2)\n" +
			"        SB(Preference = 0, NumSuccessfulPolls[0] = 0, NumSuccessfulPolls[1] = 0, SF(Confidence = 0, Finalized = false, SL(Preference = 0))) Bit = 2\n" +
			"            SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [3, 256)\n" +
			"            SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [3, 256)\n" +
			"    SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [1, 256)"
		require.Equal(expected, tree.String())
		require.Equal(c0000, tree.Preference())
		require.False(tree.Finalized())
	}

	c0000Bag := ids.Bag{}
	c0000Bag.Add(c0000)
	require.True(tree.RecordPoll(c0000Bag))

	{
		expected := "SB(Preference = 0, NumSuccessfulPolls[0] = 1, NumSuccessfulPolls[1] = 0, SF(Confidence = 1, Finalized = false, SL(Preference = 0))) Bit = 0\n" +
			"    SB(Preference = 0, NumSuccessfulPolls[0] = 1, NumSuccessfulPolls[1] = 0, SF(Confidence = 1, Finalized = false, SL(Preference = 0))) Bit = 2\n" +
			"        SB(NumSuccessfulPolls = 1, SF(Confidence = 1, Finalized = true)) Bits = [3, 256)\n" +
			"        SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [3, 256)\n" +
			"    SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [1, 256)"
		require.Equal(expected, tree.String())
		require.Equal(c0000, tree.Preference())
		require.False(tree.Finalized())
	}

	tree.Add(c0100)

	{
		expected := "SB(Preference = 0, NumSuccessfulPolls[0] = 1, NumSuccessfulPolls[1] = 0, SF(Confidence = 1, Finalized = false, SL(Preference = 0))) Bit = 0\n" +
			"    SB(Preference = 0, NumSuccessfulPolls[0] = 1, NumSuccessfulPolls[1] = 0, SF(Confidence = 1, Finalized = false, SL(Preference = 0))) Bit = 2\n" +
			"        SB(NumSuccessfulPolls = 1, SF(Confidence = 1, Finalized = true)) Bits = [3, 256)\n" +
			"        SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [3, 256)\n" +
			"    SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [1, 256)"
		require.Equal(expected, tree.String())
		require.Equal(c0000, tree.Preference())
		require.False(tree.Finalized())
	}

	c0100Bag := ids.Bag{}
	c0100Bag.Add(c0100)
	require.True(tree.RecordPoll(c0100Bag))

	{
		expected := "SB(Preference = 0, NumSuccessfulPolls[0] = 1, NumSuccessfulPolls[1] = 0, SF(Confidence = 0, Finalized = false, SL(Preference = 0))) Bit = 2\n" +
			"    SB(NumSuccessfulPolls = 1, SF(Confidence = 1, Finalized = true)) Bits = [3, 256)\n" +
			"    SB(NumSuccessfulPolls = 0, SF(Confidence = 0, Finalized = false)) Bits = [3, 256)"
		require.Equal(expected, tree.String())
		require.Equal(c0000, tree.Preference())
		require.False(tree.Finalized())
	}
}

```

avalanchego/snow/consensus/snowball/unary_snowball.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowball

import (
	"fmt"
)

var _ UnarySnowball = &unarySnowball{}

// unarySnowball is the implementation of a unary snowball instance
type unarySnowball struct {
	// wrap the unary snowflake logic
	unarySnowflake

	// numSuccessfulPolls tracks the total number of successful network polls
	numSuccessfulPolls int
}

func (sb *unarySnowball) RecordSuccessfulPoll() {
	sb.numSuccessfulPolls++
	sb.unarySnowflake.RecordSuccessfulPoll()
}

func (sb *unarySnowball) Extend(beta int, choice int) BinarySnowball {
	bs := &binarySnowball{
		binarySnowflake: binarySnowflake{
			binarySlush: binarySlush{preference: choice},
			confidence:  sb.confidence,
			beta:        beta,
			finalized:   sb.Finalized(),
		},
		preference: choice,
	}
	bs.numSuccessfulPolls[choice] = sb.numSuccessfulPolls
	return bs
}

func (sb *unarySnowball) Clone() UnarySnowball {
	newSnowball := *sb
	return &newSnowball
}

func (sb *unarySnowball) String() string {
	return fmt.Sprintf("SB(NumSuccessfulPolls = %d, %s)",
		sb.numSuccessfulPolls,
		&sb.unarySnowflake)
}

```

avalanchego/snow/consensus/snowball/unary_snowball_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowball

import (
	"testing"
)

func UnarySnowballStateTest(t *testing.T, sb *unarySnowball, expectedNumSuccessfulPolls, expectedConfidence int, expectedFinalized bool) {
	if numSuccessfulPolls := sb.numSuccessfulPolls; numSuccessfulPolls != expectedNumSuccessfulPolls {
		t.Fatalf("Wrong numSuccessfulPolls. Expected %d got %d", expectedNumSuccessfulPolls, numSuccessfulPolls)
	} else if confidence := sb.confidence; confidence != expectedConfidence {
		t.Fatalf("Wrong confidence. Expected %d got %d", expectedConfidence, confidence)
	} else if finalized := sb.Finalized(); finalized != expectedFinalized {
		t.Fatalf("Wrong finalized status. Expected %v got %v", expectedFinalized, finalized)
	}
}

func TestUnarySnowball(t *testing.T) {
	beta := 2

	sb := &unarySnowball{}
	sb.Initialize(beta)

	sb.RecordSuccessfulPoll()
	UnarySnowballStateTest(t, sb, 1, 1, false)

	sb.RecordUnsuccessfulPoll()
	UnarySnowballStateTest(t, sb, 1, 0, false)

	sb.RecordSuccessfulPoll()
	UnarySnowballStateTest(t, sb, 2, 1, false)

	sbCloneIntf := sb.Clone()
	sbClone, ok := sbCloneIntf.(*unarySnowball)
	if !ok {
		t.Fatalf("Unexpected clone type")
	}

	UnarySnowballStateTest(t, sbClone, 2, 1, false)

	binarySnowball := sbClone.Extend(beta, 0)

	expected := "SB(Preference = 0, NumSuccessfulPolls[0] = 2, NumSuccessfulPolls[1] = 0, SF(Confidence = 1, Finalized = false, SL(Preference = 0)))"
	if result := binarySnowball.String(); result != expected {
		t.Fatalf("Expected:\n%s\nReturned:\n%s", expected, result)
	}

	binarySnowball.RecordUnsuccessfulPoll()
	for i := 0; i < 3; i++ {
		if binarySnowball.Preference() != 0 {
			t.Fatalf("Wrong preference")
		} else if binarySnowball.Finalized() {
			t.Fatalf("Should not have finalized")
		}
		binarySnowball.RecordSuccessfulPoll(1)
		binarySnowball.RecordUnsuccessfulPoll()
	}

	if binarySnowball.Preference() != 1 {
		t.Fatalf("Wrong preference")
	} else if binarySnowball.Finalized() {
		t.Fatalf("Should not have finalized")
	}

	binarySnowball.RecordSuccessfulPoll(1)
	if binarySnowball.Preference() != 1 {
		t.Fatalf("Wrong preference")
	} else if binarySnowball.Finalized() {
		t.Fatalf("Should not have finalized")
	}

	binarySnowball.RecordSuccessfulPoll(1)

	if binarySnowball.Preference() != 1 {
		t.Fatalf("Wrong preference")
	} else if !binarySnowball.Finalized() {
		t.Fatalf("Should have finalized")
	}

	expected = "SB(NumSuccessfulPolls = 2, SF(Confidence = 1, Finalized = false))"
	if str := sb.String(); str != expected {
		t.Fatalf("Wrong state. Expected:\n%s\nGot:\n%s", expected, str)
	}
}

```

avalanchego/snow/consensus/snowball/unary_snowflake.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowball

import (
	"fmt"
)

var _ UnarySnowflake = &unarySnowflake{}

// unarySnowflake is the implementation of a unary snowflake instance
type unarySnowflake struct {
	// beta is the number of consecutive successful queries required for
	// finalization.
	beta int

	// confidence tracks the number of successful polls in a row that have
	// returned the preference
	confidence int

	// finalized prevents the state from changing after the required number of
	// consecutive polls has been reached
	finalized bool
}

func (sf *unarySnowflake) Initialize(beta int) { sf.beta = beta }

func (sf *unarySnowflake) RecordSuccessfulPoll() {
	sf.confidence++
	sf.finalized = sf.finalized || sf.confidence >= sf.beta
}

func (sf *unarySnowflake) RecordUnsuccessfulPoll() { sf.confidence = 0 }

func (sf *unarySnowflake) Finalized() bool { return sf.finalized }

func (sf *unarySnowflake) Extend(beta int, choice int) BinarySnowflake {
	return &binarySnowflake{
		binarySlush: binarySlush{preference: choice},
		confidence:  sf.confidence,
		beta:        beta,
		finalized:   sf.finalized,
	}
}

func (sf *unarySnowflake) Clone() UnarySnowflake {
	newSnowflake := *sf
	return &newSnowflake
}

func (sf *unarySnowflake) String() string {
	return fmt.Sprintf("SF(Confidence = %d, Finalized = %v)",
		sf.confidence,
		sf.finalized)
}

```

avalanchego/snow/consensus/snowball/unary_snowflake_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowball

import (
	"testing"
)

func UnarySnowflakeStateTest(t *testing.T, sf *unarySnowflake, expectedConfidence int, expectedFinalized bool) {
	if confidence := sf.confidence; confidence != expectedConfidence {
		t.Fatalf("Wrong confidence. Expected %d got %d", expectedConfidence, confidence)
	} else if finalized := sf.Finalized(); finalized != expectedFinalized {
		t.Fatalf("Wrong finalized status. Expected %v got %v", expectedFinalized, finalized)
	}
}

func TestUnarySnowflake(t *testing.T) {
	beta := 2

	sf := &unarySnowflake{}
	sf.Initialize(beta)

	sf.RecordSuccessfulPoll()
	UnarySnowflakeStateTest(t, sf, 1, false)

	sf.RecordUnsuccessfulPoll()
	UnarySnowflakeStateTest(t, sf, 0, false)

	sf.RecordSuccessfulPoll()
	UnarySnowflakeStateTest(t, sf, 1, false)

	sfCloneIntf := sf.Clone()
	sfClone, ok := sfCloneIntf.(*unarySnowflake)
	if !ok {
		t.Fatalf("Unexpected clone type")
	}

	UnarySnowflakeStateTest(t, sfClone, 1, false)

	binarySnowflake := sfClone.Extend(beta, 0)

	binarySnowflake.RecordUnsuccessfulPoll()

	binarySnowflake.RecordSuccessfulPoll(1)

	if binarySnowflake.Finalized() {
		t.Fatalf("Should not have finalized")
	}

	binarySnowflake.RecordSuccessfulPoll(1)

	if binarySnowflake.Preference() != 1 {
		t.Fatalf("Wrong preference")
	} else if !binarySnowflake.Finalized() {
		t.Fatalf("Should have finalized")
	}

	sf.RecordSuccessfulPoll()
	UnarySnowflakeStateTest(t, sf, 2, true)

	sf.RecordUnsuccessfulPoll()
	UnarySnowflakeStateTest(t, sf, 0, true)

	sf.RecordSuccessfulPoll()
	UnarySnowflakeStateTest(t, sf, 1, true)
}

```

avalanchego/snow/consensus/snowman/block.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowman

import (
	"time"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/choices"
)

// Block is a possible decision that dictates the next canonical block.
//
// Blocks are guaranteed to be Verified, Accepted, and Rejected in topological
// order. Specifically, if Verify is called, then the parent has already been
// verified. If Accept is called, then the parent has already been accepted. If
// Reject is called, the parent has already been accepted or rejected.
//
// If the status of the block is Unknown, ID is assumed to be able to be called.
// If the status of the block is Accepted or Rejected; Parent, Verify, Accept,
// and Reject will never be called.
type Block interface {
	choices.Decidable

	// Parent returns the ID of this block's parent.
	Parent() ids.ID

	// Verify that the state transition this block would make if accepted is
	// valid. If the state transition is invalid, a non-nil error should be
	// returned.
	//
	// It is guaranteed that the Parent has been successfully verified.
	//
	// If nil is returned, it is guaranteed that either Accept or Reject will be
	// called on this block, unless the VM is shut down.
	Verify() error

	// Bytes returns the binary representation of this block.
	//
	// This is used for sending blocks to peers. The bytes should be able to be
	// parsed into the same block on another node.
	Bytes() []byte

	// Height returns the height of this block in the chain.
	Height() uint64

	// Time this block was proposed at. This value should be consistent across
	// all nodes. If this block hasn't been successfully verified, any value can
	// be returned. If this block is the last accepted block, the timestamp must
	// be returned correctly. Otherwise, accepted blocks can return any value.
	Timestamp() time.Time
}

```

avalanchego/snow/consensus/snowman/consensus.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowman

import (
	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow"
	"github.com/ava-labs/avalanchego/snow/consensus/snowball"
)

// Consensus represents a general snowman instance that can be used directly to
// process a series of dependent operations.
type Consensus interface {
	// Takes in the context, snowball parameters, and the last accepted block.
	Initialize(
		ctx *snow.ConsensusContext,
		params snowball.Parameters,
		lastAcceptedID ids.ID,
		lastAcceptedHeight uint64,
	) error

	// Returns the parameters that describe this snowman instance
	Parameters() snowball.Parameters

	// Returns the number of blocks processing
	NumProcessing() int

	// Adds a new decision. Assumes the dependency has already been added.
	// Returns if a critical error has occurred.
	Add(Block) error

	// Decided returns true if the block has been decided.
	Decided(Block) bool

	// Processing returns true if the block ID is currently processing.
	Processing(ids.ID) bool

	// IsPreferred returns true if the block is currently on the preferred
	// chain.
	IsPreferred(Block) bool

	// Returns the ID of the tail of the strongly preferred sequence of
	// decisions.
	Preference() ids.ID

	// RecordPoll collects the results of a network poll. Assumes all decisions
	// have been previously added. Returns if a critical error has occurred.
	RecordPoll(ids.Bag) error

	// Finalized returns true if all decisions that have been added have been
	// finalized. Note, it is possible that after returning finalized, a new
	// decision may be added such that this instance is no longer finalized.
	Finalized() bool

	// HealthCheck returns information about the consensus health.
	HealthCheck() (interface{}, error)
}

```

avalanchego/snow/consensus/snowman/consensus_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowman

import (
	"errors"
	"path"
	"reflect"
	"runtime"
	"strings"
	"testing"

	"github.com/prometheus/client_golang/prometheus"

	"github.com/stretchr/testify/require"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow"
	"github.com/ava-labs/avalanchego/snow/choices"
	"github.com/ava-labs/avalanchego/snow/consensus/snowball"
	"github.com/ava-labs/avalanchego/utils/sampler"
)

type testFunc func(*testing.T, Factory)

var (
	GenesisID     = ids.Empty.Prefix(0)
	GenesisHeight = uint64(0)
	Genesis       = &TestBlock{TestDecidable: choices.TestDecidable{
		IDV:     GenesisID,
		StatusV: choices.Accepted,
	}}

	testFuncs = []testFunc{
		InitializeTest,
		NumProcessingTest,
		AddToTailTest,
		AddToNonTailTest,
		AddToUnknownTest,
		StatusOrProcessingPreviouslyAcceptedTest,
		StatusOrProcessingPreviouslyRejectedTest,
		StatusOrProcessingUnissuedTest,
		StatusOrProcessingIssuedTest,
		RecordPollAcceptSingleBlockTest,
		RecordPollAcceptAndRejectTest,
		RecordPollSplitVoteNoChangeTest,
		RecordPollWhenFinalizedTest,
		RecordPollRejectTransitivelyTest,
		RecordPollTransitivelyResetConfidenceTest,
		RecordPollInvalidVoteTest,
		RecordPollTransitiveVotingTest,
		RecordPollDivergedVotingTest,
		RecordPollDivergedVotingWithNoConflictingBitTest,
		RecordPollChangePreferredChainTest,
		MetricsProcessingErrorTest,
		MetricsAcceptedErrorTest,
		MetricsRejectedErrorTest,
		ErrorOnInitialRejectionTest,
		ErrorOnAcceptTest,
		ErrorOnRejectSiblingTest,
		ErrorOnTransitiveRejectionTest,
		RandomizedConsistencyTest,
		ErrorOnAddDecidedBlock,
		ErrorOnAddDuplicateBlockID,
	}
)

// Execute all tests against a consensus implementation
func runConsensusTests(t *testing.T, factory Factory) {
	for _, test := range testFuncs {
		t.Run(getTestName(test), func(tt *testing.T) {
			test(tt, factory)
		})
	}
}

func getTestName(i interface{}) string {
	return strings.Split(path.Base(runtime.FuncForPC(reflect.ValueOf(i).Pointer()).Name()), ".")[1]
}

// Make sure that initialize sets the state correctly
func InitializeTest(t *testing.T, factory Factory) {
	sm := factory.New()

	ctx := snow.DefaultConsensusContextTest()
	params := snowball.Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          3,
		BetaRogue:             5,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}

	if err := sm.Initialize(ctx, params, GenesisID, GenesisHeight); err != nil {
		t.Fatal(err)
	}

	if p := sm.Parameters(); p != params {
		t.Fatalf("Wrong returned parameters")
	} else if pref := sm.Preference(); pref != GenesisID {
		t.Fatalf("Wrong preference returned")
	} else if !sm.Finalized() {
		t.Fatalf("Wrong should have marked the instance as being finalized")
	}
}

// Make sure that the number of processing blocks is tracked correctly
func NumProcessingTest(t *testing.T, factory Factory) {
	sm := factory.New()

	ctx := snow.DefaultConsensusContextTest()
	params := snowball.Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          1,
		BetaRogue:             1,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}
	if err := sm.Initialize(ctx, params, GenesisID, GenesisHeight); err != nil {
		t.Fatal(err)
	}

	block := &TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.Empty.Prefix(1),
			StatusV: choices.Processing,
		},
		ParentV: Genesis.IDV,
		HeightV: Genesis.HeightV + 1,
	}

	if numProcessing := sm.NumProcessing(); numProcessing != 0 {
		t.Fatalf("expected %d blocks to be processing but returned %d", 0, numProcessing)
	}

	// Adding to the previous preference will update the preference
	if err := sm.Add(block); err != nil {
		t.Fatal(err)
	}

	if numProcessing := sm.NumProcessing(); numProcessing != 1 {
		t.Fatalf("expected %d blocks to be processing but returned %d", 1, numProcessing)
	}

	votes := ids.Bag{}
	votes.Add(block.ID())
	if err := sm.RecordPoll(votes); err != nil {
		t.Fatal(err)
	}

	if numProcessing := sm.NumProcessing(); numProcessing != 0 {
		t.Fatalf("expected %d blocks to be processing but returned %d", 0, numProcessing)
	}
}

// Make sure that adding a block to the tail updates the preference
func AddToTailTest(t *testing.T, factory Factory) {
	sm := factory.New()

	ctx := snow.DefaultConsensusContextTest()
	params := snowball.Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          3,
		BetaRogue:             5,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}
	if err := sm.Initialize(ctx, params, GenesisID, GenesisHeight); err != nil {
		t.Fatal(err)
	}

	block := &TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.Empty.Prefix(1),
			StatusV: choices.Processing,
		},
		ParentV: Genesis.IDV,
		HeightV: Genesis.HeightV + 1,
	}

	// Adding to the previous preference will update the preference
	if err := sm.Add(block); err != nil {
		t.Fatal(err)
	} else if pref := sm.Preference(); pref != block.ID() {
		t.Fatalf("Wrong preference. Expected %s, got %s", block.ID(), pref)
	} else if !sm.IsPreferred(block) {
		t.Fatalf("Should have marked %s as being Preferred", pref)
	}
}

// Make sure that adding a block not to the tail doesn't change the preference
func AddToNonTailTest(t *testing.T, factory Factory) {
	sm := factory.New()

	ctx := snow.DefaultConsensusContextTest()
	params := snowball.Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          3,
		BetaRogue:             5,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}
	if err := sm.Initialize(ctx, params, GenesisID, GenesisHeight); err != nil {
		t.Fatal(err)
	}

	firstBlock := &TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.Empty.Prefix(1),
			StatusV: choices.Processing,
		},
		ParentV: Genesis.IDV,
		HeightV: Genesis.HeightV + 1,
	}
	secondBlock := &TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.Empty.Prefix(2),
			StatusV: choices.Processing,
		},
		ParentV: Genesis.IDV,
		HeightV: Genesis.HeightV + 1,
	}

	// Adding to the previous preference will update the preference
	if err := sm.Add(firstBlock); err != nil {
		t.Fatal(err)
	} else if pref := sm.Preference(); pref != firstBlock.IDV {
		t.Fatalf("Wrong preference. Expected %s, got %s", firstBlock.IDV, pref)
	}

	// Adding to something other than the previous preference won't update the
	// preference
	if err := sm.Add(secondBlock); err != nil {
		t.Fatal(err)
	} else if pref := sm.Preference(); pref != firstBlock.IDV {
		t.Fatalf("Wrong preference. Expected %s, got %s", firstBlock.IDV, pref)
	}
}

// Make sure that adding a block that is detached from the rest of the tree
// rejects the block
func AddToUnknownTest(t *testing.T, factory Factory) {
	sm := factory.New()

	ctx := snow.DefaultConsensusContextTest()
	params := snowball.Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          3,
		BetaRogue:             5,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}
	if err := sm.Initialize(ctx, params, GenesisID, GenesisHeight); err != nil {
		t.Fatal(err)
	}

	parent := &TestBlock{TestDecidable: choices.TestDecidable{
		IDV:     ids.Empty.Prefix(1),
		StatusV: choices.Unknown,
	}}

	block := &TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.Empty.Prefix(2),
			StatusV: choices.Processing,
		},
		ParentV: parent.IDV,
		HeightV: parent.HeightV + 1,
	}

	// Adding a block with an unknown parent means the parent must have already
	// been rejected. Therefore the block should be immediately rejected
	if err := sm.Add(block); err != nil {
		t.Fatal(err)
	} else if pref := sm.Preference(); pref != GenesisID {
		t.Fatalf("Wrong preference. Expected %s, got %s", GenesisID, pref)
	} else if status := block.Status(); status != choices.Rejected {
		t.Fatalf("Should have rejected the block")
	}
}

func StatusOrProcessingPreviouslyAcceptedTest(t *testing.T, factory Factory) {
	sm := factory.New()

	ctx := snow.DefaultConsensusContextTest()
	params := snowball.Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          3,
		BetaRogue:             5,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}
	if err := sm.Initialize(ctx, params, GenesisID, GenesisHeight); err != nil {
		t.Fatal(err)
	}

	if Genesis.Status() != choices.Accepted {
		t.Fatalf("Should have marked an accepted block as having been accepted")
	}
	if sm.Processing(Genesis.ID()) {
		t.Fatalf("Shouldn't have marked an accepted block as having been processing")
	}
	if !sm.Decided(Genesis) {
		t.Fatalf("Should have marked an accepted block as having been decided")
	}
	if !sm.IsPreferred(Genesis) {
		t.Fatalf("Should have marked an accepted block as being preferred")
	}
}

func StatusOrProcessingPreviouslyRejectedTest(t *testing.T, factory Factory) {
	sm := factory.New()

	ctx := snow.DefaultConsensusContextTest()
	params := snowball.Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          3,
		BetaRogue:             5,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}
	if err := sm.Initialize(ctx, params, GenesisID, GenesisHeight); err != nil {
		t.Fatal(err)
	}

	block := &TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.Empty.Prefix(1),
			StatusV: choices.Rejected,
		},
		ParentV: Genesis.IDV,
		HeightV: Genesis.HeightV + 1,
	}

	if block.Status() == choices.Accepted {
		t.Fatalf("Shouldn't have marked a rejected block as having been accepted")
	}
	if sm.Processing(block.ID()) {
		t.Fatalf("Shouldn't have marked a rejected block as having been processing")
	}
	if !sm.Decided(block) {
		t.Fatalf("Should have marked a rejected block as having been decided")
	}
	if sm.IsPreferred(block) {
		t.Fatalf("Shouldn't have marked a rejected block as being preferred")
	}
}

func StatusOrProcessingUnissuedTest(t *testing.T, factory Factory) {
	sm := factory.New()

	ctx := snow.DefaultConsensusContextTest()
	params := snowball.Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          3,
		BetaRogue:             5,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}
	if err := sm.Initialize(ctx, params, GenesisID, GenesisHeight); err != nil {
		t.Fatal(err)
	}

	block := &TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.Empty.Prefix(1),
			StatusV: choices.Processing,
		},
		ParentV: Genesis.IDV,
		HeightV: Genesis.HeightV + 1,
	}

	if block.Status() == choices.Accepted {
		t.Fatalf("Shouldn't have marked an unissued block as having been accepted")
	}
	if sm.Processing(block.ID()) {
		t.Fatalf("Shouldn't have marked an unissued block as having been processing")
	}
	if sm.Decided(block) {
		t.Fatalf("Should't have marked an unissued block as having been decided")
	}
	if sm.IsPreferred(block) {
		t.Fatalf("Shouldn't have marked an unissued block as being preferred")
	}
}

func StatusOrProcessingIssuedTest(t *testing.T, factory Factory) {
	sm := factory.New()

	ctx := snow.DefaultConsensusContextTest()
	params := snowball.Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          3,
		BetaRogue:             5,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}
	if err := sm.Initialize(ctx, params, GenesisID, GenesisHeight); err != nil {
		t.Fatal(err)
	}

	block := &TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.Empty.Prefix(1),
			StatusV: choices.Processing,
		},
		ParentV: Genesis.IDV,
		HeightV: Genesis.HeightV + 1,
	}

	if err := sm.Add(block); err != nil {
		t.Fatal(err)
	}
	if block.Status() == choices.Accepted {
		t.Fatalf("Shouldn't have marked the block as accepted")
	}
	if !sm.Processing(block.ID()) {
		t.Fatalf("Should have marked the block as processing")
	}
	if sm.Decided(block) {
		t.Fatalf("Shouldn't have marked the block as decided")
	}
	if !sm.IsPreferred(block) {
		t.Fatalf("Should have marked the tail as being preferred")
	}
}

func RecordPollAcceptSingleBlockTest(t *testing.T, factory Factory) {
	sm := factory.New()

	ctx := snow.DefaultConsensusContextTest()
	params := snowball.Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          2,
		BetaRogue:             3,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}
	if err := sm.Initialize(ctx, params, GenesisID, GenesisHeight); err != nil {
		t.Fatal(err)
	}

	block := &TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.Empty.Prefix(1),
			StatusV: choices.Processing,
		},
		ParentV: Genesis.IDV,
		HeightV: Genesis.HeightV + 1,
	}

	if err := sm.Add(block); err != nil {
		t.Fatal(err)
	}

	votes := ids.Bag{}
	votes.Add(block.ID())
	if err := sm.RecordPoll(votes); err != nil {
		t.Fatal(err)
	} else if pref := sm.Preference(); pref != block.ID() {
		t.Fatalf("Preference returned the wrong block")
	} else if sm.Finalized() {
		t.Fatalf("Snowman instance finalized too soon")
	} else if status := block.Status(); status != choices.Processing {
		t.Fatalf("Block's status changed unexpectedly")
	} else if err := sm.RecordPoll(votes); err != nil {
		t.Fatal(err)
	} else if pref := sm.Preference(); pref != block.ID() {
		t.Fatalf("Preference returned the wrong block")
	} else if !sm.Finalized() {
		t.Fatalf("Snowman instance didn't finalize")
	} else if status := block.Status(); status != choices.Accepted {
		t.Fatalf("Block's status should have been set to accepted")
	}
}

func RecordPollAcceptAndRejectTest(t *testing.T, factory Factory) {
	sm := factory.New()

	ctx := snow.DefaultConsensusContextTest()
	params := snowball.Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          1,
		BetaRogue:             2,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}
	if err := sm.Initialize(ctx, params, GenesisID, GenesisHeight); err != nil {
		t.Fatal(err)
	}

	firstBlock := &TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.Empty.Prefix(1),
			StatusV: choices.Processing,
		},
		ParentV: Genesis.IDV,
		HeightV: Genesis.HeightV + 1,
	}
	secondBlock := &TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.Empty.Prefix(2),
			StatusV: choices.Processing,
		},
		ParentV: Genesis.IDV,
		HeightV: Genesis.HeightV + 1,
	}

	if err := sm.Add(firstBlock); err != nil {
		t.Fatal(err)
	} else if err := sm.Add(secondBlock); err != nil {
		t.Fatal(err)
	}

	votes := ids.Bag{}
	votes.Add(firstBlock.ID())

	if err := sm.RecordPoll(votes); err != nil {
		t.Fatal(err)
	} else if pref := sm.Preference(); pref != firstBlock.ID() {
		t.Fatalf("Preference returned the wrong block")
	} else if sm.Finalized() {
		t.Fatalf("Snowman instance finalized too soon")
	} else if status := firstBlock.Status(); status != choices.Processing {
		t.Fatalf("Block's status changed unexpectedly")
	} else if status := secondBlock.Status(); status != choices.Processing {
		t.Fatalf("Block's status changed unexpectedly")
	} else if err := sm.RecordPoll(votes); err != nil {
		t.Fatal(err)
	} else if pref := sm.Preference(); pref != firstBlock.ID() {
		t.Fatalf("Preference returned the wrong block")
	} else if !sm.Finalized() {
		t.Fatalf("Snowman instance didn't finalize")
	} else if status := firstBlock.Status(); status != choices.Accepted {
		t.Fatalf("Block's status should have been set to accepted")
	} else if status := secondBlock.Status(); status != choices.Rejected {
		t.Fatalf("Block's status should have been set to rejected")
	}
}

func RecordPollSplitVoteNoChangeTest(t *testing.T, factory Factory) {
	require := require.New(t)
	sm := factory.New()

	ctx := snow.DefaultConsensusContextTest()
	registerer := prometheus.NewRegistry()
	ctx.Registerer = registerer

	params := snowball.Parameters{
		K:                     2,
		Alpha:                 2,
		BetaVirtuous:          1,
		BetaRogue:             2,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}
	require.NoError(sm.Initialize(ctx, params, GenesisID, GenesisHeight))

	firstBlock := &TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.Empty.Prefix(1),
			StatusV: choices.Processing,
		},
		ParentV: Genesis.IDV,
		HeightV: Genesis.HeightV + 1,
	}
	secondBlock := &TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.Empty.Prefix(2),
			StatusV: choices.Processing,
		},
		ParentV: Genesis.IDV,
		HeightV: Genesis.HeightV + 1,
	}

	require.NoError(sm.Add(firstBlock))
	require.NoError(sm.Add(secondBlock))

	votes := ids.Bag{}
	votes.Add(firstBlock.ID())
	votes.Add(secondBlock.ID())

	// The first poll will accept shared bits
	require.NoError(sm.RecordPoll(votes))
	require.Equal(firstBlock.ID(), sm.Preference())
	require.False(sm.Finalized())

	metrics := gatherCounterGauge(t, registerer)
	require.EqualValues(0, metrics["polls_failed"])
	require.EqualValues(1, metrics["polls_successful"])

	// The second poll will do nothing
	require.NoError(sm.RecordPoll(votes))
	require.Equal(firstBlock.ID(), sm.Preference())
	require.False(sm.Finalized())

	metrics = gatherCounterGauge(t, registerer)
	require.EqualValues(1, metrics["polls_failed"])
	require.EqualValues(1, metrics["polls_successful"])
}

func RecordPollWhenFinalizedTest(t *testing.T, factory Factory) {
	sm := factory.New()

	ctx := snow.DefaultConsensusContextTest()
	params := snowball.Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          1,
		BetaRogue:             2,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}
	if err := sm.Initialize(ctx, params, GenesisID, GenesisHeight); err != nil {
		t.Fatal(err)
	}

	votes := ids.Bag{}
	votes.Add(GenesisID)
	if err := sm.RecordPoll(votes); err != nil {
		t.Fatal(err)
	} else if !sm.Finalized() {
		t.Fatalf("Consensus should still be finalized")
	} else if pref := sm.Preference(); GenesisID != pref {
		t.Fatalf("Wrong preference listed")
	}
}

func RecordPollRejectTransitivelyTest(t *testing.T, factory Factory) {
	sm := factory.New()

	ctx := snow.DefaultConsensusContextTest()
	params := snowball.Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          1,
		BetaRogue:             1,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}
	if err := sm.Initialize(ctx, params, GenesisID, GenesisHeight); err != nil {
		t.Fatal(err)
	}

	block0 := &TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.Empty.Prefix(1),
			StatusV: choices.Processing,
		},
		ParentV: Genesis.IDV,
		HeightV: Genesis.HeightV + 1,
	}
	block1 := &TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.Empty.Prefix(2),
			StatusV: choices.Processing,
		},
		ParentV: Genesis.IDV,
		HeightV: Genesis.HeightV + 1,
	}
	block2 := &TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.Empty.Prefix(3),
			StatusV: choices.Processing,
		},
		ParentV: block1.IDV,
		HeightV: block1.HeightV + 1,
	}

	if err := sm.Add(block0); err != nil {
		t.Fatal(err)
	} else if err := sm.Add(block1); err != nil {
		t.Fatal(err)
	} else if err := sm.Add(block2); err != nil {
		t.Fatal(err)
	}

	// Current graph structure:
	//   G
	//  / \
	// 0   1
	//     |
	//     2
	// Tail = 0

	votes := ids.Bag{}
	votes.Add(block0.ID())
	if err := sm.RecordPoll(votes); err != nil {
		t.Fatal(err)
	}

	// Current graph structure:
	// 0
	// Tail = 0

	if !sm.Finalized() {
		t.Fatalf("Finalized too late")
	} else if pref := sm.Preference(); block0.ID() != pref {
		t.Fatalf("Wrong preference listed")
	} else if status := block0.Status(); status != choices.Accepted {
		t.Fatalf("Wrong status returned")
	} else if status := block1.Status(); status != choices.Rejected {
		t.Fatalf("Wrong status returned")
	} else if status := block2.Status(); status != choices.Rejected {
		t.Fatalf("Wrong status returned")
	}
}

func RecordPollTransitivelyResetConfidenceTest(t *testing.T, factory Factory) {
	sm := factory.New()

	ctx := snow.DefaultConsensusContextTest()
	params := snowball.Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          2,
		BetaRogue:             2,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}
	if err := sm.Initialize(ctx, params, GenesisID, GenesisHeight); err != nil {
		t.Fatal(err)
	}

	block0 := &TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.Empty.Prefix(1),
			StatusV: choices.Processing,
		},
		ParentV: Genesis.IDV,
		HeightV: Genesis.HeightV + 1,
	}
	block1 := &TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.Empty.Prefix(2),
			StatusV: choices.Processing,
		},
		ParentV: Genesis.IDV,
		HeightV: Genesis.HeightV + 1,
	}
	block2 := &TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.Empty.Prefix(3),
			StatusV: choices.Processing,
		},
		ParentV: block1.IDV,
		HeightV: block1.HeightV + 1,
	}
	block3 := &TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.Empty.Prefix(4),
			StatusV: choices.Processing,
		},
		ParentV: block1.IDV,
		HeightV: block1.HeightV + 1,
	}

	if err := sm.Add(block0); err != nil {
		t.Fatal(err)
	} else if err := sm.Add(block1); err != nil {
		t.Fatal(err)
	} else if err := sm.Add(block2); err != nil {
		t.Fatal(err)
	} else if err := sm.Add(block3); err != nil {
		t.Fatal(err)
	}

	// Current graph structure:
	//   G
	//  / \
	// 0   1
	//    / \
	//   2   3

	votesFor2 := ids.Bag{}
	votesFor2.Add(block2.ID())
	if err := sm.RecordPoll(votesFor2); err != nil {
		t.Fatal(err)
	} else if sm.Finalized() {
		t.Fatalf("Finalized too early")
	} else if pref := sm.Preference(); block2.ID() != pref {
		t.Fatalf("Wrong preference listed")
	}

	emptyVotes := ids.Bag{}
	if err := sm.RecordPoll(emptyVotes); err != nil {
		t.Fatal(err)
	} else if sm.Finalized() {
		t.Fatalf("Finalized too early")
	} else if pref := sm.Preference(); block2.ID() != pref {
		t.Fatalf("Wrong preference listed")
	} else if err := sm.RecordPoll(votesFor2); err != nil {
		t.Fatal(err)
	} else if sm.Finalized() {
		t.Fatalf("Finalized too early")
	} else if pref := sm.Preference(); block2.ID() != pref {
		t.Fatalf("Wrong preference listed")
	}

	votesFor3 := ids.Bag{}
	votesFor3.Add(block3.ID())
	if err := sm.RecordPoll(votesFor3); err != nil {
		t.Fatal(err)
	} else if sm.Finalized() {
		t.Fatalf("Finalized too early")
	} else if pref := sm.Preference(); block2.ID() != pref {
		t.Fatalf("Wrong preference listed")
	} else if err := sm.RecordPoll(votesFor3); err != nil {
		t.Fatal(err)
	} else if !sm.Finalized() {
		t.Fatalf("Finalized too late")
	} else if pref := sm.Preference(); block3.ID() != pref {
		t.Fatalf("Wrong preference listed")
	} else if status := block0.Status(); status != choices.Rejected {
		t.Fatalf("Wrong status returned")
	} else if status := block1.Status(); status != choices.Accepted {
		t.Fatalf("Wrong status returned")
	} else if status := block2.Status(); status != choices.Rejected {
		t.Fatalf("Wrong status returned")
	} else if status := block3.Status(); status != choices.Accepted {
		t.Fatalf("Wrong status returned")
	}
}

func RecordPollInvalidVoteTest(t *testing.T, factory Factory) {
	sm := factory.New()

	ctx := snow.DefaultConsensusContextTest()
	params := snowball.Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          2,
		BetaRogue:             2,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}
	if err := sm.Initialize(ctx, params, GenesisID, GenesisHeight); err != nil {
		t.Fatal(err)
	}

	block := &TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.Empty.Prefix(1),
			StatusV: choices.Processing,
		},
		ParentV: Genesis.IDV,
		HeightV: Genesis.HeightV + 1,
	}
	unknownBlockID := ids.Empty.Prefix(2)

	if err := sm.Add(block); err != nil {
		t.Fatal(err)
	}

	validVotes := ids.Bag{}
	validVotes.Add(block.ID())
	if err := sm.RecordPoll(validVotes); err != nil {
		t.Fatal(err)
	}

	invalidVotes := ids.Bag{}
	invalidVotes.Add(unknownBlockID)
	if err := sm.RecordPoll(invalidVotes); err != nil {
		t.Fatal(err)
	} else if err := sm.RecordPoll(validVotes); err != nil {
		t.Fatal(err)
	} else if sm.Finalized() {
		t.Fatalf("Finalized too early")
	} else if pref := sm.Preference(); block.ID() != pref {
		t.Fatalf("Wrong preference listed")
	}
}

func RecordPollTransitiveVotingTest(t *testing.T, factory Factory) {
	sm := factory.New()

	ctx := snow.DefaultConsensusContextTest()
	params := snowball.Parameters{
		K:                     3,
		Alpha:                 3,
		BetaVirtuous:          1,
		BetaRogue:             1,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}
	if err := sm.Initialize(ctx, params, GenesisID, GenesisHeight); err != nil {
		t.Fatal(err)
	}

	block0 := &TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.Empty.Prefix(1),
			StatusV: choices.Processing,
		},
		ParentV: Genesis.IDV,
		HeightV: Genesis.HeightV + 1,
	}
	block1 := &TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.Empty.Prefix(2),
			StatusV: choices.Processing,
		},
		ParentV: block0.IDV,
		HeightV: block0.HeightV + 1,
	}
	block2 := &TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.Empty.Prefix(3),
			StatusV: choices.Processing,
		},
		ParentV: block1.IDV,
		HeightV: block1.HeightV + 1,
	}
	block3 := &TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.Empty.Prefix(4),
			StatusV: choices.Processing,
		},
		ParentV: block0.IDV,
		HeightV: block0.HeightV + 1,
	}
	block4 := &TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.Empty.Prefix(5),
			StatusV: choices.Processing,
		},
		ParentV: block3.IDV,
		HeightV: block3.HeightV + 1,
	}

	if err := sm.Add(block0); err != nil {
		t.Fatal(err)
	} else if err := sm.Add(block1); err != nil {
		t.Fatal(err)
	} else if err := sm.Add(block2); err != nil {
		t.Fatal(err)
	} else if err := sm.Add(block3); err != nil {
		t.Fatal(err)
	} else if err := sm.Add(block4); err != nil {
		t.Fatal(err)
	}

	// Current graph structure:
	//   G
	//   |
	//   0
	//  / \
	// 1   3
	// |   |
	// 2   4
	// Tail = 2

	votes0_2_4 := ids.Bag{}
	votes0_2_4.Add(
		block0.ID(),
		block2.ID(),
		block4.ID(),
	)
	if err := sm.RecordPoll(votes0_2_4); err != nil {
		t.Fatal(err)
	}

	// Current graph structure:
	//   0
	//  / \
	// 1   3
	// |   |
	// 2   4
	// Tail = 2

	pref := sm.Preference()
	switch {
	case block2.ID() != pref:
		t.Fatalf("Wrong preference listed")
	case sm.Finalized():
		t.Fatalf("Finalized too early")
	case block0.Status() != choices.Accepted:
		t.Fatalf("Should have accepted")
	case block1.Status() != choices.Processing:
		t.Fatalf("Should have accepted")
	case block2.Status() != choices.Processing:
		t.Fatalf("Should have accepted")
	case block3.Status() != choices.Processing:
		t.Fatalf("Should have rejected")
	case block4.Status() != choices.Processing:
		t.Fatalf("Should have rejected")
	}

	dep2_2_2 := ids.Bag{}
	dep2_2_2.AddCount(block2.ID(), 3)
	if err := sm.RecordPoll(dep2_2_2); err != nil {
		t.Fatal(err)
	}

	// Current graph structure:
	//   2
	// Tail = 2

	pref = sm.Preference()
	switch {
	case block2.ID() != pref:
		t.Fatalf("Wrong preference listed")
	case !sm.Finalized():
		t.Fatalf("Finalized too late")
	case block0.Status() != choices.Accepted:
		t.Fatalf("Should have accepted")
	case block1.Status() != choices.Accepted:
		t.Fatalf("Should have accepted")
	case block2.Status() != choices.Accepted:
		t.Fatalf("Should have accepted")
	case block3.Status() != choices.Rejected:
		t.Fatalf("Should have rejected")
	case block4.Status() != choices.Rejected:
		t.Fatalf("Should have rejected")
	}
}

func RecordPollDivergedVotingTest(t *testing.T, factory Factory) {
	sm := factory.New()
	require := require.New(t)

	ctx := snow.DefaultConsensusContextTest()
	params := snowball.Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          1,
		BetaRogue:             2,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}
	err := sm.Initialize(ctx, params, GenesisID, GenesisHeight)
	require.NoError(err)

	block0 := &TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.ID{0x0f}, // 1111
			StatusV: choices.Processing,
		},
		ParentV: Genesis.IDV,
		HeightV: Genesis.HeightV + 1,
	}
	block1 := &TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.ID{0x08}, // 0001
			StatusV: choices.Processing,
		},
		ParentV: Genesis.IDV,
		HeightV: Genesis.HeightV + 1,
	}
	block2 := &TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.ID{0x01}, // 1000
			StatusV: choices.Processing,
		},
		ParentV: Genesis.IDV,
		HeightV: Genesis.HeightV + 1,
	}
	block3 := &TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.Empty.Prefix(1),
			StatusV: choices.Processing,
		},
		ParentV: block2.IDV,
		HeightV: block2.HeightV + 1,
	}

	err = sm.Add(block0)
	require.NoError(err)

	err = sm.Add(block1)
	require.NoError(err)

	// The first bit is contested as either 0 or 1. When voting for [block0] and
	// when the first bit is 1, the following bits have been decided to follow
	// the 255 remaining bits of [block0].
	votes0 := ids.Bag{}
	votes0.Add(block0.ID())
	err = sm.RecordPoll(votes0)
	require.NoError(err)

	// Although we are adding in [block2] here - the underlying snowball
	// instance has already decided it is rejected. Snowman doesn't actually
	// know that though, because that is an implementation detail of the
	// Snowball trie that is used.
	err = sm.Add(block2)
	require.NoError(err)

	// Because [block2] is effectively rejected, [block3] is also effectively
	// rejected.
	err = sm.Add(block3)
	require.NoError(err)

	require.Equal(block0.ID(), sm.Preference())
	require.Equal(choices.Processing, block0.Status(), "should not be accepted yet")
	require.Equal(choices.Processing, block1.Status(), "should not be rejected yet")
	require.Equal(choices.Processing, block2.Status(), "should not be rejected yet")
	require.Equal(choices.Processing, block3.Status(), "should not be rejected yet")

	// Current graph structure:
	//       G
	//     /   \
	//    *     |
	//   / \    |
	//  0   2   1
	//      |
	//      3
	// Tail = 0

	// Transitively votes for [block2] by voting for its child [block3].
	// Because [block2] shares the first bit with [block0] and the following
	// bits have been finalized for [block0], the voting results in accepting
	// [block0]. When [block0] is accepted, [block1] and [block2] are rejected
	// as conflicting. [block2]'s child, [block3], is then rejected
	// transitively.
	votes3 := ids.Bag{}
	votes3.Add(block3.ID())
	err = sm.RecordPoll(votes3)
	require.NoError(err)

	require.True(sm.Finalized(), "finalized too late")
	require.Equal(choices.Accepted, block0.Status(), "should be accepted")
	require.Equal(choices.Rejected, block1.Status())
	require.Equal(choices.Rejected, block2.Status())
	require.Equal(choices.Rejected, block3.Status())
}

func RecordPollDivergedVotingWithNoConflictingBitTest(t *testing.T, factory Factory) {
	sm := factory.New()
	require := require.New(t)

	ctx := snow.DefaultConsensusContextTest()
	params := snowball.Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          1,
		BetaRogue:             2,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}
	require.NoError(sm.Initialize(ctx, params, GenesisID, GenesisHeight))

	block0 := &TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.ID{0x06}, // 0110
			StatusV: choices.Processing,
		},
		ParentV: Genesis.IDV,
		HeightV: Genesis.HeightV + 1,
	}
	block1 := &TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.ID{0x08}, // 0001
			StatusV: choices.Processing,
		},
		ParentV: Genesis.IDV,
		HeightV: Genesis.HeightV + 1,
	}
	block2 := &TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.ID{0x01}, // 1000
			StatusV: choices.Processing,
		},
		ParentV: Genesis.IDV,
		HeightV: Genesis.HeightV + 1,
	}
	block3 := &TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.Empty.Prefix(1),
			StatusV: choices.Processing,
		},
		ParentV: block2.IDV,
		HeightV: block2.HeightV + 1,
	}

	require.NoError(sm.Add(block0))
	require.NoError(sm.Add(block1))

	// When voting for [block0], we end up finalizing the first bit as 0. The
	// second bit is contested as either 0 or 1. For when the second bit is 1,
	// the following bits have been decided to follow the 254 remaining bits of
	// [block0].
	votes0 := ids.Bag{}
	votes0.Add(block0.ID())
	require.NoError(sm.RecordPoll(votes0))

	// Although we are adding in [block2] here - the underlying snowball
	// instance has already decided it is rejected. Snowman doesn't actually
	// know that though, because that is an implementation detail of the
	// Snowball trie that is used.
	require.NoError(sm.Add(block2))

	// Because [block2] is effectively rejected, [block3] is also effectively
	// rejected.
	require.NoError(sm.Add(block3))

	require.Equal(block0.ID(), sm.Preference())
	require.Equal(choices.Processing, block0.Status(), "should not be decided yet")
	require.Equal(choices.Processing, block1.Status(), "should not be decided yet")
	require.Equal(choices.Processing, block2.Status(), "should not be decided yet")
	require.Equal(choices.Processing, block3.Status(), "should not be decided yet")

	// Current graph structure:
	//       G
	//     /   \
	//    *     |
	//   / \    |
	//  0   1   2
	//          |
	//          3
	// Tail = 0

	// Transitively votes for [block2] by voting for its child [block3]. Because
	// [block2] doesn't share any processing bits with [block0] or [block1], the
	// votes are over only rejected bits. Therefore, the votes for [block2] are
	// dropped. Although the votes for [block3] are still applied, [block3] will
	// only be marked as accepted after [block2] is marked as accepted; which
	// will never happen.
	votes3 := ids.Bag{}
	votes3.Add(block3.ID())
	require.NoError(sm.RecordPoll(votes3))

	require.False(sm.Finalized(), "finalized too early")
	require.Equal(choices.Processing, block0.Status())
	require.Equal(choices.Processing, block1.Status())
	require.Equal(choices.Processing, block2.Status())
	require.Equal(choices.Processing, block3.Status())
}

func RecordPollChangePreferredChainTest(t *testing.T, factory Factory) {
	sm := factory.New()

	ctx := snow.DefaultConsensusContextTest()
	params := snowball.Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          10,
		BetaRogue:             10,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}
	if err := sm.Initialize(ctx, params, GenesisID, GenesisHeight); err != nil {
		t.Fatal(err)
	}

	a1Block := &TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentV: Genesis.IDV,
		HeightV: Genesis.HeightV + 1,
	}
	b1Block := &TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentV: Genesis.IDV,
		HeightV: Genesis.HeightV + 1,
	}
	a2Block := &TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentV: a1Block.IDV,
		HeightV: a1Block.HeightV + 1,
	}
	b2Block := &TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentV: b1Block.IDV,
		HeightV: b1Block.HeightV + 1,
	}

	if err := sm.Add(a1Block); err != nil {
		t.Fatal(err)
	}
	if err := sm.Add(a2Block); err != nil {
		t.Fatal(err)
	}
	if err := sm.Add(b1Block); err != nil {
		t.Fatal(err)
	}
	if err := sm.Add(b2Block); err != nil {
		t.Fatal(err)
	}

	if sm.Preference() != a2Block.ID() {
		t.Fatal("Wrong preference reported")
	}

	if !sm.IsPreferred(a1Block) {
		t.Fatalf("Should have reported a1 as being preferred")
	}
	if !sm.IsPreferred(a2Block) {
		t.Fatalf("Should have reported a2 as being preferred")
	}
	if sm.IsPreferred(b1Block) {
		t.Fatalf("Shouldn't have reported b1 as being preferred")
	}
	if sm.IsPreferred(b2Block) {
		t.Fatalf("Shouldn't have reported b2 as being preferred")
	}

	b2Votes := ids.Bag{}
	b2Votes.Add(b2Block.ID())

	if err := sm.RecordPoll(b2Votes); err != nil {
		t.Fatal(err)
	}

	if sm.Preference() != b2Block.ID() {
		t.Fatal("Wrong preference reported")
	}

	if sm.IsPreferred(a1Block) {
		t.Fatalf("Shouldn't have reported a1 as being preferred")
	}
	if sm.IsPreferred(a2Block) {
		t.Fatalf("Shouldn't have reported a2 as being preferred")
	}
	if !sm.IsPreferred(b1Block) {
		t.Fatalf("Should have reported b1 as being preferred")
	}
	if !sm.IsPreferred(b2Block) {
		t.Fatalf("Should have reported b2 as being preferred")
	}

	a1Votes := ids.Bag{}
	a1Votes.Add(a1Block.ID())

	if err := sm.RecordPoll(a1Votes); err != nil {
		t.Fatal(err)
	}
	if err := sm.RecordPoll(a1Votes); err != nil {
		t.Fatal(err)
	}

	if sm.Preference() != a2Block.ID() {
		t.Fatal("Wrong preference reported")
	}

	if !sm.IsPreferred(a1Block) {
		t.Fatalf("Should have reported a1 as being preferred")
	}
	if !sm.IsPreferred(a2Block) {
		t.Fatalf("Should have reported a2 as being preferred")
	}
	if sm.IsPreferred(b1Block) {
		t.Fatalf("Shouldn't have reported b1 as being preferred")
	}
	if sm.IsPreferred(b2Block) {
		t.Fatalf("Shouldn't have reported b2 as being preferred")
	}
}

func MetricsProcessingErrorTest(t *testing.T, factory Factory) {
	sm := factory.New()

	ctx := snow.DefaultConsensusContextTest()
	params := snowball.Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          1,
		BetaRogue:             1,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}

	numProcessing := prometheus.NewGauge(
		prometheus.GaugeOpts{
			Name: "blks_processing",
		})

	if err := ctx.Registerer.Register(numProcessing); err != nil {
		t.Fatal(err)
	}

	if err := sm.Initialize(ctx, params, GenesisID, GenesisHeight); err == nil {
		t.Fatalf("should have errored during initialization due to a duplicate metric")
	}
}

func MetricsAcceptedErrorTest(t *testing.T, factory Factory) {
	sm := factory.New()

	ctx := snow.DefaultConsensusContextTest()
	params := snowball.Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          1,
		BetaRogue:             1,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}

	numAccepted := prometheus.NewGauge(
		prometheus.GaugeOpts{
			Name: "blks_accepted_count",
		})

	if err := ctx.Registerer.Register(numAccepted); err != nil {
		t.Fatal(err)
	}

	if err := sm.Initialize(ctx, params, GenesisID, GenesisHeight); err == nil {
		t.Fatalf("should have errored during initialization due to a duplicate metric")
	}
}

func MetricsRejectedErrorTest(t *testing.T, factory Factory) {
	sm := factory.New()

	ctx := snow.DefaultConsensusContextTest()
	params := snowball.Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          1,
		BetaRogue:             1,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}

	numRejected := prometheus.NewGauge(
		prometheus.GaugeOpts{
			Name: "blks_rejected_count",
		})

	if err := ctx.Registerer.Register(numRejected); err != nil {
		t.Fatal(err)
	}

	if err := sm.Initialize(ctx, params, GenesisID, GenesisHeight); err == nil {
		t.Fatalf("should have errored during initialization due to a duplicate metric")
	}
}

func ErrorOnInitialRejectionTest(t *testing.T, factory Factory) {
	sm := factory.New()

	ctx := snow.DefaultConsensusContextTest()
	params := snowball.Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          1,
		BetaRogue:             1,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}

	if err := sm.Initialize(ctx, params, GenesisID, GenesisHeight); err != nil {
		t.Fatal(err)
	}

	rejectedBlock := &TestBlock{TestDecidable: choices.TestDecidable{
		IDV:     ids.Empty.Prefix(1),
		StatusV: choices.Rejected,
	}}

	block := &TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.Empty.Prefix(2),
			RejectV: errors.New(""),
			StatusV: choices.Processing,
		},
		ParentV: rejectedBlock.IDV,
		HeightV: rejectedBlock.HeightV + 1,
	}

	if err := sm.Add(block); err == nil {
		t.Fatalf("Should have errored on rejecting the rejectable block")
	}
}

func ErrorOnAcceptTest(t *testing.T, factory Factory) {
	sm := factory.New()

	ctx := snow.DefaultConsensusContextTest()
	params := snowball.Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          1,
		BetaRogue:             1,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}

	if err := sm.Initialize(ctx, params, GenesisID, GenesisHeight); err != nil {
		t.Fatal(err)
	}

	block := &TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.Empty.Prefix(1),
			AcceptV: errors.New(""),
			StatusV: choices.Processing,
		},
		ParentV: Genesis.IDV,
		HeightV: Genesis.HeightV + 1,
	}

	if err := sm.Add(block); err != nil {
		t.Fatal(err)
	}

	votes := ids.Bag{}
	votes.Add(block.ID())
	if err := sm.RecordPoll(votes); err == nil {
		t.Fatalf("Should have errored on accepted the block")
	}
}

func ErrorOnRejectSiblingTest(t *testing.T, factory Factory) {
	sm := factory.New()

	ctx := snow.DefaultConsensusContextTest()
	params := snowball.Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          1,
		BetaRogue:             1,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}

	if err := sm.Initialize(ctx, params, GenesisID, GenesisHeight); err != nil {
		t.Fatal(err)
	}

	block0 := &TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.Empty.Prefix(1),
			StatusV: choices.Processing,
		},
		ParentV: Genesis.IDV,
		HeightV: Genesis.HeightV + 1,
	}
	block1 := &TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.Empty.Prefix(2),
			RejectV: errors.New(""),
			StatusV: choices.Processing,
		},
		ParentV: Genesis.IDV,
		HeightV: Genesis.HeightV + 1,
	}

	if err := sm.Add(block0); err != nil {
		t.Fatal(err)
	} else if err := sm.Add(block1); err != nil {
		t.Fatal(err)
	}

	votes := ids.Bag{}
	votes.Add(block0.ID())
	if err := sm.RecordPoll(votes); err == nil {
		t.Fatalf("Should have errored on rejecting the block's sibling")
	}
}

func ErrorOnTransitiveRejectionTest(t *testing.T, factory Factory) {
	sm := factory.New()

	ctx := snow.DefaultConsensusContextTest()
	params := snowball.Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          1,
		BetaRogue:             1,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}

	if err := sm.Initialize(ctx, params, GenesisID, GenesisHeight); err != nil {
		t.Fatal(err)
	}

	block0 := &TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.Empty.Prefix(1),
			StatusV: choices.Processing,
		},
		ParentV: Genesis.IDV,
		HeightV: Genesis.HeightV + 1,
	}
	block1 := &TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.Empty.Prefix(2),
			StatusV: choices.Processing,
		},
		ParentV: Genesis.IDV,
		HeightV: Genesis.HeightV + 1,
	}
	block2 := &TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.Empty.Prefix(3),
			RejectV: errors.New(""),
			StatusV: choices.Processing,
		},
		ParentV: block1.IDV,
		HeightV: block1.HeightV + 1,
	}

	if err := sm.Add(block0); err != nil {
		t.Fatal(err)
	} else if err := sm.Add(block1); err != nil {
		t.Fatal(err)
	} else if err := sm.Add(block2); err != nil {
		t.Fatal(err)
	}

	votes := ids.Bag{}
	votes.Add(block0.ID())
	if err := sm.RecordPoll(votes); err == nil {
		t.Fatalf("Should have errored on transitively rejecting the block")
	}
}

func RandomizedConsistencyTest(t *testing.T, factory Factory) {
	numColors := 50
	numNodes := 100
	params := snowball.Parameters{
		K:                     20,
		Alpha:                 15,
		BetaVirtuous:          20,
		BetaRogue:             30,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}
	seed := int64(0)

	sampler.Seed(seed)

	n := Network{}
	n.Initialize(params, numColors)

	for i := 0; i < numNodes; i++ {
		if err := n.AddNode(factory.New()); err != nil {
			t.Fatal(err)
		}
	}

	for !n.Finalized() {
		if err := n.Round(); err != nil {
			t.Fatal(err)
		}
	}

	if !n.Agreement() {
		t.Fatalf("Network agreed on inconsistent values")
	}
}

func ErrorOnAddDecidedBlock(t *testing.T, factory Factory) {
	sm := factory.New()
	require := require.New(t)

	ctx := snow.DefaultConsensusContextTest()
	params := snowball.Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          1,
		BetaRogue:             1,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}
	require.NoError(sm.Initialize(ctx, params, GenesisID, GenesisHeight))

	block0 := &TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.ID{0x03}, // 0b0011
			StatusV: choices.Accepted,
		},
		ParentV: Genesis.IDV,
		HeightV: Genesis.HeightV + 1,
	}
	require.ErrorIs(sm.Add(block0), errDuplicateAdd)
}

func ErrorOnAddDuplicateBlockID(t *testing.T, factory Factory) {
	sm := factory.New()
	require := require.New(t)

	ctx := snow.DefaultConsensusContextTest()
	params := snowball.Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          1,
		BetaRogue:             1,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}
	require.NoError(sm.Initialize(ctx, params, GenesisID, GenesisHeight))

	block0 := &TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.ID{0x03}, // 0b0011
			StatusV: choices.Processing,
		},
		ParentV: Genesis.IDV,
		HeightV: Genesis.HeightV + 1,
	}
	block1 := &TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.ID{0x03}, // 0b0011, same as block0
			StatusV: choices.Processing,
		},
		ParentV: block0.IDV,
		HeightV: block0.HeightV + 1,
	}

	require.NoError(sm.Add(block0))
	require.ErrorIs(sm.Add(block1), errDuplicateAdd)
}

func gatherCounterGauge(t *testing.T, reg *prometheus.Registry) map[string]float64 {
	ms, err := reg.Gather()
	if err != nil {
		t.Fatal(err)
	}
	mss := make(map[string]float64)
	for _, mf := range ms {
		name := mf.GetName()
		for _, m := range mf.GetMetric() {
			cnt := m.GetCounter()
			if cnt != nil {
				mss[name] = cnt.GetValue()
				break
			}
			gg := m.GetGauge()
			if gg != nil {
				mss[name] = gg.GetValue()
				break
			}
		}
	}
	return mss
}

```

avalanchego/snow/consensus/snowman/factory.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowman

// Factory returns new instances of Consensus
type Factory interface {
	New() Consensus
}

```

avalanchego/snow/consensus/snowman/mock_block.go:
```
// Code generated by MockGen. DO NOT EDIT.
// Source: github.com/ava-labs/avalanchego/snow/consensus/snowman (interfaces: Block)

// Package snowman is a generated GoMock package.
package snowman

import (
	reflect "reflect"
	time "time"

	ids "github.com/ava-labs/avalanchego/ids"
	choices "github.com/ava-labs/avalanchego/snow/choices"
	gomock "github.com/golang/mock/gomock"
)

// MockBlock is a mock of Block interface.
type MockBlock struct {
	ctrl     *gomock.Controller
	recorder *MockBlockMockRecorder
}

// MockBlockMockRecorder is the mock recorder for MockBlock.
type MockBlockMockRecorder struct {
	mock *MockBlock
}

// NewMockBlock creates a new mock instance.
func NewMockBlock(ctrl *gomock.Controller) *MockBlock {
	mock := &MockBlock{ctrl: ctrl}
	mock.recorder = &MockBlockMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockBlock) EXPECT() *MockBlockMockRecorder {
	return m.recorder
}

// Accept mocks base method.
func (m *MockBlock) Accept() error {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "Accept")
	ret0, _ := ret[0].(error)
	return ret0
}

// Accept indicates an expected call of Accept.
func (mr *MockBlockMockRecorder) Accept() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Accept", reflect.TypeOf((*MockBlock)(nil).Accept))
}

// Bytes mocks base method.
func (m *MockBlock) Bytes() []byte {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "Bytes")
	ret0, _ := ret[0].([]byte)
	return ret0
}

// Bytes indicates an expected call of Bytes.
func (mr *MockBlockMockRecorder) Bytes() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Bytes", reflect.TypeOf((*MockBlock)(nil).Bytes))
}

// Height mocks base method.
func (m *MockBlock) Height() uint64 {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "Height")
	ret0, _ := ret[0].(uint64)
	return ret0
}

// Height indicates an expected call of Height.
func (mr *MockBlockMockRecorder) Height() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Height", reflect.TypeOf((*MockBlock)(nil).Height))
}

// ID mocks base method.
func (m *MockBlock) ID() ids.ID {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "ID")
	ret0, _ := ret[0].(ids.ID)
	return ret0
}

// ID indicates an expected call of ID.
func (mr *MockBlockMockRecorder) ID() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "ID", reflect.TypeOf((*MockBlock)(nil).ID))
}

// Parent mocks base method.
func (m *MockBlock) Parent() ids.ID {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "Parent")
	ret0, _ := ret[0].(ids.ID)
	return ret0
}

// Parent indicates an expected call of Parent.
func (mr *MockBlockMockRecorder) Parent() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Parent", reflect.TypeOf((*MockBlock)(nil).Parent))
}

// Reject mocks base method.
func (m *MockBlock) Reject() error {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "Reject")
	ret0, _ := ret[0].(error)
	return ret0
}

// Reject indicates an expected call of Reject.
func (mr *MockBlockMockRecorder) Reject() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Reject", reflect.TypeOf((*MockBlock)(nil).Reject))
}

// Status mocks base method.
func (m *MockBlock) Status() choices.Status {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "Status")
	ret0, _ := ret[0].(choices.Status)
	return ret0
}

// Status indicates an expected call of Status.
func (mr *MockBlockMockRecorder) Status() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Status", reflect.TypeOf((*MockBlock)(nil).Status))
}

// Timestamp mocks base method.
func (m *MockBlock) Timestamp() time.Time {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "Timestamp")
	ret0, _ := ret[0].(time.Time)
	return ret0
}

// Timestamp indicates an expected call of Timestamp.
func (mr *MockBlockMockRecorder) Timestamp() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Timestamp", reflect.TypeOf((*MockBlock)(nil).Timestamp))
}

// Verify mocks base method.
func (m *MockBlock) Verify() error {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "Verify")
	ret0, _ := ret[0].(error)
	return ret0
}

// Verify indicates an expected call of Verify.
func (mr *MockBlockMockRecorder) Verify() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Verify", reflect.TypeOf((*MockBlock)(nil).Verify))
}

```

avalanchego/snow/consensus/snowman/network_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowman

import (
	"math/rand"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow"
	"github.com/ava-labs/avalanchego/snow/choices"
	"github.com/ava-labs/avalanchego/snow/consensus/snowball"
	"github.com/ava-labs/avalanchego/utils/sampler"
)

type Network struct {
	params         snowball.Parameters
	colors         []*TestBlock
	nodes, running []Consensus
}

func (n *Network) shuffleColors() {
	s := sampler.NewUniform()
	_ = s.Initialize(uint64(len(n.colors)))
	indices, _ := s.Sample(len(n.colors))
	colors := []*TestBlock(nil)
	for _, index := range indices {
		colors = append(colors, n.colors[int(index)])
	}
	n.colors = colors
	SortTestBlocks(n.colors)
}

func (n *Network) Initialize(params snowball.Parameters, numColors int) {
	n.params = params
	// #nosec G404
	n.colors = append(n.colors, &TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.Empty.Prefix(uint64(rand.Int63())),
			StatusV: choices.Processing,
		},
		ParentV: Genesis.IDV,
		HeightV: 1,
	})

	for i := 1; i < numColors; i++ {
		dependency := n.colors[rand.Intn(len(n.colors))] // #nosec G404
		// #nosec G404
		n.colors = append(n.colors, &TestBlock{
			TestDecidable: choices.TestDecidable{
				IDV:     ids.Empty.Prefix(uint64(rand.Int63())),
				StatusV: choices.Processing,
			},
			ParentV: dependency.IDV,
			HeightV: dependency.HeightV + 1,
		})
	}
}

func (n *Network) AddNode(sm Consensus) error {
	if err := sm.Initialize(snow.DefaultConsensusContextTest(), n.params, Genesis.ID(), Genesis.Height()); err != nil {
		return err
	}

	n.shuffleColors()
	deps := map[ids.ID]ids.ID{}
	for _, blk := range n.colors {
		myDep, found := deps[blk.ParentV]
		if !found {
			myDep = blk.Parent()
		}
		myVtx := &TestBlock{
			TestDecidable: choices.TestDecidable{
				IDV:     blk.ID(),
				StatusV: blk.Status(),
			},
			ParentV: myDep,
			HeightV: blk.Height(),
			VerifyV: blk.Verify(),
			BytesV:  blk.Bytes(),
		}
		if err := sm.Add(myVtx); err != nil {
			return err
		}
		deps[myVtx.ID()] = myDep
	}
	n.nodes = append(n.nodes, sm)
	n.running = append(n.running, sm)
	return nil
}

func (n *Network) Finalized() bool { return len(n.running) == 0 }

func (n *Network) Round() error {
	if len(n.running) == 0 {
		return nil
	}

	runningInd := rand.Intn(len(n.running)) // #nosec G404
	running := n.running[runningInd]

	s := sampler.NewUniform()
	_ = s.Initialize(uint64(len(n.nodes)))
	indices, _ := s.Sample(n.params.K)
	sampledColors := ids.Bag{}
	for _, index := range indices {
		peer := n.nodes[int(index)]
		sampledColors.Add(peer.Preference())
	}

	if err := running.RecordPoll(sampledColors); err != nil {
		return err
	}

	// If this node has been finalized, remove it from the poller
	if running.Finalized() {
		newSize := len(n.running) - 1
		n.running[runningInd] = n.running[newSize]
		n.running = n.running[:newSize]
	}

	return nil
}

func (n *Network) Agreement() bool {
	if len(n.nodes) == 0 {
		return true
	}
	pref := n.nodes[0].Preference()
	for _, node := range n.nodes {
		if pref != node.Preference() {
			return false
		}
	}
	return true
}

```

avalanchego/snow/consensus/snowman/oracle_block.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowman

import "errors"

var ErrNotOracle = errors.New("block isn't an oracle")

// OracleBlock is a block that only has two valid children. The children should
// be returned in preferential order.
//
// This ordering does not need to be deterministically created from the chain
// state.
type OracleBlock interface {
	Block

	// Options returns the possible children of this block in the order this
	// validator prefers the blocks.
	// Options is guaranteed to only be called on a verified block.
	Options() ([2]Block, error)
}

```

avalanchego/snow/consensus/snowman/poll/early_term_no_traversal.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package poll

import (
	"fmt"

	"github.com/ava-labs/avalanchego/ids"
)

type earlyTermNoTraversalFactory struct {
	alpha int
}

// NewEarlyTermNoTraversalFactory returns a factory that returns polls with
// early termination, without doing DAG traversals
func NewEarlyTermNoTraversalFactory(alpha int) Factory {
	return &earlyTermNoTraversalFactory{alpha: alpha}
}

func (f *earlyTermNoTraversalFactory) New(vdrs ids.NodeIDBag) Poll {
	return &earlyTermNoTraversalPoll{
		polled: vdrs,
		alpha:  f.alpha,
	}
}

// earlyTermNoTraversalPoll finishes when any remaining validators can't change
// the result of the poll. However, does not terminate tightly with this bound.
// It terminates as quickly as it can without performing any DAG traversals.
type earlyTermNoTraversalPoll struct {
	votes  ids.Bag
	polled ids.NodeIDBag
	alpha  int
}

// Vote registers a response for this poll
func (p *earlyTermNoTraversalPoll) Vote(vdr ids.NodeID, vote ids.ID) {
	count := p.polled.Count(vdr)
	// make sure that a validator can't respond multiple times
	p.polled.Remove(vdr)

	// track the votes the validator responded with
	p.votes.AddCount(vote, count)
}

// Drop any future response for this poll
func (p *earlyTermNoTraversalPoll) Drop(vdr ids.NodeID) {
	p.polled.Remove(vdr)
}

// Finished returns true when all validators have voted
func (p *earlyTermNoTraversalPoll) Finished() bool {
	remaining := p.polled.Len()
	received := p.votes.Len()
	_, freq := p.votes.Mode()
	return remaining == 0 || // All k nodes responded
		freq >= p.alpha || // An alpha majority has returned
		received+remaining < p.alpha // An alpha majority can never return
}

// Result returns the result of this poll
func (p *earlyTermNoTraversalPoll) Result() ids.Bag { return p.votes }

func (p *earlyTermNoTraversalPoll) PrefixedString(prefix string) string {
	return fmt.Sprintf(
		"waiting on %s\n%sreceived %s",
		p.polled.PrefixedString(prefix),
		prefix,
		p.votes.PrefixedString(prefix),
	)
}

func (p *earlyTermNoTraversalPoll) String() string { return p.PrefixedString("") }

```

avalanchego/snow/consensus/snowman/poll/early_term_no_traversal_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package poll

import (
	"testing"

	"github.com/ava-labs/avalanchego/ids"
)

func TestEarlyTermNoTraversalResults(t *testing.T) {
	alpha := 1

	vtxID := ids.ID{1}

	vdr1 := ids.NodeID{1} // k = 1

	vdrs := ids.NodeIDBag{}
	vdrs.Add(vdr1)

	factory := NewEarlyTermNoTraversalFactory(alpha)
	poll := factory.New(vdrs)

	poll.Vote(vdr1, vtxID)
	if !poll.Finished() {
		t.Fatalf("Poll did not terminate after receiving k votes")
	}

	result := poll.Result()
	if list := result.List(); len(list) != 1 {
		t.Fatalf("Wrong number of vertices returned")
	} else if retVtxID := list[0]; retVtxID != vtxID {
		t.Fatalf("Wrong vertex returned")
	} else if result.Count(vtxID) != 1 {
		t.Fatalf("Wrong number of votes returned")
	}
}

func TestEarlyTermNoTraversalString(t *testing.T) {
	alpha := 2

	vtxID := ids.ID{1}

	vdr1 := ids.NodeID{1}
	vdr2 := ids.NodeID{2} // k = 2

	vdrs := ids.NodeIDBag{}
	vdrs.Add(
		vdr1,
		vdr2,
	)

	factory := NewEarlyTermNoTraversalFactory(alpha)
	poll := factory.New(vdrs)

	poll.Vote(vdr1, vtxID)

	expected := `waiting on Bag: (Size = 1)
    ID[NodeID-BaMPFdqMUQ46BV8iRcwbVfsam55kMqcp]: Count = 1
received Bag: (Size = 1)
    ID[SYXsAycDPUu4z2ZksJD5fh5nTDcH3vCFHnpcVye5XuJ2jArg]: Count = 1`
	if result := poll.String(); expected != result {
		t.Fatalf("Poll should have returned %s but returned %s", expected, result)
	}
}

func TestEarlyTermNoTraversalDropsDuplicatedVotes(t *testing.T) {
	alpha := 2

	vtxID := ids.ID{1}

	vdr1 := ids.NodeID{1}
	vdr2 := ids.NodeID{2} // k = 2

	vdrs := ids.NodeIDBag{}
	vdrs.Add(
		vdr1,
		vdr2,
	)

	factory := NewEarlyTermNoTraversalFactory(alpha)
	poll := factory.New(vdrs)

	poll.Vote(vdr1, vtxID)
	if poll.Finished() {
		t.Fatalf("Poll finished after less than alpha votes")
	}
	poll.Vote(vdr1, vtxID)
	if poll.Finished() {
		t.Fatalf("Poll finished after getting a duplicated vote")
	}
	poll.Vote(vdr2, vtxID)
	if !poll.Finished() {
		t.Fatalf("Poll did not terminate after receiving k votes")
	}
}

func TestEarlyTermNoTraversalTerminatesEarly(t *testing.T) {
	alpha := 3

	vtxID := ids.ID{1}

	vdr1 := ids.NodeID{1}
	vdr2 := ids.NodeID{2}
	vdr3 := ids.NodeID{3}
	vdr4 := ids.NodeID{4}
	vdr5 := ids.NodeID{5} // k = 5

	vdrs := ids.NodeIDBag{}
	vdrs.Add(
		vdr1,
		vdr2,
		vdr3,
		vdr4,
		vdr5,
	)

	factory := NewEarlyTermNoTraversalFactory(alpha)
	poll := factory.New(vdrs)

	poll.Vote(vdr1, vtxID)
	if poll.Finished() {
		t.Fatalf("Poll finished after less than alpha votes")
	}
	poll.Vote(vdr2, vtxID)
	if poll.Finished() {
		t.Fatalf("Poll finished after less than alpha votes")
	}
	poll.Vote(vdr3, vtxID)
	if !poll.Finished() {
		t.Fatalf("Poll did not terminate early after receiving alpha votes for one vertex and none for other vertices")
	}
}

func TestEarlyTermNoTraversalForSharedAncestor(t *testing.T) {
	alpha := 4

	vtxA := ids.ID{1}
	vtxB := ids.ID{2}
	vtxC := ids.ID{3}
	vtxD := ids.ID{4}

	// If validators 1-3 vote for frontier vertices
	// B, C, and D respectively, which all share the common ancestor
	// A, then we cannot terminate early with alpha = k = 4
	// If the final vote is cast for any of A, B, C, or D, then
	// vertex A will have transitively received alpha = 4 votes
	vdr1 := ids.NodeID{1}
	vdr2 := ids.NodeID{2}
	vdr3 := ids.NodeID{3}
	vdr4 := ids.NodeID{4}

	vdrs := ids.NodeIDBag{}
	vdrs.Add(
		vdr1,
		vdr2,
		vdr3,
		vdr4,
	)

	factory := NewEarlyTermNoTraversalFactory(alpha)
	poll := factory.New(vdrs)

	poll.Vote(vdr1, vtxB)
	if poll.Finished() {
		t.Fatalf("Poll finished early after receiving one vote")
	}
	poll.Vote(vdr2, vtxC)
	if poll.Finished() {
		t.Fatalf("Poll finished early after receiving two votes")
	}
	poll.Vote(vdr3, vtxD)
	if poll.Finished() {
		t.Fatalf("Poll terminated early, when a shared ancestor could have received alpha votes")
	}
	poll.Vote(vdr4, vtxA)
	if !poll.Finished() {
		t.Fatalf("Poll did not terminate after receiving all outstanding votes")
	}
}

func TestEarlyTermNoTraversalWithFastDrops(t *testing.T) {
	alpha := 2

	vdr1 := ids.NodeID{1}
	vdr2 := ids.NodeID{2}
	vdr3 := ids.NodeID{3} // k = 3

	vdrs := ids.NodeIDBag{}
	vdrs.Add(
		vdr1,
		vdr2,
		vdr3,
	)

	factory := NewEarlyTermNoTraversalFactory(alpha)
	poll := factory.New(vdrs)

	poll.Drop(vdr1)
	if poll.Finished() {
		t.Fatalf("Poll finished early after dropping one vote")
	}
	poll.Drop(vdr2)
	if !poll.Finished() {
		t.Fatalf("Poll did not terminate after dropping two votes")
	}
}

func TestEarlyTermNoTraversalWithWeightedResponses(t *testing.T) {
	alpha := 2

	vtxID := ids.ID{1}

	vdr1 := ids.NodeID{2}
	vdr2 := ids.NodeID{3}

	vdrs := ids.NodeIDBag{}
	vdrs.Add(
		vdr1,
		vdr2,
		vdr2,
	) // k = 3

	factory := NewEarlyTermNoTraversalFactory(alpha)
	poll := factory.New(vdrs)

	poll.Vote(vdr2, vtxID)
	if !poll.Finished() {
		t.Fatalf("Poll did not terminate after receiving two votes")
	}

	result := poll.Result()
	if list := result.List(); len(list) != 1 {
		t.Fatalf("Wrong number of vertices returned")
	} else if retVtxID := list[0]; retVtxID != vtxID {
		t.Fatalf("Wrong vertex returned")
	} else if result.Count(vtxID) != 2 {
		t.Fatalf("Wrong number of votes returned")
	}
}

func TestEarlyTermNoTraversalDropWithWeightedResponses(t *testing.T) {
	alpha := 2

	vdr1 := ids.NodeID{1}
	vdr2 := ids.NodeID{2}

	vdrs := ids.NodeIDBag{}
	vdrs.Add(
		vdr1,
		vdr2,
		vdr2,
	) // k = 3

	factory := NewEarlyTermNoTraversalFactory(alpha)
	poll := factory.New(vdrs)

	poll.Drop(vdr2)
	if !poll.Finished() {
		t.Fatalf("Poll did not terminate after dropping two votes")
	}
}

```

avalanchego/snow/consensus/snowman/poll/interfaces.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package poll

import (
	"fmt"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/utils/formatting"
)

// Set is a collection of polls
type Set interface {
	fmt.Stringer

	Add(requestID uint32, vdrs ids.NodeIDBag) bool
	Vote(requestID uint32, vdr ids.NodeID, vote ids.ID) []ids.Bag
	Drop(requestID uint32, vdr ids.NodeID) []ids.Bag
	Len() int
}

// Poll is an outstanding poll
type Poll interface {
	formatting.PrefixedStringer

	Vote(vdr ids.NodeID, vote ids.ID)
	Drop(vdr ids.NodeID)
	Finished() bool
	Result() ids.Bag
}

// Factory creates a new Poll
type Factory interface {
	New(vdrs ids.NodeIDBag) Poll
}

```

avalanchego/snow/consensus/snowman/poll/no_early_term.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package poll

import (
	"fmt"

	"github.com/ava-labs/avalanchego/ids"
)

type noEarlyTermFactory struct{}

// NewNoEarlyTermFactory returns a factory that returns polls with no early
// termination
func NewNoEarlyTermFactory() Factory { return noEarlyTermFactory{} }

func (noEarlyTermFactory) New(vdrs ids.NodeIDBag) Poll {
	return &noEarlyTermPoll{polled: vdrs}
}

// noEarlyTermPoll finishes when all polled validators either respond to the
// query or a timeout occurs
type noEarlyTermPoll struct {
	votes  ids.Bag
	polled ids.NodeIDBag
}

// Vote registers a response for this poll
func (p *noEarlyTermPoll) Vote(vdr ids.NodeID, vote ids.ID) {
	count := p.polled.Count(vdr)
	// make sure that a validator can't respond multiple times
	p.polled.Remove(vdr)

	// track the votes the validator responded with
	p.votes.AddCount(vote, count)
}

// Drop any future response for this poll
func (p *noEarlyTermPoll) Drop(vdr ids.NodeID) { p.polled.Remove(vdr) }

// Finished returns true when all validators have voted
func (p *noEarlyTermPoll) Finished() bool {
	return p.polled.Len() == 0
}

// Result returns the result of this poll
func (p *noEarlyTermPoll) Result() ids.Bag { return p.votes }

func (p *noEarlyTermPoll) PrefixedString(prefix string) string {
	return fmt.Sprintf(
		"waiting on %s\n%sreceived %s",
		p.polled.PrefixedString(prefix),
		prefix,
		p.votes.PrefixedString(prefix),
	)
}

func (p *noEarlyTermPoll) String() string { return p.PrefixedString("") }

```

avalanchego/snow/consensus/snowman/poll/no_early_term_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package poll

import (
	"testing"

	"github.com/ava-labs/avalanchego/ids"
)

func TestNoEarlyTermResults(t *testing.T) {
	vtxID := ids.ID{1}

	vdr1 := ids.NodeID{1} // k = 1

	vdrs := ids.NodeIDBag{}
	vdrs.Add(vdr1)

	factory := NewNoEarlyTermFactory()
	poll := factory.New(vdrs)

	poll.Vote(vdr1, vtxID)
	if !poll.Finished() {
		t.Fatalf("Poll did not terminate after receiving k votes")
	}

	result := poll.Result()
	if list := result.List(); len(list) != 1 {
		t.Fatalf("Wrong number of vertices returned")
	} else if retVtxID := list[0]; retVtxID != vtxID {
		t.Fatalf("Wrong vertex returned")
	} else if result.Count(vtxID) != 1 {
		t.Fatalf("Wrong number of votes returned")
	}
}

func TestNoEarlyTermString(t *testing.T) {
	vtxID := ids.ID{1}

	vdr1 := ids.NodeID{1}
	vdr2 := ids.NodeID{2} // k = 2

	vdrs := ids.NodeIDBag{}
	vdrs.Add(
		vdr1,
		vdr2,
	)

	factory := NewNoEarlyTermFactory()
	poll := factory.New(vdrs)

	poll.Vote(vdr1, vtxID)

	expected := `waiting on Bag: (Size = 1)
    ID[NodeID-BaMPFdqMUQ46BV8iRcwbVfsam55kMqcp]: Count = 1
received Bag: (Size = 1)
    ID[SYXsAycDPUu4z2ZksJD5fh5nTDcH3vCFHnpcVye5XuJ2jArg]: Count = 1`
	if result := poll.String(); expected != result {
		t.Fatalf("Poll should have returned %s but returned %s", expected, result)
	}
}

func TestNoEarlyTermDropsDuplicatedVotes(t *testing.T) {
	vtxID := ids.ID{1}

	vdr1 := ids.NodeID{1}
	vdr2 := ids.NodeID{2} // k = 2

	vdrs := ids.NodeIDBag{}
	vdrs.Add(
		vdr1,
		vdr2,
	)

	factory := NewNoEarlyTermFactory()
	poll := factory.New(vdrs)

	poll.Vote(vdr1, vtxID)
	if poll.Finished() {
		t.Fatalf("Poll finished after less than alpha votes")
	}
	poll.Vote(vdr1, vtxID)
	if poll.Finished() {
		t.Fatalf("Poll finished after getting a duplicated vote")
	}
	poll.Drop(vdr1)
	if poll.Finished() {
		t.Fatalf("Poll finished after getting a duplicated vote")
	}
	poll.Vote(vdr2, vtxID)
	if !poll.Finished() {
		t.Fatalf("Poll did not terminate after receiving k votes")
	}
}

```

avalanchego/snow/consensus/snowman/poll/set.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package poll

import (
	"fmt"
	"strings"
	"time"

	"github.com/prometheus/client_golang/prometheus"

	"go.uber.org/zap"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/utils/linkedhashmap"
	"github.com/ava-labs/avalanchego/utils/logging"
	"github.com/ava-labs/avalanchego/utils/metric"
)

type pollHolder interface {
	GetPoll() Poll
	StartTime() time.Time
}

type poll struct {
	Poll
	start time.Time
}

func (p poll) GetPoll() Poll {
	return p
}

func (p poll) StartTime() time.Time {
	return p.start
}

type set struct {
	log      logging.Logger
	numPolls prometheus.Gauge
	durPolls metric.Averager
	factory  Factory
	// maps requestID -> poll
	polls linkedhashmap.LinkedHashmap[uint32, pollHolder]
}

// NewSet returns a new empty set of polls
func NewSet(
	factory Factory,
	log logging.Logger,
	namespace string,
	reg prometheus.Registerer,
) Set {
	numPolls := prometheus.NewGauge(prometheus.GaugeOpts{
		Namespace: namespace,
		Name:      "polls",
		Help:      "Number of pending network polls",
	})
	if err := reg.Register(numPolls); err != nil {
		log.Error("failed to register polls statistics",
			zap.Error(err),
		)
	}

	durPolls, err := metric.NewAverager(
		namespace,
		"poll_duration",
		"time (in ns) this poll took to complete",
		reg,
	)
	if err != nil {
		log.Error("failed to register poll_duration statistics",
			zap.Error(err),
		)
	}

	return &set{
		log:      log,
		numPolls: numPolls,
		durPolls: durPolls,
		factory:  factory,
		polls:    linkedhashmap.New[uint32, pollHolder](),
	}
}

// Add to the current set of polls
// Returns true if the poll was registered correctly and the network sample
//         should be made.
func (s *set) Add(requestID uint32, vdrs ids.NodeIDBag) bool {
	if _, exists := s.polls.Get(requestID); exists {
		s.log.Debug("dropping poll",
			zap.String("reason", "duplicated request"),
			zap.Uint32("requestID", requestID),
		)
		return false
	}

	s.log.Verbo("creating poll",
		zap.Uint32("requestID", requestID),
		zap.Stringer("validators", &vdrs),
	)

	s.polls.Put(requestID, poll{
		Poll:  s.factory.New(vdrs), // create the new poll
		start: time.Now(),
	})
	s.numPolls.Inc() // increase the metrics
	return true
}

// Vote registers the connections response to a query for [id]. If there was no
// query, or the response has already be registered, nothing is performed.
func (s *set) Vote(requestID uint32, vdr ids.NodeID, vote ids.ID) []ids.Bag {
	holder, exists := s.polls.Get(requestID)
	if !exists {
		s.log.Verbo("dropping vote",
			zap.String("reason", "unknown poll"),
			zap.Stringer("validator", vdr),
			zap.Uint32("requestID", requestID),
		)
		return nil
	}

	p := holder.GetPoll()

	s.log.Verbo("processing vote",
		zap.Stringer("validator", vdr),
		zap.Uint32("requestID", requestID),
		zap.Stringer("vote", vote),
	)

	p.Vote(vdr, vote)
	if !p.Finished() {
		return nil
	}

	return s.processFinishedPolls()
}

// processFinishedPolls checks for other dependent finished polls and returns them all if finished
func (s *set) processFinishedPolls() []ids.Bag {
	var results []ids.Bag

	// iterate from oldest to newest
	iter := s.polls.NewIterator()
	for iter.Next() {
		holder := iter.Value()
		p := holder.GetPoll()
		if !p.Finished() {
			// since we're iterating from oldest to newest, if the next poll has not finished,
			// we can break and return what we have so far
			break
		}

		s.log.Verbo("poll finished",
			zap.Any("requestID", iter.Key()),
			zap.Stringer("poll", holder.GetPoll()),
		)
		s.durPolls.Observe(float64(time.Since(holder.StartTime())))
		s.numPolls.Dec() // decrease the metrics

		results = append(results, p.Result())
		s.polls.Delete(iter.Key())
	}

	// only gets here if the poll has finished
	// results will have values if this and other newer polls have finished
	return results
}

// Drop registers the connections response to a query for [id]. If there was no
// query, or the response has already be registered, nothing is performed.
func (s *set) Drop(requestID uint32, vdr ids.NodeID) []ids.Bag {
	holder, exists := s.polls.Get(requestID)
	if !exists {
		s.log.Verbo("dropping vote",
			zap.String("reason", "unknown poll"),
			zap.Stringer("validator", vdr),
			zap.Uint32("requestID", requestID),
		)
		return nil
	}

	s.log.Verbo("processing dropped vote",
		zap.Stringer("validator", vdr),
		zap.Uint32("requestID", requestID),
	)

	poll := holder.GetPoll()

	poll.Drop(vdr)
	if !poll.Finished() {
		return nil
	}

	return s.processFinishedPolls()
}

// Len returns the number of outstanding polls
func (s *set) Len() int { return s.polls.Len() }

func (s *set) String() string {
	sb := strings.Builder{}
	sb.WriteString(fmt.Sprintf("current polls: (Size = %d)", s.polls.Len()))
	iter := s.polls.NewIterator()
	for iter.Next() {
		requestID := iter.Key()
		poll := iter.Value().(Poll)
		sb.WriteString(fmt.Sprintf("\n    RequestID %d:\n        %s", requestID, poll.PrefixedString("        ")))
	}
	return sb.String()
}

```

avalanchego/snow/consensus/snowman/poll/set_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package poll

import (
	"testing"

	"github.com/stretchr/testify/require"

	"github.com/prometheus/client_golang/prometheus"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/utils/logging"
	"github.com/ava-labs/avalanchego/utils/wrappers"
)

func TestNewSetErrorOnMetrics(t *testing.T) {
	factory := NewNoEarlyTermFactory()
	log := logging.NoLog{}
	namespace := ""
	registerer := prometheus.NewRegistry()

	errs := wrappers.Errs{}
	errs.Add(
		registerer.Register(prometheus.NewCounter(prometheus.CounterOpts{
			Name: "polls",
		})),
		registerer.Register(prometheus.NewCounter(prometheus.CounterOpts{
			Name: "poll_duration",
		})),
	)
	if errs.Errored() {
		t.Fatal(errs.Err)
	}

	s := NewSet(factory, log, namespace, registerer)
	if s == nil {
		t.Fatalf("shouldn't have failed due to a metrics initialization err")
	}
}

func TestCreateAndFinishPollOutOfOrder_NewerFinishesFirst(t *testing.T) {
	factory := NewNoEarlyTermFactory()
	log := logging.NoLog{}
	namespace := ""
	registerer := prometheus.NewRegistry()
	s := NewSet(factory, log, namespace, registerer)

	// create validators
	vdr1 := ids.NodeID{1}
	vdr2 := ids.NodeID{2}
	vdr3 := ids.NodeID{3}

	vdrs := []ids.NodeID{vdr1, vdr2, vdr3}

	// create two polls for the two vtxs
	vdrBag := ids.NodeIDBag{}
	vdrBag.Add(vdrs...)
	added := s.Add(1, vdrBag)
	require.True(t, added)

	vdrBag = ids.NodeIDBag{}
	vdrBag.Add(vdrs...)
	added = s.Add(2, vdrBag)
	require.True(t, added)
	require.Equal(t, s.Len(), 2)

	// vote vtx1 for poll 1
	// vote vtx2 for poll 2
	vtx1 := ids.ID{1}
	vtx2 := ids.ID{2}

	var results []ids.Bag

	// vote out of order
	results = s.Vote(1, vdr1, vtx1)
	require.Len(t, results, 0)
	results = s.Vote(2, vdr2, vtx2)
	require.Len(t, results, 0)
	results = s.Vote(2, vdr3, vtx2)
	require.Len(t, results, 0)

	results = s.Vote(2, vdr1, vtx2) // poll 2 finished
	require.Len(t, results, 0)      // expect 2 to not have finished because 1 is still pending

	results = s.Vote(1, vdr2, vtx1)
	require.Len(t, results, 0)

	results = s.Vote(1, vdr3, vtx1) // poll 1 finished, poll 2 should be finished as well
	require.Len(t, results, 2)
	require.Equal(t, vtx1, results[0].List()[0])
	require.Equal(t, vtx2, results[1].List()[0])
}

func TestCreateAndFinishPollOutOfOrder_OlderFinishesFirst(t *testing.T) {
	factory := NewNoEarlyTermFactory()
	log := logging.NoLog{}
	namespace := ""
	registerer := prometheus.NewRegistry()
	s := NewSet(factory, log, namespace, registerer)

	// create validators
	vdr1 := ids.NodeID{1}
	vdr2 := ids.NodeID{2}
	vdr3 := ids.NodeID{3}

	vdrs := []ids.NodeID{vdr1, vdr2, vdr3}

	// create two polls for the two vtxs
	vdrBag := ids.NodeIDBag{}
	vdrBag.Add(vdrs...)
	added := s.Add(1, vdrBag)
	require.True(t, added)

	vdrBag = ids.NodeIDBag{}
	vdrBag.Add(vdrs...)
	added = s.Add(2, vdrBag)
	require.True(t, added)
	require.Equal(t, s.Len(), 2)

	// vote vtx1 for poll 1
	// vote vtx2 for poll 2
	vtx1 := ids.ID{1}
	vtx2 := ids.ID{2}

	var results []ids.Bag

	// vote out of order
	results = s.Vote(1, vdr1, vtx1)
	require.Len(t, results, 0)
	results = s.Vote(2, vdr2, vtx2)
	require.Len(t, results, 0)
	results = s.Vote(2, vdr3, vtx2)
	require.Len(t, results, 0)

	results = s.Vote(1, vdr2, vtx1)
	require.Len(t, results, 0)

	results = s.Vote(1, vdr3, vtx1) // poll 1 finished, poll 2 still remaining
	require.Len(t, results, 1)      // because 1 is the oldest
	require.Equal(t, vtx1, results[0].List()[0])

	results = s.Vote(2, vdr1, vtx2) // poll 2 finished
	require.Len(t, results, 1)      // because 2 is the oldest now
	require.Equal(t, vtx2, results[0].List()[0])
}

func TestCreateAndFinishPollOutOfOrder_UnfinishedPollsGaps(t *testing.T) {
	factory := NewNoEarlyTermFactory()
	log := logging.NoLog{}
	namespace := ""
	registerer := prometheus.NewRegistry()
	s := NewSet(factory, log, namespace, registerer)

	// create validators
	vdr1 := ids.NodeID{1}
	vdr2 := ids.NodeID{2}
	vdr3 := ids.NodeID{3}

	vdrs := []ids.NodeID{vdr1, vdr2, vdr3}

	// create three polls for the two vtxs
	vdrBag := ids.NodeIDBag{}
	vdrBag.Add(vdrs...)
	added := s.Add(1, vdrBag)
	require.True(t, added)

	vdrBag = ids.NodeIDBag{}
	vdrBag.Add(vdrs...)
	added = s.Add(2, vdrBag)
	require.True(t, added)

	vdrBag = ids.NodeIDBag{}
	vdrBag.Add(vdrs...)
	added = s.Add(3, vdrBag)
	require.True(t, added)
	require.Equal(t, s.Len(), 3)

	// vote vtx1 for poll 1
	// vote vtx2 for poll 2
	// vote vtx3 for poll 3
	vtx1 := ids.ID{1}
	vtx2 := ids.ID{2}
	vtx3 := ids.ID{3}

	var results []ids.Bag

	// vote out of order
	// 2 finishes first to create a gap of finished poll between two unfinished polls 1 and 3
	results = s.Vote(2, vdr3, vtx2)
	require.Len(t, results, 0)
	results = s.Vote(2, vdr2, vtx2)
	require.Len(t, results, 0)
	results = s.Vote(2, vdr1, vtx2)
	require.Len(t, results, 0)

	// 3 finishes now, 2 has already finished but 1 is not finished so we expect to receive no results still
	results = s.Vote(3, vdr2, vtx3)
	require.Len(t, results, 0)
	results = s.Vote(3, vdr3, vtx3)
	require.Len(t, results, 0)
	results = s.Vote(3, vdr1, vtx3)
	require.Len(t, results, 0)

	// 1 finishes now, 2 and 3 have already finished so we expect 3 items in results
	results = s.Vote(1, vdr1, vtx1)
	require.Len(t, results, 0)
	results = s.Vote(1, vdr2, vtx1)
	require.Len(t, results, 0)
	results = s.Vote(1, vdr3, vtx1)
	require.Len(t, results, 3)
	require.Equal(t, vtx1, results[0].List()[0])
	require.Equal(t, vtx2, results[1].List()[0])
	require.Equal(t, vtx3, results[2].List()[0])
}

func TestCreateAndFinishSuccessfulPoll(t *testing.T) {
	factory := NewNoEarlyTermFactory()
	log := logging.NoLog{}
	namespace := ""
	registerer := prometheus.NewRegistry()
	s := NewSet(factory, log, namespace, registerer)

	vtxID := ids.ID{1}

	vdr1 := ids.NodeID{1}
	vdr2 := ids.NodeID{2} // k = 2

	vdrs := ids.NodeIDBag{}
	vdrs.Add(
		vdr1,
		vdr2,
	)

	if s.Len() != 0 {
		t.Fatalf("Shouldn't have any active polls yet")
	} else if !s.Add(0, vdrs) {
		t.Fatalf("Should have been able to add a new poll")
	} else if s.Len() != 1 {
		t.Fatalf("Should only have one active poll")
	} else if s.Add(0, vdrs) {
		t.Fatalf("Shouldn't have been able to add a duplicated poll")
	} else if s.Len() != 1 {
		t.Fatalf("Should only have one active poll")
	} else if results := s.Vote(1, vdr1, vtxID); len(results) > 0 {
		t.Fatalf("Shouldn't have been able to finish a non-existent poll")
	} else if results = s.Vote(0, vdr1, vtxID); len(results) > 0 {
		t.Fatalf("Shouldn't have been able to finish an ongoing poll")
	} else if results = s.Vote(0, vdr1, vtxID); len(results) > 0 {
		t.Fatalf("Should have dropped a duplicated poll")
	} else if results = s.Vote(0, vdr2, vtxID); len(results) == 0 {
		t.Fatalf("Should have finished the")
	} else if len(results) != 1 {
		t.Fatalf("Wrong number of results returned")
	} else if list := results[0].List(); len(list) != 1 {
		t.Fatalf("Wrong number of vertices returned")
	} else if retVtxID := list[0]; retVtxID != vtxID {
		t.Fatalf("Wrong vertex returned")
	} else if results[0].Count(vtxID) != 2 {
		t.Fatalf("Wrong number of votes returned")
	}
}

func TestCreateAndFinishFailedPoll(t *testing.T) {
	factory := NewNoEarlyTermFactory()
	log := logging.NoLog{}
	namespace := ""
	registerer := prometheus.NewRegistry()
	s := NewSet(factory, log, namespace, registerer)

	vdr1 := ids.NodeID{1}
	vdr2 := ids.NodeID{2} // k = 2

	vdrs := ids.NodeIDBag{}
	vdrs.Add(
		vdr1,
		vdr2,
	)

	if s.Len() != 0 {
		t.Fatalf("Shouldn't have any active polls yet")
	} else if !s.Add(0, vdrs) {
		t.Fatalf("Should have been able to add a new poll")
	} else if s.Len() != 1 {
		t.Fatalf("Should only have one active poll")
	} else if s.Add(0, vdrs) {
		t.Fatalf("Shouldn't have been able to add a duplicated poll")
	} else if s.Len() != 1 {
		t.Fatalf("Should only have one active poll")
	} else if results := s.Drop(1, vdr1); len(results) > 0 {
		t.Fatalf("Shouldn't have been able to finish a non-existent poll")
	} else if results = s.Drop(0, vdr1); len(results) > 0 {
		t.Fatalf("Shouldn't have been able to finish an ongoing poll")
	} else if results = s.Drop(0, vdr1); len(results) > 0 {
		t.Fatalf("Should have dropped a duplicated poll")
	} else if results = s.Drop(0, vdr2); len(results) == 0 {
		t.Fatalf("Should have finished the")
	} else if list := results[0].List(); len(list) != 0 {
		t.Fatalf("Wrong number of vertices returned")
	}
}

func TestSetString(t *testing.T) {
	factory := NewNoEarlyTermFactory()
	log := logging.NoLog{}
	namespace := ""
	registerer := prometheus.NewRegistry()
	s := NewSet(factory, log, namespace, registerer)

	vdr1 := ids.NodeID{1} // k = 1

	vdrs := ids.NodeIDBag{}
	vdrs.Add(vdr1)

	expected := `current polls: (Size = 1)
    RequestID 0:
        waiting on Bag: (Size = 1)
            ID[NodeID-6HgC8KRBEhXYbF4riJyJFLSHt37UNuRt]: Count = 1
        received Bag: (Size = 0)`
	if !s.Add(0, vdrs) {
		t.Fatalf("Should have been able to add a new poll")
	} else if str := s.String(); expected != str {
		t.Fatalf("Set return wrong string, Expected:\n%s\nReturned:\n%s",
			expected,
			str)
	}
}

```

avalanchego/snow/consensus/snowman/snowman_block.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowman

import (
	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/choices"
	"github.com/ava-labs/avalanchego/snow/consensus/snowball"
)

// Tracks the state of a snowman block
type snowmanBlock struct {
	// pointer to the snowman instance this node is managed by
	sm Consensus

	// block that this node contains. For the genesis, this value will be nil
	blk Block

	// shouldFalter is set to true if this node, and all its descendants received
	// less than Alpha votes
	shouldFalter bool

	// sb is the snowball instance used to decide which child is the canonical
	// child of this block. If this node has not had a child issued under it,
	// this value will be nil
	sb snowball.Consensus

	// children is the set of blocks that have been issued that name this block
	// as their parent. If this node has not had a child issued under it, this value
	// will be nil
	children map[ids.ID]Block
}

func (n *snowmanBlock) AddChild(child Block) {
	childID := child.ID()

	// if the snowball instance is nil, this is the first child. So the instance
	// should be initialized.
	if n.sb == nil {
		n.sb = &snowball.Tree{}
		n.sb.Initialize(n.sm.Parameters(), childID)
		n.children = make(map[ids.ID]Block)
	} else {
		n.sb.Add(childID)
	}

	n.children[childID] = child
}

func (n *snowmanBlock) Accepted() bool {
	// if the block is nil, then this is the genesis which is defined as
	// accepted
	if n.blk == nil {
		return true
	}
	return n.blk.Status() == choices.Accepted
}

```

avalanchego/snow/consensus/snowman/test_block.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowman

import (
	"sort"
	"time"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/choices"
)

var _ Block = &TestBlock{}

// TestBlock is a useful test block
type TestBlock struct {
	choices.TestDecidable

	ParentV    ids.ID
	HeightV    uint64
	TimestampV time.Time
	VerifyV    error
	BytesV     []byte
}

func (b *TestBlock) Parent() ids.ID       { return b.ParentV }
func (b *TestBlock) Height() uint64       { return b.HeightV }
func (b *TestBlock) Timestamp() time.Time { return b.TimestampV }
func (b *TestBlock) Verify() error        { return b.VerifyV }
func (b *TestBlock) Bytes() []byte        { return b.BytesV }

type sortBlocks []*TestBlock

func (sb sortBlocks) Less(i, j int) bool { return sb[i].HeightV < sb[j].HeightV }
func (sb sortBlocks) Len() int           { return len(sb) }
func (sb sortBlocks) Swap(i, j int)      { sb[j], sb[i] = sb[i], sb[j] }

// SortTestBlocks sorts the array of blocks by height
func SortTestBlocks(blocks []*TestBlock) { sort.Sort(sortBlocks(blocks)) }

```

avalanchego/snow/consensus/snowman/topological.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowman

import (
	"errors"
	"fmt"
	"strings"

	"go.uber.org/zap"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow"
	"github.com/ava-labs/avalanchego/snow/choices"
	"github.com/ava-labs/avalanchego/snow/consensus/metrics"
	"github.com/ava-labs/avalanchego/snow/consensus/snowball"
)

var (
	errDuplicateAdd = errors.New("duplicate block add")

	_ Factory   = &TopologicalFactory{}
	_ Consensus = &Topological{}
)

// TopologicalFactory implements Factory by returning a topological struct
type TopologicalFactory struct{}

func (TopologicalFactory) New() Consensus { return &Topological{} }

// Topological implements the Snowman interface by using a tree tracking the
// strongly preferred branch. This tree structure amortizes network polls to
// vote on more than just the next block.
type Topological struct {
	metrics.Latency
	metrics.Polls
	metrics.Height

	// pollNumber is the number of times RecordPolls has been called
	pollNumber uint64

	// ctx is the context this snowman instance is executing in
	ctx *snow.ConsensusContext

	// params are the parameters that should be used to initialize snowball
	// instances
	params snowball.Parameters

	// head is the last accepted block
	head ids.ID

	// height is the height of the last accepted block
	height uint64

	// blocks stores the last accepted block and all the pending blocks
	blocks map[ids.ID]*snowmanBlock // blockID -> snowmanBlock

	// preferredIDs stores the set of IDs that are currently preferred.
	preferredIDs ids.Set

	// tail is the preferred block with no children
	tail ids.ID

	// Used in [calculateInDegree] and.
	// Should only be accessed in that method.
	// We use this one instance of ids.Set instead of creating a
	// new ids.Set during each call to [calculateInDegree].
	leaves ids.Set

	// Kahn nodes used in [calculateInDegree] and [markAncestorInDegrees].
	// Should only be accessed in those methods.
	// We use this one map instead of creating a new map
	// during each call to [calculateInDegree].
	kahnNodes map[ids.ID]kahnNode
}

// Used to track the kahn topological sort status
type kahnNode struct {
	// inDegree is the number of children that haven't been processed yet. If
	// inDegree is 0, then this node is a leaf
	inDegree int
	// votes for all the children of this node, so far
	votes ids.Bag
}

// Used to track which children should receive votes
type votes struct {
	// parentID is the parent of all the votes provided in the votes bag
	parentID ids.ID
	// votes for all the children of the parent
	votes ids.Bag
}

func (ts *Topological) Initialize(ctx *snow.ConsensusContext, params snowball.Parameters, rootID ids.ID, rootHeight uint64) error {
	if err := params.Verify(); err != nil {
		return err
	}

	latencyMetrics, err := metrics.NewLatency("blks", "block(s)", ctx.Log, "", ctx.Registerer)
	if err != nil {
		return err
	}
	ts.Latency = latencyMetrics

	pollsMetrics, err := metrics.NewPolls("", ctx.Registerer)
	if err != nil {
		return err
	}
	ts.Polls = pollsMetrics

	heightMetrics, err := metrics.NewHeight("", ctx.Registerer)
	if err != nil {
		return err
	}
	ts.Height = heightMetrics

	ts.leaves = ids.Set{}
	ts.kahnNodes = make(map[ids.ID]kahnNode)
	ts.ctx = ctx
	ts.params = params
	ts.head = rootID
	ts.height = rootHeight
	ts.blocks = map[ids.ID]*snowmanBlock{
		rootID: {sm: ts},
	}
	ts.tail = rootID

	// Initially set the height to the last accepted block.
	ts.Height.Accepted(ts.height)
	return nil
}

func (ts *Topological) Parameters() snowball.Parameters { return ts.params }

func (ts *Topological) NumProcessing() int { return len(ts.blocks) - 1 }

func (ts *Topological) Add(blk Block) error {
	blkID := blk.ID()

	// Make sure a block is not inserted twice. This enforces the invariant that
	// blocks are always added in topological order. Essentially, a block that
	// is being added should never have a child that was already added.
	// Additionally, this prevents any edge cases that may occur due to adding
	// different blocks with the same ID.
	if ts.Decided(blk) || ts.Processing(blkID) {
		return errDuplicateAdd
	}

	ts.Latency.Issued(blkID, ts.pollNumber)

	parentID := blk.Parent()
	parentNode, ok := ts.blocks[parentID]
	if !ok {
		// If the ancestor is missing, this means the ancestor must have already
		// been pruned. Therefore, the dependent should be transitively
		// rejected.
		if err := blk.Reject(); err != nil {
			return err
		}
		ts.Latency.Rejected(blkID, ts.pollNumber)
		return nil
	}

	// add the block as a child of its parent, and add the block to the tree
	parentNode.AddChild(blk)
	ts.blocks[blkID] = &snowmanBlock{
		sm:  ts,
		blk: blk,
	}

	// If we are extending the tail, this is the new tail
	if ts.tail == parentID {
		ts.tail = blkID
		ts.preferredIDs.Add(blkID)
	}
	return nil
}

func (ts *Topological) Decided(blk Block) bool {
	// If the block is decided, then it must have been previously issued.
	if blk.Status().Decided() {
		return true
	}
	// If the block is marked as fetched, we can check if it has been
	// transitively rejected.
	return blk.Status() == choices.Processing && blk.Height() <= ts.height
}

func (ts *Topological) Processing(blkID ids.ID) bool {
	// The last accepted block is in the blocks map, so we first must ensure the
	// requested block isn't the last accepted block.
	if blkID == ts.head {
		return false
	}
	// If the block is in the map of current blocks and not the head, then the
	// block is currently processing.
	_, ok := ts.blocks[blkID]
	return ok
}

func (ts *Topological) IsPreferred(blk Block) bool {
	// If the block is accepted, then it must be transitively preferred.
	if blk.Status() == choices.Accepted {
		return true
	}
	return ts.preferredIDs.Contains(blk.ID())
}

func (ts *Topological) Preference() ids.ID { return ts.tail }

// The votes bag contains at most K votes for blocks in the tree. If there is a
// vote for a block that isn't in the tree, the vote is dropped.
//
// Votes are propagated transitively towards the genesis. All blocks in the tree
// that result in at least Alpha votes will record the poll on their children.
// Every other block will have an unsuccessful poll registered.
//
// After collecting which blocks should be voted on, the polls are registered
// and blocks are accepted/rejected as needed. The tail is then updated to equal
// the leaf on the preferred branch.
//
// To optimize the theoretical complexity of the vote propagation, a topological
// sort is done over the blocks that are reachable from the provided votes.
// During the sort, votes are pushed towards the genesis. To prevent interating
// over all blocks that had unsuccessful polls, we set a flag on the block to
// know that any future traversal through that block should register an
// unsuccessful poll on that block and every descendant block.
//
// The complexity of this function is:
// - Runtime = 3 * |live set| + |votes|
// - Space = 2 * |live set| + |votes|
func (ts *Topological) RecordPoll(voteBag ids.Bag) error {
	// Register a new poll call
	ts.pollNumber++

	var voteStack []votes
	if voteBag.Len() >= ts.params.Alpha {
		// Since we received at least alpha votes, it's possible that
		// we reached an alpha majority on a processing block.
		// We must perform the traversals to calculate all block
		// that reached an alpha majority.

		// Populates [ts.kahnNodes] and [ts.leaves]
		// Runtime = |live set| + |votes| ; Space = |live set| + |votes|
		ts.calculateInDegree(voteBag)

		// Runtime = |live set| ; Space = |live set|
		voteStack = ts.pushVotes()
	}

	// Runtime = |live set| ; Space = Constant
	preferred, err := ts.vote(voteStack)
	if err != nil {
		return err
	}

	// If the set of preferred IDs already contains the preference, then the
	// tail is guaranteed to already be set correctly. This is because the value
	// returned from vote reports the next preferred block after the last
	// preferred block that was voted for. If this block was previously
	// preferred, then we know that following the preferences down the chain
	// will return the current tail.
	if ts.preferredIDs.Contains(preferred) {
		return nil
	}

	// Runtime = |live set| ; Space = Constant
	ts.preferredIDs.Clear()

	ts.tail = preferred
	startBlock := ts.blocks[ts.tail]

	// Runtime = |live set| ; Space = Constant
	// Traverse from the preferred ID to the last accepted ancestor.
	for block := startBlock; !block.Accepted(); {
		ts.preferredIDs.Add(block.blk.ID())
		block = ts.blocks[block.blk.Parent()]
	}
	// Traverse from the preferred ID to the preferred child until there are no
	// children.
	for block := startBlock; block.sb != nil; block = ts.blocks[ts.tail] {
		ts.tail = block.sb.Preference()
		ts.preferredIDs.Add(ts.tail)
	}
	return nil
}

func (ts *Topological) Finalized() bool { return len(ts.blocks) == 1 }

// HealthCheck returns information about the consensus health.
func (ts *Topological) HealthCheck() (interface{}, error) {
	numOutstandingBlks := ts.Latency.NumProcessing()
	isOutstandingBlks := numOutstandingBlks <= ts.params.MaxOutstandingItems
	healthy := isOutstandingBlks
	details := map[string]interface{}{
		"outstandingBlocks": numOutstandingBlks,
	}

	// check for long running blocks
	timeReqRunning := ts.Latency.MeasureAndGetOldestDuration()
	isProcessingTime := timeReqRunning <= ts.params.MaxItemProcessingTime
	healthy = healthy && isProcessingTime
	details["longestRunningBlock"] = timeReqRunning.String()

	if !healthy {
		var errorReasons []string
		if !isOutstandingBlks {
			errorReasons = append(errorReasons, fmt.Sprintf("number of outstanding blocks %d > %d", numOutstandingBlks, ts.params.MaxOutstandingItems))
		}
		if !isProcessingTime {
			errorReasons = append(errorReasons, fmt.Sprintf("block processing time %s > %s", timeReqRunning, ts.params.MaxItemProcessingTime))
		}
		return details, fmt.Errorf("snowman consensus is not healthy reason: %s", strings.Join(errorReasons, ", "))
	}
	return details, nil
}

// takes in a list of votes and sets up the topological ordering. Returns the
// reachable section of the graph annotated with the number of inbound edges and
// the non-transitively applied votes. Also returns the list of leaf blocks.
func (ts *Topological) calculateInDegree(votes ids.Bag) {
	// Clear the Kahn node set
	for k := range ts.kahnNodes {
		delete(ts.kahnNodes, k)
	}
	// Clear the leaf set
	ts.leaves.Clear()

	for _, vote := range votes.List() {
		votedBlock, validVote := ts.blocks[vote]

		// If the vote is for a block that isn't in the current pending set,
		// then the vote is dropped
		if !validVote {
			continue
		}

		// If the vote is for the last accepted block, the vote is dropped
		if votedBlock.Accepted() {
			continue
		}

		// The parent contains the snowball instance of its children
		parentID := votedBlock.blk.Parent()

		// Add the votes for this block to the parent's set of responses
		numVotes := votes.Count(vote)
		kahn, previouslySeen := ts.kahnNodes[parentID]
		kahn.votes.AddCount(vote, numVotes)
		ts.kahnNodes[parentID] = kahn

		// If the parent block already had registered votes, then there is no
		// need to iterate into the parents
		if previouslySeen {
			continue
		}

		// If I've never seen this parent block before, it is currently a leaf.
		ts.leaves.Add(parentID)

		// iterate through all the block's ancestors and set up the inDegrees of
		// the blocks
		for n := ts.blocks[parentID]; !n.Accepted(); n = ts.blocks[parentID] {
			parentID = n.blk.Parent()

			// Increase the inDegree by one
			kahn := ts.kahnNodes[parentID]
			kahn.inDegree++
			ts.kahnNodes[parentID] = kahn

			// If we have already seen this block, then we shouldn't increase
			// the inDegree of the ancestors through this block again.
			if kahn.inDegree != 1 {
				break
			}

			// If I am transitively seeing this block for the first time, either
			// the block was previously unknown or it was previously a leaf.
			// Regardless, it shouldn't be tracked as a leaf.
			ts.leaves.Remove(parentID)
		}
	}
}

// convert the tree into a branch of snowball instances with at least alpha
// votes
func (ts *Topological) pushVotes() []votes {
	voteStack := make([]votes, 0, len(ts.kahnNodes))
	for ts.leaves.Len() > 0 {
		// Pop one element of [leaves]
		leafID, _ := ts.leaves.Pop()
		// Should never return false because we just
		// checked that [ts.leaves] is non-empty.

		// get the block and sort information about the block
		kahnNode := ts.kahnNodes[leafID]
		block := ts.blocks[leafID]

		// If there are at least Alpha votes, then this block needs to record
		// the poll on the snowball instance
		if kahnNode.votes.Len() >= ts.params.Alpha {
			voteStack = append(voteStack, votes{
				parentID: leafID,
				votes:    kahnNode.votes,
			})
		}

		// If the block is accepted, then we don't need to push votes to the
		// parent block
		if block.Accepted() {
			continue
		}

		parentID := block.blk.Parent()

		// Remove an inbound edge from the parent kahn node and push the votes.
		parentKahnNode := ts.kahnNodes[parentID]
		parentKahnNode.inDegree--
		parentKahnNode.votes.AddCount(leafID, kahnNode.votes.Len())
		ts.kahnNodes[parentID] = parentKahnNode

		// If the inDegree is zero, then the parent node is now a leaf
		if parentKahnNode.inDegree == 0 {
			ts.leaves.Add(parentID)
		}
	}
	return voteStack
}

// apply votes to the branch that received an Alpha threshold and returns the
// next preferred block after the last preferred block that received an Alpha
// threshold.
func (ts *Topological) vote(voteStack []votes) (ids.ID, error) {
	// If the voteStack is empty, then the full tree should falter. This won't
	// change the preferred branch.
	if len(voteStack) == 0 {
		headBlock := ts.blocks[ts.head]
		headBlock.shouldFalter = true

		if numProcessing := len(ts.blocks) - 1; numProcessing > 0 {
			ts.ctx.Log.Verbo("no progress was made after processing pending blocks",
				zap.Int("numProcessing", numProcessing),
			)
			ts.Polls.Failed()
		}
		return ts.tail, nil
	}

	// keep track of the new preferred block
	newPreferred := ts.head
	onPreferredBranch := true
	pollSuccessful := false
	for len(voteStack) > 0 {
		// pop a vote off the stack
		newStackSize := len(voteStack) - 1
		vote := voteStack[newStackSize]
		voteStack = voteStack[:newStackSize]

		// get the block that we are going to vote on
		parentBlock, notRejected := ts.blocks[vote.parentID]

		// if the block block we are going to vote on was already rejected, then
		// we should stop applying the votes
		if !notRejected {
			break
		}

		// keep track of transitive falters to propagate to this block's
		// children
		shouldTransitivelyFalter := parentBlock.shouldFalter

		// if the block was previously marked as needing to falter, the block
		// should falter before applying the vote
		if shouldTransitivelyFalter {
			ts.ctx.Log.Verbo("resetting confidence below parent",
				zap.Stringer("parentID", vote.parentID),
			)

			parentBlock.sb.RecordUnsuccessfulPoll()
			parentBlock.shouldFalter = false
		}

		// apply the votes for this snowball instance
		pollSuccessful = parentBlock.sb.RecordPoll(vote.votes) || pollSuccessful

		// Only accept when you are finalized and the head.
		if parentBlock.sb.Finalized() && ts.head == vote.parentID {
			if err := ts.acceptPreferredChild(parentBlock); err != nil {
				return ids.ID{}, err
			}

			// by accepting the child of parentBlock, the last accepted block is
			// no longer voteParentID, but its child. So, voteParentID can be
			// removed from the tree.
			delete(ts.blocks, vote.parentID)
		}

		// If we are on the preferred branch, then the parent's preference is
		// the next block on the preferred branch.
		parentPreference := parentBlock.sb.Preference()
		if onPreferredBranch {
			newPreferred = parentPreference
		}

		// Get the ID of the child that is having a RecordPoll called. All other
		// children will need to have their confidence reset. If there isn't a
		// child having RecordPoll called, then the nextID will default to the
		// nil ID.
		nextID := ids.ID{}
		if len(voteStack) > 0 {
			nextID = voteStack[newStackSize-1].parentID
		}

		// If we are on the preferred branch and the nextID is the preference of
		// the snowball instance, then we are following the preferred branch.
		onPreferredBranch = onPreferredBranch && nextID == parentPreference

		// If there wasn't an alpha threshold on the branch (either on this vote
		// or a past transitive vote), I should falter now.
		for childID := range parentBlock.children {
			// If we don't need to transitively falter and the child is going to
			// have RecordPoll called on it, then there is no reason to reset
			// the block's confidence
			if !shouldTransitivelyFalter && childID == nextID {
				continue
			}

			// If we finalized a child of the current block, then all other
			// children will have been rejected and removed from the tree.
			// Therefore, we need to make sure the child is still in the tree.
			childBlock, notRejected := ts.blocks[childID]
			if notRejected {
				ts.ctx.Log.Verbo("defering confidence reset of child block",
					zap.Stringer("childID", childID),
				)

				ts.ctx.Log.Verbo("voting for next block",
					zap.Stringer("nextID", nextID),
				)

				// If the child is ever voted for positively, the confidence
				// must be reset first.
				childBlock.shouldFalter = true
			}
		}
	}

	if pollSuccessful {
		ts.Polls.Successful()
	} else {
		ts.Polls.Failed()
	}
	return newPreferred, nil
}

// Accepts the preferred child of the provided snowman block. By accepting the
// preferred child, all other children will be rejected. When these children are
// rejected, all their descendants will be rejected.
//
// We accept a block once its parent's snowball instance has finalized
// with it as the preference.
func (ts *Topological) acceptPreferredChild(n *snowmanBlock) error {
	// We are finalizing the block's child, so we need to get the preference
	pref := n.sb.Preference()

	// Get the child and accept it
	child := n.children[pref]
	// Notify anyone listening that this block was accepted.
	bytes := child.Bytes()
	// Note that DecisionAcceptor.Accept / ConsensusAcceptor.Accept must be
	// called before child.Accept to honor Acceptor.Accept's invariant.
	if err := ts.ctx.DecisionAcceptor.Accept(ts.ctx, pref, bytes); err != nil {
		return err
	}
	if err := ts.ctx.ConsensusAcceptor.Accept(ts.ctx, pref, bytes); err != nil {
		return err
	}

	ts.ctx.Log.Trace("accepting block",
		zap.Stringer("blkID", pref),
	)
	if err := child.Accept(); err != nil {
		return err
	}

	// Because this is the newest accepted block, this is the new head.
	ts.head = pref
	ts.height = child.Height()
	// Remove the decided block from the set of processing IDs, as its status
	// now implies its preferredness.
	ts.preferredIDs.Remove(pref)

	ts.Latency.Accepted(pref, ts.pollNumber)
	ts.Height.Accepted(ts.height)

	// Because ts.blocks contains the last accepted block, we don't delete the
	// block from the blocks map here.

	rejects := make([]ids.ID, 0, len(n.children)-1)
	for childID, child := range n.children {
		if childID == pref {
			// don't reject the block we just accepted
			continue
		}

		ts.ctx.Log.Trace("rejecting block",
			zap.String("reason", "conflict with accepted block"),
			zap.Stringer("rejectedID", childID),
			zap.Stringer("conflictedID", pref),
		)
		if err := child.Reject(); err != nil {
			return err
		}
		ts.Latency.Rejected(childID, ts.pollNumber)

		// Track which blocks have been directly rejected
		rejects = append(rejects, childID)
	}

	// reject all the descendants of the blocks we just rejected
	return ts.rejectTransitively(rejects)
}

// Takes in a list of rejected ids and rejects all descendants of these IDs
func (ts *Topological) rejectTransitively(rejected []ids.ID) error {
	// the rejected array is treated as a stack, with the next element at index
	// 0 and the last element at the end of the slice.
	for len(rejected) > 0 {
		// pop the rejected ID off the stack
		newRejectedSize := len(rejected) - 1
		rejectedID := rejected[newRejectedSize]
		rejected = rejected[:newRejectedSize]

		// get the rejected node, and remove it from the tree
		rejectedNode := ts.blocks[rejectedID]
		delete(ts.blocks, rejectedID)

		for childID, child := range rejectedNode.children {
			if err := child.Reject(); err != nil {
				return err
			}
			ts.Latency.Rejected(childID, ts.pollNumber)

			// add the newly rejected block to the end of the stack
			rejected = append(rejected, childID)
		}
	}
	return nil
}

```

avalanchego/snow/consensus/snowman/topological_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowman

import (
	"testing"
)

func TestTopological(t *testing.T) { runConsensusTests(t, TopologicalFactory{}) }

```

avalanchego/snow/consensus/snowstorm/acceptor.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowstorm

import (
	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/events"
	"github.com/ava-labs/avalanchego/utils/wrappers"
)

var _ events.Blockable = &acceptor{}

type acceptor struct {
	g        *Directed
	errs     *wrappers.Errs
	deps     ids.Set
	rejected bool
	txID     ids.ID
}

func (a *acceptor) Dependencies() ids.Set { return a.deps }

func (a *acceptor) Fulfill(id ids.ID) {
	a.deps.Remove(id)
	a.Update()
}

func (a *acceptor) Abandon(id ids.ID) { a.rejected = true }

func (a *acceptor) Update() {
	// If I was rejected or I am still waiting on dependencies to finish or an
	// error has occurred, I shouldn't do anything.
	if a.rejected || a.deps.Len() != 0 || a.errs.Errored() {
		return
	}
	a.errs.Add(a.g.accept(a.txID))
}

```

avalanchego/snow/consensus/snowstorm/benchmark_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowstorm

import (
	"testing"

	"github.com/ava-labs/avalanchego/utils/sampler"

	sbcon "github.com/ava-labs/avalanchego/snow/consensus/snowball"
)

func Simulate(
	numColors, colorsPerConsumer, maxInputConflicts, numNodes int,
	params sbcon.Parameters,
	seed int64,
	fact Factory,
) error {
	net := Network{}
	sampler.Seed(seed)
	net.Initialize(
		params,
		numColors,
		colorsPerConsumer,
		maxInputConflicts,
	)

	sampler.Seed(seed)
	for i := 0; i < numNodes; i++ {
		if err := net.AddNode(fact.New()); err != nil {
			return err
		}
	}

	numRounds := 0
	for !net.Finalized() && !net.Disagreement() && numRounds < 50 {
		sampler.Seed(int64(numRounds) + seed)
		if err := net.Round(); err != nil {
			return err
		}
		numRounds++
	}
	return nil
}

/*
 ******************************************************************************
 ********************************** Virtuous **********************************
 ******************************************************************************
 */

func BenchmarkVirtuousDirected(b *testing.B) {
	for n := 0; n < b.N; n++ {
		err := Simulate(
			/*numColors=*/ 25,
			/*colorsPerConsumer=*/ 1,
			/*maxInputConflicts=*/ 1,
			/*numNodes=*/ 50,
			/*params=*/ sbcon.Parameters{
				K:                 20,
				Alpha:             11,
				BetaVirtuous:      20,
				BetaRogue:         30,
				ConcurrentRepolls: 1,
			},
			/*seed=*/ 0,
			/*fact=*/ DirectedFactory{},
		)
		if err != nil {
			b.Fatal(err)
		}
	}
}

/*
 ******************************************************************************
 *********************************** Rogue ************************************
 ******************************************************************************
 */

func BenchmarkRogueDirected(b *testing.B) {
	for n := 0; n < b.N; n++ {
		err := Simulate(
			/*numColors=*/ 25,
			/*colorsPerConsumer=*/ 1,
			/*maxInputConflicts=*/ 3,
			/*numNodes=*/ 50,
			/*params=*/ sbcon.Parameters{
				K:                 20,
				Alpha:             11,
				BetaVirtuous:      20,
				BetaRogue:         30,
				ConcurrentRepolls: 1,
			},
			/*seed=*/ 0,
			/*fact=*/ DirectedFactory{},
		)
		if err != nil {
			b.Fatal(err)
		}
	}
}

/*
 ******************************************************************************
 ******************************** Many Inputs *********************************
 ******************************************************************************
 */

func BenchmarkMultiDirected(b *testing.B) {
	for n := 0; n < b.N; n++ {
		err := Simulate(
			/*numColors=*/ 50,
			/*colorsPerConsumer=*/ 10,
			/*maxInputConflicts=*/ 1,
			/*numNodes=*/ 50,
			/*params=*/ sbcon.Parameters{
				K:                 20,
				Alpha:             11,
				BetaVirtuous:      20,
				BetaRogue:         30,
				ConcurrentRepolls: 1,
			},
			/*seed=*/ 0,
			/*fact=*/ DirectedFactory{},
		)
		if err != nil {
			b.Fatal(err)
		}
	}
}

/*
 ******************************************************************************
 ***************************** Many Rogue Inputs ******************************
 ******************************************************************************
 */

func BenchmarkMultiRogueDirected(b *testing.B) {
	for n := 0; n < b.N; n++ {
		err := Simulate(
			/*numColors=*/ 50,
			/*colorsPerConsumer=*/ 10,
			/*maxInputConflicts=*/ 3,
			/*numNodes=*/ 50,
			/*params=*/ sbcon.Parameters{
				K:                 20,
				Alpha:             11,
				BetaVirtuous:      20,
				BetaRogue:         30,
				ConcurrentRepolls: 1,
			},
			/*seed=*/ 0,
			/*fact=*/ DirectedFactory{},
		)
		if err != nil {
			b.Fatal(err)
		}
	}
}

```

avalanchego/snow/consensus/snowstorm/consensus.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowstorm

import (
	"fmt"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow"

	sbcon "github.com/ava-labs/avalanchego/snow/consensus/snowball"
)

// Consensus is a snowball instance deciding between an unbounded number of
// non-transitive conflicts. After performing a network sample of k nodes, you
// should call collect with the responses.
type Consensus interface {
	fmt.Stringer

	// Takes in the context, alpha, betaVirtuous, and betaRogue
	Initialize(*snow.ConsensusContext, sbcon.Parameters) error

	// Returns the parameters that describe this snowstorm instance
	Parameters() sbcon.Parameters

	// Returns true if transaction <Tx> is virtuous.
	// That is, no transaction has been added that conflicts with <Tx>
	IsVirtuous(Tx) bool

	// Adds a new transaction to vote on. Returns if a critical error has
	// occurred.
	Add(Tx) error

	// Remove a transaction from the set of currently processing txs. It is
	// assumed that the provided transaction ID is currently processing.
	Remove(ids.ID) error

	// Returns true iff transaction <Tx> has been added
	Issued(Tx) bool

	// Returns the set of virtuous transactions
	// that have not yet been accepted or rejected
	Virtuous() ids.Set

	// Returns the currently preferred transactions to be finalized
	Preferences() ids.Set

	// Return the current virtuous transactions that are being voted on.
	VirtuousVoting() ids.Set

	// Returns the set of transactions conflicting with <Tx>
	Conflicts(Tx) ids.Set

	// Collects the results of a network poll. Assumes all transactions
	// have been previously added. Returns true if any statuses or preferences
	// changed. Returns if a critical error has occurred.
	RecordPoll(ids.Bag) (bool, error)

	// Returns true iff all remaining transactions are rogue. Note, it is
	// possible that after returning quiesce, a new decision may be added such
	// that this instance should no longer quiesce.
	Quiesce() bool

	// Returns true iff all added transactions have been finalized. Note, it is
	// possible that after returning finalized, a new decision may be added such
	// that this instance is no longer finalized.
	Finalized() bool

	// HealthCheck returns information about the consensus health.
	HealthCheck() (interface{}, error)
}

```

avalanchego/snow/consensus/snowstorm/consensus_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowstorm

import (
	"errors"
	"path"
	"reflect"
	"runtime"
	"strings"
	"testing"

	"github.com/prometheus/client_golang/prometheus"

	"github.com/stretchr/testify/require"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow"
	"github.com/ava-labs/avalanchego/snow/choices"
	"github.com/ava-labs/avalanchego/utils"
	"github.com/ava-labs/avalanchego/utils/wrappers"

	sbcon "github.com/ava-labs/avalanchego/snow/consensus/snowball"
)

type testFunc func(*testing.T, Factory)

var (
	testFuncs = []testFunc{
		MetricsTest,
		ParamsTest,
		IssuedTest,
		LeftoverInputTest,
		LowerConfidenceTest,
		MiddleConfidenceTest,
		IndependentTest,
		VirtuousTest,
		IsVirtuousTest,
		QuiesceTest,
		AddNonEmptyWhitelistTest,
		AddWhitelistedVirtuousTest,
		WhitelistConflictsTest,
		AcceptingDependencyTest,
		AcceptingSlowDependencyTest,
		RejectingDependencyTest,
		VacuouslyAcceptedTest,
		ConflictsTest,
		VirtuousDependsOnRogueTest,
		ErrorOnVacuouslyAcceptedTest,
		ErrorOnAcceptedTest,
		ErrorOnRejectingLowerConfidenceConflictTest,
		ErrorOnRejectingHigherConfidenceConflictTest,
		UTXOCleanupTest,
		RemoveVirtuousTest,
	}

	Red, Green, Blue, Alpha *TestTx
)

//  R - G - B - A
func Setup() {
	Red = &TestTx{}
	Green = &TestTx{}
	Blue = &TestTx{}
	Alpha = &TestTx{}

	for i, color := range []*TestTx{Red, Green, Blue, Alpha} {
		color.IDV = ids.Empty.Prefix(uint64(i))
		color.AcceptV = nil
		color.RejectV = nil
		color.StatusV = choices.Processing

		color.DependenciesV = nil
		color.InputIDsV = []ids.ID{}
		color.VerifyV = nil
		color.BytesV = []byte{byte(i)}
	}

	X := ids.Empty.Prefix(4)
	Y := ids.Empty.Prefix(5)
	Z := ids.Empty.Prefix(6)

	Red.InputIDsV = append(Red.InputIDsV, X)
	Green.InputIDsV = append(Green.InputIDsV, X)
	Green.InputIDsV = append(Green.InputIDsV, Y)

	Blue.InputIDsV = append(Blue.InputIDsV, Y)
	Blue.InputIDsV = append(Blue.InputIDsV, Z)

	Alpha.InputIDsV = append(Alpha.InputIDsV, Z)

	errs := wrappers.Errs{}
	errs.Add(
		Red.Verify(),
		Green.Verify(),
		Blue.Verify(),
		Alpha.Verify(),
	)
	if errs.Errored() {
		panic(errs.Err)
	}
}

// Execute all tests against a consensus implementation
func runConsensusTests(t *testing.T, factory Factory, prefix string) {
	for _, test := range testFuncs {
		Setup()
		t.Run(getTestName(test), func(tt *testing.T) {
			test(tt, factory)
		})
	}
	Setup()
	StringTest(t, factory, prefix)
}

func getTestName(i interface{}) string {
	return strings.Split(path.Base(runtime.FuncForPC(reflect.ValueOf(i).Pointer()).Name()), ".")[1]
}

func MetricsTest(t *testing.T, factory Factory) {
	{
		ctx := snow.DefaultConsensusContextTest()
		params := sbcon.Parameters{
			K:                 2,
			Alpha:             2,
			BetaVirtuous:      1,
			BetaRogue:         2,
			ConcurrentRepolls: 1,
		}
		err := ctx.Registerer.Register(prometheus.NewCounter(prometheus.CounterOpts{
			Name: "tx_processing",
		}))
		if err != nil {
			t.Fatal(err)
		}
		graph := factory.New()
		if err := graph.Initialize(ctx, params); err == nil {
			t.Fatalf("should have errored due to a duplicated metric")
		}
	}
	{
		ctx := snow.DefaultConsensusContextTest()
		params := sbcon.Parameters{
			K:                 2,
			Alpha:             2,
			BetaVirtuous:      1,
			BetaRogue:         2,
			ConcurrentRepolls: 1,
		}
		err := ctx.Registerer.Register(prometheus.NewCounter(prometheus.CounterOpts{
			Name: "tx_accepted",
		}))
		if err != nil {
			t.Fatal(err)
		}
		graph := factory.New()
		if err := graph.Initialize(ctx, params); err == nil {
			t.Fatalf("should have errored due to a duplicated metric")
		}
	}
	{
		ctx := snow.DefaultConsensusContextTest()
		params := sbcon.Parameters{
			K:                 2,
			Alpha:             2,
			BetaVirtuous:      1,
			BetaRogue:         2,
			ConcurrentRepolls: 1,
		}
		err := ctx.Registerer.Register(prometheus.NewCounter(prometheus.CounterOpts{
			Name: "tx_rejected",
		}))
		if err != nil {
			t.Fatal(err)
		}
		graph := factory.New()
		if err := graph.Initialize(ctx, params); err == nil {
			t.Fatalf("should have errored due to a duplicated metric")
		}
	}
}

func ParamsTest(t *testing.T, factory Factory) {
	graph := factory.New()

	params := sbcon.Parameters{
		K:                     2,
		Alpha:                 2,
		BetaVirtuous:          1,
		BetaRogue:             2,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}
	err := graph.Initialize(snow.DefaultConsensusContextTest(), params)
	if err != nil {
		t.Fatal(err)
	}

	if p := graph.Parameters(); p.K != params.K {
		t.Fatalf("Wrong K parameter")
	} else if p := graph.Parameters(); p.Alpha != params.Alpha {
		t.Fatalf("Wrong Alpha parameter")
	} else if p := graph.Parameters(); p.BetaVirtuous != params.BetaVirtuous {
		t.Fatalf("Wrong Beta1 parameter")
	} else if p := graph.Parameters(); p.BetaRogue != params.BetaRogue {
		t.Fatalf("Wrong Beta2 parameter")
	}
}

func IssuedTest(t *testing.T, factory Factory) {
	graph := factory.New()

	params := sbcon.Parameters{
		K:                     2,
		Alpha:                 2,
		BetaVirtuous:          1,
		BetaRogue:             1,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}
	err := graph.Initialize(snow.DefaultConsensusContextTest(), params)
	if err != nil {
		t.Fatal(err)
	}

	if issued := graph.Issued(Red); issued {
		t.Fatalf("Haven't issued anything yet.")
	} else if err := graph.Add(Red); err != nil {
		t.Fatal(err)
	} else if issued := graph.Issued(Red); !issued {
		t.Fatalf("Have already issued.")
	}

	_ = Blue.Accept()

	if issued := graph.Issued(Blue); !issued {
		t.Fatalf("Have already accepted.")
	}
}

func LeftoverInputTest(t *testing.T, factory Factory) {
	graph := factory.New()

	params := sbcon.Parameters{
		K:                     2,
		Alpha:                 2,
		BetaVirtuous:          1,
		BetaRogue:             1,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}
	err := graph.Initialize(snow.DefaultConsensusContextTest(), params)
	if err != nil {
		t.Fatal(err)
	}

	if err := graph.Add(Red); err != nil {
		t.Fatal(err)
	} else if err := graph.Add(Green); err != nil {
		t.Fatal(err)
	}

	prefs := graph.Preferences()
	switch {
	case prefs.Len() != 1:
		t.Fatalf("Wrong number of preferences.")
	case !prefs.Contains(Red.ID()):
		t.Fatalf("Wrong preference. Expected %s got %s", Red.ID(), prefs.List()[0])
	case graph.Finalized():
		t.Fatalf("Finalized too early")
	}

	r := ids.Bag{}
	r.SetThreshold(2)
	r.AddCount(Red.ID(), 2)
	if updated, err := graph.RecordPoll(r); err != nil {
		t.Fatal(err)
	} else if !updated {
		t.Fatalf("Should have updated the frontiers")
	}

	prefs = graph.Preferences()
	switch {
	case prefs.Len() != 0:
		t.Fatalf("Wrong number of preferences.")
	case !graph.Finalized():
		t.Fatalf("Finalized too late")
	case Red.Status() != choices.Accepted:
		t.Fatalf("%s should have been accepted", Red.ID())
	case Green.Status() != choices.Rejected:
		t.Fatalf("%s should have been rejected", Green.ID())
	}
}

func LowerConfidenceTest(t *testing.T, factory Factory) {
	graph := factory.New()

	params := sbcon.Parameters{
		K:                     2,
		Alpha:                 2,
		BetaVirtuous:          1,
		BetaRogue:             1,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}
	err := graph.Initialize(snow.DefaultConsensusContextTest(), params)
	if err != nil {
		t.Fatal(err)
	}

	if err := graph.Add(Red); err != nil {
		t.Fatal(err)
	}
	if err := graph.Add(Green); err != nil {
		t.Fatal(err)
	}
	if err := graph.Add(Blue); err != nil {
		t.Fatal(err)
	}

	prefs := graph.Preferences()
	switch {
	case prefs.Len() != 1:
		t.Fatalf("Wrong number of preferences.")
	case !prefs.Contains(Red.ID()):
		t.Fatalf("Wrong preference. Expected %s got %s", Red.ID(), prefs.List()[0])
	case graph.Finalized():
		t.Fatalf("Finalized too early")
	}

	r := ids.Bag{}
	r.SetThreshold(2)
	r.AddCount(Red.ID(), 2)
	if updated, err := graph.RecordPoll(r); err != nil {
		t.Fatal(err)
	} else if !updated {
		t.Fatalf("Should have updated the frontiers")
	}

	prefs = graph.Preferences()
	switch {
	case prefs.Len() != 1:
		t.Fatalf("Wrong number of preferences.")
	case !prefs.Contains(Blue.ID()):
		t.Fatalf("Wrong preference. Expected %s", Blue.ID())
	case graph.Finalized():
		t.Fatalf("Finalized too early")
	}
}

func MiddleConfidenceTest(t *testing.T, factory Factory) {
	graph := factory.New()

	params := sbcon.Parameters{
		K:                     2,
		Alpha:                 2,
		BetaVirtuous:          1,
		BetaRogue:             1,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}
	err := graph.Initialize(snow.DefaultConsensusContextTest(), params)
	if err != nil {
		t.Fatal(err)
	}

	if err := graph.Add(Red); err != nil {
		t.Fatal(err)
	}
	if err := graph.Add(Green); err != nil {
		t.Fatal(err)
	}
	if err := graph.Add(Alpha); err != nil {
		t.Fatal(err)
	}
	if err := graph.Add(Blue); err != nil {
		t.Fatal(err)
	}

	prefs := graph.Preferences()
	switch {
	case prefs.Len() != 2:
		t.Fatalf("Wrong number of preferences.")
	case !prefs.Contains(Red.ID()):
		t.Fatalf("Wrong preference. Expected %s", Red.ID())
	case !prefs.Contains(Alpha.ID()):
		t.Fatalf("Wrong preference. Expected %s", Alpha.ID())
	case graph.Finalized():
		t.Fatalf("Finalized too early")
	}

	r := ids.Bag{}
	r.SetThreshold(2)
	r.AddCount(Red.ID(), 2)
	if updated, err := graph.RecordPoll(r); err != nil {
		t.Fatal(err)
	} else if !updated {
		t.Fatalf("Should have updated the frontiers")
	}

	prefs = graph.Preferences()
	switch {
	case prefs.Len() != 1:
		t.Fatalf("Wrong number of preferences.")
	case !prefs.Contains(Alpha.ID()):
		t.Fatalf("Wrong preference. Expected %s", Alpha.ID())
	case graph.Finalized():
		t.Fatalf("Finalized too early")
	}
}

func IndependentTest(t *testing.T, factory Factory) {
	graph := factory.New()

	params := sbcon.Parameters{
		K:                     2,
		Alpha:                 2,
		BetaVirtuous:          2,
		BetaRogue:             2,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}
	err := graph.Initialize(snow.DefaultConsensusContextTest(), params)
	if err != nil {
		t.Fatal(err)
	}

	if err := graph.Add(Red); err != nil {
		t.Fatal(err)
	}
	if err := graph.Add(Alpha); err != nil {
		t.Fatal(err)
	}

	prefs := graph.Preferences()
	switch {
	case prefs.Len() != 2:
		t.Fatalf("Wrong number of preferences.")
	case !prefs.Contains(Red.ID()):
		t.Fatalf("Wrong preference. Expected %s", Red.ID())
	case !prefs.Contains(Alpha.ID()):
		t.Fatalf("Wrong preference. Expected %s", Alpha.ID())
	case graph.Finalized():
		t.Fatalf("Finalized too early")
	}

	ra := ids.Bag{}
	ra.SetThreshold(2)
	ra.AddCount(Red.ID(), 2)
	ra.AddCount(Alpha.ID(), 2)
	if updated, err := graph.RecordPoll(ra); err != nil {
		t.Fatal(err)
	} else if updated {
		t.Fatalf("Shouldn't have updated the frontiers")
	} else if prefs := graph.Preferences(); prefs.Len() != 2 {
		t.Fatalf("Wrong number of preferences.")
	} else if !prefs.Contains(Red.ID()) {
		t.Fatalf("Wrong preference. Expected %s", Red.ID())
	} else if !prefs.Contains(Alpha.ID()) {
		t.Fatalf("Wrong preference. Expected %s", Alpha.ID())
	} else if graph.Finalized() {
		t.Fatalf("Finalized too early")
	} else if updated, err := graph.RecordPoll(ra); err != nil {
		t.Fatal(err)
	} else if !updated {
		t.Fatalf("Should have updated the frontiers")
	} else if prefs := graph.Preferences(); prefs.Len() != 0 {
		t.Fatalf("Wrong number of preferences.")
	} else if !graph.Finalized() {
		t.Fatalf("Finalized too late")
	}
}

func VirtuousTest(t *testing.T, factory Factory) {
	graph := factory.New()

	params := sbcon.Parameters{
		K:                     2,
		Alpha:                 2,
		BetaVirtuous:          1,
		BetaRogue:             1,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}
	err := graph.Initialize(snow.DefaultConsensusContextTest(), params)
	if err != nil {
		t.Fatal(err)
	}

	if err := graph.Add(Red); err != nil {
		t.Fatal(err)
	} else if virtuous := graph.Virtuous(); virtuous.Len() != 1 {
		t.Fatalf("Wrong number of virtuous.")
	} else if !virtuous.Contains(Red.ID()) {
		t.Fatalf("Wrong virtuous. Expected %s", Red.ID())
	} else if err := graph.Add(Alpha); err != nil {
		t.Fatal(err)
	} else if virtuous := graph.Virtuous(); virtuous.Len() != 2 {
		t.Fatalf("Wrong number of virtuous.")
	} else if !virtuous.Contains(Red.ID()) {
		t.Fatalf("Wrong virtuous. Expected %s", Red.ID())
	} else if !virtuous.Contains(Alpha.ID()) {
		t.Fatalf("Wrong virtuous. Expected %s", Alpha.ID())
	} else if err := graph.Add(Green); err != nil {
		t.Fatal(err)
	} else if virtuous := graph.Virtuous(); virtuous.Len() != 1 {
		t.Fatalf("Wrong number of virtuous.")
	} else if !virtuous.Contains(Alpha.ID()) {
		t.Fatalf("Wrong virtuous. Expected %s", Alpha.ID())
	} else if err := graph.Add(Blue); err != nil {
		t.Fatal(err)
	} else if virtuous := graph.Virtuous(); virtuous.Len() != 0 {
		t.Fatalf("Wrong number of virtuous.")
	}
}

func IsVirtuousTest(t *testing.T, factory Factory) {
	graph := factory.New()

	params := sbcon.Parameters{
		K:                     2,
		Alpha:                 2,
		BetaVirtuous:          1,
		BetaRogue:             1,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}
	if err := graph.Initialize(snow.DefaultConsensusContextTest(), params); err != nil {
		t.Fatal(err)
	}

	switch {
	case !graph.IsVirtuous(Red):
		t.Fatalf("Should be virtuous")
	case !graph.IsVirtuous(Green):
		t.Fatalf("Should be virtuous")
	case !graph.IsVirtuous(Blue):
		t.Fatalf("Should be virtuous")
	case !graph.IsVirtuous(Alpha):
		t.Fatalf("Should be virtuous")
	}

	err := graph.Add(Red)
	switch {
	case err != nil:
		t.Fatal(err)
	case !graph.IsVirtuous(Red):
		t.Fatalf("Should be virtuous")
	case graph.IsVirtuous(Green):
		t.Fatalf("Should not be virtuous")
	case !graph.IsVirtuous(Blue):
		t.Fatalf("Should be virtuous")
	case !graph.IsVirtuous(Alpha):
		t.Fatalf("Should be virtuous")
	}

	err = graph.Add(Green)
	switch {
	case err != nil:
		t.Fatal(err)
	case graph.IsVirtuous(Red):
		t.Fatalf("Should not be virtuous")
	case graph.IsVirtuous(Green):
		t.Fatalf("Should not be virtuous")
	case graph.IsVirtuous(Blue):
		t.Fatalf("Should not be virtuous")
	}
}

func QuiesceTest(t *testing.T, factory Factory) {
	graph := factory.New()

	params := sbcon.Parameters{
		K:                     2,
		Alpha:                 2,
		BetaVirtuous:          1,
		BetaRogue:             1,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}
	err := graph.Initialize(snow.DefaultConsensusContextTest(), params)
	if err != nil {
		t.Fatal(err)
	}

	if !graph.Quiesce() {
		t.Fatalf("Should quiesce")
	} else if err := graph.Add(Red); err != nil {
		t.Fatal(err)
	} else if graph.Quiesce() {
		t.Fatalf("Shouldn't quiesce")
	} else if err := graph.Add(Green); err != nil {
		t.Fatal(err)
	} else if !graph.Quiesce() {
		t.Fatalf("Should quiesce")
	}
}

func AddNonEmptyWhitelistTest(t *testing.T, factory Factory) {
	graph := factory.New()

	params := sbcon.Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          1,
		BetaRogue:             1,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}
	ctx := snow.DefaultConsensusContextTest()
	reg := prometheus.NewRegistry()
	ctx.Registerer = reg
	err := graph.Initialize(ctx, params)
	if err != nil {
		t.Fatal(err)
	}

	/*
	                    [tx1]
	                   ⬈     ⬉
	              [tx2]       [tx3]
	            ⬈      ⬉      ⬈
	       [tx6]         [tx4]
	         ⬆             ⬆
	    {stop stx7}    {stop stx5}
	   Add stx5 => no conflict
	   Add  tx6 => stx5 conflicts with tx6
	   Add stx7 => stx5 conflicts with tx6
	               stx5 conflicts with stx7
	               stx7 conflicts with tx3
	               stx7 conflicts with tx4
	*/
	tx1 := &TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.Empty.Prefix(1),
			StatusV: choices.Processing,
		},
		InputIDsV: []ids.ID{ids.GenerateTestID()},
		BytesV:    []byte{1},
	}
	tx2 := &TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.Empty.Prefix(2),
			StatusV: choices.Processing,
		},
		InputIDsV:     []ids.ID{ids.GenerateTestID()},
		DependenciesV: []Tx{tx1},
		BytesV:        []byte{2},
	}
	tx3 := &TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.Empty.Prefix(3),
			StatusV: choices.Processing,
		},
		InputIDsV:     []ids.ID{ids.GenerateTestID()},
		DependenciesV: []Tx{tx1},
		BytesV:        []byte{3},
	}
	tx4 := &TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.Empty.Prefix(4),
			StatusV: choices.Processing,
		},
		InputIDsV:     []ids.ID{ids.GenerateTestID()},
		DependenciesV: []Tx{tx2, tx3},
		BytesV:        []byte{4},
	}
	stx5 := &TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.Empty.Prefix(5),
			StatusV: choices.Processing,
		},
		InputIDsV:     []ids.ID{ids.GenerateTestID()},
		DependenciesV: []Tx{tx1, tx2, tx3, tx4},
		HasWhitelistV: true,
		WhitelistV: ids.Set{
			tx1.IDV: struct{}{},
			tx2.IDV: struct{}{},
			tx3.IDV: struct{}{},
			tx4.IDV: struct{}{},
		},
		BytesV: []byte{5},
	}
	tx6 := &TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.Empty.Prefix(6),
			StatusV: choices.Processing,
		},
		InputIDsV:     []ids.ID{ids.GenerateTestID()},
		DependenciesV: []Tx{tx2},
		BytesV:        []byte{6},
	}
	stx7 := &TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.Empty.Prefix(7),
			StatusV: choices.Processing,
		},
		InputIDsV:     []ids.ID{ids.GenerateTestID()},
		DependenciesV: []Tx{tx1, tx2, tx6},
		HasWhitelistV: true,
		WhitelistV: ids.Set{
			tx1.IDV: struct{}{},
			tx2.IDV: struct{}{},
			tx6.IDV: struct{}{},
		},
		BytesV: []byte{7},
	}

	txs := []*TestTx{tx1, tx2, tx3, tx4, stx5, tx6, stx7}
	for _, tx := range txs {
		if err := graph.Add(tx); err != nil {
			t.Fatal(err)
		}
	}

	// check if stop vertex has been issued but not accepted
	mss := gatherCounterGauge(t, reg)
	require.Equal(t, 5., mss["rogue_tx_processing"])
	require.Equal(t, 2., mss["virtuous_tx_processing"])
	require.Equal(t, 0., mss["whitelist_tx_accepted_count"])
	require.Equal(t, 2., mss["whitelist_tx_processing"])

	vset1 := graph.Virtuous()
	if !vset1.Equals(ids.Set{
		tx1.IDV: struct{}{},
		tx2.IDV: struct{}{},
	}) {
		t.Fatalf("unexpected virtuous %v", vset1)
	}
	pset1 := graph.Preferences()
	if !pset1.Equals(ids.Set{
		tx1.IDV:  struct{}{},
		tx2.IDV:  struct{}{},
		tx3.IDV:  struct{}{},
		tx4.IDV:  struct{}{},
		stx5.IDV: struct{}{},
	}) {
		t.Fatalf("unexpected preferences %v", pset1)
	}
	if graph.Finalized() {
		t.Fatal("unexpected Finalized")
	}

	r := ids.Bag{}
	r.SetThreshold(2)
	r.AddCount(tx1.ID(), 2)

	updated, err := graph.RecordPoll(r)
	if err != nil {
		t.Fatal(err)
	}
	if !updated {
		t.Fatal("should have updated the frontiers")
	}

	vset2 := graph.Virtuous()
	if !vset2.Equals(ids.Set{
		tx2.IDV: struct{}{},
	}) {
		t.Fatalf("unexpected virtuous %v", vset2)
	}
	pset2 := graph.Preferences()
	if !pset2.Equals(ids.Set{
		tx2.IDV:  struct{}{},
		tx3.IDV:  struct{}{},
		tx4.IDV:  struct{}{},
		stx5.IDV: struct{}{},
	}) {
		t.Fatalf("unexpected preferences %v", pset2)
	}

	mss = gatherCounterGauge(t, reg)
	require.Equal(t, 5., mss["rogue_tx_processing"])
	require.Equal(t, 1., mss["virtuous_tx_processing"])
	require.Equal(t, 0., mss["whitelist_tx_accepted_count"])
	require.Equal(t, 2., mss["whitelist_tx_processing"])
}

func gatherCounterGauge(t *testing.T, reg *prometheus.Registry) map[string]float64 {
	ms, err := reg.Gather()
	if err != nil {
		t.Fatal(err)
	}
	mss := make(map[string]float64)
	for _, mf := range ms {
		name := mf.GetName()
		for _, m := range mf.GetMetric() {
			cnt := m.GetCounter()
			if cnt != nil {
				mss[name] = cnt.GetValue()
				break
			}
			gg := m.GetGauge()
			if gg != nil {
				mss[name] = gg.GetValue()
				break
			}
		}
	}
	return mss
}

func AddWhitelistedVirtuousTest(t *testing.T, factory Factory) {
	graph := factory.New()

	params := sbcon.Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          1,
		BetaRogue:             1,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}
	err := graph.Initialize(snow.DefaultConsensusContextTest(), params)
	if err != nil {
		t.Fatal(err)
	}

	tx0 := &TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		InputIDsV: []ids.ID{ids.GenerateTestID()},
		BytesV:    utils.RandomBytes(32),
	}
	tx1 := &TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		InputIDsV:     []ids.ID{ids.GenerateTestID()},
		BytesV:        utils.RandomBytes(32),
		HasWhitelistV: true,
	}

	txs := []*TestTx{tx0, tx1}
	for _, tx := range txs {
		if err := graph.Add(tx); err != nil {
			t.Fatal(err)
		}
	}

	vset := graph.Virtuous()
	if vset.Len() != 0 {
		t.Fatalf("unexpected virtuous %v", vset)
	}
}

// When a transaction supporting whitelisting is added to the conflict graph,
// all txs outside of its whitelist should be marked in conflict.
func WhitelistConflictsTest(t *testing.T, factory Factory) {
	graph := factory.New()

	params := sbcon.Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          1,
		BetaRogue:             2,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}
	err := graph.Initialize(snow.DefaultConsensusContextTest(), params)
	if err != nil {
		t.Fatal(err)
	}

	n := 10
	txIDs := make([]ids.ID, n)
	for i := range txIDs {
		txIDs[i] = ids.GenerateTestID()
	}
	allTxIDs := ids.NewSet(n)
	allTxIDs.Add(txIDs...)

	// each spending each other
	allTxs := make([]Tx, n)
	for i, txID := range txIDs {
		tx := &TestTx{
			TestDecidable: choices.TestDecidable{
				IDV:     txID,
				AcceptV: nil,
				StatusV: choices.Processing,
			},
			InputIDsV:     []ids.ID{txID},
			HasWhitelistV: false,
			WhitelistV:    nil,
		}
		allTxs[i] = tx
		if err := graph.Add(tx); err != nil {
			t.Fatal(err)
		}
	}

	whitelist := ids.NewSet(1)
	whitelist.Add(ids.GenerateTestID())

	// make whitelist transaction that conflicts with tx outside of its
	// whitelist
	wlTxID := ids.GenerateTestID()
	wlTx := &TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     wlTxID,
			AcceptV: nil,
			StatusV: choices.Processing,
		},
		InputIDsV:     []ids.ID{wlTxID},
		HasWhitelistV: true,
		WhitelistV:    whitelist,
		WhitelistErrV: nil,
	}
	if err := graph.Add(wlTx); err != nil {
		t.Fatal(err)
	}

	for _, tx := range allTxs {
		conflicts := graph.Conflicts(tx)
		if conflicts.Len() != 1 {
			t.Fatal("wrong number of conflicts")
		}
		if !conflicts.Contains(wlTxID) {
			t.Fatal("unexpected conflict")
		}
	}

	// the transitive vertex should be conflicting with everything
	conflicts := graph.Conflicts(wlTx)
	if !allTxIDs.Equals(conflicts) {
		t.Fatal("transitive vertex outs != all txs")
	}
}

func AcceptingDependencyTest(t *testing.T, factory Factory) {
	graph := factory.New()

	purple := &TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.Empty.Prefix(7),
			StatusV: choices.Processing,
		},
		DependenciesV: []Tx{Red},
	}
	purple.InputIDsV = append(purple.InputIDsV, ids.Empty.Prefix(8))

	params := sbcon.Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          1,
		BetaRogue:             2,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}
	if err := graph.Initialize(snow.DefaultConsensusContextTest(), params); err != nil {
		t.Fatal(err)
	}

	if err := graph.Add(Red); err != nil {
		t.Fatal(err)
	}
	if err := graph.Add(Green); err != nil {
		t.Fatal(err)
	}
	if err := graph.Add(purple); err != nil {
		t.Fatal(err)
	}

	prefs := graph.Preferences()
	switch {
	case prefs.Len() != 2:
		t.Fatalf("Wrong number of preferences.")
	case !prefs.Contains(Red.ID()):
		t.Fatalf("Wrong preference. Expected %s", Red.ID())
	case !prefs.Contains(purple.ID()):
		t.Fatalf("Wrong preference. Expected %s", purple.ID())
	case Red.Status() != choices.Processing:
		t.Fatalf("Wrong status. %s should be %s", Red.ID(), choices.Processing)
	case Green.Status() != choices.Processing:
		t.Fatalf("Wrong status. %s should be %s", Green.ID(), choices.Processing)
	case purple.Status() != choices.Processing:
		t.Fatalf("Wrong status. %s should be %s", purple.ID(), choices.Processing)
	}

	g := ids.Bag{}
	g.Add(Green.ID())
	if updated, err := graph.RecordPoll(g); err != nil {
		t.Fatal(err)
	} else if !updated {
		t.Fatalf("Should have updated the frontiers")
	}

	prefs = graph.Preferences()
	switch {
	case prefs.Len() != 2:
		t.Fatalf("Wrong number of preferences.")
	case !prefs.Contains(Green.ID()):
		t.Fatalf("Wrong preference. Expected %s", Green.ID())
	case !prefs.Contains(purple.ID()):
		t.Fatalf("Wrong preference. Expected %s", purple.ID())
	case Red.Status() != choices.Processing:
		t.Fatalf("Wrong status. %s should be %s", Red.ID(), choices.Processing)
	case Green.Status() != choices.Processing:
		t.Fatalf("Wrong status. %s should be %s", Green.ID(), choices.Processing)
	case purple.Status() != choices.Processing:
		t.Fatalf("Wrong status. %s should be %s", purple.ID(), choices.Processing)
	}

	rp := ids.Bag{}
	rp.Add(Red.ID(), purple.ID())
	if updated, err := graph.RecordPoll(rp); err != nil {
		t.Fatal(err)
	} else if updated {
		t.Fatalf("Shouldn't have updated the frontiers")
	}

	prefs = graph.Preferences()
	switch {
	case prefs.Len() != 2:
		t.Fatalf("Wrong number of preferences.")
	case !prefs.Contains(Green.ID()):
		t.Fatalf("Wrong preference. Expected %s", Green.ID())
	case !prefs.Contains(purple.ID()):
		t.Fatalf("Wrong preference. Expected %s", purple.ID())
	case Red.Status() != choices.Processing:
		t.Fatalf("Wrong status. %s should be %s", Red.ID(), choices.Processing)
	case Green.Status() != choices.Processing:
		t.Fatalf("Wrong status. %s should be %s", Green.ID(), choices.Processing)
	case purple.Status() != choices.Processing:
		t.Fatalf("Wrong status. %s should be %s", purple.ID(), choices.Processing)
	}

	r := ids.Bag{}
	r.Add(Red.ID())
	if updated, err := graph.RecordPoll(r); err != nil {
		t.Fatal(err)
	} else if !updated {
		t.Fatalf("Should have updated the frontiers")
	}

	prefs = graph.Preferences()
	switch {
	case prefs.Len() != 0:
		t.Fatalf("Wrong number of preferences.")
	case Red.Status() != choices.Accepted:
		t.Fatalf("Wrong status. %s should be %s", Red.ID(), choices.Accepted)
	case Green.Status() != choices.Rejected:
		t.Fatalf("Wrong status. %s should be %s", Green.ID(), choices.Rejected)
	case purple.Status() != choices.Accepted:
		t.Fatalf("Wrong status. %s should be %s", purple.ID(), choices.Accepted)
	}
}

type singleAcceptTx struct {
	Tx

	t        *testing.T
	accepted bool
}

func (tx *singleAcceptTx) Accept() error {
	if tx.accepted {
		tx.t.Fatalf("accept called multiple times")
	}
	tx.accepted = true
	return tx.Tx.Accept()
}

func AcceptingSlowDependencyTest(t *testing.T, factory Factory) {
	graph := factory.New()

	rawPurple := &TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.Empty.Prefix(7),
			StatusV: choices.Processing,
		},
		DependenciesV: []Tx{Red},
	}
	rawPurple.InputIDsV = append(rawPurple.InputIDsV, ids.Empty.Prefix(8))

	purple := &singleAcceptTx{
		Tx: rawPurple,
		t:  t,
	}

	params := sbcon.Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          1,
		BetaRogue:             2,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}
	err := graph.Initialize(snow.DefaultConsensusContextTest(), params)
	if err != nil {
		t.Fatal(err)
	}

	if err := graph.Add(Red); err != nil {
		t.Fatal(err)
	}
	if err := graph.Add(Green); err != nil {
		t.Fatal(err)
	}
	if err := graph.Add(purple); err != nil {
		t.Fatal(err)
	}

	prefs := graph.Preferences()
	switch {
	case prefs.Len() != 2:
		t.Fatalf("Wrong number of preferences.")
	case !prefs.Contains(Red.ID()):
		t.Fatalf("Wrong preference. Expected %s", Red.ID())
	case !prefs.Contains(purple.ID()):
		t.Fatalf("Wrong preference. Expected %s", purple.ID())
	case Red.Status() != choices.Processing:
		t.Fatalf("Wrong status. %s should be %s", Red.ID(), choices.Processing)
	case Green.Status() != choices.Processing:
		t.Fatalf("Wrong status. %s should be %s", Green.ID(), choices.Processing)
	case purple.Status() != choices.Processing:
		t.Fatalf("Wrong status. %s should be %s", purple.ID(), choices.Processing)
	}

	g := ids.Bag{}
	g.Add(Green.ID())
	if updated, err := graph.RecordPoll(g); err != nil {
		t.Fatal(err)
	} else if !updated {
		t.Fatalf("Should have updated the frontiers")
	}

	prefs = graph.Preferences()
	switch {
	case prefs.Len() != 2:
		t.Fatalf("Wrong number of preferences.")
	case !prefs.Contains(Green.ID()):
		t.Fatalf("Wrong preference. Expected %s", Green.ID())
	case !prefs.Contains(purple.ID()):
		t.Fatalf("Wrong preference. Expected %s", purple.ID())
	case Red.Status() != choices.Processing:
		t.Fatalf("Wrong status. %s should be %s", Red.ID(), choices.Processing)
	case Green.Status() != choices.Processing:
		t.Fatalf("Wrong status. %s should be %s", Green.ID(), choices.Processing)
	case purple.Status() != choices.Processing:
		t.Fatalf("Wrong status. %s should be %s", purple.ID(), choices.Processing)
	}

	p := ids.Bag{}
	p.Add(purple.ID())
	if updated, err := graph.RecordPoll(p); err != nil {
		t.Fatal(err)
	} else if updated {
		t.Fatalf("Shouldn't have updated the frontiers")
	}

	prefs = graph.Preferences()
	switch {
	case prefs.Len() != 2:
		t.Fatalf("Wrong number of preferences.")
	case !prefs.Contains(Green.ID()):
		t.Fatalf("Wrong preference. Expected %s", Green.ID())
	case !prefs.Contains(purple.ID()):
		t.Fatalf("Wrong preference. Expected %s", purple.ID())
	case Red.Status() != choices.Processing:
		t.Fatalf("Wrong status. %s should be %s", Red.ID(), choices.Processing)
	case Green.Status() != choices.Processing:
		t.Fatalf("Wrong status. %s should be %s", Green.ID(), choices.Processing)
	case purple.Status() != choices.Processing:
		t.Fatalf("Wrong status. %s should be %s", purple.ID(), choices.Processing)
	}

	rp := ids.Bag{}
	rp.Add(Red.ID(), purple.ID())
	if updated, err := graph.RecordPoll(rp); err != nil {
		t.Fatal(err)
	} else if updated {
		t.Fatalf("Shouldn't have updated the frontiers")
	}

	prefs = graph.Preferences()
	switch {
	case prefs.Len() != 2:
		t.Fatalf("Wrong number of preferences.")
	case !prefs.Contains(Green.ID()):
		t.Fatalf("Wrong preference. Expected %s", Green.ID())
	case !prefs.Contains(purple.ID()):
		t.Fatalf("Wrong preference. Expected %s", purple.ID())
	case Red.Status() != choices.Processing:
		t.Fatalf("Wrong status. %s should be %s", Red.ID(), choices.Processing)
	case Green.Status() != choices.Processing:
		t.Fatalf("Wrong status. %s should be %s", Green.ID(), choices.Processing)
	case purple.Status() != choices.Processing:
		t.Fatalf("Wrong status. %s should be %s", purple.ID(), choices.Processing)
	}

	r := ids.Bag{}
	r.Add(Red.ID())
	if updated, err := graph.RecordPoll(r); err != nil {
		t.Fatal(err)
	} else if !updated {
		t.Fatalf("Should have updated the frontiers")
	}

	prefs = graph.Preferences()
	switch {
	case prefs.Len() != 0:
		t.Fatalf("Wrong number of preferences.")
	case Red.Status() != choices.Accepted:
		t.Fatalf("Wrong status. %s should be %s", Red.ID(), choices.Accepted)
	case Green.Status() != choices.Rejected:
		t.Fatalf("Wrong status. %s should be %s", Green.ID(), choices.Rejected)
	case purple.Status() != choices.Accepted:
		t.Fatalf("Wrong status. %s should be %s", purple.ID(), choices.Accepted)
	}
}

func RejectingDependencyTest(t *testing.T, factory Factory) {
	graph := factory.New()

	purple := &TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.Empty.Prefix(7),
			StatusV: choices.Processing,
		},
		DependenciesV: []Tx{Red, Blue},
	}
	purple.InputIDsV = append(purple.InputIDsV, ids.Empty.Prefix(8))

	params := sbcon.Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          1,
		BetaRogue:             2,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}
	err := graph.Initialize(snow.DefaultConsensusContextTest(), params)
	if err != nil {
		t.Fatal(err)
	}

	if err := graph.Add(Red); err != nil {
		t.Fatal(err)
	}
	if err := graph.Add(Green); err != nil {
		t.Fatal(err)
	}
	if err := graph.Add(Blue); err != nil {
		t.Fatal(err)
	}
	if err := graph.Add(purple); err != nil {
		t.Fatal(err)
	}

	prefs := graph.Preferences()
	switch {
	case prefs.Len() != 2:
		t.Fatalf("Wrong number of preferences.")
	case !prefs.Contains(Red.ID()):
		t.Fatalf("Wrong preference. Expected %s", Red.ID())
	case !prefs.Contains(purple.ID()):
		t.Fatalf("Wrong preference. Expected %s", purple.ID())
	case Red.Status() != choices.Processing:
		t.Fatalf("Wrong status. %s should be %s", Red.ID(), choices.Processing)
	case Green.Status() != choices.Processing:
		t.Fatalf("Wrong status. %s should be %s", Green.ID(), choices.Processing)
	case Blue.Status() != choices.Processing:
		t.Fatalf("Wrong status. %s should be %s", Blue.ID(), choices.Processing)
	case purple.Status() != choices.Processing:
		t.Fatalf("Wrong status. %s should be %s", purple.ID(), choices.Processing)
	}

	gp := ids.Bag{}
	gp.Add(Green.ID(), purple.ID())
	if updated, err := graph.RecordPoll(gp); err != nil {
		t.Fatal(err)
	} else if !updated {
		t.Fatalf("Should have updated the frontiers")
	}

	prefs = graph.Preferences()
	switch {
	case prefs.Len() != 2:
		t.Fatalf("Wrong number of preferences.")
	case !prefs.Contains(Green.ID()):
		t.Fatalf("Wrong preference. Expected %s", Green.ID())
	case !prefs.Contains(purple.ID()):
		t.Fatalf("Wrong preference. Expected %s", purple.ID())
	case Red.Status() != choices.Processing:
		t.Fatalf("Wrong status. %s should be %s", Red.ID(), choices.Processing)
	case Green.Status() != choices.Processing:
		t.Fatalf("Wrong status. %s should be %s", Green.ID(), choices.Processing)
	case Blue.Status() != choices.Processing:
		t.Fatalf("Wrong status. %s should be %s", Blue.ID(), choices.Processing)
	case purple.Status() != choices.Processing:
		t.Fatalf("Wrong status. %s should be %s", purple.ID(), choices.Processing)
	}

	if updated, err := graph.RecordPoll(gp); err != nil {
		t.Fatal(err)
	} else if !updated {
		t.Fatalf("Should have updated the frontiers")
	}

	prefs = graph.Preferences()
	switch {
	case prefs.Len() != 0:
		t.Fatalf("Wrong number of preferences.")
	case Red.Status() != choices.Rejected:
		t.Fatalf("Wrong status. %s should be %s", Red.ID(), choices.Rejected)
	case Green.Status() != choices.Accepted:
		t.Fatalf("Wrong status. %s should be %s", Green.ID(), choices.Accepted)
	case Blue.Status() != choices.Rejected:
		t.Fatalf("Wrong status. %s should be %s", Blue.ID(), choices.Rejected)
	case purple.Status() != choices.Rejected:
		t.Fatalf("Wrong status. %s should be %s", purple.ID(), choices.Rejected)
	}
}

func VacuouslyAcceptedTest(t *testing.T, factory Factory) {
	graph := factory.New()

	purple := &TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.Empty.Prefix(7),
		StatusV: choices.Processing,
	}}

	params := sbcon.Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          1,
		BetaRogue:             2,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}
	err := graph.Initialize(snow.DefaultConsensusContextTest(), params)
	if err != nil {
		t.Fatal(err)
	}

	if err := graph.Add(purple); err != nil {
		t.Fatal(err)
	} else if prefs := graph.Preferences(); prefs.Len() != 0 {
		t.Fatalf("Wrong number of preferences.")
	} else if status := purple.Status(); status != choices.Accepted {
		t.Fatalf("Wrong status. %s should be %s", purple.ID(), choices.Accepted)
	}
}

func ConflictsTest(t *testing.T, factory Factory) {
	graph := factory.New()

	params := sbcon.Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          1,
		BetaRogue:             2,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}
	err := graph.Initialize(snow.DefaultConsensusContextTest(), params)
	if err != nil {
		t.Fatal(err)
	}

	conflictInputID := ids.Empty.Prefix(0)

	purple := &TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.Empty.Prefix(6),
			StatusV: choices.Processing,
		},
		InputIDsV: []ids.ID{conflictInputID},
	}

	orange := &TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.Empty.Prefix(7),
			StatusV: choices.Processing,
		},
		InputIDsV: []ids.ID{conflictInputID},
	}

	if err := graph.Add(purple); err != nil {
		t.Fatal(err)
	} else if orangeConflicts := graph.Conflicts(orange); orangeConflicts.Len() != 1 {
		t.Fatalf("Wrong number of conflicts")
	} else if !orangeConflicts.Contains(purple.IDV) {
		t.Fatalf("Conflicts does not contain the right transaction")
	} else if err := graph.Add(orange); err != nil {
		t.Fatal(err)
	} else if orangeConflicts := graph.Conflicts(orange); orangeConflicts.Len() != 1 {
		t.Fatalf("Wrong number of conflicts")
	} else if !orangeConflicts.Contains(purple.IDV) {
		t.Fatalf("Conflicts does not contain the right transaction")
	}
}

func VirtuousDependsOnRogueTest(t *testing.T, factory Factory) {
	graph := factory.New()

	params := sbcon.Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          1,
		BetaRogue:             2,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}
	err := graph.Initialize(snow.DefaultConsensusContextTest(), params)
	if err != nil {
		t.Fatal(err)
	}

	rogue1 := &TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.Empty.Prefix(0),
		StatusV: choices.Processing,
	}}
	rogue2 := &TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.Empty.Prefix(1),
		StatusV: choices.Processing,
	}}
	virtuous := &TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.Empty.Prefix(2),
			StatusV: choices.Processing,
		},
		DependenciesV: []Tx{rogue1},
	}

	input1 := ids.Empty.Prefix(3)
	input2 := ids.Empty.Prefix(4)

	rogue1.InputIDsV = append(rogue1.InputIDsV, input1)
	rogue2.InputIDsV = append(rogue2.InputIDsV, input1)

	virtuous.InputIDsV = append(virtuous.InputIDsV, input2)

	if err := graph.Add(rogue1); err != nil {
		t.Fatal(err)
	} else if err := graph.Add(rogue2); err != nil {
		t.Fatal(err)
	} else if err := graph.Add(virtuous); err != nil {
		t.Fatal(err)
	}

	votes := ids.Bag{}
	votes.Add(rogue1.ID())
	votes.Add(virtuous.ID())
	if updated, err := graph.RecordPoll(votes); err != nil {
		t.Fatal(err)
	} else if updated {
		t.Fatalf("Shouldn't have updated the frontiers")
	} else if status := rogue1.Status(); status != choices.Processing {
		t.Fatalf("Rogue Tx is %s expected %s", status, choices.Processing)
	} else if status := rogue2.Status(); status != choices.Processing {
		t.Fatalf("Rogue Tx is %s expected %s", status, choices.Processing)
	} else if status := virtuous.Status(); status != choices.Processing {
		t.Fatalf("Virtuous Tx is %s expected %s", status, choices.Processing)
	} else if !graph.Quiesce() {
		t.Fatalf("Should quiesce as there are no pending virtuous transactions")
	}
}

func ErrorOnVacuouslyAcceptedTest(t *testing.T, factory Factory) {
	graph := factory.New()

	purple := &TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.Empty.Prefix(7),
		AcceptV: errors.New(""),
		StatusV: choices.Processing,
	}}

	params := sbcon.Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          1,
		BetaRogue:             2,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}
	err := graph.Initialize(snow.DefaultConsensusContextTest(), params)
	if err != nil {
		t.Fatal(err)
	}

	if err := graph.Add(purple); err == nil {
		t.Fatalf("Should have errored on acceptance")
	}
}

func ErrorOnAcceptedTest(t *testing.T, factory Factory) {
	graph := factory.New()

	purple := &TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.Empty.Prefix(7),
		AcceptV: errors.New(""),
		StatusV: choices.Processing,
	}}
	purple.InputIDsV = append(purple.InputIDsV, ids.Empty.Prefix(4))

	params := sbcon.Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          1,
		BetaRogue:             2,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}
	err := graph.Initialize(snow.DefaultConsensusContextTest(), params)
	if err != nil {
		t.Fatal(err)
	}

	if err := graph.Add(purple); err != nil {
		t.Fatal(err)
	}

	votes := ids.Bag{}
	votes.Add(purple.ID())
	if _, err := graph.RecordPoll(votes); err == nil {
		t.Fatalf("Should have errored on accepting an invalid tx")
	}
}

func ErrorOnRejectingLowerConfidenceConflictTest(t *testing.T, factory Factory) {
	graph := factory.New()

	X := ids.Empty.Prefix(4)

	purple := &TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.Empty.Prefix(7),
		StatusV: choices.Processing,
	}}
	purple.InputIDsV = append(purple.InputIDsV, X)

	pink := &TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.Empty.Prefix(8),
		RejectV: errors.New(""),
		StatusV: choices.Processing,
	}}
	pink.InputIDsV = append(pink.InputIDsV, X)

	params := sbcon.Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          1,
		BetaRogue:             1,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}
	err := graph.Initialize(snow.DefaultConsensusContextTest(), params)
	if err != nil {
		t.Fatal(err)
	}

	if err := graph.Add(purple); err != nil {
		t.Fatal(err)
	} else if err := graph.Add(pink); err != nil {
		t.Fatal(err)
	}

	votes := ids.Bag{}
	votes.Add(purple.ID())
	if _, err := graph.RecordPoll(votes); err == nil {
		t.Fatalf("Should have errored on rejecting an invalid tx")
	}
}

func ErrorOnRejectingHigherConfidenceConflictTest(t *testing.T, factory Factory) {
	graph := factory.New()

	X := ids.Empty.Prefix(4)

	purple := &TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.Empty.Prefix(7),
		StatusV: choices.Processing,
	}}
	purple.InputIDsV = append(purple.InputIDsV, X)

	pink := &TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.Empty.Prefix(8),
		RejectV: errors.New(""),
		StatusV: choices.Processing,
	}}
	pink.InputIDsV = append(pink.InputIDsV, X)

	params := sbcon.Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          1,
		BetaRogue:             1,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}
	err := graph.Initialize(snow.DefaultConsensusContextTest(), params)
	if err != nil {
		t.Fatal(err)
	}

	if err := graph.Add(pink); err != nil {
		t.Fatal(err)
	} else if err := graph.Add(purple); err != nil {
		t.Fatal(err)
	}

	votes := ids.Bag{}
	votes.Add(purple.ID())
	if _, err := graph.RecordPoll(votes); err == nil {
		t.Fatalf("Should have errored on rejecting an invalid tx")
	}
}

func UTXOCleanupTest(t *testing.T, factory Factory) {
	graph := factory.New()

	params := sbcon.Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          1,
		BetaRogue:             2,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}
	err := graph.Initialize(snow.DefaultConsensusContextTest(), params)
	require.NoError(t, err)

	err = graph.Add(Red)
	require.NoError(t, err)

	err = graph.Add(Green)
	require.NoError(t, err)

	redVotes := ids.Bag{}
	redVotes.Add(Red.ID())
	changed, err := graph.RecordPoll(redVotes)
	require.NoError(t, err)
	require.False(t, changed, "shouldn't have accepted the red tx")

	changed, err = graph.RecordPoll(redVotes)
	require.NoError(t, err)
	require.True(t, changed, "should have accepted the red tx")

	require.Equal(t, choices.Accepted, Red.Status())
	require.Equal(t, choices.Rejected, Green.Status())

	err = graph.Add(Blue)
	require.NoError(t, err)

	blueVotes := ids.Bag{}
	blueVotes.Add(Blue.ID())
	changed, err = graph.RecordPoll(blueVotes)
	require.NoError(t, err)
	require.True(t, changed, "should have accepted the blue tx")

	require.Equal(t, choices.Accepted, Blue.Status())
}

func RemoveVirtuousTest(t *testing.T, factory Factory) {
	graph := factory.New()

	params := sbcon.Parameters{
		K:                     1,
		Alpha:                 1,
		BetaVirtuous:          1,
		BetaRogue:             2,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}
	err := graph.Initialize(snow.DefaultConsensusContextTest(), params)
	require.NoError(t, err)

	err = graph.Add(Red)
	require.NoError(t, err)

	virtuous := graph.Virtuous()
	require.NotEmpty(t, virtuous, "a virtuous transaction was added but not tracked")

	err = graph.Remove(Red.ID())
	require.NoError(t, err)

	virtuous = graph.Virtuous()
	require.Empty(t, virtuous, "removal of a virtuous transaction should have emptied the virtuous set")
}

func StringTest(t *testing.T, factory Factory, prefix string) {
	graph := factory.New()

	params := sbcon.Parameters{
		K:                     2,
		Alpha:                 2,
		BetaVirtuous:          1,
		BetaRogue:             2,
		ConcurrentRepolls:     1,
		OptimalProcessing:     1,
		MaxOutstandingItems:   1,
		MaxItemProcessingTime: 1,
	}
	err := graph.Initialize(snow.DefaultConsensusContextTest(), params)
	if err != nil {
		t.Fatal(err)
	}

	if err := graph.Add(Red); err != nil {
		t.Fatal(err)
	}
	if err := graph.Add(Green); err != nil {
		t.Fatal(err)
	}
	if err := graph.Add(Blue); err != nil {
		t.Fatal(err)
	}
	if err := graph.Add(Alpha); err != nil {
		t.Fatal(err)
	}

	prefs := graph.Preferences()
	switch {
	case prefs.Len() != 1:
		t.Fatalf("Wrong number of preferences.")
	case !prefs.Contains(Red.ID()):
		t.Fatalf("Wrong preference. Expected %s got %s", Red.ID(), prefs.List()[0])
	case graph.Finalized():
		t.Fatalf("Finalized too early")
	}

	rb := ids.Bag{}
	rb.SetThreshold(2)
	rb.AddCount(Red.ID(), 2)
	rb.AddCount(Blue.ID(), 2)
	if changed, err := graph.RecordPoll(rb); err != nil {
		t.Fatal(err)
	} else if !changed {
		t.Fatalf("Should have caused the frontiers to recalculate")
	} else if err := graph.Add(Blue); err != nil {
		t.Fatal(err)
	}

	{
		expected := prefix + "(\n" +
			"    Choice[0] = ID:  LUC1cmcxnfNR9LdkACS2ccGKLEK7SYqB4gLLTycQfg1koyfSq SB(NumSuccessfulPolls = 1, Confidence = 1)\n" +
			"    Choice[1] = ID:  TtF4d2QWbk5vzQGTEPrN48x6vwgAoAmKQ9cbp79inpQmcRKES SB(NumSuccessfulPolls = 0, Confidence = 0)\n" +
			"    Choice[2] = ID:  Zda4gsqTjRaX6XVZekVNi3ovMFPHDRQiGbzYuAb7Nwqy1rGBc SB(NumSuccessfulPolls = 0, Confidence = 0)\n" +
			"    Choice[3] = ID: 2mcwQKiD8VEspmMJpL1dc7okQQ5dDVAWeCBZ7FWBFAbxpv3t7w SB(NumSuccessfulPolls = 1, Confidence = 1)\n" +
			")"
		if str := graph.String(); str != expected {
			t.Fatalf("Expected %s, got %s", expected, str)
		}
	}

	prefs = graph.Preferences()
	switch {
	case prefs.Len() != 2:
		t.Fatalf("Wrong number of preferences.")
	case !prefs.Contains(Red.ID()):
		t.Fatalf("Wrong preference. Expected %s", Red.ID())
	case !prefs.Contains(Blue.ID()):
		t.Fatalf("Wrong preference. Expected %s", Blue.ID())
	case graph.Finalized():
		t.Fatalf("Finalized too early")
	}

	ga := ids.Bag{}
	ga.SetThreshold(2)
	ga.AddCount(Green.ID(), 2)
	ga.AddCount(Alpha.ID(), 2)
	if changed, err := graph.RecordPoll(ga); err != nil {
		t.Fatal(err)
	} else if changed {
		t.Fatalf("Shouldn't have caused the frontiers to recalculate")
	}

	{
		expected := prefix + "(\n" +
			"    Choice[0] = ID:  LUC1cmcxnfNR9LdkACS2ccGKLEK7SYqB4gLLTycQfg1koyfSq SB(NumSuccessfulPolls = 1, Confidence = 0)\n" +
			"    Choice[1] = ID:  TtF4d2QWbk5vzQGTEPrN48x6vwgAoAmKQ9cbp79inpQmcRKES SB(NumSuccessfulPolls = 1, Confidence = 1)\n" +
			"    Choice[2] = ID:  Zda4gsqTjRaX6XVZekVNi3ovMFPHDRQiGbzYuAb7Nwqy1rGBc SB(NumSuccessfulPolls = 1, Confidence = 1)\n" +
			"    Choice[3] = ID: 2mcwQKiD8VEspmMJpL1dc7okQQ5dDVAWeCBZ7FWBFAbxpv3t7w SB(NumSuccessfulPolls = 1, Confidence = 0)\n" +
			")"
		if str := graph.String(); str != expected {
			t.Fatalf("Expected %s, got %s", expected, str)
		}
	}

	prefs = graph.Preferences()
	switch {
	case prefs.Len() != 2:
		t.Fatalf("Wrong number of preferences.")
	case !prefs.Contains(Red.ID()):
		t.Fatalf("Wrong preference. Expected %s", Red.ID())
	case !prefs.Contains(Blue.ID()):
		t.Fatalf("Wrong preference. Expected %s", Blue.ID())
	case graph.Finalized():
		t.Fatalf("Finalized too early")
	}

	empty := ids.Bag{}
	if changed, err := graph.RecordPoll(empty); err != nil {
		t.Fatal(err)
	} else if changed {
		t.Fatalf("Shouldn't have caused the frontiers to recalculate")
	}

	{
		expected := prefix + "(\n" +
			"    Choice[0] = ID:  LUC1cmcxnfNR9LdkACS2ccGKLEK7SYqB4gLLTycQfg1koyfSq SB(NumSuccessfulPolls = 1, Confidence = 0)\n" +
			"    Choice[1] = ID:  TtF4d2QWbk5vzQGTEPrN48x6vwgAoAmKQ9cbp79inpQmcRKES SB(NumSuccessfulPolls = 1, Confidence = 0)\n" +
			"    Choice[2] = ID:  Zda4gsqTjRaX6XVZekVNi3ovMFPHDRQiGbzYuAb7Nwqy1rGBc SB(NumSuccessfulPolls = 1, Confidence = 0)\n" +
			"    Choice[3] = ID: 2mcwQKiD8VEspmMJpL1dc7okQQ5dDVAWeCBZ7FWBFAbxpv3t7w SB(NumSuccessfulPolls = 1, Confidence = 0)\n" +
			")"
		if str := graph.String(); str != expected {
			t.Fatalf("Expected %s, got %s", expected, str)
		}
	}

	prefs = graph.Preferences()
	switch {
	case prefs.Len() != 2:
		t.Fatalf("Wrong number of preferences.")
	case !prefs.Contains(Red.ID()):
		t.Fatalf("Wrong preference. Expected %s", Red.ID())
	case !prefs.Contains(Blue.ID()):
		t.Fatalf("Wrong preference. Expected %s", Blue.ID())
	case graph.Finalized():
		t.Fatalf("Finalized too early")
	}

	if changed, err := graph.RecordPoll(ga); err != nil {
		t.Fatal(err)
	} else if !changed {
		t.Fatalf("Should have caused the frontiers to recalculate")
	}

	{
		expected := prefix + "(\n" +
			"    Choice[0] = ID:  LUC1cmcxnfNR9LdkACS2ccGKLEK7SYqB4gLLTycQfg1koyfSq SB(NumSuccessfulPolls = 1, Confidence = 0)\n" +
			"    Choice[1] = ID:  TtF4d2QWbk5vzQGTEPrN48x6vwgAoAmKQ9cbp79inpQmcRKES SB(NumSuccessfulPolls = 2, Confidence = 1)\n" +
			"    Choice[2] = ID:  Zda4gsqTjRaX6XVZekVNi3ovMFPHDRQiGbzYuAb7Nwqy1rGBc SB(NumSuccessfulPolls = 2, Confidence = 1)\n" +
			"    Choice[3] = ID: 2mcwQKiD8VEspmMJpL1dc7okQQ5dDVAWeCBZ7FWBFAbxpv3t7w SB(NumSuccessfulPolls = 1, Confidence = 0)\n" +
			")"
		if str := graph.String(); str != expected {
			t.Fatalf("Expected %s, got %s", expected, str)
		}
	}

	prefs = graph.Preferences()
	switch {
	case prefs.Len() != 2:
		t.Fatalf("Wrong number of preferences.")
	case !prefs.Contains(Green.ID()):
		t.Fatalf("Wrong preference. Expected %s", Green.ID())
	case !prefs.Contains(Alpha.ID()):
		t.Fatalf("Wrong preference. Expected %s", Alpha.ID())
	case graph.Finalized():
		t.Fatalf("Finalized too early")
	}

	if changed, err := graph.RecordPoll(ga); err != nil {
		t.Fatal(err)
	} else if !changed {
		t.Fatalf("Should have caused the frontiers to recalculate")
	}

	{
		expected := prefix + "()"
		if str := graph.String(); str != expected {
			t.Fatalf("Expected %s, got %s", expected, str)
		}
	}

	prefs = graph.Preferences()
	switch {
	case prefs.Len() != 0:
		t.Fatalf("Wrong number of preferences.")
	case !graph.Finalized():
		t.Fatalf("Finalized too late")
	case Green.Status() != choices.Accepted:
		t.Fatalf("%s should have been accepted", Green.ID())
	case Alpha.Status() != choices.Accepted:
		t.Fatalf("%s should have been accepted", Alpha.ID())
	case Red.Status() != choices.Rejected:
		t.Fatalf("%s should have been rejected", Red.ID())
	case Blue.Status() != choices.Rejected:
		t.Fatalf("%s should have been rejected", Blue.ID())
	}

	if changed, err := graph.RecordPoll(rb); err != nil {
		t.Fatal(err)
	} else if changed {
		t.Fatalf("Shouldn't have caused the frontiers to recalculate")
	}

	{
		expected := prefix + "()"
		if str := graph.String(); str != expected {
			t.Fatalf("Expected %s, got %s", expected, str)
		}
	}

	prefs = graph.Preferences()
	switch {
	case prefs.Len() != 0:
		t.Fatalf("Wrong number of preferences.")
	case !graph.Finalized():
		t.Fatalf("Finalized too late")
	case Green.Status() != choices.Accepted:
		t.Fatalf("%s should have been accepted", Green.ID())
	case Alpha.Status() != choices.Accepted:
		t.Fatalf("%s should have been accepted", Alpha.ID())
	case Red.Status() != choices.Rejected:
		t.Fatalf("%s should have been rejected", Red.ID())
	case Blue.Status() != choices.Rejected:
		t.Fatalf("%s should have been rejected", Blue.ID())
	}
}

```

avalanchego/snow/consensus/snowstorm/directed.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowstorm

import (
	"fmt"

	"github.com/prometheus/client_golang/prometheus"

	"go.uber.org/zap"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow"
	"github.com/ava-labs/avalanchego/snow/choices"
	"github.com/ava-labs/avalanchego/snow/consensus/metrics"
	"github.com/ava-labs/avalanchego/snow/events"
	"github.com/ava-labs/avalanchego/utils/wrappers"

	sbcon "github.com/ava-labs/avalanchego/snow/consensus/snowball"
)

var (
	_ Factory   = &DirectedFactory{}
	_ Consensus = &Directed{}
)

// DirectedFactory implements Factory by returning a directed struct
type DirectedFactory struct{}

func (DirectedFactory) New() Consensus { return &Directed{} }

// Directed is an implementation of a multi-color, non-transitive, snowball
// instance
type Directed struct {
	metrics.Polls
	metrics.Latency
	whitelistTxLatency metrics.Latency
	numVirtuousTxs     prometheus.Gauge
	numRogueTxs        prometheus.Gauge

	// context that this consensus instance is executing in
	ctx *snow.ConsensusContext

	// params describes how this instance was parameterized
	params sbcon.Parameters

	// each element of preferences is the ID of a transaction that is preferred
	preferences ids.Set

	// each element of virtuous is the ID of a transaction that is virtuous
	virtuous ids.Set

	// each element is in the virtuous set and is still being voted on
	virtuousVoting ids.Set

	// number of times RecordPoll has been called
	pollNumber uint64

	// keeps track of whether dependencies have been accepted
	pendingAccept events.Blocker

	// keeps track of whether dependencies have been rejected
	pendingReject events.Blocker

	// track any errors that occurred during callbacks
	errs wrappers.Errs

	// Key: Transaction ID
	// Value: Node that represents this transaction in the conflict graph
	txs map[ids.ID]*directedTx

	// Key: UTXO ID
	// Value: IDs of transactions that consume the UTXO specified in the key
	utxos map[ids.ID]ids.Set

	// map transaction ID to the set of whitelisted transaction IDs.
	whitelists map[ids.ID]ids.Set
}

type directedTx struct {
	snowball

	// pendingAccept identifies if this transaction has been marked as accepted
	// once its transitive dependencies have also been accepted
	pendingAccept bool

	// ins is the set of txIDs that this tx conflicts with that are less
	// preferred than this tx
	ins ids.Set

	// outs is the set of txIDs that this tx conflicts with that are more
	// preferred than this tx
	outs ids.Set

	// tx is the actual transaction this node represents
	tx Tx
}

func (dg *Directed) Initialize(
	ctx *snow.ConsensusContext,
	params sbcon.Parameters,
) error {
	dg.ctx = ctx
	dg.params = params

	var err error
	dg.Polls, err = metrics.NewPolls("", ctx.Registerer)
	if err != nil {
		return fmt.Errorf("failed to create poll metrics: %w", err)
	}

	dg.Latency, err = metrics.NewLatency("txs", "transaction(s)", ctx.Log, "", ctx.Registerer)
	if err != nil {
		return fmt.Errorf("failed to create latency metrics: %w", err)
	}

	dg.whitelistTxLatency, err = metrics.NewLatency("whitelist_tx", "whitelist transaction(s)", ctx.Log, "", ctx.Registerer)
	if err != nil {
		return fmt.Errorf("failed to create whitelist tx metrics: %w", err)
	}

	dg.numVirtuousTxs = prometheus.NewGauge(prometheus.GaugeOpts{
		Name: "virtuous_tx_processing",
		Help: "Number of currently processing virtuous transaction(s)",
	})
	err = ctx.Registerer.Register(dg.numVirtuousTxs)
	if err != nil {
		return fmt.Errorf("failed to create virtuous tx metrics: %w", err)
	}

	dg.numRogueTxs = prometheus.NewGauge(prometheus.GaugeOpts{
		Name: "rogue_tx_processing",
		Help: "Number of currently processing rogue transaction(s)",
	})
	err = ctx.Registerer.Register(dg.numRogueTxs)
	if err != nil {
		return fmt.Errorf("failed to create rogue tx metrics: %w", err)
	}

	dg.txs = make(map[ids.ID]*directedTx)
	dg.utxos = make(map[ids.ID]ids.Set)
	dg.whitelists = make(map[ids.ID]ids.Set)

	return params.Verify()
}

func (dg *Directed) Parameters() sbcon.Parameters { return dg.params }

func (dg *Directed) Virtuous() ids.Set { return dg.virtuous }

func (dg *Directed) Preferences() ids.Set { return dg.preferences }

func (dg *Directed) VirtuousVoting() ids.Set { return dg.virtuousVoting }

func (dg *Directed) Quiesce() bool {
	numVirtuous := dg.virtuousVoting.Len()
	dg.ctx.Log.Verbo("conflict graph Quiesce was called",
		zap.Int("numVirtuous", numVirtuous),
	)
	return numVirtuous == 0
}

func (dg *Directed) Finalized() bool {
	numPreferences := dg.preferences.Len()
	dg.ctx.Log.Verbo("conflict graph Finalized was called",
		zap.Int("numPreferences", numPreferences),
	)
	return numPreferences == 0
}

// HealthCheck returns information about the consensus health.
func (dg *Directed) HealthCheck() (interface{}, error) {
	numOutstandingTxs := dg.Latency.NumProcessing()
	isOutstandingTxs := numOutstandingTxs <= dg.params.MaxOutstandingItems
	details := map[string]interface{}{
		"outstandingTransactions": numOutstandingTxs,
	}
	if !isOutstandingTxs {
		errorReason := fmt.Sprintf("number of outstanding txs %d > %d", numOutstandingTxs, dg.params.MaxOutstandingItems)
		return details, fmt.Errorf("snowstorm consensus is not healthy reason: %s", errorReason)
	}
	return details, nil
}

// shouldVote returns if the provided tx should be voted on to determine if it
// can be accepted. If the tx can be vacuously accepted, the tx will be accepted
// and will therefore not be valid to be voted on.
func (dg *Directed) shouldVote(tx Tx) (bool, error) {
	if dg.Issued(tx) {
		// If the tx was previously inserted, it shouldn't be re-inserted.
		return false, nil
	}

	txID := tx.ID()

	// Notify the metrics that this transaction is being issued.
	if tx.HasWhitelist() {
		dg.ctx.Log.Info("whitelist tx successfully issued",
			zap.Stringer("txID", txID),
		)
		dg.whitelistTxLatency.Issued(txID, dg.pollNumber)
	} else {
		dg.Latency.Issued(txID, dg.pollNumber)
	}

	// If this tx has inputs, it needs to be voted on before being accepted.
	if inputs := tx.InputIDs(); len(inputs) != 0 {
		return true, nil
	}

	// Since this tx doesn't have any inputs, it's impossible for there to be
	// any conflicting transactions. Therefore, this transaction is treated as
	// vacuously accepted and doesn't need to be voted on.

	// Notify those listening for accepted txs if the transaction has a binary
	// format.
	if bytes := tx.Bytes(); len(bytes) > 0 {
		// Note that DecisionAcceptor.Accept must be called before tx.Accept to
		// honor Acceptor.Accept's invariant.
		if err := dg.ctx.DecisionAcceptor.Accept(dg.ctx, txID, bytes); err != nil {
			return false, err
		}
	}

	if err := tx.Accept(); err != nil {
		return false, err
	}

	// Notify the metrics that this transaction was accepted.
	dg.Latency.Accepted(txID, dg.pollNumber)
	return false, nil
}

func (dg *Directed) IsVirtuous(tx Tx) bool {
	txID := tx.ID()
	// If the tx is currently processing, we should just return whether it was
	// registered as rogue or not.
	if node, exists := dg.txs[txID]; exists {
		return !node.rogue
	}

	// The tx isn't processing, so we need to check if it conflicts with any of
	// the other txs that are currently processing.
	for _, utxoID := range tx.InputIDs() {
		if _, exists := dg.utxos[utxoID]; exists {
			// A currently processing tx names the same input as the provided
			// tx, so the provided tx would be rogue.
			return false
		}
	}

	// This tx is virtuous as far as this consensus instance knows.
	return true
}

func (dg *Directed) Conflicts(tx Tx) ids.Set {
	var conflicts ids.Set
	if node, exists := dg.txs[tx.ID()]; exists {
		// If the tx is currently processing, the conflicting txs are just the
		// union of the inbound conflicts and the outbound conflicts.
		//
		// Only bother to call Union, which will do a memory allocation, if ins
		// or outs are non-empty.
		if node.ins.Len() > 0 || node.outs.Len() > 0 {
			conflicts.Union(node.ins)
			conflicts.Union(node.outs)
		}
	} else {
		// If the tx isn't currently processing, the conflicting txs are the
		// union of all the txs that spend an input that this tx spends.
		for _, inputID := range tx.InputIDs() {
			if spends, exists := dg.utxos[inputID]; exists {
				conflicts.Union(spends)
			}
		}
	}
	return conflicts
}

func (dg *Directed) Add(tx Tx) error {
	if shouldVote, err := dg.shouldVote(tx); !shouldVote || err != nil {
		return err
	}

	txID := tx.ID()
	txNode := &directedTx{tx: tx}

	// First check the other whitelist transactions.
	for otherID, otherWhitelist := range dg.whitelists {
		// [txID] is not whitelisted by [otherWhitelist]
		if !otherWhitelist.Contains(txID) {
			otherNode := dg.txs[otherID]

			// The [otherNode] should be preferred over [txNode] because a newly
			// issued transaction's confidence is always 0 and ties are broken
			// by the issuance order ("other_node" was issued before "tx_node").
			dg.addEdge(txNode, otherNode)
		}
	}
	if tx.HasWhitelist() {
		whitelist, err := tx.Whitelist()
		if err != nil {
			return err
		}
		dg.ctx.Log.Info("processing whitelist tx",
			zap.Stringer("txID", txID),
		)

		// Find all transactions that are not explicitly whitelisted and mark
		// them as conflicting.
		for otherID, otherNode := range dg.txs {
			// [otherID] is not whitelisted by [whitelist]
			if !whitelist.Contains(otherID) {
				// The [otherNode] should be preferred over [txNode] because a
				// newly issued transaction's confidence is always 0 and ties
				// are broken by the issuance order ("other_node" was issued
				// before "tx_node").
				dg.addEdge(txNode, otherNode)
			}
		}

		// Record the whitelist for future calls.
		dg.whitelists[txID] = whitelist
	}

	// For each UTXO consumed by the tx:
	// * Add edges between this tx and txs that consume this UTXO
	// * Mark this tx as attempting to consume this UTXO
	for _, inputID := range tx.InputIDs() {
		// Get the set of txs that are currently processing that also consume
		// this UTXO
		spenders := dg.utxos[inputID]

		// Update txs conflicting with tx to account for its issuance
		for conflictIDKey := range spenders {
			// Get the node that contains this conflicting tx
			conflict := dg.txs[conflictIDKey]

			// Add all the txs that spend this UTXO to this txs conflicts. These
			// conflicting txs must be preferred over this tx. We know this
			// because this tx currently has a bias of 0 and the tie goes to the
			// tx whose bias was updated first.
			dg.addEdge(txNode, conflict)
		}

		// Add this tx to list of txs consuming the current UTXO
		spenders.Add(txID)

		// spenders may be nil initially, so we should re-map the set.
		dg.utxos[inputID] = spenders
	}

	// Mark this transaction as rogue if it had any conflicts registered above
	txNode.rogue = txNode.outs.Len() != 0
	if !txNode.rogue {
		// If this tx is currently virtuous, add it to the virtuous sets
		dg.virtuous.Add(txID)
		dg.virtuousVoting.Add(txID)

		// If a tx is virtuous, it must be preferred.
		dg.preferences.Add(txID)
	}

	// Add this tx to the set of currently processing txs
	dg.txs[txID] = txNode

	// If a tx that this tx depends on is rejected, this tx should also be
	// rejected.
	err := dg.registerRejector(tx)

	numVirtuous := dg.virtuous.Len()
	dg.numVirtuousTxs.Set(float64(numVirtuous))
	dg.numRogueTxs.Set(float64(len(dg.txs) - numVirtuous))
	return err
}

// addEdge between the [src] and [dst] txs to represent a conflict.
//
// The edge goes from [src] to [dst]: [src] -> [dst].
//
// It is assumed that this is only called when [src] is being added. Which is
// why only [dst] is removed from the virtuous set and marked as rogue. [src]
// must be marked as rogue externally.
//
// For example:
// - TxA is issued
// - TxB is issued that consumes the same UTXO as TxA.
//   - [addEdge(TxB, TxA)] would be called to register the conflict.
func (dg *Directed) addEdge(src, dst *directedTx) {
	srcID, dstID := src.tx.ID(), dst.tx.ID()

	// Track the outbound edge from [src] to [dst].
	src.outs.Add(dstID)

	// Because we are adding a conflict, the transaction can't be virtuous.
	dg.virtuous.Remove(dstID)
	dg.virtuousVoting.Remove(dstID)
	dst.rogue = true

	// Track the inbound edge to [dst] from [src].
	dst.ins.Add(srcID)
}

func (dg *Directed) Remove(txID ids.ID) error {
	err := dg.reject(ids.Set{
		txID: struct{}{},
	})

	numVirtuous := dg.virtuous.Len()
	dg.numVirtuousTxs.Set(float64(numVirtuous))
	dg.numRogueTxs.Set(float64(len(dg.txs) - numVirtuous))
	return err
}

func (dg *Directed) Issued(tx Tx) bool {
	// If the tx is either Accepted or Rejected, then it must have been issued
	// previously.
	if tx.Status().Decided() {
		return true
	}

	// If the tx is currently processing, then it must have been issued.
	_, ok := dg.txs[tx.ID()]
	return ok
}

func (dg *Directed) RecordPoll(votes ids.Bag) (bool, error) {
	// Increase the vote ID. This is only updated here and is used to reset the
	// confidence values of transactions lazily.
	// This is also used to track the number of polls required to accept/reject
	// a transaction.
	dg.pollNumber++

	// This flag tracks if the Avalanche instance needs to recompute its
	// frontiers. Frontiers only need to be recalculated if preferences change
	// or if a tx was accepted.
	changed := false

	// We only want to iterate over txs that received alpha votes
	votes.SetThreshold(dg.params.Alpha)
	// Get the set of IDs that meet this alpha threshold
	metThreshold := votes.Threshold()
	for txIDKey := range metThreshold {
		// Get the node this tx represents
		txNode, exist := dg.txs[txIDKey]
		if !exist {
			// This tx may have already been accepted because of its
			// dependencies. If this is the case, we can just drop the vote.
			continue
		}

		txNode.recordSuccessfulPoll(dg.pollNumber)

		// If the tx should be accepted, then we should defer its acceptance
		// until its dependencies are decided. If this tx was already marked to
		// be accepted, we shouldn't register it again.
		if !txNode.pendingAccept &&
			txNode.finalized(dg.params.BetaVirtuous, dg.params.BetaRogue) {
			// Mark that this tx is pending acceptance so acceptance is only
			// registered once.
			txNode.pendingAccept = true

			if err := dg.registerAcceptor(txNode.tx); err != nil {
				return false, err
			}
			if dg.errs.Errored() {
				return changed, dg.errs.Err
			}
		}

		if txNode.tx.Status() != choices.Accepted {
			// If this tx wasn't accepted, then this instance is only changed if
			// preferences changed.
			changed = dg.redirectEdges(txNode) || changed
		} else {
			// By accepting a tx, the state of this instance has changed.
			changed = true
		}
	}

	if len(dg.txs) > 0 {
		if metThreshold.Len() == 0 {
			dg.Failed()
		} else {
			dg.Successful()
		}
	}

	numVirtuous := dg.virtuous.Len()
	dg.numVirtuousTxs.Set(float64(numVirtuous))
	dg.numRogueTxs.Set(float64(len(dg.txs) - numVirtuous))
	return changed, dg.errs.Err
}

func (dg *Directed) String() string {
	nodes := make([]*snowballNode, 0, len(dg.txs))
	for _, txNode := range dg.txs {
		nodes = append(nodes, &snowballNode{
			txID:               txNode.tx.ID(),
			numSuccessfulPolls: txNode.numSuccessfulPolls,
			confidence:         txNode.getConfidence(dg.pollNumber),
		})
	}
	return consensusString(nodes)
}

// accept the named txID and remove it from the graph
func (dg *Directed) accept(txID ids.ID) error {
	txNode := dg.txs[txID]
	// We are accepting the tx, so we should remove the node from the graph.
	delete(dg.txs, txID)
	delete(dg.whitelists, txID)

	// This tx is consuming all the UTXOs from its inputs, so we can prune them
	// all from memory
	for _, inputID := range txNode.tx.InputIDs() {
		delete(dg.utxos, inputID)
	}

	// This tx is now accepted, so it shouldn't be part of the virtuous set or
	// the preferred set. Its status as Accepted implies these descriptions.
	dg.virtuous.Remove(txID)
	dg.preferences.Remove(txID)

	// Reject all the txs that conflicted with this tx.
	if err := dg.reject(txNode.ins); err != nil {
		return err
	}
	// While it is typically true that a tx that is being accepted is preferred,
	// it is possible for this to not be the case.
	if err := dg.reject(txNode.outs); err != nil {
		return err
	}
	return dg.acceptTx(txNode.tx)
}

// reject all the named txIDs and remove them from the graph
func (dg *Directed) reject(conflictIDs ids.Set) error {
	for conflictKey := range conflictIDs {
		conflict := dg.txs[conflictKey]
		// This tx is no longer an option for consuming the UTXOs from its
		// inputs, so we should remove their reference to this tx.
		for _, inputID := range conflict.tx.InputIDs() {
			txIDs, exists := dg.utxos[inputID]
			if !exists {
				// This UTXO may no longer exist because it was removed due to
				// the acceptance of a tx. If that is the case, there is nothing
				// left to remove from memory.
				continue
			}
			delete(txIDs, conflictKey)
			delete(dg.whitelists, conflictKey)
			if txIDs.Len() == 0 {
				// If this tx was the last tx consuming this UTXO, we should
				// prune the UTXO from memory entirely.
				delete(dg.utxos, inputID)
			} else {
				// If this UTXO still has txs consuming it, then we should make
				// sure this update is written back to the UTXOs map.
				dg.utxos[inputID] = txIDs
			}
		}

		// We are rejecting the tx, so we should remove it from the graph
		delete(dg.txs, conflictKey)

		// It's statistically unlikely that something being rejected is
		// preferred. However, it's possible. Additionally, any transaction may
		// be removed at any time.
		delete(dg.preferences, conflictKey)
		delete(dg.virtuous, conflictKey)
		delete(dg.virtuousVoting, conflictKey)

		// remove the edge between this node and all its neighbors
		dg.removeConflict(conflictKey, conflict.ins)
		dg.removeConflict(conflictKey, conflict.outs)

		if err := dg.rejectTx(conflict.tx); err != nil {
			return err
		}
	}
	return nil
}

// redirectEdges attempts to turn outbound edges into inbound edges if the
// preferences have changed
func (dg *Directed) redirectEdges(tx *directedTx) bool {
	changed := false
	for conflictID := range tx.outs {
		changed = dg.redirectEdge(tx, conflictID) || changed
	}
	return changed
}

// Fixes the direction of the edge between [txNode] and [conflictID] if needed.
//
// It is assumed the edge is currently directed as [txNode] -> [conflictID].
//
// If [conflictID] has less successful polls than [txNode], the direction of the
// edge will be set to [conflictID] -> [txNode].
//
// Returns true if the direction was switched.
func (dg *Directed) redirectEdge(txNode *directedTx, conflictID ids.ID) bool {
	conflict := dg.txs[conflictID]
	if txNode.numSuccessfulPolls <= conflict.numSuccessfulPolls {
		return false
	}

	// Because this tx has a higher preference than the conflicting tx, we must
	// ensure that the edge is directed towards this tx.
	nodeID := txNode.tx.ID()

	// Change the edge direction according to the conflict tx
	conflict.ins.Remove(nodeID)
	conflict.outs.Add(nodeID)
	dg.preferences.Remove(conflictID) // This conflict has an outbound edge

	// Change the edge direction according to this tx
	txNode.ins.Add(conflictID)
	txNode.outs.Remove(conflictID)
	if txNode.outs.Len() == 0 {
		// If this tx doesn't have any outbound edges, it's preferred
		dg.preferences.Add(nodeID)
	}
	return true
}

func (dg *Directed) removeConflict(txIDKey ids.ID, neighborIDs ids.Set) {
	for neighborID := range neighborIDs {
		neighbor, exists := dg.txs[neighborID]
		if !exists {
			// If the neighbor doesn't exist, they may have already been
			// rejected, so this mapping can be skipped.
			continue
		}

		// Remove any edge to this tx.
		delete(neighbor.ins, txIDKey)
		delete(neighbor.outs, txIDKey)

		if neighbor.outs.Len() == 0 {
			// If this tx should now be preferred, make sure its status is
			// updated.
			dg.preferences.Add(neighborID)
		}
	}
}

// accept the provided tx.
func (dg *Directed) acceptTx(tx Tx) error {
	txID := tx.ID()
	dg.ctx.Log.Trace("accepting transaction",
		zap.Stringer("txID", txID),
	)

	// Notify those listening that this tx has been accepted if the transaction
	// has a binary format.
	if bytes := tx.Bytes(); len(bytes) > 0 {
		// Note that DecisionAcceptor.Accept must be called before tx.Accept to
		// honor Acceptor.Accept's invariant.
		if err := dg.ctx.DecisionAcceptor.Accept(dg.ctx, txID, bytes); err != nil {
			return err
		}
	}

	if err := tx.Accept(); err != nil {
		return err
	}

	// Update the metrics to account for this transaction's acceptance
	if tx.HasWhitelist() {
		dg.ctx.Log.Info("whitelist tx accepted",
			zap.Stringer("txID", txID),
		)
		dg.whitelistTxLatency.Accepted(txID, dg.pollNumber)
	} else {
		// just regular tx
		dg.Latency.Accepted(txID, dg.pollNumber)
	}

	// If there is a tx that was accepted pending on this tx, the ancestor
	// should be notified that it doesn't need to block on this tx anymore.
	dg.pendingAccept.Fulfill(txID)
	// If there is a tx that was issued pending on this tx, the ancestor tx
	// doesn't need to be rejected because of this tx.
	dg.pendingReject.Abandon(txID)

	return nil
}

// reject the provided tx.
func (dg *Directed) rejectTx(tx Tx) error {
	txID := tx.ID()
	dg.ctx.Log.Trace("rejecting transaction",
		zap.String("reason", "conflicting acceptance"),
		zap.Stringer("txID", txID),
	)

	// Reject is called before notifying the IPC so that rejections that
	// cause fatal errors aren't sent to an IPC peer.
	if err := tx.Reject(); err != nil {
		return err
	}

	// Update the metrics to account for this transaction's rejection
	if tx.HasWhitelist() {
		dg.ctx.Log.Info("whitelist tx rejected",
			zap.Stringer("txID", txID),
		)
		dg.whitelistTxLatency.Rejected(txID, dg.pollNumber)
	} else {
		dg.Latency.Rejected(txID, dg.pollNumber)
	}

	// If there is a tx that was accepted pending on this tx, the ancestor tx
	// can't be accepted.
	dg.pendingAccept.Abandon(txID)
	// If there is a tx that was issued pending on this tx, the ancestor tx must
	// be rejected.
	dg.pendingReject.Fulfill(txID)
	return nil
}

// registerAcceptor attempts to accept this tx once all its dependencies are
// accepted. If all the dependencies are already accepted, this function will
// immediately accept the tx.
func (dg *Directed) registerAcceptor(tx Tx) error {
	txID := tx.ID()

	toAccept := &acceptor{
		g:    dg,
		errs: &dg.errs,
		txID: txID,
	}

	deps, err := tx.Dependencies()
	if err != nil {
		return err
	}
	for _, dependency := range deps {
		if dependency.Status() != choices.Accepted {
			// If the dependency isn't accepted, then it must be processing.
			// This tx should be accepted after this tx is accepted. Note that
			// the dependencies can't already be rejected, because it is assumed
			// that this tx is currently considered valid.
			toAccept.deps.Add(dependency.ID())
		}
	}

	// This tx is no longer being voted on, so we remove it from the voting set.
	// This ensures that virtuous txs built on top of rogue txs don't force the
	// node to treat the rogue tx as virtuous.
	dg.virtuousVoting.Remove(txID)
	dg.pendingAccept.Register(toAccept)
	return nil
}

// registerRejector rejects this tx if any of its dependencies are rejected.
func (dg *Directed) registerRejector(tx Tx) error {
	// If a tx that this tx depends on is rejected, this tx should also be
	// rejected.
	toReject := &rejector{
		g:    dg,
		errs: &dg.errs,
		txID: tx.ID(),
	}

	// Register all of this txs dependencies as possibilities to reject this tx.
	deps, err := tx.Dependencies()
	if err != nil {
		return err
	}
	for _, dependency := range deps {
		if dependency.Status() != choices.Accepted {
			// If the dependency isn't accepted, then it must be processing. So,
			// this tx should be rejected if any of these processing txs are
			// rejected. Note that the dependencies can't already be rejected,
			// because it is assumed that this tx is currently considered valid.
			toReject.deps.Add(dependency.ID())
		}
	}

	// Register these dependencies
	dg.pendingReject.Register(toReject)
	return nil
}

```

avalanchego/snow/consensus/snowstorm/directed_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowstorm

import (
	"testing"
)

func TestDirectedConsensus(t *testing.T) { runConsensusTests(t, DirectedFactory{}, "DG") }

```

avalanchego/snow/consensus/snowstorm/factory.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowstorm

// Factory returns new instances of Consensus
type Factory interface {
	New() Consensus
}

```

avalanchego/snow/consensus/snowstorm/network_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowstorm

import (
	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow"
	"github.com/ava-labs/avalanchego/snow/choices"
	"github.com/ava-labs/avalanchego/utils/sampler"

	sbcon "github.com/ava-labs/avalanchego/snow/consensus/snowball"
)

type Network struct {
	params         sbcon.Parameters
	consumers      []*TestTx
	nodeTxs        []map[ids.ID]*TestTx
	nodes, running []Consensus
}

func (n *Network) shuffleConsumers() {
	s := sampler.NewUniform()
	_ = s.Initialize(uint64(len(n.consumers)))
	indices, _ := s.Sample(len(n.consumers))
	consumers := []*TestTx(nil)
	for _, index := range indices {
		consumers = append(consumers, n.consumers[int(index)])
	}
	n.consumers = consumers
}

func (n *Network) Initialize(
	params sbcon.Parameters,
	numColors,
	colorsPerConsumer,
	maxInputConflicts int,
) {
	n.params = params

	idCount := uint64(0)

	colorMap := map[ids.ID]int{}
	colors := []ids.ID{}
	for i := 0; i < numColors; i++ {
		idCount++
		color := ids.Empty.Prefix(idCount)
		colorMap[color] = i
		colors = append(colors, color)
	}

	count := map[ids.ID]int{}
	for len(colors) > 0 {
		selected := []ids.ID{}
		s := sampler.NewUniform()
		_ = s.Initialize(uint64(len(colors)))
		size := len(colors)
		if size > colorsPerConsumer {
			size = colorsPerConsumer
		}
		indices, _ := s.Sample(size)
		for _, index := range indices {
			selected = append(selected, colors[int(index)])
		}

		for _, sID := range selected {
			newCount := count[sID] + 1
			count[sID] = newCount
			if newCount >= maxInputConflicts {
				i := colorMap[sID]
				e := len(colorMap) - 1

				eID := colors[e]

				colorMap[eID] = i
				colors[i] = eID

				delete(colorMap, sID)
				colors = colors[:e]
			}
		}

		idCount++
		tx := &TestTx{TestDecidable: choices.TestDecidable{
			IDV:     ids.Empty.Prefix(idCount),
			StatusV: choices.Processing,
		}}
		tx.InputIDsV = append(tx.InputIDsV, selected...)

		n.consumers = append(n.consumers, tx)
	}
}

func (n *Network) AddNode(cg Consensus) error {
	if err := cg.Initialize(snow.DefaultConsensusContextTest(), n.params); err != nil {
		return err
	}

	n.shuffleConsumers()

	txs := map[ids.ID]*TestTx{}
	for _, tx := range n.consumers {
		newTx := &TestTx{
			TestDecidable: choices.TestDecidable{
				IDV:     tx.ID(),
				StatusV: choices.Processing,
			},
			InputIDsV: tx.InputIDs(),
		}
		txs[newTx.ID()] = newTx

		if err := cg.Add(newTx); err != nil {
			return err
		}
	}

	n.nodeTxs = append(n.nodeTxs, txs)
	n.nodes = append(n.nodes, cg)
	n.running = append(n.running, cg)

	return nil
}

func (n *Network) Finalized() bool {
	return len(n.running) == 0
}

func (n *Network) Round() error {
	if len(n.running) == 0 {
		return nil
	}

	s := sampler.NewUniform()
	_ = s.Initialize(uint64(len(n.running)))
	runningInd, _ := s.Next()

	running := n.running[runningInd]

	_ = s.Initialize(uint64(len(n.nodes)))
	indices, _ := s.Sample(n.params.K)
	sampledColors := ids.Bag{}
	sampledColors.SetThreshold(n.params.Alpha)
	for _, index := range indices {
		peer := n.nodes[int(index)]
		peerTxs := n.nodeTxs[int(index)]

		preferences := peer.Preferences()
		for _, color := range preferences.List() {
			sampledColors.Add(color)
		}
		for _, tx := range peerTxs {
			if tx.Status() == choices.Accepted {
				sampledColors.Add(tx.ID())
			}
		}
	}

	if _, err := running.RecordPoll(sampledColors); err != nil {
		return err
	}

	// If this node has been finalized, remove it from the poller
	if running.Finalized() {
		newSize := len(n.running) - 1
		n.running[runningInd] = n.running[newSize]
		n.running = n.running[:newSize]
	}
	return nil
}

func (n *Network) Disagreement() bool {
	for _, color := range n.consumers {
		accepted := false
		rejected := false
		for _, nodeTx := range n.nodeTxs {
			tx := nodeTx[color.ID()]
			accepted = accepted || tx.Status() == choices.Accepted
			rejected = rejected || tx.Status() == choices.Rejected
		}
		if accepted && rejected {
			return true
		}
	}
	return false
}

func (n *Network) Agreement() bool {
	statuses := map[ids.ID]choices.Status{}
	for _, color := range n.consumers {
		for _, nodeTx := range n.nodeTxs {
			colorID := color.ID()
			tx := nodeTx[colorID]
			prevStatus, exists := statuses[colorID]
			if exists && prevStatus != tx.Status() {
				return false
			}
			statuses[colorID] = tx.Status()
		}
	}
	return !n.Disagreement()
}

```

avalanchego/snow/consensus/snowstorm/rejector.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowstorm

import (
	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/events"
	"github.com/ava-labs/avalanchego/utils/wrappers"
)

var _ events.Blockable = &rejector{}

type rejector struct {
	g        *Directed
	errs     *wrappers.Errs
	deps     ids.Set
	rejected bool // true if the tx has been rejected
	txID     ids.ID
}

func (r *rejector) Dependencies() ids.Set { return r.deps }

func (r *rejector) Fulfill(ids.ID) {
	if r.rejected || r.errs.Errored() {
		return
	}
	r.rejected = true
	asSet := ids.NewSet(1)
	asSet.Add(r.txID)
	r.errs.Add(r.g.reject(asSet))
}

func (*rejector) Abandon(ids.ID) {}
func (*rejector) Update()        {}

```

avalanchego/snow/consensus/snowstorm/snowball.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowstorm

type snowball struct {
	// numSuccessfulPolls is the number of times this choice was the successful
	// result of a network poll
	numSuccessfulPolls int

	// confidence is the number of consecutive times this choice was the
	// successful result of a network poll as of [lastVote]
	confidence int

	// lastVote is the last poll number that this choice was included in a
	// successful network poll
	lastVote uint64

	// rogue identifies if there is a known conflict with this choice
	rogue bool
}

func (sb *snowball) getConfidence(currentVote uint64) int {
	if sb.lastVote != currentVote {
		return 0
	}
	return sb.confidence
}

func (sb *snowball) recordSuccessfulPoll(currentVote uint64) {
	// If this choice wasn't voted for during the last poll, the confidence
	// should have been reset during the last poll. So, we reset it now.
	if sb.lastVote+1 != currentVote {
		sb.confidence = 0
	}

	// This choice was voted for in this poll. Mark it as such.
	sb.lastVote = currentVote

	// An affirmative vote increases both the snowball and snowflake counters.
	sb.numSuccessfulPolls++
	sb.confidence++
}

func (sb *snowball) finalized(betaVirtuous, betaRogue int) bool {
	// This choice is finalized if the snowflake counter is at least
	// [betaRogue]. If there are no known conflicts with this operation, it can
	// be accepted with a snowflake counter of at least [betaVirtuous].
	return (!sb.rogue && sb.confidence >= betaVirtuous) ||
		sb.confidence >= betaRogue
}

```

avalanchego/snow/consensus/snowstorm/stringer.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowstorm

import (
	"bytes"
	"fmt"
	"sort"
	"strings"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/utils/formatting"
)

type snowballNode struct {
	txID               ids.ID
	numSuccessfulPolls int
	confidence         int
}

func (sb *snowballNode) String() string {
	return fmt.Sprintf(
		"SB(NumSuccessfulPolls = %d, Confidence = %d)",
		sb.numSuccessfulPolls,
		sb.confidence)
}

type sortSnowballNodeData []*snowballNode

func (sb sortSnowballNodeData) Less(i, j int) bool {
	return bytes.Compare(sb[i].txID[:], sb[j].txID[:]) == -1
}
func (sb sortSnowballNodeData) Len() int      { return len(sb) }
func (sb sortSnowballNodeData) Swap(i, j int) { sb[j], sb[i] = sb[i], sb[j] }

func sortSnowballNodes(nodes []*snowballNode) {
	sort.Sort(sortSnowballNodeData(nodes))
}

// consensusString converts a list of snowball nodes into a human-readable
// string.
func consensusString(nodes []*snowballNode) string {
	// Sort the nodes so that the string representation is canonical
	sortSnowballNodes(nodes)

	sb := strings.Builder{}
	sb.WriteString("DG(")

	format := fmt.Sprintf(
		"\n    Choice[%s] = ID: %%50s %%s",
		formatting.IntFormat(len(nodes)-1))
	for i, txNode := range nodes {
		sb.WriteString(fmt.Sprintf(format, i, txNode.txID, txNode))
	}

	if len(nodes) > 0 {
		sb.WriteString("\n")
	}
	sb.WriteString(")")
	return sb.String()
}

```

avalanchego/snow/consensus/snowstorm/test_tx.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowstorm

import (
	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/choices"
)

var _ Tx = &TestTx{}

// TestTx is a useful test tx
type TestTx struct {
	choices.TestDecidable

	DependenciesV    []Tx
	DependenciesErrV error
	InputIDsV        []ids.ID
	HasWhitelistV    bool
	WhitelistV       ids.Set
	WhitelistErrV    error
	VerifyV          error
	BytesV           []byte
}

func (t *TestTx) Dependencies() ([]Tx, error) { return t.DependenciesV, t.DependenciesErrV }
func (t *TestTx) InputIDs() []ids.ID          { return t.InputIDsV }
func (t *TestTx) HasWhitelist() bool          { return t.HasWhitelistV }
func (t *TestTx) Whitelist() (ids.Set, error) { return t.WhitelistV, t.WhitelistErrV }
func (t *TestTx) Verify() error               { return t.VerifyV }
func (t *TestTx) Bytes() []byte               { return t.BytesV }

```

avalanchego/snow/consensus/snowstorm/tx.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowstorm

import (
	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/choices"
)

// Whitelister defines the interface for specifying whitelisted operations.
type Whitelister interface {
	// Returns [true] if the underlying instance does implement whitelisted
	// conflicts.
	HasWhitelist() bool

	// Whitelist returns the set of transaction IDs that are explicitly
	// whitelisted. Transactions that are not explicitly whitelisted are
	// considered conflicting.
	Whitelist() (ids.Set, error)
}

// Tx consumes state.
type Tx interface {
	choices.Decidable
	Whitelister

	// Dependencies is a list of transactions upon which this transaction
	// depends. Each element of Dependencies must be verified before Verify is
	// called on this transaction.
	//
	// Similarly, each element of Dependencies must be accepted before this
	// transaction is accepted.
	Dependencies() ([]Tx, error)

	// InputIDs is a set where each element is the ID of a piece of state that
	// will be consumed if this transaction is accepted.
	//
	// In the context of a UTXO-based payments system, for example, this would
	// be the IDs of the UTXOs consumed by this transaction
	InputIDs() []ids.ID

	// Verify that the state transition this transaction would make if it were
	// accepted is valid. If the state transition is invalid, a non-nil error
	// should be returned.
	//
	// It is guaranteed that when Verify is called, all the dependencies of
	// this transaction have already been successfully verified.
	Verify() error

	// Bytes returns the binary representation of this transaction.
	//
	// This is used for sending transactions to peers. Another node should be
	// able to parse these bytes to the same transaction.
	Bytes() []byte
}

```

avalanchego/snow/context.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snow

import (
	"crypto"
	"crypto/x509"
	"sync"

	"github.com/prometheus/client_golang/prometheus"

	"github.com/ava-labs/avalanchego/api/keystore"
	"github.com/ava-labs/avalanchego/api/metrics"
	"github.com/ava-labs/avalanchego/chains/atomic"
	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/validators"
	"github.com/ava-labs/avalanchego/utils"
	"github.com/ava-labs/avalanchego/utils/logging"
)

type SubnetLookup interface {
	SubnetID(chainID ids.ID) (ids.ID, error)
}

// ContextInitializable represents an object that can be initialized
// given a *Context object
type ContextInitializable interface {
	// InitCtx initializes an object provided a *Context object
	InitCtx(ctx *Context)
}

// Context is information about the current execution.
// [NetworkID] is the ID of the network this context exists within.
// [ChainID] is the ID of the chain this context exists within.
// [NodeID] is the ID of this node
type Context struct {
	NetworkID uint32
	SubnetID  ids.ID
	ChainID   ids.ID
	NodeID    ids.NodeID

	XChainID    ids.ID
	AVAXAssetID ids.ID

	Log          logging.Logger
	Lock         sync.RWMutex
	Keystore     keystore.BlockchainKeystore
	SharedMemory atomic.SharedMemory
	BCLookup     ids.AliaserReader
	SNLookup     SubnetLookup
	Metrics      metrics.OptionalGatherer

	// snowman++ attributes
	ValidatorState    validators.State  // interface for P-Chain validators
	StakingLeafSigner crypto.Signer     // block signer
	StakingCertLeaf   *x509.Certificate // block certificate
}

// Expose gatherer interface for unit testing.
type Registerer interface {
	prometheus.Registerer
	prometheus.Gatherer
}

type ConsensusContext struct {
	*Context

	Registerer Registerer

	// DecisionAcceptor is the callback that will be fired whenever a VM is
	// notified that their object, either a block in snowman or a transaction
	// in avalanche, was accepted.
	DecisionAcceptor Acceptor

	// ConsensusAcceptor is the callback that will be fired whenever a
	// container, either a block in snowman or a vertex in avalanche, was
	// accepted.
	ConsensusAcceptor Acceptor

	// Non-zero iff this chain bootstrapped.
	state utils.AtomicInterface

	// Non-zero iff this chain is executing transactions.
	executing utils.AtomicBool

	// Indicates this chain is available to only validators.
	validatorOnly utils.AtomicBool
}

func (ctx *ConsensusContext) SetState(newState State) {
	ctx.state.SetValue(newState)
}

func (ctx *ConsensusContext) GetState() State {
	stateInf := ctx.state.GetValue()
	return stateInf.(State)
}

// IsExecuting returns true iff this chain is still executing transactions.
func (ctx *ConsensusContext) IsExecuting() bool {
	return ctx.executing.GetValue()
}

// Executing marks this chain as executing or not.
// Set to "true" if there's an ongoing transaction.
func (ctx *ConsensusContext) Executing(b bool) {
	ctx.executing.SetValue(b)
}

// IsValidatorOnly returns true iff this chain is available only to validators
func (ctx *ConsensusContext) IsValidatorOnly() bool {
	return ctx.validatorOnly.GetValue()
}

// SetValidatorOnly  marks this chain as available only to validators
func (ctx *ConsensusContext) SetValidatorOnly() {
	ctx.validatorOnly.SetValue(true)
}

func DefaultContextTest() *Context {
	return &Context{
		NetworkID: 0,
		SubnetID:  ids.Empty,
		ChainID:   ids.Empty,
		NodeID:    ids.EmptyNodeID,
		Log:       logging.NoLog{},
		BCLookup:  ids.NewAliaser(),
		Metrics:   metrics.NewOptionalGatherer(),
	}
}

func DefaultConsensusContextTest() *ConsensusContext {
	return &ConsensusContext{
		Context:           DefaultContextTest(),
		Registerer:        prometheus.NewRegistry(),
		DecisionAcceptor:  noOpAcceptor{},
		ConsensusAcceptor: noOpAcceptor{},
	}
}

```

avalanchego/snow/engine/avalanche/bootstrap/bootstrapper.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package bootstrap

import (
	"errors"
	"fmt"
	"math"
	"time"

	"go.uber.org/zap"

	"github.com/ava-labs/avalanchego/cache"
	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow"
	"github.com/ava-labs/avalanchego/snow/choices"
	"github.com/ava-labs/avalanchego/snow/consensus/avalanche"
	"github.com/ava-labs/avalanchego/snow/engine/avalanche/vertex"
	"github.com/ava-labs/avalanchego/snow/engine/common"
	"github.com/ava-labs/avalanchego/version"
)

const (
	// We cache processed vertices where height = c * stripeDistance for c = {1,2,3...}
	// This forms a "stripe" of cached DAG vertices at height stripeDistance, 2*stripeDistance, etc.
	// This helps to limit the number of repeated DAG traversals performed
	stripeDistance = 2000
	stripeWidth    = 5
	cacheSize      = 100000

	// Parameters for delaying bootstrapping to avoid potential CPU burns
	bootstrappingDelay = 10 * time.Second
)

var (
	_ common.BootstrapableEngine = &bootstrapper{}

	errUnexpectedTimeout = errors.New("unexpected timeout fired")
)

func New(config Config, onFinished func(lastReqID uint32) error) (common.BootstrapableEngine, error) {
	b := &bootstrapper{
		Config: config,

		StateSummaryFrontierHandler: common.NewNoOpStateSummaryFrontierHandler(config.Ctx.Log),
		AcceptedStateSummaryHandler: common.NewNoOpAcceptedStateSummaryHandler(config.Ctx.Log),
		PutHandler:                  common.NewNoOpPutHandler(config.Ctx.Log),
		QueryHandler:                common.NewNoOpQueryHandler(config.Ctx.Log),
		ChitsHandler:                common.NewNoOpChitsHandler(config.Ctx.Log),
		AppHandler:                  common.NewNoOpAppHandler(config.Ctx.Log),

		processedCache:           &cache.LRU{Size: cacheSize},
		Fetcher:                  common.Fetcher{OnFinished: onFinished},
		executedStateTransitions: math.MaxInt32,
	}

	if err := b.metrics.Initialize("bs", config.Ctx.Registerer); err != nil {
		return nil, err
	}

	if err := b.VtxBlocked.SetParser(&vtxParser{
		log:         config.Ctx.Log,
		numAccepted: b.numAcceptedVts,
		numDropped:  b.numDroppedVts,
		manager:     b.Manager,
	}); err != nil {
		return nil, err
	}

	if err := b.TxBlocked.SetParser(&txParser{
		log:         config.Ctx.Log,
		numAccepted: b.numAcceptedTxs,
		numDropped:  b.numDroppedTxs,
		vm:          b.VM,
	}); err != nil {
		return nil, err
	}

	config.Config.Bootstrapable = b
	b.Bootstrapper = common.NewCommonBootstrapper(config.Config)
	return b, nil
}

type bootstrapper struct {
	Config

	// list of NoOpsHandler for messages dropped by bootstrapper
	common.StateSummaryFrontierHandler
	common.AcceptedStateSummaryHandler
	common.PutHandler
	common.QueryHandler
	common.ChitsHandler
	common.AppHandler

	common.Bootstrapper
	common.Fetcher
	metrics

	started bool

	// IDs of vertices that we will send a GetAncestors request for once we are
	// not at the max number of outstanding requests
	needToFetch ids.Set

	// Contains IDs of vertices that have recently been processed
	processedCache *cache.LRU
	// number of state transitions executed
	executedStateTransitions int

	awaitingTimeout bool
}

func (b *bootstrapper) Clear() error {
	if err := b.VtxBlocked.Clear(); err != nil {
		return err
	}
	if err := b.TxBlocked.Clear(); err != nil {
		return err
	}
	if err := b.VtxBlocked.Commit(); err != nil {
		return err
	}
	return b.TxBlocked.Commit()
}

// Ancestors handles the receipt of multiple containers. Should be received in
// response to a GetAncestors message to [nodeID] with request ID [requestID].
// Expects vtxs[0] to be the vertex requested in the corresponding GetAncestors.
func (b *bootstrapper) Ancestors(nodeID ids.NodeID, requestID uint32, vtxs [][]byte) error {
	lenVtxs := len(vtxs)
	if lenVtxs == 0 {
		b.Ctx.Log.Debug("Ancestors contains no vertices",
			zap.Stringer("nodeID", nodeID),
			zap.Uint32("requestID", requestID),
		)
		return b.GetAncestorsFailed(nodeID, requestID)
	}
	if lenVtxs > b.Config.AncestorsMaxContainersReceived {
		b.Ctx.Log.Debug("ignoring containers in Ancestors",
			zap.Stringer("nodeID", nodeID),
			zap.Uint32("requestID", requestID),
			zap.Int("numIgnored", lenVtxs-b.Config.AncestorsMaxContainersReceived),
		)

		vtxs = vtxs[:b.Config.AncestorsMaxContainersReceived]
	}

	requestedVtxID, requested := b.OutstandingRequests.Remove(nodeID, requestID)
	vtx, err := b.Manager.ParseVtx(vtxs[0]) // first vertex should be the one we requested in GetAncestors request
	if err != nil {
		if !requested {
			b.Ctx.Log.Debug("failed to parse unrequested vertex",
				zap.Stringer("nodeID", nodeID),
				zap.Uint32("requestID", requestID),
				zap.Error(err),
			)
			return nil
		}
		b.Ctx.Log.Debug("failed to parse requested vertex",
			zap.Stringer("nodeID", nodeID),
			zap.Uint32("requestID", requestID),
			zap.Stringer("vtxID", requestedVtxID),
			zap.Error(err),
		)
		b.Ctx.Log.Verbo("failed to parse requested vertex",
			zap.Stringer("nodeID", nodeID),
			zap.Uint32("requestID", requestID),
			zap.Stringer("vtxID", requestedVtxID),
			zap.Binary("vtxBytes", vtxs[0]),
			zap.Error(err),
		)
		return b.fetch(requestedVtxID)
	}

	vtxID := vtx.ID()
	// If the vertex is neither the requested vertex nor a needed vertex, return early and re-fetch if necessary
	if requested && requestedVtxID != vtxID {
		b.Ctx.Log.Debug("received incorrect vertex",
			zap.Stringer("nodeID", nodeID),
			zap.Uint32("requestID", requestID),
			zap.Stringer("vtxID", vtxID),
		)
		return b.fetch(requestedVtxID)
	}
	if !requested && !b.OutstandingRequests.Contains(vtxID) && !b.needToFetch.Contains(vtxID) {
		b.Ctx.Log.Debug("received un-needed vertex",
			zap.Stringer("nodeID", nodeID),
			zap.Uint32("requestID", requestID),
			zap.Stringer("vtxID", vtxID),
		)
		return nil
	}

	// Do not remove from outstanding requests if this did not answer a specific outstanding request
	// to ensure that real responses are not dropped in favor of potentially byzantine Ancestors messages that
	// could force the node to bootstrap 1 vertex at a time.
	b.needToFetch.Remove(vtxID)

	// All vertices added to [processVertices] have received transitive votes from the accepted frontier
	processVertices := make([]avalanche.Vertex, 1, len(vtxs)) // Process all of the valid vertices in this message
	processVertices[0] = vtx
	parents, err := vtx.Parents()
	if err != nil {
		return err
	}
	eligibleVertices := ids.NewSet(len(parents))
	for _, parent := range parents {
		eligibleVertices.Add(parent.ID())
	}

	for _, vtxBytes := range vtxs[1:] { // Parse/persist all the vertices
		vtx, err := b.Manager.ParseVtx(vtxBytes) // Persists the vtx
		if err != nil {
			b.Ctx.Log.Debug("failed to parse vertex",
				zap.Stringer("nodeID", nodeID),
				zap.Uint32("requestID", requestID),
				zap.Error(err),
			)
			b.Ctx.Log.Debug("failed to parse vertex",
				zap.Stringer("nodeID", nodeID),
				zap.Uint32("requestID", requestID),
				zap.Binary("vtxBytes", vtxBytes),
				zap.Error(err),
			)
			break
		}
		vtxID := vtx.ID()
		if !eligibleVertices.Contains(vtxID) {
			b.Ctx.Log.Debug("received vertex that should not have been included",
				zap.Stringer("nodeID", nodeID),
				zap.Uint32("requestID", requestID),
				zap.Stringer("vtxID", vtxID),
			)
			break
		}
		eligibleVertices.Remove(vtxID)
		parents, err := vtx.Parents()
		if err != nil {
			return err
		}
		for _, parent := range parents {
			eligibleVertices.Add(parent.ID())
		}
		processVertices = append(processVertices, vtx)
		b.needToFetch.Remove(vtxID) // No need to fetch this vertex since we have it now
	}

	return b.process(processVertices...)
}

func (b *bootstrapper) GetAncestorsFailed(nodeID ids.NodeID, requestID uint32) error {
	vtxID, ok := b.OutstandingRequests.Remove(nodeID, requestID)
	if !ok {
		b.Ctx.Log.Debug("skipping GetAncestorsFailed call",
			zap.String("reason", "no matching outstanding request"),
			zap.Stringer("nodeID", nodeID),
			zap.Uint32("requestID", requestID),
		)
		return nil
	}
	// Send another request for the vertex
	return b.fetch(vtxID)
}

func (b *bootstrapper) Connected(nodeID ids.NodeID, nodeVersion *version.Application) error {
	if err := b.VM.Connected(nodeID, nodeVersion); err != nil {
		return err
	}

	if err := b.StartupTracker.Connected(nodeID, nodeVersion); err != nil {
		return err
	}

	if b.started || !b.StartupTracker.ShouldStart() {
		return nil
	}

	b.started = true
	return b.Startup()
}

func (b *bootstrapper) Disconnected(nodeID ids.NodeID) error {
	if err := b.VM.Disconnected(nodeID); err != nil {
		return err
	}

	return b.StartupTracker.Disconnected(nodeID)
}

func (b *bootstrapper) Timeout() error {
	if !b.awaitingTimeout {
		return errUnexpectedTimeout
	}
	b.awaitingTimeout = false

	if !b.Config.Subnet.IsBootstrapped() {
		return b.Restart(true)
	}
	return b.OnFinished(b.Config.SharedCfg.RequestID)
}

func (b *bootstrapper) Gossip() error { return nil }

func (b *bootstrapper) Shutdown() error {
	b.Ctx.Log.Info("shutting down bootstrapper")
	return b.VM.Shutdown()
}

func (b *bootstrapper) Notify(common.Message) error { return nil }

func (b *bootstrapper) Start(startReqID uint32) error {
	b.Ctx.Log.Info("starting bootstrap")

	b.Ctx.SetState(snow.Bootstrapping)
	if err := b.VM.SetState(snow.Bootstrapping); err != nil {
		return fmt.Errorf("failed to notify VM that bootstrapping has started: %w",
			err)
	}

	b.Config.SharedCfg.RequestID = startReqID

	if !b.StartupTracker.ShouldStart() {
		return nil
	}

	b.started = true
	return b.Startup()
}

func (b *bootstrapper) HealthCheck() (interface{}, error) {
	vmIntf, vmErr := b.VM.HealthCheck()
	intf := map[string]interface{}{
		"consensus": struct{}{},
		"vm":        vmIntf,
	}
	return intf, vmErr
}

func (b *bootstrapper) GetVM() common.VM { return b.VM }

// Add the vertices in [vtxIDs] to the set of vertices that we need to fetch,
// and then fetch vertices (and their ancestors) until either there are no more
// to fetch or we are at the maximum number of outstanding requests.
func (b *bootstrapper) fetch(vtxIDs ...ids.ID) error {
	b.needToFetch.Add(vtxIDs...)
	for b.needToFetch.Len() > 0 && b.OutstandingRequests.Len() < common.MaxOutstandingGetAncestorsRequests {
		vtxID := b.needToFetch.CappedList(1)[0]
		b.needToFetch.Remove(vtxID)

		// Make sure we haven't already requested this vertex
		if b.OutstandingRequests.Contains(vtxID) {
			continue
		}

		// Make sure we don't already have this vertex
		if _, err := b.Manager.GetVtx(vtxID); err == nil {
			continue
		}

		validators, err := b.Config.Beacons.Sample(1) // validator to send request to
		if err != nil {
			return fmt.Errorf("dropping request for %s as there are no validators", vtxID)
		}
		validatorID := validators[0].ID()
		b.Config.SharedCfg.RequestID++

		b.OutstandingRequests.Add(validatorID, b.Config.SharedCfg.RequestID, vtxID)
		b.Config.Sender.SendGetAncestors(validatorID, b.Config.SharedCfg.RequestID, vtxID) // request vertex and ancestors
	}
	return b.checkFinish()
}

// Process the vertices in [vtxs].
func (b *bootstrapper) process(vtxs ...avalanche.Vertex) error {
	// Vertices that we need to process. Store them in a heap for deduplication
	// and so we always process vertices further down in the DAG first. This helps
	// to reduce the number of repeated DAG traversals.
	toProcess := vertex.NewHeap()
	for _, vtx := range vtxs {
		vtxID := vtx.ID()
		if _, ok := b.processedCache.Get(vtxID); !ok { // only process a vertex if we haven't already
			toProcess.Push(vtx)
		} else {
			b.VtxBlocked.RemoveMissingID(vtxID)
		}
	}

	vtxHeightSet := ids.Set{}
	prevHeight := uint64(0)

	for toProcess.Len() > 0 { // While there are unprocessed vertices
		if b.Halted() {
			return nil
		}

		vtx := toProcess.Pop() // Get an unknown vertex or one furthest down the DAG
		vtxID := vtx.ID()

		switch vtx.Status() {
		case choices.Unknown:
			b.VtxBlocked.AddMissingID(vtxID)
			b.needToFetch.Add(vtxID) // We don't have this vertex locally. Mark that we need to fetch it.
		case choices.Rejected:
			return fmt.Errorf("tried to accept %s even though it was previously rejected", vtxID)
		case choices.Processing:
			b.needToFetch.Remove(vtxID)
			b.VtxBlocked.RemoveMissingID(vtxID)

			// Add to queue of vertices to execute when bootstrapping finishes.
			if pushed, err := b.VtxBlocked.Push(&vertexJob{
				log:         b.Ctx.Log,
				numAccepted: b.numAcceptedVts,
				numDropped:  b.numDroppedVts,
				vtx:         vtx,
			}); err != nil {
				return err
			} else if !pushed {
				// If the vertex is already on the queue, then we have already
				// pushed [vtx]'s transactions and traversed into its parents.
				continue
			}

			txs, err := vtx.Txs()
			if err != nil {
				return err
			}
			for _, tx := range txs {
				// Add to queue of txs to execute when bootstrapping finishes.
				if pushed, err := b.TxBlocked.Push(&txJob{
					log:         b.Ctx.Log,
					numAccepted: b.numAcceptedTxs,
					numDropped:  b.numDroppedTxs,
					tx:          tx,
				}); err != nil {
					return err
				} else if pushed {
					b.numFetchedTxs.Inc()
				}
			}

			b.numFetchedVts.Inc()

			verticesFetchedSoFar := b.VtxBlocked.Jobs.PendingJobs()
			if verticesFetchedSoFar%common.StatusUpdateFrequency == 0 { // Periodically print progress
				if !b.Config.SharedCfg.Restarted {
					b.Ctx.Log.Info("fetched vertices",
						zap.Uint64("numVerticesFetched", verticesFetchedSoFar),
					)
				} else {
					b.Ctx.Log.Debug("fetched vertices",
						zap.Uint64("numVerticesFetched", verticesFetchedSoFar),
					)
				}
			}

			parents, err := vtx.Parents()
			if err != nil {
				return err
			}
			for _, parent := range parents { // Process the parents of this vertex (traverse up the DAG)
				parentID := parent.ID()
				if _, ok := b.processedCache.Get(parentID); !ok { // But only if we haven't processed the parent
					if !vtxHeightSet.Contains(parentID) {
						toProcess.Push(parent)
					}
				}
			}
			height, err := vtx.Height()
			if err != nil {
				return err
			}
			if height%stripeDistance < stripeWidth { // See comment for stripeDistance
				b.processedCache.Put(vtxID, nil)
			}
			if height == prevHeight {
				vtxHeightSet.Add(vtxID)
			} else {
				// Set new height and reset [vtxHeightSet]
				prevHeight = height
				vtxHeightSet.Clear()
				vtxHeightSet.Add(vtxID)
			}
		}
	}

	if err := b.TxBlocked.Commit(); err != nil {
		return err
	}
	if err := b.VtxBlocked.Commit(); err != nil {
		return err
	}

	return b.fetch()
}

// ForceAccepted starts bootstrapping. Process the vertices in [accepterContainerIDs].
func (b *bootstrapper) ForceAccepted(acceptedContainerIDs []ids.ID) error {
	pendingContainerIDs := b.VtxBlocked.MissingIDs()
	// Append the list of accepted container IDs to pendingContainerIDs to ensure
	// we iterate over every container that must be traversed.
	pendingContainerIDs = append(pendingContainerIDs, acceptedContainerIDs...)
	b.Ctx.Log.Debug("starting bootstrapping",
		zap.Int("numMissingVertices", len(pendingContainerIDs)),
		zap.Int("numAcceptedVertices", len(acceptedContainerIDs)),
	)
	toProcess := make([]avalanche.Vertex, 0, len(pendingContainerIDs))
	for _, vtxID := range pendingContainerIDs {
		if vtx, err := b.Manager.GetVtx(vtxID); err == nil {
			if vtx.Status() == choices.Accepted {
				b.VtxBlocked.RemoveMissingID(vtxID)
			} else {
				toProcess = append(toProcess, vtx) // Process this vertex.
			}
		} else {
			b.VtxBlocked.AddMissingID(vtxID)
			b.needToFetch.Add(vtxID) // We don't have this vertex. Mark that we have to fetch it.
		}
	}
	return b.process(toProcess...)
}

// checkFinish repeatedly executes pending transactions and requests new frontier blocks until there aren't any new ones
// after which it finishes the bootstrap process
func (b *bootstrapper) checkFinish() error {
	// If there are outstanding requests for vertices or we still need to fetch vertices, we can't finish
	pendingJobs := b.VtxBlocked.MissingIDs()
	if b.IsBootstrapped() || len(pendingJobs) > 0 || b.awaitingTimeout {
		return nil
	}

	if !b.Config.SharedCfg.Restarted {
		b.Ctx.Log.Info("executing transactions")
	} else {
		b.Ctx.Log.Debug("executing transactions")
	}

	_, err := b.TxBlocked.ExecuteAll(b.Config.Ctx, b, b.Config.SharedCfg.Restarted, b.Ctx.DecisionAcceptor)
	if err != nil || b.Halted() {
		return err
	}

	if !b.Config.SharedCfg.Restarted {
		b.Ctx.Log.Info("executing vertices")
	} else {
		b.Ctx.Log.Debug("executing vertices")
	}

	executedVts, err := b.VtxBlocked.ExecuteAll(b.Config.Ctx, b, b.Config.SharedCfg.Restarted, b.Ctx.ConsensusAcceptor)
	if err != nil || b.Halted() {
		return err
	}

	previouslyExecuted := b.executedStateTransitions
	b.executedStateTransitions = executedVts

	// Note that executedVts < c*previouslyExecuted is enforced so that the
	// bootstrapping process will terminate even as new vertices are being
	// issued.
	if executedVts > 0 && executedVts < previouslyExecuted/2 && b.Config.RetryBootstrap {
		b.Ctx.Log.Debug("checking for more vertices before finishing bootstrapping")
		return b.Restart(true)
	}

	// Notify the subnet that this chain is synced
	b.Config.Subnet.Bootstrapped(b.Ctx.ChainID)
	b.processedCache.Flush()

	// If the subnet hasn't finished bootstrapping, this chain should remain
	// syncing.
	if !b.Config.Subnet.IsBootstrapped() {
		if !b.Config.SharedCfg.Restarted {
			b.Ctx.Log.Info("waiting for the remaining chains in this subnet to finish syncing")
		} else {
			b.Ctx.Log.Debug("waiting for the remaining chains in this subnet to finish syncing")
		}
		// Restart bootstrapping after [bootstrappingDelay] to keep up to date
		// on the latest tip.
		b.Config.Timer.RegisterTimeout(bootstrappingDelay)
		b.awaitingTimeout = true
		return nil
	}
	return b.OnFinished(b.Config.SharedCfg.RequestID)
}

```

avalanchego/snow/engine/avalanche/bootstrap/bootstrapper_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package bootstrap

import (
	"bytes"
	"errors"
	"testing"

	"github.com/ava-labs/avalanchego/database/memdb"
	"github.com/ava-labs/avalanchego/database/prefixdb"
	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow"
	"github.com/ava-labs/avalanchego/snow/choices"
	"github.com/ava-labs/avalanchego/snow/consensus/avalanche"
	"github.com/ava-labs/avalanchego/snow/consensus/snowstorm"
	"github.com/ava-labs/avalanchego/snow/engine/avalanche/vertex"
	"github.com/ava-labs/avalanchego/snow/engine/common"
	"github.com/ava-labs/avalanchego/snow/engine/common/queue"
	"github.com/ava-labs/avalanchego/snow/engine/common/tracker"
	"github.com/ava-labs/avalanchego/snow/validators"

	avagetter "github.com/ava-labs/avalanchego/snow/engine/avalanche/getter"
)

var (
	errUnknownVertex       = errors.New("unknown vertex")
	errParsedUnknownVertex = errors.New("parsed unknown vertex")
)

func newConfig(t *testing.T) (Config, ids.NodeID, *common.SenderTest, *vertex.TestManager, *vertex.TestVM) {
	ctx := snow.DefaultConsensusContextTest()

	peers := validators.NewSet()
	db := memdb.New()
	sender := &common.SenderTest{T: t}
	manager := vertex.NewTestManager(t)
	vm := &vertex.TestVM{}
	vm.T = t

	isBootstrapped := false
	subnet := &common.SubnetTest{
		T:               t,
		IsBootstrappedF: func() bool { return isBootstrapped },
		BootstrappedF:   func(ids.ID) { isBootstrapped = true },
	}

	sender.Default(true)
	manager.Default(true)
	vm.Default(true)

	sender.CantSendGetAcceptedFrontier = false

	peer := ids.GenerateTestNodeID()
	if err := peers.AddWeight(peer, 1); err != nil {
		t.Fatal(err)
	}

	vtxBlocker, err := queue.NewWithMissing(prefixdb.New([]byte("vtx"), db), "vtx", ctx.Registerer)
	if err != nil {
		t.Fatal(err)
	}
	txBlocker, err := queue.New(prefixdb.New([]byte("tx"), db), "tx", ctx.Registerer)
	if err != nil {
		t.Fatal(err)
	}

	peerTracker := tracker.NewPeers()
	startupTracker := tracker.NewStartup(peerTracker, peers.Weight()/2+1)
	peers.RegisterCallbackListener(startupTracker)

	commonConfig := common.Config{
		Ctx:                            ctx,
		Validators:                     peers,
		Beacons:                        peers,
		SampleK:                        peers.Len(),
		Alpha:                          peers.Weight()/2 + 1,
		StartupTracker:                 startupTracker,
		Sender:                         sender,
		Subnet:                         subnet,
		Timer:                          &common.TimerTest{},
		AncestorsMaxContainersSent:     2000,
		AncestorsMaxContainersReceived: 2000,
		SharedCfg:                      &common.SharedConfig{},
	}

	avaGetHandler, err := avagetter.New(manager, commonConfig)
	if err != nil {
		t.Fatal(err)
	}

	return Config{
		Config:        commonConfig,
		AllGetsServer: avaGetHandler,
		VtxBlocked:    vtxBlocker,
		TxBlocked:     txBlocker,
		Manager:       manager,
		VM:            vm,
	}, peer, sender, manager, vm
}

// Three vertices in the accepted frontier. None have parents. No need to fetch anything
func TestBootstrapperSingleFrontier(t *testing.T) {
	config, _, _, manager, vm := newConfig(t)

	vtxID0 := ids.Empty.Prefix(0)
	vtxID1 := ids.Empty.Prefix(1)
	vtxID2 := ids.Empty.Prefix(2)

	vtxBytes0 := []byte{0}
	vtxBytes1 := []byte{1}
	vtxBytes2 := []byte{2}

	vtx0 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     vtxID0,
			StatusV: choices.Processing,
		},
		HeightV: 0,
		BytesV:  vtxBytes0,
	}
	vtx1 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     vtxID1,
			StatusV: choices.Processing,
		},
		HeightV: 0,
		BytesV:  vtxBytes1,
	}
	vtx2 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     vtxID2,
			StatusV: choices.Processing,
		},
		HeightV: 0,
		BytesV:  vtxBytes2,
	}

	bs, err := New(
		config,
		func(lastReqID uint32) error { config.Ctx.SetState(snow.NormalOp); return nil },
	)
	if err != nil {
		t.Fatal(err)
	}

	vm.CantSetState = false
	if err := bs.Start(0); err != nil {
		t.Fatal(err)
	}

	acceptedIDs := []ids.ID{vtxID0, vtxID1, vtxID2}

	manager.GetVtxF = func(vtxID ids.ID) (avalanche.Vertex, error) {
		switch vtxID {
		case vtxID0:
			return vtx0, nil
		case vtxID1:
			return vtx1, nil
		case vtxID2:
			return vtx2, nil
		default:
			t.Fatal(errUnknownVertex)
			panic(errUnknownVertex)
		}
	}

	manager.ParseVtxF = func(vtxBytes []byte) (avalanche.Vertex, error) {
		switch {
		case bytes.Equal(vtxBytes, vtxBytes0):
			return vtx0, nil
		case bytes.Equal(vtxBytes, vtxBytes1):
			return vtx1, nil
		case bytes.Equal(vtxBytes, vtxBytes2):
			return vtx2, nil
		}
		t.Fatal(errParsedUnknownVertex)
		return nil, errParsedUnknownVertex
	}

	if err := bs.ForceAccepted(acceptedIDs); err != nil {
		t.Fatal(err)
	}

	switch {
	case config.Ctx.GetState() != snow.NormalOp:
		t.Fatalf("Bootstrapping should have finished")
	case vtx0.Status() != choices.Accepted:
		t.Fatalf("Vertex should be accepted")
	case vtx1.Status() != choices.Accepted:
		t.Fatalf("Vertex should be accepted")
	case vtx2.Status() != choices.Accepted:
		t.Fatalf("Vertex should be accepted")
	}
}

// Accepted frontier has one vertex, which has one vertex as a dependency.
// Requests again and gets an unexpected vertex.
// Requests again and gets the expected vertex and an additional vertex that should not be accepted.
func TestBootstrapperByzantineResponses(t *testing.T) {
	config, peerID, sender, manager, vm := newConfig(t)

	vtxID0 := ids.Empty.Prefix(0)
	vtxID1 := ids.Empty.Prefix(1)
	vtxID2 := ids.Empty.Prefix(2)

	vtxBytes0 := []byte{0}
	vtxBytes1 := []byte{1}
	vtxBytes2 := []byte{2}

	vtx0 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     vtxID0,
			StatusV: choices.Unknown,
		},
		HeightV: 0,
		BytesV:  vtxBytes0,
	}
	vtx1 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     vtxID1,
			StatusV: choices.Processing,
		},
		ParentsV: []avalanche.Vertex{vtx0},
		HeightV:  1,
		BytesV:   vtxBytes1,
	}
	// Should not receive transitive votes from [vtx1]
	vtx2 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     vtxID2,
			StatusV: choices.Unknown,
		},
		HeightV: 0,
		BytesV:  vtxBytes2,
	}

	bs, err := New(
		config,
		func(lastReqID uint32) error { config.Ctx.SetState(snow.NormalOp); return nil },
	)
	if err != nil {
		t.Fatal(err)
	}

	vm.CantSetState = false
	if err := bs.Start(0); err != nil {
		t.Fatal(err)
	}

	acceptedIDs := []ids.ID{vtxID1}

	manager.GetVtxF = func(vtxID ids.ID) (avalanche.Vertex, error) {
		switch vtxID {
		case vtxID1:
			return vtx1, nil
		case vtxID0:
			return nil, errUnknownVertex
		default:
			t.Fatal(errUnknownVertex)
			panic(errUnknownVertex)
		}
	}

	requestID := new(uint32)
	reqVtxID := ids.Empty
	sender.SendGetAncestorsF = func(vdr ids.NodeID, reqID uint32, vtxID ids.ID) {
		switch {
		case vdr != peerID:
			t.Fatalf("Should have requested vertex from %s, requested from %s",
				peerID, vdr)
		case vtxID != vtxID0:
			t.Fatalf("should have requested vtx0")
		}
		*requestID = reqID
		reqVtxID = vtxID
	}

	manager.ParseVtxF = func(vtxBytes []byte) (avalanche.Vertex, error) {
		switch {
		case bytes.Equal(vtxBytes, vtxBytes0):
			vtx0.StatusV = choices.Processing
			return vtx0, nil
		case bytes.Equal(vtxBytes, vtxBytes1):
			vtx1.StatusV = choices.Processing
			return vtx1, nil
		case bytes.Equal(vtxBytes, vtxBytes2):
			vtx2.StatusV = choices.Processing
			return vtx2, nil
		}
		t.Fatal(errParsedUnknownVertex)
		return nil, errParsedUnknownVertex
	}

	if err := bs.ForceAccepted(acceptedIDs); err != nil { // should request vtx0
		t.Fatal(err)
	} else if reqVtxID != vtxID0 {
		t.Fatalf("should have requested vtxID0 but requested %s", reqVtxID)
	}

	oldReqID := *requestID
	err = bs.Ancestors(peerID, *requestID, [][]byte{vtxBytes2})
	switch {
	case err != nil: // send unexpected vertex
		t.Fatal(err)
	case *requestID == oldReqID:
		t.Fatal("should have issued new request")
	case reqVtxID != vtxID0:
		t.Fatalf("should have requested vtxID0 but requested %s", reqVtxID)
	}

	oldReqID = *requestID
	manager.GetVtxF = func(vtxID ids.ID) (avalanche.Vertex, error) {
		switch vtxID {
		case vtxID1:
			return vtx1, nil
		case vtxID0:
			return vtx0, nil
		default:
			t.Fatal(errUnknownVertex)
			panic(errUnknownVertex)
		}
	}

	if err := bs.Ancestors(peerID, *requestID, [][]byte{vtxBytes0, vtxBytes2}); err != nil { // send expected vertex and vertex that should not be accepted
		t.Fatal(err)
	}

	switch {
	case *requestID != oldReqID:
		t.Fatal("should not have issued new request")
	case config.Ctx.GetState() != snow.NormalOp:
		t.Fatalf("Bootstrapping should have finished")
	case vtx0.Status() != choices.Accepted:
		t.Fatalf("Vertex should be accepted")
	case vtx1.Status() != choices.Accepted:
		t.Fatalf("Vertex should be accepted")
	}
	if vtx2.Status() == choices.Accepted {
		t.Fatalf("Vertex should not have been accepted")
	}
}

// Vertex has a dependency and tx has a dependency
func TestBootstrapperTxDependencies(t *testing.T) {
	config, peerID, sender, manager, vm := newConfig(t)

	utxos := []ids.ID{ids.GenerateTestID(), ids.GenerateTestID()}

	txID0 := ids.GenerateTestID()
	txID1 := ids.GenerateTestID()

	txBytes0 := []byte{0}
	txBytes1 := []byte{1}

	tx0 := &snowstorm.TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     txID0,
			StatusV: choices.Processing,
		},
		BytesV: txBytes0,
	}
	tx0.InputIDsV = append(tx0.InputIDsV, utxos[0])

	// Depends on tx0
	tx1 := &snowstorm.TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     txID1,
			StatusV: choices.Processing,
		},
		DependenciesV: []snowstorm.Tx{tx0},
		BytesV:        txBytes1,
	}
	tx1.InputIDsV = append(tx1.InputIDsV, utxos[1])

	vtxID0 := ids.GenerateTestID()
	vtxID1 := ids.GenerateTestID()

	vtxBytes0 := []byte{2}
	vtxBytes1 := []byte{3}
	vm.ParseTxF = func(b []byte) (snowstorm.Tx, error) {
		switch {
		case bytes.Equal(b, txBytes0):
			return tx0, nil
		case bytes.Equal(b, txBytes1):
			return tx1, nil
		default:
			return nil, errors.New("wrong tx")
		}
	}

	vtx0 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     vtxID0,
			StatusV: choices.Unknown,
		},
		HeightV: 0,
		TxsV:    []snowstorm.Tx{tx1},
		BytesV:  vtxBytes0,
	}
	vtx1 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     vtxID1,
			StatusV: choices.Processing,
		},
		ParentsV: []avalanche.Vertex{vtx0}, // Depends on vtx0
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx0},
		BytesV:   vtxBytes1,
	}

	bs, err := New(
		config,
		func(lastReqID uint32) error { config.Ctx.SetState(snow.NormalOp); return nil },
	)
	if err != nil {
		t.Fatal(err)
	}

	vm.CantSetState = false
	if err := bs.Start(0); err != nil {
		t.Fatal(err)
	}

	acceptedIDs := []ids.ID{vtxID1}

	manager.ParseVtxF = func(vtxBytes []byte) (avalanche.Vertex, error) {
		switch {
		case bytes.Equal(vtxBytes, vtxBytes1):
			return vtx1, nil
		case bytes.Equal(vtxBytes, vtxBytes0):
			return vtx0, nil
		}
		t.Fatal(errParsedUnknownVertex)
		return nil, errParsedUnknownVertex
	}
	manager.GetVtxF = func(vtxID ids.ID) (avalanche.Vertex, error) {
		switch vtxID {
		case vtxID1:
			return vtx1, nil
		case vtxID0:
			return nil, errUnknownVertex
		default:
			t.Fatal(errUnknownVertex)
			panic(errUnknownVertex)
		}
	}

	reqIDPtr := new(uint32)
	sender.SendGetAncestorsF = func(vdr ids.NodeID, reqID uint32, vtxID ids.ID) {
		if vdr != peerID {
			t.Fatalf("Should have requested vertex from %s, requested from %s", peerID, vdr)
		}
		switch vtxID {
		case vtxID0:
		default:
			t.Fatal(errUnknownVertex)
		}

		*reqIDPtr = reqID
	}

	if err := bs.ForceAccepted(acceptedIDs); err != nil { // should request vtx0
		t.Fatal(err)
	}

	manager.ParseVtxF = func(vtxBytes []byte) (avalanche.Vertex, error) {
		switch {
		case bytes.Equal(vtxBytes, vtxBytes1):
			return vtx1, nil
		case bytes.Equal(vtxBytes, vtxBytes0):
			vtx0.StatusV = choices.Processing
			return vtx0, nil
		}
		t.Fatal(errParsedUnknownVertex)
		return nil, errParsedUnknownVertex
	}

	if err := bs.Ancestors(peerID, *reqIDPtr, [][]byte{vtxBytes0}); err != nil {
		t.Fatal(err)
	}

	if config.Ctx.GetState() != snow.NormalOp {
		t.Fatalf("Should have finished bootstrapping")
	}
	if tx0.Status() != choices.Accepted {
		t.Fatalf("Tx should be accepted")
	}
	if tx1.Status() != choices.Accepted {
		t.Fatalf("Tx should be accepted")
	}

	if vtx0.Status() != choices.Accepted {
		t.Fatalf("Vertex should be accepted")
	}
	if vtx1.Status() != choices.Accepted {
		t.Fatalf("Vertex should be accepted")
	}
}

// Unfulfilled tx dependency
func TestBootstrapperMissingTxDependency(t *testing.T) {
	config, peerID, sender, manager, vm := newConfig(t)

	utxos := []ids.ID{ids.GenerateTestID(), ids.GenerateTestID()}

	txID0 := ids.GenerateTestID()
	txID1 := ids.GenerateTestID()

	txBytes1 := []byte{1}

	tx0 := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     txID0,
		StatusV: choices.Unknown,
	}}

	tx1 := &snowstorm.TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     txID1,
			StatusV: choices.Processing,
		},
		DependenciesV: []snowstorm.Tx{tx0},
		BytesV:        txBytes1,
	}
	tx1.InputIDsV = append(tx1.InputIDsV, utxos[1])

	vtxID0 := ids.GenerateTestID()
	vtxID1 := ids.GenerateTestID()

	vtxBytes0 := []byte{2}
	vtxBytes1 := []byte{3}

	vtx0 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     vtxID0,
			StatusV: choices.Unknown,
		},
		HeightV: 0,
		BytesV:  vtxBytes0,
	}
	vtx1 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     vtxID1,
			StatusV: choices.Processing,
		},
		ParentsV: []avalanche.Vertex{vtx0}, // depends on vtx0
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx1},
		BytesV:   vtxBytes1,
	}

	bs, err := New(
		config,
		func(lastReqID uint32) error { config.Ctx.SetState(snow.NormalOp); return nil },
	)
	if err != nil {
		t.Fatal(err)
	}

	vm.CantSetState = false
	if err := bs.Start(0); err != nil {
		t.Fatal(err)
	}

	acceptedIDs := []ids.ID{vtxID1}

	manager.GetVtxF = func(vtxID ids.ID) (avalanche.Vertex, error) {
		switch vtxID {
		case vtxID1:
			return vtx1, nil
		case vtxID0:
			return nil, errUnknownVertex
		default:
			t.Fatal(errUnknownVertex)
			panic(errUnknownVertex)
		}
	}
	manager.ParseVtxF = func(vtxBytes []byte) (avalanche.Vertex, error) {
		switch {
		case bytes.Equal(vtxBytes, vtxBytes1):
			return vtx1, nil
		case bytes.Equal(vtxBytes, vtxBytes0):
			vtx0.StatusV = choices.Processing
			return vtx0, nil
		}
		t.Fatal(errParsedUnknownVertex)
		return nil, errParsedUnknownVertex
	}

	reqIDPtr := new(uint32)
	sender.SendGetAncestorsF = func(vdr ids.NodeID, reqID uint32, vtxID ids.ID) {
		if vdr != peerID {
			t.Fatalf("Should have requested vertex from %s, requested from %s", peerID, vdr)
		}
		switch {
		case vtxID == vtxID0:
		default:
			t.Fatalf("Requested wrong vertex")
		}

		*reqIDPtr = reqID
	}

	if err := bs.ForceAccepted(acceptedIDs); err != nil { // should request vtx1
		t.Fatal(err)
	}

	if err := bs.Ancestors(peerID, *reqIDPtr, [][]byte{vtxBytes0}); err != nil {
		t.Fatal(err)
	}

	if config.Ctx.GetState() != snow.NormalOp {
		t.Fatalf("Bootstrapping should have finished")
	}
	if tx0.Status() != choices.Unknown { // never saw this tx
		t.Fatalf("Tx should be unknown")
	}
	if tx1.Status() != choices.Processing { // can't accept because we don't have tx0
		t.Fatalf("Tx should be processing")
	}

	if vtx0.Status() != choices.Accepted {
		t.Fatalf("Vertex should be accepted")
	}
	if vtx1.Status() != choices.Processing { // can't accept because we don't have tx1 accepted
		t.Fatalf("Vertex should be processing")
	}
}

// Ancestors only contains 1 of the two needed vertices; have to issue another GetAncestors
func TestBootstrapperIncompleteAncestors(t *testing.T) {
	config, peerID, sender, manager, vm := newConfig(t)

	vtxID0 := ids.Empty.Prefix(0)
	vtxID1 := ids.Empty.Prefix(1)
	vtxID2 := ids.Empty.Prefix(2)

	vtxBytes0 := []byte{0}
	vtxBytes1 := []byte{1}
	vtxBytes2 := []byte{2}

	vtx0 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     vtxID0,
			StatusV: choices.Unknown,
		},
		HeightV: 0,
		BytesV:  vtxBytes0,
	}
	vtx1 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     vtxID1,
			StatusV: choices.Unknown,
		},
		ParentsV: []avalanche.Vertex{vtx0},
		HeightV:  1,
		BytesV:   vtxBytes1,
	}
	vtx2 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     vtxID2,
			StatusV: choices.Processing,
		},
		ParentsV: []avalanche.Vertex{vtx1},
		HeightV:  2,
		BytesV:   vtxBytes2,
	}

	bs, err := New(
		config,
		func(lastReqID uint32) error { config.Ctx.SetState(snow.NormalOp); return nil },
	)
	if err != nil {
		t.Fatal(err)
	}

	vm.CantSetState = false
	if err := bs.Start(0); err != nil {
		t.Fatal(err)
	}

	acceptedIDs := []ids.ID{vtxID2}

	manager.GetVtxF = func(vtxID ids.ID) (avalanche.Vertex, error) {
		switch {
		case vtxID == vtxID0:
			return nil, errUnknownVertex
		case vtxID == vtxID1:
			return nil, errUnknownVertex
		case vtxID == vtxID2:
			return vtx2, nil
		default:
			t.Fatal(errUnknownVertex)
			panic(errUnknownVertex)
		}
	}
	manager.ParseVtxF = func(vtxBytes []byte) (avalanche.Vertex, error) {
		switch {
		case bytes.Equal(vtxBytes, vtxBytes0):
			vtx0.StatusV = choices.Processing
			return vtx0, nil

		case bytes.Equal(vtxBytes, vtxBytes1):
			vtx1.StatusV = choices.Processing
			return vtx1, nil
		case bytes.Equal(vtxBytes, vtxBytes2):
			return vtx2, nil
		}
		t.Fatal(errParsedUnknownVertex)
		return nil, errParsedUnknownVertex
	}
	reqIDPtr := new(uint32)
	requested := ids.Empty
	sender.SendGetAncestorsF = func(vdr ids.NodeID, reqID uint32, vtxID ids.ID) {
		if vdr != peerID {
			t.Fatalf("Should have requested vertex from %s, requested from %s", peerID, vdr)
		}
		switch vtxID {
		case vtxID1, vtxID0:
		default:
			t.Fatal(errUnknownVertex)
		}
		*reqIDPtr = reqID
		requested = vtxID
	}

	if err := bs.ForceAccepted(acceptedIDs); err != nil { // should request vtx1
		t.Fatal(err)
	} else if requested != vtxID1 {
		t.Fatal("requested wrong vtx")
	}

	err = bs.Ancestors(peerID, *reqIDPtr, [][]byte{vtxBytes1})
	switch {
	case err != nil: // Provide vtx1; should request vtx0
		t.Fatal(err)
	case bs.Context().GetState() == snow.NormalOp:
		t.Fatalf("should not have finished")
	case requested != vtxID0:
		t.Fatal("should hae requested vtx0")
	}

	err = bs.Ancestors(peerID, *reqIDPtr, [][]byte{vtxBytes0})
	switch {
	case err != nil: // Provide vtx0; can finish now
		t.Fatal(err)
	case bs.Context().GetState() != snow.NormalOp:
		t.Fatal("should have finished")
	case vtx0.Status() != choices.Accepted:
		t.Fatal("should be accepted")
	case vtx1.Status() != choices.Accepted:
		t.Fatal("should be accepted")
	case vtx2.Status() != choices.Accepted:
		t.Fatal("should be accepted")
	}
}

func TestBootstrapperFinalized(t *testing.T) {
	config, peerID, sender, manager, vm := newConfig(t)

	vtxID0 := ids.Empty.Prefix(0)
	vtxID1 := ids.Empty.Prefix(1)

	vtxBytes0 := []byte{0}
	vtxBytes1 := []byte{1}

	vtx0 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     vtxID0,
			StatusV: choices.Unknown,
		},
		HeightV: 0,
		BytesV:  vtxBytes0,
	}
	vtx1 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     vtxID1,
			StatusV: choices.Unknown,
		},
		ParentsV: []avalanche.Vertex{vtx0},
		HeightV:  1,
		BytesV:   vtxBytes1,
	}

	bs, err := New(
		config,
		func(lastReqID uint32) error { config.Ctx.SetState(snow.NormalOp); return nil },
	)
	if err != nil {
		t.Fatal(err)
	}

	vm.CantSetState = false
	if err := bs.Start(0); err != nil {
		t.Fatal(err)
	}

	acceptedIDs := []ids.ID{vtxID0, vtxID1}

	parsedVtx0 := false
	parsedVtx1 := false
	manager.GetVtxF = func(vtxID ids.ID) (avalanche.Vertex, error) {
		switch vtxID {
		case vtxID0:
			if parsedVtx0 {
				return vtx0, nil
			}
			return nil, errUnknownVertex
		case vtxID1:
			if parsedVtx1 {
				return vtx1, nil
			}
			return nil, errUnknownVertex
		default:
			t.Fatal(errUnknownVertex)
			panic(errUnknownVertex)
		}
	}
	manager.ParseVtxF = func(vtxBytes []byte) (avalanche.Vertex, error) {
		switch {
		case bytes.Equal(vtxBytes, vtxBytes0):
			vtx0.StatusV = choices.Processing
			parsedVtx0 = true
			return vtx0, nil
		case bytes.Equal(vtxBytes, vtxBytes1):
			vtx1.StatusV = choices.Processing
			parsedVtx1 = true
			return vtx1, nil
		}
		t.Fatal(errUnknownVertex)
		return nil, errUnknownVertex
	}

	requestIDs := map[ids.ID]uint32{}
	sender.SendGetAncestorsF = func(vdr ids.NodeID, reqID uint32, vtxID ids.ID) {
		if vdr != peerID {
			t.Fatalf("Should have requested block from %s, requested from %s", peerID, vdr)
		}
		requestIDs[vtxID] = reqID
	}

	if err := bs.ForceAccepted(acceptedIDs); err != nil { // should request vtx0 and vtx1
		t.Fatal(err)
	}

	reqID, ok := requestIDs[vtxID1]
	if !ok {
		t.Fatalf("should have requested vtx1")
	}

	if err := bs.Ancestors(peerID, reqID, [][]byte{vtxBytes1, vtxBytes0}); err != nil {
		t.Fatal(err)
	}

	reqID, ok = requestIDs[vtxID0]
	if !ok {
		t.Fatalf("should have requested vtx0")
	}

	err = bs.GetAncestorsFailed(peerID, reqID)
	switch {
	case err != nil:
		t.Fatal(err)
	case config.Ctx.GetState() != snow.NormalOp:
		t.Fatalf("Bootstrapping should have finished")
	case vtx0.Status() != choices.Accepted:
		t.Fatalf("Vertex should be accepted")
	case vtx1.Status() != choices.Accepted:
		t.Fatalf("Vertex should be accepted")
	}
}

// Test that Ancestors accepts the parents of the first vertex returned
func TestBootstrapperAcceptsAncestorsParents(t *testing.T) {
	config, peerID, sender, manager, vm := newConfig(t)

	vtxID0 := ids.Empty.Prefix(0)
	vtxID1 := ids.Empty.Prefix(1)
	vtxID2 := ids.Empty.Prefix(2)

	vtxBytes0 := []byte{0}
	vtxBytes1 := []byte{1}
	vtxBytes2 := []byte{2}

	vtx0 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     vtxID0,
			StatusV: choices.Unknown,
		},
		HeightV: 0,
		BytesV:  vtxBytes0,
	}
	vtx1 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     vtxID1,
			StatusV: choices.Unknown,
		},
		ParentsV: []avalanche.Vertex{vtx0},
		HeightV:  1,
		BytesV:   vtxBytes1,
	}
	vtx2 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     vtxID2,
			StatusV: choices.Unknown,
		},
		ParentsV: []avalanche.Vertex{vtx1},
		HeightV:  2,
		BytesV:   vtxBytes2,
	}

	bs, err := New(
		config,
		func(lastReqID uint32) error { config.Ctx.SetState(snow.NormalOp); return nil },
	)
	if err != nil {
		t.Fatal(err)
	}

	vm.CantSetState = false
	if err := bs.Start(0); err != nil {
		t.Fatal(err)
	}

	acceptedIDs := []ids.ID{vtxID2}

	parsedVtx0 := false
	parsedVtx1 := false
	parsedVtx2 := false
	manager.GetVtxF = func(vtxID ids.ID) (avalanche.Vertex, error) {
		switch vtxID {
		case vtxID0:
			if parsedVtx0 {
				return vtx0, nil
			}
			return nil, errUnknownVertex
		case vtxID1:
			if parsedVtx1 {
				return vtx1, nil
			}
			return nil, errUnknownVertex
		case vtxID2:
			if parsedVtx2 {
				return vtx2, nil
			}
		default:
			t.Fatal(errUnknownVertex)
			panic(errUnknownVertex)
		}
		return nil, errUnknownVertex
	}
	manager.ParseVtxF = func(vtxBytes []byte) (avalanche.Vertex, error) {
		switch {
		case bytes.Equal(vtxBytes, vtxBytes0):
			vtx0.StatusV = choices.Processing
			parsedVtx0 = true
			return vtx0, nil
		case bytes.Equal(vtxBytes, vtxBytes1):
			vtx1.StatusV = choices.Processing
			parsedVtx1 = true
			return vtx1, nil
		case bytes.Equal(vtxBytes, vtxBytes2):
			vtx2.StatusV = choices.Processing
			parsedVtx2 = true
			return vtx2, nil
		}
		t.Fatal(errUnknownVertex)
		return nil, errUnknownVertex
	}

	requestIDs := map[ids.ID]uint32{}
	sender.SendGetAncestorsF = func(vdr ids.NodeID, reqID uint32, vtxID ids.ID) {
		if vdr != peerID {
			t.Fatalf("Should have requested block from %s, requested from %s", peerID, vdr)
		}
		requestIDs[vtxID] = reqID
	}

	if err := bs.ForceAccepted(acceptedIDs); err != nil { // should request vtx2
		t.Fatal(err)
	}

	reqID, ok := requestIDs[vtxID2]
	if !ok {
		t.Fatalf("should have requested vtx2")
	}

	if err := bs.Ancestors(peerID, reqID, [][]byte{vtxBytes2, vtxBytes1, vtxBytes0}); err != nil {
		t.Fatal(err)
	}

	switch {
	case config.Ctx.GetState() != snow.NormalOp:
		t.Fatalf("Bootstrapping should have finished")
	case vtx0.Status() != choices.Accepted:
		t.Fatalf("Vertex should be accepted")
	case vtx1.Status() != choices.Accepted:
		t.Fatalf("Vertex should be accepted")
	case vtx2.Status() != choices.Accepted:
		t.Fatalf("Vertex should be accepted")
	}
}

func TestRestartBootstrapping(t *testing.T) {
	config, peerID, sender, manager, vm := newConfig(t)

	vtxID0 := ids.GenerateTestID()
	vtxID1 := ids.GenerateTestID()
	vtxID2 := ids.GenerateTestID()
	vtxID3 := ids.GenerateTestID()
	vtxID4 := ids.GenerateTestID()
	vtxID5 := ids.GenerateTestID()

	vtxBytes0 := []byte{0}
	vtxBytes1 := []byte{1}
	vtxBytes2 := []byte{2}
	vtxBytes3 := []byte{3}
	vtxBytes4 := []byte{4}
	vtxBytes5 := []byte{5}

	vtx0 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     vtxID0,
			StatusV: choices.Unknown,
		},
		HeightV: 0,
		BytesV:  vtxBytes0,
	}
	vtx1 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     vtxID1,
			StatusV: choices.Unknown,
		},
		ParentsV: []avalanche.Vertex{vtx0},
		HeightV:  1,
		BytesV:   vtxBytes1,
	}
	vtx2 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     vtxID2,
			StatusV: choices.Unknown,
		},
		ParentsV: []avalanche.Vertex{vtx1},
		HeightV:  2,
		BytesV:   vtxBytes2,
	}
	vtx3 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     vtxID3,
			StatusV: choices.Unknown,
		},
		ParentsV: []avalanche.Vertex{vtx2},
		HeightV:  3,
		BytesV:   vtxBytes3,
	}
	vtx4 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     vtxID4,
			StatusV: choices.Unknown,
		},
		ParentsV: []avalanche.Vertex{vtx2},
		HeightV:  3,
		BytesV:   vtxBytes4,
	}
	vtx5 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     vtxID5,
			StatusV: choices.Unknown,
		},
		ParentsV: []avalanche.Vertex{vtx4},
		HeightV:  4,
		BytesV:   vtxBytes5,
	}

	bsIntf, err := New(
		config,
		func(lastReqID uint32) error { config.Ctx.SetState(snow.NormalOp); return nil },
	)
	if err != nil {
		t.Fatal(err)
	}
	bs, ok := bsIntf.(*bootstrapper)
	if !ok {
		t.Fatal("unexpected bootstrapper type")
	}

	vm.CantSetState = false
	if err := bs.Start(0); err != nil {
		t.Fatal(err)
	}

	parsedVtx0 := false
	parsedVtx1 := false
	parsedVtx2 := false
	parsedVtx3 := false
	parsedVtx4 := false
	parsedVtx5 := false
	manager.GetVtxF = func(vtxID ids.ID) (avalanche.Vertex, error) {
		switch vtxID {
		case vtxID0:
			if parsedVtx0 {
				return vtx0, nil
			}
			return nil, errUnknownVertex
		case vtxID1:
			if parsedVtx1 {
				return vtx1, nil
			}
			return nil, errUnknownVertex
		case vtxID2:
			if parsedVtx2 {
				return vtx2, nil
			}
		case vtxID3:
			if parsedVtx3 {
				return vtx3, nil
			}
		case vtxID4:
			if parsedVtx4 {
				return vtx4, nil
			}
		case vtxID5:
			if parsedVtx5 {
				return vtx5, nil
			}
		default:
			t.Fatal(errUnknownVertex)
			panic(errUnknownVertex)
		}
		return nil, errUnknownVertex
	}
	manager.ParseVtxF = func(vtxBytes []byte) (avalanche.Vertex, error) {
		switch {
		case bytes.Equal(vtxBytes, vtxBytes0):
			vtx0.StatusV = choices.Processing
			parsedVtx0 = true
			return vtx0, nil
		case bytes.Equal(vtxBytes, vtxBytes1):
			vtx1.StatusV = choices.Processing
			parsedVtx1 = true
			return vtx1, nil
		case bytes.Equal(vtxBytes, vtxBytes2):
			vtx2.StatusV = choices.Processing
			parsedVtx2 = true
			return vtx2, nil
		case bytes.Equal(vtxBytes, vtxBytes3):
			vtx3.StatusV = choices.Processing
			parsedVtx3 = true
			return vtx3, nil
		case bytes.Equal(vtxBytes, vtxBytes4):
			vtx4.StatusV = choices.Processing
			parsedVtx4 = true
			return vtx4, nil
		case bytes.Equal(vtxBytes, vtxBytes5):
			vtx5.StatusV = choices.Processing
			parsedVtx5 = true
			return vtx5, nil
		}
		t.Fatal(errUnknownVertex)
		return nil, errUnknownVertex
	}

	requestIDs := map[ids.ID]uint32{}
	sender.SendGetAncestorsF = func(vdr ids.NodeID, reqID uint32, vtxID ids.ID) {
		if vdr != peerID {
			t.Fatalf("Should have requested block from %s, requested from %s", peerID, vdr)
		}
		requestIDs[vtxID] = reqID
	}

	if err := bs.ForceAccepted([]ids.ID{vtxID3, vtxID4}); err != nil { // should request vtx3 and vtx4
		t.Fatal(err)
	}

	vtx3ReqID, ok := requestIDs[vtxID3]
	if !ok {
		t.Fatal("should have requested vtx4")
	}
	_, ok = requestIDs[vtxID4]
	if !ok {
		t.Fatal("should have requested vtx4")
	}

	if err := bs.Ancestors(peerID, vtx3ReqID, [][]byte{vtxBytes3, vtxBytes2}); err != nil {
		t.Fatal(err)
	}

	_, ok = requestIDs[vtxID1]
	if !ok {
		t.Fatal("should have requested vtx1")
	}

	if removed := bs.OutstandingRequests.RemoveAny(vtxID4); !removed {
		t.Fatal("expected to find outstanding requested for vtx4")
	}

	if removed := bs.OutstandingRequests.RemoveAny(vtxID1); !removed {
		t.Fatal("expected to find outstanding requested for vtx1")
	}
	bs.needToFetch.Clear()
	requestIDs = map[ids.ID]uint32{}

	if err := bs.ForceAccepted([]ids.ID{vtxID5, vtxID3}); err != nil {
		t.Fatal(err)
	}

	vtx1ReqID, ok := requestIDs[vtxID1]
	if !ok {
		t.Fatal("should have re-requested vtx1 from pending on prior run")
	}
	_, ok = requestIDs[vtxID4]
	if !ok {
		t.Fatal("should have re-requested vtx4 from pending on prior run")
	}
	vtx5ReqID, ok := requestIDs[vtxID5]
	if !ok {
		t.Fatal("should have requested vtx5")
	}
	if _, ok := requestIDs[vtxID3]; ok {
		t.Fatal("should not have re-requested vtx3 since it has been processed")
	}

	if err := bs.Ancestors(peerID, vtx5ReqID, [][]byte{vtxBytes5, vtxBytes4, vtxBytes2, vtxBytes1}); err != nil {
		t.Fatal(err)
	}

	_, ok = requestIDs[vtxID0]
	if !ok {
		t.Fatal("should have requested vtx0 after ancestors ended prior to it")
	}

	if err := bs.Ancestors(peerID, vtx1ReqID, [][]byte{vtxBytes1, vtxBytes0}); err != nil {
		t.Fatal(err)
	}

	switch {
	case config.Ctx.GetState() != snow.NormalOp:
		t.Fatalf("Bootstrapping should have finished")
	case vtx0.Status() != choices.Accepted:
		t.Fatalf("Vertex should be accepted")
	case vtx1.Status() != choices.Accepted:
		t.Fatalf("Vertex should be accepted")
	case vtx2.Status() != choices.Accepted:
		t.Fatalf("Vertex should be accepted")
	case vtx3.Status() != choices.Accepted:
		t.Fatalf("Vertex should be accepted")
	case vtx4.Status() != choices.Accepted:
		t.Fatalf("Vertex should be accepted")
	case vtx5.Status() != choices.Accepted:
		t.Fatalf("Vertex should be accepted")
	}
}

```

avalanchego/snow/engine/avalanche/bootstrap/config.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package bootstrap

import (
	"github.com/ava-labs/avalanchego/snow/engine/avalanche/vertex"
	"github.com/ava-labs/avalanchego/snow/engine/common"
	"github.com/ava-labs/avalanchego/snow/engine/common/queue"
)

type Config struct {
	common.Config
	common.AllGetsServer

	// VtxBlocked tracks operations that are blocked on vertices
	VtxBlocked *queue.JobsWithMissing
	// TxBlocked tracks operations that are blocked on transactions
	TxBlocked *queue.Jobs

	Manager vertex.Manager
	VM      vertex.DAGVM
}

```

avalanchego/snow/engine/avalanche/bootstrap/metrics.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package bootstrap

import (
	"github.com/prometheus/client_golang/prometheus"

	"github.com/ava-labs/avalanchego/utils/wrappers"
)

type metrics struct {
	numFetchedVts, numDroppedVts, numAcceptedVts,
	numFetchedTxs, numDroppedTxs, numAcceptedTxs prometheus.Counter
}

func (m *metrics) Initialize(
	namespace string,
	registerer prometheus.Registerer,
) error {
	m.numFetchedVts = prometheus.NewCounter(prometheus.CounterOpts{
		Namespace: namespace,
		Name:      "fetched_vts",
		Help:      "Number of vertices fetched during bootstrapping",
	})
	m.numDroppedVts = prometheus.NewCounter(prometheus.CounterOpts{
		Namespace: namespace,
		Name:      "dropped_vts",
		Help:      "Number of vertices dropped during bootstrapping",
	})
	m.numAcceptedVts = prometheus.NewCounter(prometheus.CounterOpts{
		Namespace: namespace,
		Name:      "accepted_vts",
		Help:      "Number of vertices accepted during bootstrapping",
	})

	m.numFetchedTxs = prometheus.NewCounter(prometheus.CounterOpts{
		Namespace: namespace,
		Name:      "fetched_txs",
		Help:      "Number of transactions fetched during bootstrapping",
	})
	m.numDroppedTxs = prometheus.NewCounter(prometheus.CounterOpts{
		Namespace: namespace,
		Name:      "dropped_txs",
		Help:      "Number of transactions dropped during bootstrapping",
	})
	m.numAcceptedTxs = prometheus.NewCounter(prometheus.CounterOpts{
		Namespace: namespace,
		Name:      "accepted_txs",
		Help:      "Number of transactions accepted during bootstrapping",
	})

	errs := wrappers.Errs{}
	errs.Add(
		registerer.Register(m.numFetchedVts),
		registerer.Register(m.numDroppedVts),
		registerer.Register(m.numAcceptedVts),
		registerer.Register(m.numFetchedTxs),
		registerer.Register(m.numDroppedTxs),
		registerer.Register(m.numAcceptedTxs),
	)
	return errs.Err
}

```

avalanchego/snow/engine/avalanche/bootstrap/tx_job.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package bootstrap

import (
	"errors"
	"fmt"

	"github.com/prometheus/client_golang/prometheus"

	"go.uber.org/zap"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/choices"
	"github.com/ava-labs/avalanchego/snow/consensus/snowstorm"
	"github.com/ava-labs/avalanchego/snow/engine/avalanche/vertex"
	"github.com/ava-labs/avalanchego/snow/engine/common/queue"
	"github.com/ava-labs/avalanchego/utils/logging"
)

var errMissingTxDependenciesOnAccept = errors.New("attempting to accept a transaction with missing dependencies")

type txParser struct {
	log                     logging.Logger
	numAccepted, numDropped prometheus.Counter
	vm                      vertex.DAGVM
}

func (p *txParser) Parse(txBytes []byte) (queue.Job, error) {
	tx, err := p.vm.ParseTx(txBytes)
	if err != nil {
		return nil, err
	}
	return &txJob{
		log:         p.log,
		numAccepted: p.numAccepted,
		numDropped:  p.numDropped,
		tx:          tx,
	}, nil
}

type txJob struct {
	log                     logging.Logger
	numAccepted, numDropped prometheus.Counter
	tx                      snowstorm.Tx
}

func (t *txJob) ID() ids.ID { return t.tx.ID() }
func (t *txJob) MissingDependencies() (ids.Set, error) {
	missing := ids.Set{}
	deps, err := t.tx.Dependencies()
	if err != nil {
		return missing, err
	}
	for _, dep := range deps {
		if dep.Status() != choices.Accepted {
			missing.Add(dep.ID())
		}
	}
	return missing, nil
}

// Returns true if this tx job has at least 1 missing dependency
func (t *txJob) HasMissingDependencies() (bool, error) {
	deps, err := t.tx.Dependencies()
	if err != nil {
		return false, err
	}
	for _, dep := range deps {
		if dep.Status() != choices.Accepted {
			return true, nil
		}
	}
	return false, nil
}

func (t *txJob) Execute() error {
	hasMissingDeps, err := t.HasMissingDependencies()
	if err != nil {
		return err
	}
	if hasMissingDeps {
		t.numDropped.Inc()
		return errMissingTxDependenciesOnAccept
	}

	status := t.tx.Status()
	switch status {
	case choices.Unknown, choices.Rejected:
		t.numDropped.Inc()
		return fmt.Errorf("attempting to execute transaction with status %s", status)
	case choices.Processing:
		txID := t.tx.ID()
		if err := t.tx.Verify(); err != nil {
			t.log.Error("transaction failed verification during bootstrapping",
				zap.Stringer("txID", txID),
				zap.Error(err),
			)
			return fmt.Errorf("failed to verify transaction in bootstrapping: %w", err)
		}

		t.numAccepted.Inc()
		t.log.Trace("accepting transaction in bootstrapping",
			zap.Stringer("txID", txID),
		)
		if err := t.tx.Accept(); err != nil {
			t.log.Error("transaction failed to accept during bootstrapping",
				zap.Stringer("txID", txID),
				zap.Error(err),
			)
			return fmt.Errorf("failed to accept transaction in bootstrapping: %w", err)
		}
	}
	return nil
}
func (t *txJob) Bytes() []byte { return t.tx.Bytes() }

```

avalanchego/snow/engine/avalanche/bootstrap/vertex_job.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package bootstrap

import (
	"errors"
	"fmt"

	"github.com/prometheus/client_golang/prometheus"

	"go.uber.org/zap"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/choices"
	"github.com/ava-labs/avalanchego/snow/consensus/avalanche"
	"github.com/ava-labs/avalanchego/snow/engine/avalanche/vertex"
	"github.com/ava-labs/avalanchego/snow/engine/common/queue"
	"github.com/ava-labs/avalanchego/utils/logging"
)

var errMissingVtxDependenciesOnAccept = errors.New("attempting to execute blocked vertex")

type vtxParser struct {
	log                     logging.Logger
	numAccepted, numDropped prometheus.Counter
	manager                 vertex.Manager
}

func (p *vtxParser) Parse(vtxBytes []byte) (queue.Job, error) {
	vtx, err := p.manager.ParseVtx(vtxBytes)
	if err != nil {
		return nil, err
	}
	return &vertexJob{
		log:         p.log,
		numAccepted: p.numAccepted,
		numDropped:  p.numDropped,
		vtx:         vtx,
	}, nil
}

type vertexJob struct {
	log                     logging.Logger
	numAccepted, numDropped prometheus.Counter
	vtx                     avalanche.Vertex
}

func (v *vertexJob) ID() ids.ID { return v.vtx.ID() }

func (v *vertexJob) MissingDependencies() (ids.Set, error) {
	missing := ids.Set{}
	parents, err := v.vtx.Parents()
	if err != nil {
		return missing, err
	}
	for _, parent := range parents {
		if parent.Status() != choices.Accepted {
			missing.Add(parent.ID())
		}
	}
	return missing, nil
}

// Returns true if this vertex job has at least 1 missing dependency
func (v *vertexJob) HasMissingDependencies() (bool, error) {
	parents, err := v.vtx.Parents()
	if err != nil {
		return false, err
	}
	for _, parent := range parents {
		if parent.Status() != choices.Accepted {
			return true, nil
		}
	}
	return false, nil
}

func (v *vertexJob) Execute() error {
	hasMissingDependencies, err := v.HasMissingDependencies()
	if err != nil {
		return err
	}
	if hasMissingDependencies {
		v.numDropped.Inc()
		return errMissingVtxDependenciesOnAccept
	}
	txs, err := v.vtx.Txs()
	if err != nil {
		return err
	}
	for _, tx := range txs {
		if tx.Status() != choices.Accepted {
			v.numDropped.Inc()
			v.log.Warn("attempting to execute vertex with non-accepted transactions")
			return nil
		}
	}
	status := v.vtx.Status()
	switch status {
	case choices.Unknown, choices.Rejected:
		v.numDropped.Inc()
		return fmt.Errorf("attempting to execute vertex with status %s", status)
	case choices.Processing:
		v.numAccepted.Inc()
		v.log.Trace("accepting vertex in bootstrapping",
			zap.Stringer("vtxID", v.vtx.ID()),
		)
		if err := v.vtx.Accept(); err != nil {
			return fmt.Errorf("failed to accept vertex in bootstrapping: %w", err)
		}
	}
	return nil
}

func (v *vertexJob) Bytes() []byte { return v.vtx.Bytes() }

```

avalanchego/snow/engine/avalanche/config.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package avalanche

import (
	"github.com/ava-labs/avalanchego/snow"
	"github.com/ava-labs/avalanchego/snow/consensus/avalanche"
	"github.com/ava-labs/avalanchego/snow/engine/avalanche/vertex"
	"github.com/ava-labs/avalanchego/snow/engine/common"
	"github.com/ava-labs/avalanchego/snow/validators"
)

// Config wraps all the parameters needed for an avalanche engine
type Config struct {
	Ctx *snow.ConsensusContext
	common.AllGetsServer
	VM         vertex.DAGVM
	Manager    vertex.Manager
	Sender     common.Sender
	Validators validators.Set

	Params    avalanche.Parameters
	Consensus avalanche.Consensus
}

```

avalanchego/snow/engine/avalanche/config_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package avalanche

import (
	"github.com/prometheus/client_golang/prometheus"

	"github.com/ava-labs/avalanchego/database/memdb"
	"github.com/ava-labs/avalanchego/snow/consensus/avalanche"
	"github.com/ava-labs/avalanchego/snow/consensus/snowball"
	"github.com/ava-labs/avalanchego/snow/engine/avalanche/bootstrap"
	"github.com/ava-labs/avalanchego/snow/engine/avalanche/vertex"
	"github.com/ava-labs/avalanchego/snow/engine/common"
	"github.com/ava-labs/avalanchego/snow/engine/common/queue"
)

func DefaultConfig() (common.Config, bootstrap.Config, Config) {
	vtxBlocked, _ := queue.NewWithMissing(memdb.New(), "", prometheus.NewRegistry())
	txBlocked, _ := queue.New(memdb.New(), "", prometheus.NewRegistry())

	commonCfg := common.DefaultConfigTest()

	bootstrapConfig := bootstrap.Config{
		Config:     commonCfg,
		VtxBlocked: vtxBlocked,
		TxBlocked:  txBlocked,
		Manager:    &vertex.TestManager{},
		VM:         &vertex.TestVM{},
	}

	engineConfig := Config{
		Ctx:        bootstrapConfig.Ctx,
		VM:         bootstrapConfig.VM,
		Manager:    bootstrapConfig.Manager,
		Sender:     bootstrapConfig.Sender,
		Validators: bootstrapConfig.Validators,
		Params: avalanche.Parameters{
			Parameters: snowball.Parameters{
				K:                       1,
				Alpha:                   1,
				BetaVirtuous:            1,
				BetaRogue:               2,
				ConcurrentRepolls:       1,
				OptimalProcessing:       100,
				MaxOutstandingItems:     1,
				MaxItemProcessingTime:   1,
				MixedQueryNumPushVdr:    1,
				MixedQueryNumPushNonVdr: 1,
			},
			Parents:   2,
			BatchSize: 1,
		},
		Consensus: &avalanche.Topological{},
	}

	return commonCfg, bootstrapConfig, engineConfig
}

```

avalanchego/snow/engine/avalanche/engine.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package avalanche

import (
	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/consensus/avalanche"
	"github.com/ava-labs/avalanchego/snow/engine/common"
)

// Engine describes the events that can occur on a consensus instance
type Engine interface {
	common.Engine

	// GetVtx returns a vertex by its ID.
	// Returns an error if unknown.
	GetVtx(vtxID ids.ID) (avalanche.Vertex, error)
}

```

avalanchego/snow/engine/avalanche/getter/getter.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package getter

import (
	"time"

	"go.uber.org/zap"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/message"
	"github.com/ava-labs/avalanchego/snow/choices"
	"github.com/ava-labs/avalanchego/snow/consensus/avalanche"
	"github.com/ava-labs/avalanchego/snow/engine/avalanche/vertex"
	"github.com/ava-labs/avalanchego/snow/engine/common"
	"github.com/ava-labs/avalanchego/utils/constants"
	"github.com/ava-labs/avalanchego/utils/logging"
	"github.com/ava-labs/avalanchego/utils/metric"
	"github.com/ava-labs/avalanchego/utils/wrappers"
)

// Get requests are always served, regardless node state (bootstrapping or normal operations).
var _ common.AllGetsServer = &getter{}

func New(storage vertex.Storage, commonCfg common.Config) (common.AllGetsServer, error) {
	gh := &getter{
		storage: storage,
		sender:  commonCfg.Sender,
		cfg:     commonCfg,
		log:     commonCfg.Ctx.Log,
	}

	var err error
	gh.getAncestorsVtxs, err = metric.NewAverager(
		"bs",
		"get_ancestors_vtxs",
		"vertices fetched in a call to GetAncestors",
		commonCfg.Ctx.Registerer,
	)
	return gh, err
}

type getter struct {
	storage vertex.Storage
	sender  common.Sender
	cfg     common.Config

	log              logging.Logger
	getAncestorsVtxs metric.Averager
}

func (gh *getter) GetStateSummaryFrontier(nodeID ids.NodeID, requestID uint32) error {
	gh.log.Debug("dropping request",
		zap.String("reason", "unhandled by this gear"),
		zap.Stringer("messageOp", message.GetStateSummaryFrontier),
		zap.Stringer("nodeID", nodeID),
		zap.Uint32("requestID", requestID),
	)
	return nil
}

func (gh *getter) GetAcceptedStateSummary(nodeID ids.NodeID, requestID uint32, _ []uint64) error {
	gh.log.Debug("dropping request",
		zap.String("reason", "unhandled by this gear"),
		zap.Stringer("messageOp", message.GetAcceptedStateSummary),
		zap.Stringer("nodeID", nodeID),
		zap.Uint32("requestID", requestID),
	)
	return nil
}

func (gh *getter) GetAcceptedFrontier(validatorID ids.NodeID, requestID uint32) error {
	acceptedFrontier := gh.storage.Edge()
	gh.sender.SendAcceptedFrontier(validatorID, requestID, acceptedFrontier)
	return nil
}

func (gh *getter) GetAccepted(nodeID ids.NodeID, requestID uint32, containerIDs []ids.ID) error {
	acceptedVtxIDs := make([]ids.ID, 0, len(containerIDs))
	for _, vtxID := range containerIDs {
		if vtx, err := gh.storage.GetVtx(vtxID); err == nil && vtx.Status() == choices.Accepted {
			acceptedVtxIDs = append(acceptedVtxIDs, vtxID)
		}
	}
	gh.sender.SendAccepted(nodeID, requestID, acceptedVtxIDs)
	return nil
}

func (gh *getter) GetAncestors(nodeID ids.NodeID, requestID uint32, vtxID ids.ID) error {
	startTime := time.Now()
	gh.log.Verbo("called GetAncestors",
		zap.Stringer("nodeID", nodeID),
		zap.Uint32("requestID", requestID),
		zap.Stringer("vtxID", vtxID),
	)
	vertex, err := gh.storage.GetVtx(vtxID)
	if err != nil || vertex.Status() == choices.Unknown {
		gh.log.Verbo("dropping getAncestors")
		return nil // Don't have the requested vertex. Drop message.
	}

	queue := make([]avalanche.Vertex, 1, gh.cfg.AncestorsMaxContainersSent) // for BFS
	queue[0] = vertex
	ancestorsBytesLen := 0                                                 // length, in bytes, of vertex and its ancestors
	ancestorsBytes := make([][]byte, 0, gh.cfg.AncestorsMaxContainersSent) // vertex and its ancestors in BFS order
	visited := ids.Set{}                                                   // IDs of vertices that have been in queue before
	visited.Add(vertex.ID())

	for len(ancestorsBytes) < gh.cfg.AncestorsMaxContainersSent && len(queue) > 0 && time.Since(startTime) < gh.cfg.MaxTimeGetAncestors {
		var vtx avalanche.Vertex
		vtx, queue = queue[0], queue[1:] // pop
		vtxBytes := vtx.Bytes()
		// Ensure response size isn't too large. Include wrappers.IntLen because the size of the message
		// is included with each container, and the size is repr. by an int.
		if newLen := wrappers.IntLen + ancestorsBytesLen + len(vtxBytes); newLen < constants.MaxContainersLen {
			ancestorsBytes = append(ancestorsBytes, vtxBytes)
			ancestorsBytesLen = newLen
		} else { // reached maximum response size
			break
		}
		parents, err := vtx.Parents()
		if err != nil {
			return err
		}
		for _, parent := range parents {
			if parent.Status() == choices.Unknown { // Don't have this vertex;ignore
				continue
			}
			if parentID := parent.ID(); !visited.Contains(parentID) { // If already visited, ignore
				queue = append(queue, parent)
				visited.Add(parentID)
			}
		}
	}

	gh.getAncestorsVtxs.Observe(float64(len(ancestorsBytes)))
	gh.sender.SendAncestors(nodeID, requestID, ancestorsBytes)
	return nil
}

func (gh *getter) Get(nodeID ids.NodeID, requestID uint32, vtxID ids.ID) error {
	// If this engine has access to the requested vertex, provide it
	if vtx, err := gh.storage.GetVtx(vtxID); err == nil {
		gh.sender.SendPut(nodeID, requestID, vtx.Bytes())
	}
	return nil
}

```

avalanchego/snow/engine/avalanche/getter/getter_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package getter

import (
	"errors"
	"testing"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow"
	"github.com/ava-labs/avalanchego/snow/choices"
	"github.com/ava-labs/avalanchego/snow/consensus/avalanche"
	"github.com/ava-labs/avalanchego/snow/engine/avalanche/vertex"
	"github.com/ava-labs/avalanchego/snow/engine/common"
	"github.com/ava-labs/avalanchego/snow/validators"
)

var errUnknownVertex = errors.New("unknown vertex")

func testSetup(t *testing.T) (*vertex.TestManager, *common.SenderTest, common.Config) {
	peers := validators.NewSet()
	peer := ids.GenerateTestNodeID()
	if err := peers.AddWeight(peer, 1); err != nil {
		t.Fatal(err)
	}

	sender := &common.SenderTest{T: t}
	sender.Default(true)
	sender.CantSendGetAcceptedFrontier = false

	isBootstrapped := false
	subnet := &common.SubnetTest{
		T:               t,
		IsBootstrappedF: func() bool { return isBootstrapped },
		BootstrappedF:   func(ids.ID) { isBootstrapped = true },
	}

	commonConfig := common.Config{
		Ctx:                            snow.DefaultConsensusContextTest(),
		Validators:                     peers,
		Beacons:                        peers,
		SampleK:                        peers.Len(),
		Alpha:                          peers.Weight()/2 + 1,
		Sender:                         sender,
		Subnet:                         subnet,
		Timer:                          &common.TimerTest{},
		AncestorsMaxContainersSent:     2000,
		AncestorsMaxContainersReceived: 2000,
		SharedCfg:                      &common.SharedConfig{},
	}

	manager := vertex.NewTestManager(t)
	manager.Default(true)

	return manager, sender, commonConfig
}

func TestAcceptedFrontier(t *testing.T) {
	manager, sender, config := testSetup(t)

	vtxID0 := ids.GenerateTestID()
	vtxID1 := ids.GenerateTestID()
	vtxID2 := ids.GenerateTestID()

	bsIntf, err := New(manager, config)
	if err != nil {
		t.Fatal(err)
	}
	bs, ok := bsIntf.(*getter)
	if !ok {
		t.Fatal("Unexpected get handler")
	}

	manager.EdgeF = func() []ids.ID {
		return []ids.ID{
			vtxID0,
			vtxID1,
		}
	}

	var accepted []ids.ID
	sender.SendAcceptedFrontierF = func(_ ids.NodeID, _ uint32, frontier []ids.ID) {
		accepted = frontier
	}

	if err := bs.GetAcceptedFrontier(ids.EmptyNodeID, 0); err != nil {
		t.Fatal(err)
	}

	acceptedSet := ids.Set{}
	acceptedSet.Add(accepted...)

	manager.EdgeF = nil

	if !acceptedSet.Contains(vtxID0) {
		t.Fatalf("Vtx should be accepted")
	}
	if !acceptedSet.Contains(vtxID1) {
		t.Fatalf("Vtx should be accepted")
	}
	if acceptedSet.Contains(vtxID2) {
		t.Fatalf("Vtx shouldn't be accepted")
	}
}

func TestFilterAccepted(t *testing.T) {
	manager, sender, config := testSetup(t)

	vtxID0 := ids.GenerateTestID()
	vtxID1 := ids.GenerateTestID()
	vtxID2 := ids.GenerateTestID()

	vtx0 := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     vtxID0,
		StatusV: choices.Accepted,
	}}
	vtx1 := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     vtxID1,
		StatusV: choices.Accepted,
	}}

	bsIntf, err := New(manager, config)
	if err != nil {
		t.Fatal(err)
	}
	bs, ok := bsIntf.(*getter)
	if !ok {
		t.Fatal("Unexpected get handler")
	}

	vtxIDs := []ids.ID{vtxID0, vtxID1, vtxID2}

	manager.GetVtxF = func(vtxID ids.ID) (avalanche.Vertex, error) {
		switch vtxID {
		case vtxID0:
			return vtx0, nil
		case vtxID1:
			return vtx1, nil
		case vtxID2:
			return nil, errUnknownVertex
		}
		t.Fatal(errUnknownVertex)
		return nil, errUnknownVertex
	}

	var accepted []ids.ID
	sender.SendAcceptedF = func(_ ids.NodeID, _ uint32, frontier []ids.ID) {
		accepted = frontier
	}

	if err := bs.GetAccepted(ids.EmptyNodeID, 0, vtxIDs); err != nil {
		t.Fatal(err)
	}

	acceptedSet := ids.Set{}
	acceptedSet.Add(accepted...)

	manager.GetVtxF = nil

	if !acceptedSet.Contains(vtxID0) {
		t.Fatalf("Vtx should be accepted")
	}
	if !acceptedSet.Contains(vtxID1) {
		t.Fatalf("Vtx should be accepted")
	}
	if acceptedSet.Contains(vtxID2) {
		t.Fatalf("Vtx shouldn't be accepted")
	}
}

```

avalanchego/snow/engine/avalanche/issuer.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package avalanche

import (
	"go.uber.org/zap"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/consensus/avalanche"
	"github.com/ava-labs/avalanchego/snow/consensus/snowstorm"
	"github.com/ava-labs/avalanchego/snow/engine/common"
)

// issuer issues [vtx] into consensus after its dependencies are met.
type issuer struct {
	t                 *Transitive
	vtx               avalanche.Vertex
	issued, abandoned bool
	vtxDeps, txDeps   ids.Set
}

// Register that a vertex we were waiting on has been issued to consensus.
func (i *issuer) FulfillVtx(id ids.ID) {
	i.vtxDeps.Remove(id)
	i.Update()
}

// Register that a transaction we were waiting on has been issued to consensus.
func (i *issuer) FulfillTx(id ids.ID) {
	i.txDeps.Remove(id)
	i.Update()
}

// Abandon this attempt to issue
func (i *issuer) Abandon() {
	if !i.abandoned {
		vtxID := i.vtx.ID()
		i.t.pending.Remove(vtxID)
		i.abandoned = true
		i.t.vtxBlocked.Abandon(vtxID) // Inform vertices waiting on this vtx that it won't be issued
		i.t.metrics.blockerVtxs.Set(float64(i.t.vtxBlocked.Len()))
	}
}

// Issue the poll when all dependencies are met
func (i *issuer) Update() {
	if i.abandoned || i.issued || i.vtxDeps.Len() != 0 || i.txDeps.Len() != 0 || i.t.Consensus.VertexIssued(i.vtx) || i.t.errs.Errored() {
		return
	}

	vtxID := i.vtx.ID()

	// All dependencies have been met
	i.issued = true

	// check stop vertex validity
	err := i.vtx.Verify()
	if err != nil {
		if i.vtx.HasWhitelist() {
			// do not update "i.t.errs" since it's only used for critical errors
			// which will cause chain shutdown in the engine
			// (see "handleSyncMsg" and "handleChanMsg")
			i.t.Ctx.Log.Debug("stop vertex verification failed",
				zap.Stringer("vtxID", vtxID),
				zap.Error(err),
			)
			i.t.metrics.whitelistVtxIssueFailure.Inc()
		} else {
			i.t.Ctx.Log.Debug("vertex verification failed",
				zap.Stringer("vtxID", vtxID),
				zap.Error(err),
			)
		}

		i.t.vtxBlocked.Abandon(vtxID)
		return
	}

	i.t.pending.Remove(vtxID) // Remove from set of vertices waiting to be issued.

	// Make sure the transactions in this vertex are valid
	txs, err := i.vtx.Txs()
	if err != nil {
		i.t.errs.Add(err)
		return
	}
	validTxs := make([]snowstorm.Tx, 0, len(txs))
	for _, tx := range txs {
		if err := tx.Verify(); err != nil {
			txID := tx.ID()
			i.t.Ctx.Log.Debug("transaction verification failed",
				zap.Stringer("txID", txID),
				zap.Error(err),
			)
			i.t.txBlocked.Abandon(txID)
		} else {
			validTxs = append(validTxs, tx)
		}
	}

	// Some of the transactions weren't valid. Abandon this vertex.
	// Take the valid transactions and issue a new vertex with them.
	if len(validTxs) != len(txs) {
		i.t.Ctx.Log.Debug("abandoning vertex",
			zap.String("reason", "transaction verification failed"),
			zap.Stringer("vtxID", vtxID),
		)
		if _, err := i.t.batch(validTxs, batchOption{}); err != nil {
			i.t.errs.Add(err)
		}
		i.t.vtxBlocked.Abandon(vtxID)
		i.t.metrics.blockerVtxs.Set(float64(i.t.vtxBlocked.Len()))
		return
	}

	i.t.Ctx.Log.Verbo("adding vertex to consensus",
		zap.Stringer("vtxID", vtxID),
	)

	// Add this vertex to consensus.
	if err := i.t.Consensus.Add(i.vtx); err != nil {
		i.t.errs.Add(err)
		return
	}

	// Issue a poll for this vertex.
	p := i.t.Consensus.Parameters()
	vdrs, err := i.t.Validators.Sample(p.K) // Validators to sample
	if err != nil {
		i.t.Ctx.Log.Error("dropped query",
			zap.String("reason", "insufficient number of validators"),
			zap.Stringer("vtxID", vtxID),
		)
	}

	vdrBag := ids.NodeIDBag{} // Validators to sample repr. as a set
	for _, vdr := range vdrs {
		vdrBag.Add(vdr.ID())
	}

	i.t.RequestID++
	if err == nil && i.t.polls.Add(i.t.RequestID, vdrBag) {
		numPushTo := i.t.Params.MixedQueryNumPushVdr
		if !i.t.Validators.Contains(i.t.Ctx.NodeID) {
			numPushTo = i.t.Params.MixedQueryNumPushNonVdr
		}
		common.SendMixedQuery(
			i.t.Sender,
			vdrBag.List(), // Note that this doesn't contain duplicates; length may be < k
			numPushTo,
			i.t.RequestID,
			vtxID,
			i.vtx.Bytes(),
		)
	}

	// Notify vertices waiting on this one that it (and its transactions) have been issued.
	i.t.vtxBlocked.Fulfill(vtxID)
	for _, tx := range txs {
		i.t.txBlocked.Fulfill(tx.ID())
	}
	i.t.metrics.blockerTxs.Set(float64(i.t.txBlocked.Len()))
	i.t.metrics.blockerVtxs.Set(float64(i.t.vtxBlocked.Len()))

	if i.vtx.HasWhitelist() {
		i.t.Ctx.Log.Info("successfully issued stop vertex",
			zap.Stringer("vtxID", vtxID),
		)
		i.t.metrics.whitelistVtxIssueSuccess.Inc()
	}

	// Issue a repoll
	i.t.repoll()
}

type vtxIssuer struct{ i *issuer }

func (vi *vtxIssuer) Dependencies() ids.Set { return vi.i.vtxDeps }
func (vi *vtxIssuer) Fulfill(id ids.ID)     { vi.i.FulfillVtx(id) }
func (vi *vtxIssuer) Abandon(ids.ID)        { vi.i.Abandon() }
func (vi *vtxIssuer) Update()               { vi.i.Update() }

type txIssuer struct{ i *issuer }

func (ti *txIssuer) Dependencies() ids.Set { return ti.i.txDeps }
func (ti *txIssuer) Fulfill(id ids.ID)     { ti.i.FulfillTx(id) }
func (ti *txIssuer) Abandon(ids.ID)        { ti.i.Abandon() }
func (ti *txIssuer) Update()               { ti.i.Update() }

```

avalanchego/snow/engine/avalanche/metrics.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package avalanche

import (
	"github.com/prometheus/client_golang/prometheus"

	"github.com/ava-labs/avalanchego/utils/wrappers"
)

type metrics struct {
	bootstrapFinished,
	numVtxRequests, numPendingVts,
	numMissingTxs, pendingTxs,
	blockerVtxs, blockerTxs prometheus.Gauge

	whitelistVtxIssueSuccess, whitelistVtxIssueFailure,
	numUselessPutBytes, numUselessPushQueryBytes prometheus.Counter
}

func (m *metrics) Initialize(namespace string, reg prometheus.Registerer) error {
	errs := wrappers.Errs{}
	m.bootstrapFinished = prometheus.NewGauge(prometheus.GaugeOpts{
		Namespace: namespace,
		Name:      "bootstrap_finished",
		Help:      "Whether or not bootstrap process has completed. 1 is success, 0 is fail or ongoing.",
	})
	m.numVtxRequests = prometheus.NewGauge(prometheus.GaugeOpts{
		Namespace: namespace,
		Name:      "vtx_requests",
		Help:      "Number of outstanding vertex requests",
	})
	m.numPendingVts = prometheus.NewGauge(prometheus.GaugeOpts{
		Namespace: namespace,
		Name:      "pending_vts",
		Help:      "Number of pending vertices",
	})
	m.numMissingTxs = prometheus.NewGauge(prometheus.GaugeOpts{
		Namespace: namespace,
		Name:      "missing_txs",
		Help:      "Number of missing transactions",
	})
	m.pendingTxs = prometheus.NewGauge(prometheus.GaugeOpts{
		Namespace: namespace,
		Name:      "pending_txs",
		Help:      "Number of transactions from the VM waiting to be issued",
	})
	m.blockerVtxs = prometheus.NewGauge(prometheus.GaugeOpts{
		Namespace: namespace,
		Name:      "blocker_vtxs",
		Help:      "Number of vertices that are blocking other vertices from being issued because they haven't been issued",
	})
	m.blockerTxs = prometheus.NewGauge(prometheus.GaugeOpts{
		Namespace: namespace,
		Name:      "blocker_txs",
		Help:      "Number of transactions that are blocking other transactions from being issued because they haven't been issued",
	})
	m.whitelistVtxIssueSuccess = prometheus.NewCounter(prometheus.CounterOpts{
		Namespace: namespace,
		Name:      "whitelist_vtx_issue_success",
		Help:      "Number of DAG linearization request issued (pending, not necessarily accepted)",
	})
	m.whitelistVtxIssueFailure = prometheus.NewCounter(prometheus.CounterOpts{
		Namespace: namespace,
		Name:      "whitelist_vtx_issue_failure",
		Help:      "Number of DAG linearization request issue failed (verification failure)",
	})
	m.numUselessPutBytes = prometheus.NewCounter(prometheus.CounterOpts{
		Namespace: namespace,
		Name:      "num_useless_put_bytes",
		Help:      "Amount of useless bytes received in Put messages",
	})
	m.numUselessPushQueryBytes = prometheus.NewCounter(prometheus.CounterOpts{
		Namespace: namespace,
		Name:      "num_useless_push_query_bytes",
		Help:      "Amount of useless bytes received in PushQuery messages",
	})

	errs.Add(
		reg.Register(m.bootstrapFinished),
		reg.Register(m.numVtxRequests),
		reg.Register(m.numPendingVts),
		reg.Register(m.numMissingTxs),
		reg.Register(m.pendingTxs),
		reg.Register(m.blockerVtxs),
		reg.Register(m.blockerTxs),
		reg.Register(m.whitelistVtxIssueSuccess),
		reg.Register(m.whitelistVtxIssueFailure),
		reg.Register(m.numUselessPutBytes),
		reg.Register(m.numUselessPushQueryBytes),
	)
	return errs.Err
}

```

avalanchego/snow/engine/avalanche/mocks/engine.go:
```
// Code generated by mockery v2.12.1. DO NOT EDIT.

package mocks

import (
	consensusavalanche "github.com/ava-labs/avalanchego/snow/consensus/avalanche"
	common "github.com/ava-labs/avalanchego/snow/engine/common"

	ids "github.com/ava-labs/avalanchego/ids"

	mock "github.com/stretchr/testify/mock"

	snow "github.com/ava-labs/avalanchego/snow"

	testing "testing"

	time "time"

	version "github.com/ava-labs/avalanchego/version"
)

// Engine is an autogenerated mock type for the Engine type
type Engine struct {
	mock.Mock
}

// Accepted provides a mock function with given fields: validatorID, requestID, containerIDs
func (_m *Engine) Accepted(validatorID ids.NodeID, requestID uint32, containerIDs []ids.ID) error {
	ret := _m.Called(validatorID, requestID, containerIDs)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32, []ids.ID) error); ok {
		r0 = rf(validatorID, requestID, containerIDs)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// AcceptedFrontier provides a mock function with given fields: validatorID, requestID, containerIDs
func (_m *Engine) AcceptedFrontier(validatorID ids.NodeID, requestID uint32, containerIDs []ids.ID) error {
	ret := _m.Called(validatorID, requestID, containerIDs)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32, []ids.ID) error); ok {
		r0 = rf(validatorID, requestID, containerIDs)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// AcceptedStateSummary provides a mock function with given fields: validatorID, requestID, summaryIDs
func (_m *Engine) AcceptedStateSummary(validatorID ids.NodeID, requestID uint32, summaryIDs []ids.ID) error {
	ret := _m.Called(validatorID, requestID, summaryIDs)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32, []ids.ID) error); ok {
		r0 = rf(validatorID, requestID, summaryIDs)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// Ancestors provides a mock function with given fields: validatorID, requestID, containers
func (_m *Engine) Ancestors(validatorID ids.NodeID, requestID uint32, containers [][]byte) error {
	ret := _m.Called(validatorID, requestID, containers)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32, [][]byte) error); ok {
		r0 = rf(validatorID, requestID, containers)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// AppGossip provides a mock function with given fields: nodeID, msg
func (_m *Engine) AppGossip(nodeID ids.NodeID, msg []byte) error {
	ret := _m.Called(nodeID, msg)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, []byte) error); ok {
		r0 = rf(nodeID, msg)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// AppRequest provides a mock function with given fields: nodeID, requestID, deadline, request
func (_m *Engine) AppRequest(nodeID ids.NodeID, requestID uint32, deadline time.Time, request []byte) error {
	ret := _m.Called(nodeID, requestID, deadline, request)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32, time.Time, []byte) error); ok {
		r0 = rf(nodeID, requestID, deadline, request)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// AppRequestFailed provides a mock function with given fields: nodeID, requestID
func (_m *Engine) AppRequestFailed(nodeID ids.NodeID, requestID uint32) error {
	ret := _m.Called(nodeID, requestID)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32) error); ok {
		r0 = rf(nodeID, requestID)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// AppResponse provides a mock function with given fields: nodeID, requestID, response
func (_m *Engine) AppResponse(nodeID ids.NodeID, requestID uint32, response []byte) error {
	ret := _m.Called(nodeID, requestID, response)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32, []byte) error); ok {
		r0 = rf(nodeID, requestID, response)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// Chits provides a mock function with given fields: validatorID, requestID, containerIDs
func (_m *Engine) Chits(validatorID ids.NodeID, requestID uint32, containerIDs []ids.ID) error {
	ret := _m.Called(validatorID, requestID, containerIDs)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32, []ids.ID) error); ok {
		r0 = rf(validatorID, requestID, containerIDs)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// Connected provides a mock function with given fields: id, nodeVersion
func (_m *Engine) Connected(id ids.NodeID, nodeVersion *version.Application) error {
	ret := _m.Called(id, nodeVersion)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, *version.Application) error); ok {
		r0 = rf(id, nodeVersion)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// Context provides a mock function with given fields:
func (_m *Engine) Context() *snow.ConsensusContext {
	ret := _m.Called()

	var r0 *snow.ConsensusContext
	if rf, ok := ret.Get(0).(func() *snow.ConsensusContext); ok {
		r0 = rf()
	} else {
		if ret.Get(0) != nil {
			r0 = ret.Get(0).(*snow.ConsensusContext)
		}
	}

	return r0
}

// Disconnected provides a mock function with given fields: id
func (_m *Engine) Disconnected(id ids.NodeID) error {
	ret := _m.Called(id)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID) error); ok {
		r0 = rf(id)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// Get provides a mock function with given fields: validatorID, requestID, containerID
func (_m *Engine) Get(validatorID ids.NodeID, requestID uint32, containerID ids.ID) error {
	ret := _m.Called(validatorID, requestID, containerID)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32, ids.ID) error); ok {
		r0 = rf(validatorID, requestID, containerID)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// GetAccepted provides a mock function with given fields: validatorID, requestID, containerIDs
func (_m *Engine) GetAccepted(validatorID ids.NodeID, requestID uint32, containerIDs []ids.ID) error {
	ret := _m.Called(validatorID, requestID, containerIDs)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32, []ids.ID) error); ok {
		r0 = rf(validatorID, requestID, containerIDs)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// GetAcceptedFailed provides a mock function with given fields: validatorID, requestID
func (_m *Engine) GetAcceptedFailed(validatorID ids.NodeID, requestID uint32) error {
	ret := _m.Called(validatorID, requestID)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32) error); ok {
		r0 = rf(validatorID, requestID)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// GetAcceptedFrontier provides a mock function with given fields: validatorID, requestID
func (_m *Engine) GetAcceptedFrontier(validatorID ids.NodeID, requestID uint32) error {
	ret := _m.Called(validatorID, requestID)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32) error); ok {
		r0 = rf(validatorID, requestID)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// GetAcceptedFrontierFailed provides a mock function with given fields: validatorID, requestID
func (_m *Engine) GetAcceptedFrontierFailed(validatorID ids.NodeID, requestID uint32) error {
	ret := _m.Called(validatorID, requestID)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32) error); ok {
		r0 = rf(validatorID, requestID)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// GetAcceptedStateSummary provides a mock function with given fields: validatorID, requestID, keys
func (_m *Engine) GetAcceptedStateSummary(validatorID ids.NodeID, requestID uint32, keys []uint64) error {
	ret := _m.Called(validatorID, requestID, keys)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32, []uint64) error); ok {
		r0 = rf(validatorID, requestID, keys)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// GetAcceptedStateSummaryFailed provides a mock function with given fields: validatorID, requestID
func (_m *Engine) GetAcceptedStateSummaryFailed(validatorID ids.NodeID, requestID uint32) error {
	ret := _m.Called(validatorID, requestID)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32) error); ok {
		r0 = rf(validatorID, requestID)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// GetAncestors provides a mock function with given fields: validatorID, requestID, containerID
func (_m *Engine) GetAncestors(validatorID ids.NodeID, requestID uint32, containerID ids.ID) error {
	ret := _m.Called(validatorID, requestID, containerID)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32, ids.ID) error); ok {
		r0 = rf(validatorID, requestID, containerID)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// GetAncestorsFailed provides a mock function with given fields: validatorID, requestID
func (_m *Engine) GetAncestorsFailed(validatorID ids.NodeID, requestID uint32) error {
	ret := _m.Called(validatorID, requestID)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32) error); ok {
		r0 = rf(validatorID, requestID)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// GetFailed provides a mock function with given fields: validatorID, requestID
func (_m *Engine) GetFailed(validatorID ids.NodeID, requestID uint32) error {
	ret := _m.Called(validatorID, requestID)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32) error); ok {
		r0 = rf(validatorID, requestID)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// GetStateSummaryFrontier provides a mock function with given fields: validatorID, requestID
func (_m *Engine) GetStateSummaryFrontier(validatorID ids.NodeID, requestID uint32) error {
	ret := _m.Called(validatorID, requestID)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32) error); ok {
		r0 = rf(validatorID, requestID)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// GetStateSummaryFrontierFailed provides a mock function with given fields: validatorID, requestID
func (_m *Engine) GetStateSummaryFrontierFailed(validatorID ids.NodeID, requestID uint32) error {
	ret := _m.Called(validatorID, requestID)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32) error); ok {
		r0 = rf(validatorID, requestID)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// GetVM provides a mock function with given fields:
func (_m *Engine) GetVM() common.VM {
	ret := _m.Called()

	var r0 common.VM
	if rf, ok := ret.Get(0).(func() common.VM); ok {
		r0 = rf()
	} else {
		if ret.Get(0) != nil {
			r0 = ret.Get(0).(common.VM)
		}
	}

	return r0
}

// GetVtx provides a mock function with given fields: vtxID
func (_m *Engine) GetVtx(vtxID ids.ID) (consensusavalanche.Vertex, error) {
	ret := _m.Called(vtxID)

	var r0 consensusavalanche.Vertex
	if rf, ok := ret.Get(0).(func(ids.ID) consensusavalanche.Vertex); ok {
		r0 = rf(vtxID)
	} else {
		if ret.Get(0) != nil {
			r0 = ret.Get(0).(consensusavalanche.Vertex)
		}
	}

	var r1 error
	if rf, ok := ret.Get(1).(func(ids.ID) error); ok {
		r1 = rf(vtxID)
	} else {
		r1 = ret.Error(1)
	}

	return r0, r1
}

// Gossip provides a mock function with given fields:
func (_m *Engine) Gossip() error {
	ret := _m.Called()

	var r0 error
	if rf, ok := ret.Get(0).(func() error); ok {
		r0 = rf()
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// Halt provides a mock function with given fields:
func (_m *Engine) Halt() {
	_m.Called()
}

// HealthCheck provides a mock function with given fields:
func (_m *Engine) HealthCheck() (interface{}, error) {
	ret := _m.Called()

	var r0 interface{}
	if rf, ok := ret.Get(0).(func() interface{}); ok {
		r0 = rf()
	} else {
		if ret.Get(0) != nil {
			r0 = ret.Get(0).(interface{})
		}
	}

	var r1 error
	if rf, ok := ret.Get(1).(func() error); ok {
		r1 = rf()
	} else {
		r1 = ret.Error(1)
	}

	return r0, r1
}

// Notify provides a mock function with given fields: _a0
func (_m *Engine) Notify(_a0 common.Message) error {
	ret := _m.Called(_a0)

	var r0 error
	if rf, ok := ret.Get(0).(func(common.Message) error); ok {
		r0 = rf(_a0)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// PullQuery provides a mock function with given fields: validatorID, requestID, containerID
func (_m *Engine) PullQuery(validatorID ids.NodeID, requestID uint32, containerID ids.ID) error {
	ret := _m.Called(validatorID, requestID, containerID)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32, ids.ID) error); ok {
		r0 = rf(validatorID, requestID, containerID)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// PushQuery provides a mock function with given fields: validatorID, requestID, container
func (_m *Engine) PushQuery(validatorID ids.NodeID, requestID uint32, container []byte) error {
	ret := _m.Called(validatorID, requestID, container)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32, []byte) error); ok {
		r0 = rf(validatorID, requestID, container)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// Put provides a mock function with given fields: validatorID, requestID, container
func (_m *Engine) Put(validatorID ids.NodeID, requestID uint32, container []byte) error {
	ret := _m.Called(validatorID, requestID, container)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32, []byte) error); ok {
		r0 = rf(validatorID, requestID, container)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// QueryFailed provides a mock function with given fields: validatorID, requestID
func (_m *Engine) QueryFailed(validatorID ids.NodeID, requestID uint32) error {
	ret := _m.Called(validatorID, requestID)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32) error); ok {
		r0 = rf(validatorID, requestID)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// Shutdown provides a mock function with given fields:
func (_m *Engine) Shutdown() error {
	ret := _m.Called()

	var r0 error
	if rf, ok := ret.Get(0).(func() error); ok {
		r0 = rf()
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// Start provides a mock function with given fields: startReqID
func (_m *Engine) Start(startReqID uint32) error {
	ret := _m.Called(startReqID)

	var r0 error
	if rf, ok := ret.Get(0).(func(uint32) error); ok {
		r0 = rf(startReqID)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// StateSummaryFrontier provides a mock function with given fields: validatorID, requestID, summary
func (_m *Engine) StateSummaryFrontier(validatorID ids.NodeID, requestID uint32, summary []byte) error {
	ret := _m.Called(validatorID, requestID, summary)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32, []byte) error); ok {
		r0 = rf(validatorID, requestID, summary)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// Timeout provides a mock function with given fields:
func (_m *Engine) Timeout() error {
	ret := _m.Called()

	var r0 error
	if rf, ok := ret.Get(0).(func() error); ok {
		r0 = rf()
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// NewEngine creates a new instance of Engine. It also registers the testing.TB interface on the mock and a cleanup function to assert the mocks expectations.
func NewEngine(t testing.TB) *Engine {
	mock := &Engine{}
	mock.Mock.Test(t)

	t.Cleanup(func() { mock.AssertExpectations(t) })

	return mock
}

```

avalanchego/snow/engine/avalanche/state/prefixed_state.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package state

import (
	"github.com/ava-labs/avalanchego/cache"
	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/choices"
	"github.com/ava-labs/avalanchego/snow/engine/avalanche/vertex"
)

const (
	vtxID uint64 = iota
	vtxStatusID
	edgeID
)

var uniqueEdgeID = ids.Empty.Prefix(edgeID)

type prefixedState struct {
	state *state

	vtx, status cache.Cacher
	uniqueVtx   cache.Deduplicator
}

func newPrefixedState(state *state, idCacheSizes int) *prefixedState {
	return &prefixedState{
		state:     state,
		vtx:       &cache.LRU{Size: idCacheSizes},
		status:    &cache.LRU{Size: idCacheSizes},
		uniqueVtx: &cache.EvictableLRU{Size: idCacheSizes},
	}
}

func (s *prefixedState) UniqueVertex(vtx *uniqueVertex) *uniqueVertex {
	return s.uniqueVtx.Deduplicate(vtx).(*uniqueVertex)
}

func (s *prefixedState) Vertex(id ids.ID) vertex.StatelessVertex {
	var vID ids.ID
	if cachedVtxIDIntf, found := s.vtx.Get(id); found {
		vID = cachedVtxIDIntf.(ids.ID)
	} else {
		vID = id.Prefix(vtxID)
		s.vtx.Put(id, vID)
	}

	return s.state.Vertex(vID)
}

func (s *prefixedState) SetVertex(vtx vertex.StatelessVertex) error {
	rawVertexID := vtx.ID()
	var vID ids.ID
	if cachedVtxIDIntf, found := s.vtx.Get(rawVertexID); found {
		vID = cachedVtxIDIntf.(ids.ID)
	} else {
		vID = rawVertexID.Prefix(vtxID)
		s.vtx.Put(rawVertexID, vID)
	}

	return s.state.SetVertex(vID, vtx)
}

func (s *prefixedState) Status(id ids.ID) choices.Status {
	var sID ids.ID
	if cachedStatusIDIntf, found := s.status.Get(id); found {
		sID = cachedStatusIDIntf.(ids.ID)
	} else {
		sID = id.Prefix(vtxStatusID)
		s.status.Put(id, sID)
	}

	return s.state.Status(sID)
}

func (s *prefixedState) SetStatus(id ids.ID, status choices.Status) error {
	var sID ids.ID
	if cachedStatusIDIntf, found := s.status.Get(id); found {
		sID = cachedStatusIDIntf.(ids.ID)
	} else {
		sID = id.Prefix(vtxStatusID)
		s.status.Put(id, sID)
	}

	return s.state.SetStatus(sID, status)
}

func (s *prefixedState) Edge() []ids.ID { return s.state.Edge(uniqueEdgeID) }

func (s *prefixedState) SetEdge(frontier []ids.ID) error {
	return s.state.SetEdge(uniqueEdgeID, frontier)
}

```

avalanchego/snow/engine/avalanche/state/serializer.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

// Package state manages the meta-data required by consensus for an avalanche
// dag.
package state

import (
	"errors"
	"time"

	"github.com/ava-labs/avalanchego/cache"
	"github.com/ava-labs/avalanchego/database"
	"github.com/ava-labs/avalanchego/database/versiondb"
	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/choices"
	"github.com/ava-labs/avalanchego/snow/consensus/avalanche"
	"github.com/ava-labs/avalanchego/snow/consensus/snowstorm"
	"github.com/ava-labs/avalanchego/snow/engine/avalanche/vertex"
	"github.com/ava-labs/avalanchego/utils/logging"
	"github.com/ava-labs/avalanchego/utils/math"
)

const (
	dbCacheSize = 10000
	idCacheSize = 1000
)

var (
	errUnknownVertex = errors.New("unknown vertex")
	errWrongChainID  = errors.New("wrong ChainID in vertex")
)

var _ vertex.Manager = &Serializer{}

// Serializer manages the state of multiple vertices
type Serializer struct {
	SerializerConfig
	versionDB *versiondb.Database
	state     *prefixedState
	edge      ids.Set
}

type SerializerConfig struct {
	ChainID             ids.ID
	VM                  vertex.DAGVM
	DB                  database.Database
	Log                 logging.Logger
	XChainMigrationTime time.Time
}

func NewSerializer(config SerializerConfig) vertex.Manager {
	versionDB := versiondb.New(config.DB)
	dbCache := &cache.LRU{Size: dbCacheSize}
	s := Serializer{
		SerializerConfig: config,
		versionDB:        versionDB,
	}

	rawState := &state{
		serializer: &s,
		log:        config.Log,
		dbCache:    dbCache,
		db:         versionDB,
	}

	s.state = newPrefixedState(rawState, idCacheSize)
	s.edge.Add(s.state.Edge()...)

	return &s
}

func (s *Serializer) ParseVtx(b []byte) (avalanche.Vertex, error) {
	return newUniqueVertex(s, b)
}

func (s *Serializer) BuildVtx(parentIDs []ids.ID, txs []snowstorm.Tx) (avalanche.Vertex, error) {
	return s.buildVtx(parentIDs, txs, false)
}

func (s *Serializer) BuildStopVtx(parentIDs []ids.ID) (avalanche.Vertex, error) {
	return s.buildVtx(parentIDs, nil, true)
}

func (s *Serializer) buildVtx(
	parentIDs []ids.ID,
	txs []snowstorm.Tx,
	stopVtx bool,
) (avalanche.Vertex, error) {
	height := uint64(0)
	for _, parentID := range parentIDs {
		parent, err := s.getUniqueVertex(parentID)
		if err != nil {
			return nil, err
		}
		parentHeight := parent.v.vtx.Height()
		childHeight, err := math.Add64(parentHeight, 1)
		if err != nil {
			return nil, err
		}
		height = math.Max64(height, childHeight)
	}

	var (
		vtx vertex.StatelessVertex
		err error
	)
	if !stopVtx {
		txBytes := make([][]byte, len(txs))
		for i, tx := range txs {
			txBytes[i] = tx.Bytes()
		}
		vtx, err = vertex.Build(
			s.ChainID,
			height,
			parentIDs,
			txBytes,
		)
	} else {
		vtx, err = vertex.BuildStopVertex(
			s.ChainID,
			height,
			parentIDs,
		)
	}
	if err != nil {
		return nil, err
	}

	uVtx := &uniqueVertex{
		serializer: s,
		id:         vtx.ID(),
	}
	// setVertex handles the case where this vertex already exists even
	// though we just made it
	return uVtx, uVtx.setVertex(vtx)
}

func (s *Serializer) GetVtx(vtxID ids.ID) (avalanche.Vertex, error) {
	return s.getUniqueVertex(vtxID)
}

func (s *Serializer) Edge() []ids.ID { return s.edge.List() }

func (s *Serializer) parseVertex(b []byte) (vertex.StatelessVertex, error) {
	vtx, err := vertex.Parse(b)
	if err != nil {
		return nil, err
	}
	if vtx.ChainID() != s.ChainID {
		return nil, errWrongChainID
	}
	return vtx, nil
}

func (s *Serializer) getUniqueVertex(vtxID ids.ID) (*uniqueVertex, error) {
	vtx := &uniqueVertex{
		serializer: s,
		id:         vtxID,
	}
	if vtx.Status() == choices.Unknown {
		return nil, errUnknownVertex
	}
	return vtx, nil
}

func (s *Serializer) StopVertexAccepted() (bool, error) {
	edge := s.Edge()
	if len(edge) != 1 {
		return false, nil
	}

	vtx, err := s.getUniqueVertex(edge[0])
	if err != nil {
		return false, err
	}

	return vtx.v.vtx.StopVertex(), nil
}

```

avalanchego/snow/engine/avalanche/state/state.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package state

import (
	"go.uber.org/zap"

	"github.com/ava-labs/avalanchego/cache"
	"github.com/ava-labs/avalanchego/database"
	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/choices"
	"github.com/ava-labs/avalanchego/snow/engine/avalanche/vertex"
	"github.com/ava-labs/avalanchego/utils/hashing"
	"github.com/ava-labs/avalanchego/utils/logging"
	"github.com/ava-labs/avalanchego/utils/wrappers"
)

type state struct {
	serializer *Serializer
	log        logging.Logger

	dbCache cache.Cacher
	db      database.Database
}

// Vertex retrieves the vertex with the given id from cache/disk.
// Returns nil if it's not found.
// TODO this should return an error
func (s *state) Vertex(id ids.ID) vertex.StatelessVertex {
	var (
		vtx   vertex.StatelessVertex
		bytes []byte
		err   error
	)

	if vtxIntf, found := s.dbCache.Get(id); found {
		vtx, _ = vtxIntf.(vertex.StatelessVertex)
		return vtx
	}

	if bytes, err = s.db.Get(id[:]); err != nil {
		s.log.Verbo("failed to get vertex from database",
			zap.Binary("key", id[:]),
			zap.Error(err),
		)
		s.dbCache.Put(id, nil)
		return nil
	}

	if vtx, err = s.serializer.parseVertex(bytes); err != nil {
		s.log.Error("failed parsing saved vertex",
			zap.Binary("key", id[:]),
			zap.Binary("vertex", bytes),
			zap.Error(err),
		)
		s.dbCache.Put(id, nil)
		return nil
	}

	s.dbCache.Put(id, vtx)
	return vtx
}

// SetVertex persists the vertex to the database and returns an error if it
// fails to write to the db
func (s *state) SetVertex(id ids.ID, vtx vertex.StatelessVertex) error {
	s.dbCache.Put(id, vtx)

	if vtx == nil {
		return s.db.Delete(id[:])
	}

	return s.db.Put(id[:], vtx.Bytes())
}

func (s *state) Status(id ids.ID) choices.Status {
	if statusIntf, found := s.dbCache.Get(id); found {
		status, _ := statusIntf.(choices.Status)
		return status
	}

	if val, err := database.GetUInt32(s.db, id[:]); err == nil {
		// The key was in the database
		status := choices.Status(val)
		s.dbCache.Put(id, status)
		return status
	}

	s.dbCache.Put(id, choices.Unknown)
	return choices.Unknown
}

// SetStatus sets the status of the vertex and returns an error if it fails to write to the db
func (s *state) SetStatus(id ids.ID, status choices.Status) error {
	s.dbCache.Put(id, status)

	if status == choices.Unknown {
		return s.db.Delete(id[:])
	}
	return database.PutUInt32(s.db, id[:], uint32(status))
}

func (s *state) Edge(id ids.ID) []ids.ID {
	if frontierIntf, found := s.dbCache.Get(id); found {
		frontier, _ := frontierIntf.([]ids.ID)
		return frontier
	}

	if b, err := s.db.Get(id[:]); err == nil {
		p := wrappers.Packer{Bytes: b}

		frontierSize := p.UnpackInt()
		frontier := make([]ids.ID, frontierSize)
		for i := 0; i < int(frontierSize) && !p.Errored(); i++ {
			id, err := ids.ToID(p.UnpackFixedBytes(hashing.HashLen))
			p.Add(err)
			frontier[i] = id
		}

		if p.Offset == len(b) && !p.Errored() {
			s.dbCache.Put(id, frontier)
			return frontier
		}
		s.log.Error("failed parsing saved edge",
			zap.Binary("key", id[:]),
			zap.Binary("edge", b),
			zap.Error(err),
		)
	}

	s.dbCache.Put(id, nil) // Cache the miss
	return nil
}

// SetEdge sets the frontier and returns an error if it fails to write to the db
func (s *state) SetEdge(id ids.ID, frontier []ids.ID) error {
	s.dbCache.Put(id, frontier)

	if len(frontier) == 0 {
		return s.db.Delete(id[:])
	}

	size := wrappers.IntLen + hashing.HashLen*len(frontier)
	p := wrappers.Packer{Bytes: make([]byte, size)}
	p.PackInt(uint32(len(frontier)))
	for _, id := range frontier {
		p.PackFixedBytes(id[:])
	}

	return s.db.Put(id[:], p.Bytes)
}

```

avalanchego/snow/engine/avalanche/state/unique_vertex.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package state

import (
	"errors"
	"fmt"
	"strings"
	"time"

	"github.com/ava-labs/avalanchego/cache"
	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/choices"
	"github.com/ava-labs/avalanchego/snow/consensus/avalanche"
	"github.com/ava-labs/avalanchego/snow/consensus/snowstorm"
	"github.com/ava-labs/avalanchego/snow/engine/avalanche/vertex"
	"github.com/ava-labs/avalanchego/utils/formatting"
	"github.com/ava-labs/avalanchego/utils/hashing"
)

var (
	_ cache.Evictable  = &uniqueVertex{}
	_ avalanche.Vertex = &uniqueVertex{}
)

// uniqueVertex acts as a cache for vertices in the database.
//
// If a vertex is loaded, it will have one canonical uniqueVertex. The vertex
// will eventually be evicted from memory, when the uniqueVertex is evicted from
// the cache. If the uniqueVertex has a function called again after this
// eviction, the vertex will be re-loaded from the database.
type uniqueVertex struct {
	serializer *Serializer

	id ids.ID
	v  *vertexState
	// default to "time.Now", used for testing
	time func() time.Time
}

// newUniqueVertex returns a uniqueVertex instance from [b] by checking the cache
// and then parsing the vertex bytes on a cache miss.
func newUniqueVertex(s *Serializer, b []byte) (*uniqueVertex, error) {
	vtx := &uniqueVertex{
		id:         hashing.ComputeHash256Array(b),
		serializer: s,
	}
	vtx.shallowRefresh()

	// If the vtx exists, then the vertex is already known
	if vtx.v.vtx != nil {
		return vtx, nil
	}

	// If it wasn't in the cache parse the vertex and set it
	innerVertex, err := s.parseVertex(b)
	if err != nil {
		return nil, err
	}
	if err := innerVertex.Verify(); err != nil {
		return nil, err
	}

	unparsedTxs := innerVertex.Txs()
	txs := make([]snowstorm.Tx, len(unparsedTxs))
	for i, txBytes := range unparsedTxs {
		tx, err := vtx.serializer.VM.ParseTx(txBytes)
		if err != nil {
			return nil, err
		}
		txs[i] = tx
	}

	vtx.v.vtx = innerVertex
	vtx.v.txs = txs

	// If the vertex has already been fetched,
	// skip persisting the vertex.
	if vtx.v.status.Fetched() {
		return vtx, nil
	}

	// The vertex is newly parsed, so set the status
	// and persist it.
	vtx.v.status = choices.Processing
	return vtx, vtx.persist()
}

func (vtx *uniqueVertex) refresh() {
	vtx.shallowRefresh()

	if vtx.v.vtx == nil && vtx.v.status.Fetched() {
		vtx.v.vtx = vtx.serializer.state.Vertex(vtx.ID())
	}
}

// shallowRefresh checks the cache for the uniqueVertex and gets the
// most up-to-date status for [vtx]
// ensures that the status is up-to-date for this vertex
// inner vertex may be nil after calling shallowRefresh
func (vtx *uniqueVertex) shallowRefresh() {
	if vtx.v == nil {
		vtx.v = &vertexState{}
	}
	if vtx.v.latest {
		return
	}

	latest := vtx.serializer.state.UniqueVertex(vtx)
	prevVtx := vtx.v.vtx
	if latest == vtx {
		vtx.v.status = vtx.serializer.state.Status(vtx.ID())
		vtx.v.latest = true
	} else {
		// If someone is in the cache, they must be up-to-date
		*vtx = *latest
	}

	if vtx.v.vtx == nil {
		vtx.v.vtx = prevVtx
	}
}

func (vtx *uniqueVertex) Evict() {
	if vtx.v != nil {
		vtx.v.latest = false
		// make sure the parents can be garbage collected
		vtx.v.parents = nil
	}
}

func (vtx *uniqueVertex) setVertex(innerVtx vertex.StatelessVertex) error {
	vtx.shallowRefresh()
	vtx.v.vtx = innerVtx

	if vtx.v.status.Fetched() {
		return nil
	}

	if _, err := vtx.Txs(); err != nil {
		return err
	}

	vtx.v.status = choices.Processing
	return vtx.persist()
}

func (vtx *uniqueVertex) persist() error {
	if err := vtx.serializer.state.SetVertex(vtx.v.vtx); err != nil {
		return err
	}
	if err := vtx.serializer.state.SetStatus(vtx.ID(), vtx.v.status); err != nil {
		return err
	}
	return vtx.serializer.versionDB.Commit()
}

func (vtx *uniqueVertex) setStatus(status choices.Status) error {
	vtx.shallowRefresh()
	if vtx.v.status == status {
		return nil
	}
	vtx.v.status = status
	return vtx.serializer.state.SetStatus(vtx.ID(), status)
}

func (vtx *uniqueVertex) ID() ids.ID       { return vtx.id }
func (vtx *uniqueVertex) Key() interface{} { return vtx.id }

func (vtx *uniqueVertex) Accept() error {
	if err := vtx.setStatus(choices.Accepted); err != nil {
		return err
	}

	vtx.serializer.edge.Add(vtx.id)
	parents, err := vtx.Parents()
	if err != nil {
		return err
	}

	for _, parent := range parents {
		vtx.serializer.edge.Remove(parent.ID())
	}

	if err := vtx.serializer.state.SetEdge(vtx.serializer.Edge()); err != nil {
		return fmt.Errorf("failed to set edge while accepting vertex %s due to %w", vtx.id, err)
	}

	// Should never traverse into parents of a decided vertex. Allows for the
	// parents to be garbage collected
	vtx.v.parents = nil

	return vtx.serializer.versionDB.Commit()
}

func (vtx *uniqueVertex) Reject() error {
	if err := vtx.setStatus(choices.Rejected); err != nil {
		return err
	}

	// Should never traverse into parents of a decided vertex. Allows for the
	// parents to be garbage collected
	vtx.v.parents = nil

	return vtx.serializer.versionDB.Commit()
}

// TODO: run performance test to see if shallow refreshing
// (which will mean that refresh must be called in Bytes and Verify)
// improves performance
func (vtx *uniqueVertex) Status() choices.Status { vtx.refresh(); return vtx.v.status }

func (vtx *uniqueVertex) Parents() ([]avalanche.Vertex, error) {
	vtx.refresh()

	if vtx.v.vtx == nil {
		return nil, fmt.Errorf("failed to get parents for vertex with status: %s", vtx.v.status)
	}

	parentIDs := vtx.v.vtx.ParentIDs()
	if len(vtx.v.parents) != len(parentIDs) {
		vtx.v.parents = make([]avalanche.Vertex, len(parentIDs))
		for i, parentID := range parentIDs {
			vtx.v.parents[i] = &uniqueVertex{
				serializer: vtx.serializer,
				id:         parentID,
			}
		}
	}

	return vtx.v.parents, nil
}

var (
	errStopVertexNotAllowedTimestamp = errors.New("stop vertex not allowed timestamp")
	errStopVertexAlreadyAccepted     = errors.New("stop vertex already accepted")
	errUnexpectedEdges               = errors.New("unexpected edge, expected accepted frontier")
	errUnexpectedDependencyStopVtx   = errors.New("unexpected dependencies found in stop vertex transitive path")
)

// "uniqueVertex" itself implements "Verify" regardless of whether the underlying vertex
// is stop vertex or not. Called before issuing the vertex to the consensus.
// No vertex should ever be able to refer to a stop vertex in its transitive closure.
func (vtx *uniqueVertex) Verify() error {
	// first verify the underlying stateless vertex
	if err := vtx.v.vtx.Verify(); err != nil {
		return err
	}

	whitelistVtx := vtx.v.vtx.StopVertex()
	if whitelistVtx {
		now := time.Now()
		if vtx.time != nil {
			now = vtx.time()
		}
		allowed := vtx.serializer.XChainMigrationTime
		if now.Before(allowed) {
			return errStopVertexNotAllowedTimestamp
		}
	}

	// MUST error if stop vertex has already been accepted (can't be accepted twice)
	// regardless of whether the underlying vertex is stop vertex or not
	stopVtxAccepted, err := vtx.serializer.StopVertexAccepted()
	if err != nil {
		return err
	}
	if stopVtxAccepted {
		return errStopVertexAlreadyAccepted
	}
	if !whitelistVtx {
		// below are stop vertex specific verifications
		// no need to continue
		return nil
	}

	//      (accepted)           (accepted)
	//        vtx_1                vtx_2
	//    [tx_a, tx_b]          [tx_c, tx_d]
	//          ⬆      ⬉     ⬈       ⬆
	//        vtx_3                vtx_4
	//    [tx_e, tx_f]          [tx_g, tx_h]
	//                               ⬆
	//                         stop_vertex_5
	//
	// [tx_a, tx_b] transitively referenced by "stop_vertex_5"
	// has the dependent transactions [tx_e, tx_f]
	// that are not transitively referenced by "stop_vertex_5"
	// in case "tx_g" depends on "tx_e" that is not in vtx4.
	// Thus "stop_vertex_5" is invalid!
	//
	// To make sure such transitive paths of the stop vertex reach all accepted frontier:
	// 1. check the edge of the transitive paths refers to the accepted frontier
	// 2. check dependencies of all txs must be subset of transitive paths
	queue := []avalanche.Vertex{vtx}
	visitedVtx := ids.NewSet(0)

	acceptedFrontier := ids.NewSet(0)
	transitivePaths := ids.NewSet(0)
	dependencies := ids.NewSet(0)
	for len(queue) > 0 { // perform BFS
		cur := queue[0]
		queue = queue[1:]

		curID := cur.ID()
		if cur.Status() == choices.Accepted {
			// 1. check the edge of the transitive paths refers to the accepted frontier
			acceptedFrontier.Add(curID)

			// have reached the accepted frontier on the transitive closure
			// no need to continue the search on this path
			continue
		}

		if visitedVtx.Contains(curID) {
			continue
		}
		visitedVtx.Add(curID)
		transitivePaths.Add(curID)

		txs, err := cur.Txs()
		if err != nil {
			return err
		}
		for _, tx := range txs {
			transitivePaths.Add(tx.ID())
			deps, err := tx.Dependencies()
			if err != nil {
				return err
			}
			for _, dep := range deps {
				// only add non-accepted dependencies
				if dep.Status() != choices.Accepted {
					dependencies.Add(dep.ID())
				}
			}
		}

		parents, err := cur.Parents()
		if err != nil {
			return err
		}
		queue = append(queue, parents...)
	}

	acceptedEdges := ids.NewSet(0)
	acceptedEdges.Add(vtx.serializer.Edge()...)

	// stop vertex should be able to reach all IDs
	// that are returned by the "Edge"
	if !acceptedFrontier.Equals(acceptedEdges) {
		return errUnexpectedEdges
	}

	// 2. check dependencies of all txs must be subset of transitive paths
	prev := transitivePaths.Len()
	transitivePaths.Union(dependencies)
	if prev != transitivePaths.Len() {
		return errUnexpectedDependencyStopVtx
	}

	return nil
}

func (vtx *uniqueVertex) HasWhitelist() bool {
	return vtx.v.vtx.StopVertex()
}

// "uniqueVertex" itself implements "Whitelist" traversal iff its underlying
// "vertex.StatelessVertex" is marked as a stop vertex.
func (vtx *uniqueVertex) Whitelist() (ids.Set, error) {
	if !vtx.v.vtx.StopVertex() {
		return nil, nil
	}

	// perform BFS on transitive paths until reaching the accepted frontier
	// represents all processing transaction IDs transitively referenced by the
	// vertex
	queue := []avalanche.Vertex{vtx}
	whitlist := ids.NewSet(0)
	visitedVtx := ids.NewSet(0)
	for len(queue) > 0 {
		cur := queue[0]
		queue = queue[1:]

		if cur.Status() == choices.Accepted {
			// have reached the accepted frontier on the transitive closure
			// no need to continue the search on this path
			continue
		}
		curID := cur.ID()
		if visitedVtx.Contains(curID) {
			continue
		}
		visitedVtx.Add(curID)

		txs, err := cur.Txs()
		if err != nil {
			return nil, err
		}
		for _, tx := range txs {
			whitlist.Add(tx.ID())
		}
		whitlist.Add(curID)

		parents, err := cur.Parents()
		if err != nil {
			return nil, err
		}
		queue = append(queue, parents...)
	}
	return whitlist, nil
}

func (vtx *uniqueVertex) Height() (uint64, error) {
	vtx.refresh()

	if vtx.v.vtx == nil {
		return 0, fmt.Errorf("failed to get height for vertex with status: %s", vtx.v.status)
	}

	return vtx.v.vtx.Height(), nil
}

func (vtx *uniqueVertex) Epoch() (uint32, error) {
	vtx.refresh()

	if vtx.v.vtx == nil {
		return 0, fmt.Errorf("failed to get epoch for vertex with status: %s", vtx.v.status)
	}

	return vtx.v.vtx.Epoch(), nil
}

func (vtx *uniqueVertex) Txs() ([]snowstorm.Tx, error) {
	vtx.refresh()

	if vtx.v.vtx == nil {
		return nil, fmt.Errorf("failed to get txs for vertex with status: %s", vtx.v.status)
	}

	txs := vtx.v.vtx.Txs()
	if len(txs) != len(vtx.v.txs) {
		vtx.v.txs = make([]snowstorm.Tx, len(txs))
		for i, txBytes := range txs {
			tx, err := vtx.serializer.VM.ParseTx(txBytes)
			if err != nil {
				return nil, err
			}
			vtx.v.txs[i] = tx
		}
	}

	return vtx.v.txs, nil
}

func (vtx *uniqueVertex) Bytes() []byte { return vtx.v.vtx.Bytes() }

func (vtx *uniqueVertex) String() string {
	sb := strings.Builder{}

	parents, err := vtx.Parents()
	if err != nil {
		sb.WriteString(fmt.Sprintf("Vertex(ID = %s, Error=error while retrieving vertex parents: %s)", vtx.ID(), err))
		return sb.String()
	}
	txs, err := vtx.Txs()
	if err != nil {
		sb.WriteString(fmt.Sprintf("Vertex(ID = %s, Error=error while retrieving vertex txs: %s)", vtx.ID(), err))
		return sb.String()
	}

	sb.WriteString(fmt.Sprintf(
		"Vertex(ID = %s, Status = %s, Number of Dependencies = %d, Number of Transactions = %d)",
		vtx.ID(),
		vtx.Status(),
		len(parents),
		len(txs),
	))

	parentFormat := fmt.Sprintf("\n    Parent[%s]: ID = %%s, Status = %%s",
		formatting.IntFormat(len(parents)-1))
	for i, parent := range parents {
		sb.WriteString(fmt.Sprintf(parentFormat, i, parent.ID(), parent.Status()))
	}

	txFormat := fmt.Sprintf("\n    Transaction[%s]: ID = %%s, Status = %%s",
		formatting.IntFormat(len(txs)-1))
	for i, tx := range txs {
		sb.WriteString(fmt.Sprintf(txFormat, i, tx.ID(), tx.Status()))
	}

	return sb.String()
}

type vertexState struct {
	latest bool

	vtx    vertex.StatelessVertex
	status choices.Status

	parents []avalanche.Vertex
	txs     []snowstorm.Tx
}

```

avalanchego/snow/engine/avalanche/state/unique_vertex_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package state

import (
	"bytes"
	"errors"
	"testing"
	"time"

	"github.com/ava-labs/avalanchego/database/memdb"
	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow"
	"github.com/ava-labs/avalanchego/snow/choices"
	"github.com/ava-labs/avalanchego/snow/consensus/snowstorm"
	"github.com/ava-labs/avalanchego/snow/engine/avalanche/vertex"
	"github.com/ava-labs/avalanchego/utils/hashing"
	"github.com/ava-labs/avalanchego/version"
)

func newTestSerializer(t *testing.T, parse func([]byte) (snowstorm.Tx, error)) *Serializer {
	vm := vertex.TestVM{}
	vm.T = t
	vm.Default(true)
	vm.ParseTxF = parse

	baseDB := memdb.New()
	ctx := snow.DefaultContextTest()
	s := NewSerializer(
		SerializerConfig{
			ChainID: ctx.ChainID,
			VM:      &vm,
			DB:      baseDB,
			Log:     ctx.Log,
		},
	)

	return s.(*Serializer)
}

func TestUnknownUniqueVertexErrors(t *testing.T) {
	s := newTestSerializer(t, nil)

	uVtx := &uniqueVertex{
		serializer: s,
		id:         ids.ID{},
	}

	status := uVtx.Status()
	if status != choices.Unknown {
		t.Fatalf("Expected vertex to have Unknown status")
	}

	_, err := uVtx.Parents()
	if err == nil {
		t.Fatalf("Parents should have produced error for unknown vertex")
	}

	_, err = uVtx.Height()
	if err == nil {
		t.Fatalf("Height should have produced error for unknown vertex")
	}

	_, err = uVtx.Txs()
	if err == nil {
		t.Fatalf("Txs should have produced an error for unknown vertex")
	}
}

func TestUniqueVertexCacheHit(t *testing.T) {
	testTx := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV: ids.ID{1},
	}}

	s := newTestSerializer(t, func(b []byte) (snowstorm.Tx, error) {
		if !bytes.Equal(b, []byte{0}) {
			t.Fatal("unknown tx")
		}
		return testTx, nil
	})

	id := ids.ID{2}
	parentID := ids.ID{'p', 'a', 'r', 'e', 'n', 't'}
	parentIDs := []ids.ID{parentID}
	chainID := ids.ID{} // Same as chainID of serializer
	height := uint64(1)
	vtx, err := vertex.Build( // regular, non-stop vertex
		chainID,
		height,
		parentIDs,
		[][]byte{{0}},
	)
	if err != nil {
		t.Fatal(err)
	}

	uVtx := &uniqueVertex{
		id:         id,
		serializer: s,
	}
	if err := uVtx.setVertex(vtx); err != nil {
		t.Fatalf("Failed to set vertex due to: %s", err)
	}

	newUVtx := &uniqueVertex{
		id:         id,
		serializer: s,
	}

	parents, err := newUVtx.Parents()
	if err != nil {
		t.Fatalf("Error while retrieving parents of known vertex")
	}
	if len(parents) != 1 {
		t.Fatalf("Parents should have length 1")
	}
	if parents[0].ID() != parentID {
		t.Fatalf("ParentID is incorrect")
	}

	newHeight, err := newUVtx.Height()
	if err != nil {
		t.Fatalf("Error while retrieving height of known vertex")
	}
	if height != newHeight {
		t.Fatalf("Vertex height should have been %d, but was: %d", height, newHeight)
	}

	txs, err := newUVtx.Txs()
	if err != nil {
		t.Fatalf("Error while retrieving txs of known vertex: %s", err)
	}
	if len(txs) != 1 {
		t.Fatalf("Incorrect number of transactions")
	}
	if txs[0] != testTx {
		t.Fatalf("Txs retrieved the wrong Tx")
	}

	if newUVtx.v != uVtx.v {
		t.Fatalf("Unique vertex failed to get corresponding vertex state from cache")
	}
}

func TestUniqueVertexCacheMiss(t *testing.T) {
	txBytesParent := []byte{1, 2, 3, 4, 5, 6, 7, 8, 9}
	testTxParent := &snowstorm.TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.ID{1},
			StatusV: choices.Accepted,
		},
		BytesV: txBytesParent,
	}

	txBytes := []byte{1, 2, 3, 4, 5, 6, 7, 8, 9}
	testTx := &snowstorm.TestTx{
		TestDecidable: choices.TestDecidable{
			IDV: ids.ID{1},
		},
		BytesV: txBytes,
	}
	parseTx := func(b []byte) (snowstorm.Tx, error) {
		if bytes.Equal(txBytesParent, b) {
			return testTxParent, nil
		}
		if bytes.Equal(txBytes, b) {
			return testTx, nil
		}
		t.Fatal("asked to parse unexpected transaction")
		return nil, nil
	}

	s := newTestSerializer(t, parseTx)

	uvtxParent := newTestUniqueVertex(t, s, nil, [][]byte{txBytesParent}, false)
	if err := uvtxParent.Accept(); err != nil {
		t.Fatal(err)
	}

	parentID := uvtxParent.ID()
	parentIDs := []ids.ID{parentID}
	chainID := ids.ID{}
	height := uint64(1)
	innerVertex, err := vertex.Build( // regular, non-stop vertex
		chainID,
		height,
		parentIDs,
		[][]byte{txBytes},
	)
	if err != nil {
		t.Fatal(err)
	}

	id := innerVertex.ID()
	vtxBytes := innerVertex.Bytes()

	uVtx := uniqueVertex{
		id:         id,
		serializer: s,
	}

	// Register a cache miss
	if status := uVtx.Status(); status != choices.Unknown {
		t.Fatalf("expected status to be unknown, but found: %s", status)
	}

	// Register cache hit
	vtx, err := newUniqueVertex(s, vtxBytes)
	if err != nil {
		t.Fatal(err)
	}

	if status := vtx.Status(); status != choices.Processing {
		t.Fatalf("expected status to be processing, but found: %s", status)
	}

	if err := vtx.Verify(); err != nil {
		t.Fatal(err)
	}

	validateVertex := func(vtx *uniqueVertex, expectedStatus choices.Status) {
		if status := vtx.Status(); status != expectedStatus {
			t.Fatalf("expected status to be %s, but found: %s", expectedStatus, status)
		}

		// Call bytes first to check for regression bug
		// where it's unsafe to call Bytes or Verify directly
		// after calling Status to refresh a vertex
		if !bytes.Equal(vtx.Bytes(), vtxBytes) {
			t.Fatalf("Found unexpected vertex bytes")
		}

		vtxParents, err := vtx.Parents()
		if err != nil {
			t.Fatalf("Fetching vertex parents errored with: %s", err)
		}
		vtxHeight, err := vtx.Height()
		if err != nil {
			t.Fatalf("Fetching vertex height errored with: %s", err)
		}
		vtxTxs, err := vtx.Txs()
		if err != nil {
			t.Fatalf("Fetching vertx txs errored with: %s", err)
		}
		switch {
		case vtxHeight != height:
			t.Fatalf("Expected vertex height to be %d, but found %d", height, vtxHeight)
		case len(vtxParents) != 1:
			t.Fatalf("Expected vertex to have 1 parent, but found %d", len(vtxParents))
		case vtxParents[0].ID() != parentID:
			t.Fatalf("Found unexpected parentID: %s, expected: %s", vtxParents[0].ID(), parentID)
		case len(vtxTxs) != 1:
			t.Fatalf("Exepcted vertex to have 1 transaction, but found %d", len(vtxTxs))
		case !bytes.Equal(vtxTxs[0].Bytes(), txBytes):
			t.Fatalf("Found unexpected transaction bytes")
		}
	}

	// Replace the vertex, so that it loses reference to parents, etc.
	vtx = &uniqueVertex{
		id:         id,
		serializer: s,
	}

	// Check that the vertex refreshed from the cache is valid
	validateVertex(vtx, choices.Processing)

	// Check that a newly parsed vertex refreshed from the cache is valid
	vtx, err = newUniqueVertex(s, vtxBytes)
	if err != nil {
		t.Fatal(err)
	}
	validateVertex(vtx, choices.Processing)

	// Check that refreshing a vertex when it has been removed from
	// the cache works correctly

	s.state.uniqueVtx.Flush()
	vtx = &uniqueVertex{
		id:         id,
		serializer: s,
	}
	validateVertex(vtx, choices.Processing)

	s.state.uniqueVtx.Flush()
	vtx, err = newUniqueVertex(s, vtxBytes)
	if err != nil {
		t.Fatal(err)
	}
	validateVertex(vtx, choices.Processing)
}

func TestParseVertexWithIncorrectChainID(t *testing.T) {
	statelessVertex, err := vertex.Build( // regular, non-stop vertex
		ids.GenerateTestID(),
		0,
		nil,
		[][]byte{{1}},
	)
	if err != nil {
		t.Fatal(err)
	}
	vtxBytes := statelessVertex.Bytes()

	s := newTestSerializer(t, func(b []byte) (snowstorm.Tx, error) {
		if bytes.Equal(b, []byte{1}) {
			return &snowstorm.TestTx{}, nil
		}
		return nil, errors.New("invalid tx")
	})

	if _, err := s.ParseVtx(vtxBytes); err == nil {
		t.Fatal("should have failed to parse the vertex due to invalid chainID")
	}
}

func TestParseVertexWithInvalidTxs(t *testing.T) {
	ctx := snow.DefaultContextTest()
	statelessVertex, err := vertex.Build( // regular, non-stop vertex
		ctx.ChainID,
		0,
		nil,
		[][]byte{{1}},
	)
	if err != nil {
		t.Fatal(err)
	}
	vtxBytes := statelessVertex.Bytes()

	s := newTestSerializer(t, func(b []byte) (snowstorm.Tx, error) {
		switch {
		case bytes.Equal(b, []byte{1}):
			return nil, errors.New("invalid tx")
		case bytes.Equal(b, []byte{2}):
			return &snowstorm.TestTx{}, nil
		default:
			return nil, errors.New("invalid tx")
		}
	})

	if _, err := s.ParseVtx(vtxBytes); err == nil {
		t.Fatal("should have failed to parse the vertex due to invalid transactions")
	}

	if _, err := s.ParseVtx(vtxBytes); err == nil {
		t.Fatal("should have failed to parse the vertex after previously error on parsing invalid transactions")
	}

	id := hashing.ComputeHash256Array(vtxBytes)
	if _, err := s.GetVtx(id); err == nil {
		t.Fatal("should have failed to lookup invalid vertex after previously error on parsing invalid transactions")
	}

	childStatelessVertex, err := vertex.Build( // regular, non-stop vertex
		ctx.ChainID,
		1,
		[]ids.ID{id},
		[][]byte{{2}},
	)
	if err != nil {
		t.Fatal(err)
	}
	childVtxBytes := childStatelessVertex.Bytes()

	childVtx, err := s.ParseVtx(childVtxBytes)
	if err != nil {
		t.Fatal(err)
	}

	parents, err := childVtx.Parents()
	if err != nil {
		t.Fatal(err)
	}
	if len(parents) != 1 {
		t.Fatal("wrong number of parents")
	}
	parent := parents[0]

	if parent.Status().Fetched() {
		t.Fatal("the parent is invalid, so it shouldn't be marked as fetched")
	}
}

func TestStopVertexWhitelistEmpty(t *testing.T) {
	// vtx itself is accepted, no parent ==> empty transitives
	_, parseTx := generateTestTxs('a')

	// create serializer object
	ts := newTestSerializer(t, parseTx)

	uvtx := newTestUniqueVertex(t, ts, nil, [][]byte{{'a'}}, true)
	if err := uvtx.Accept(); err != nil {
		t.Fatal(err)
	}

	tsv, err := uvtx.Whitelist()
	if err != nil {
		t.Fatalf("failed to get whitelist %v", err)
	}
	if tsv.Len() > 0 {
		t.Fatal("expected empty whitelist")
	}
}

func TestStopVertexWhitelistWithParents(t *testing.T) {
	t.Parallel()

	txs, parseTx := generateTestTxs('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h')
	ts := newTestSerializer(t, parseTx)

	//      (accepted)           (accepted)
	//        vtx_1                vtx_2
	//    [tx_a, tx_b]          [tx_c, tx_d]
	//          ⬆      ⬉     ⬈       ⬆
	//        vtx_3                vtx_4
	//    [tx_e, tx_f]          [tx_g, tx_h]
	//                    ⬉           ⬆
	//                         stop_vertex_5
	uvtx1 := newTestUniqueVertex(t, ts, nil, [][]byte{{'a'}, {'b'}}, false)
	if err := uvtx1.Accept(); err != nil {
		t.Fatal(err)
	}
	uvtx2 := newTestUniqueVertex(t, ts, nil, [][]byte{{'c'}, {'d'}}, false)
	if err := uvtx2.Accept(); err != nil {
		t.Fatal(err)
	}
	uvtx3 := newTestUniqueVertex(t, ts, []ids.ID{uvtx1.id, uvtx2.id}, [][]byte{{'e'}, {'f'}}, false)
	uvtx4 := newTestUniqueVertex(t, ts, []ids.ID{uvtx1.id, uvtx2.id}, [][]byte{{'g'}, {'h'}}, false)
	svtx5 := newTestUniqueVertex(t, ts, []ids.ID{uvtx3.id, uvtx4.id}, nil, true)

	whitelist, err := svtx5.Whitelist()
	if err != nil {
		t.Fatalf("failed to get whitelist %v", err)
	}

	expectedWhitelist := []ids.ID{
		txs[4].ID(), // 'e'
		txs[5].ID(), // 'f'
		txs[6].ID(), // 'g'
		txs[7].ID(), // 'h'
		uvtx3.ID(),
		uvtx4.ID(),
		svtx5.ID(),
	}
	if !ids.UnsortedEquals(whitelist.List(), expectedWhitelist) {
		t.Fatalf("whitelist expected %v, got %v", expectedWhitelist, whitelist)
	}
}

func TestStopVertexWhitelistWithLinearChain(t *testing.T) {
	t.Parallel()

	// 0 -> 1 -> 2 -> 3 -> 4 -> 5
	// all vertices on the transitive paths are processing
	txs, parseTx := generateTestTxs('a', 'b', 'c', 'd', 'e')

	// create serializer object
	ts := newTestSerializer(t, parseTx)

	uvtx5 := newTestUniqueVertex(t, ts, nil, [][]byte{{'e'}}, false)
	if err := uvtx5.Accept(); err != nil {
		t.Fatal(err)
	}

	uvtx4 := newTestUniqueVertex(t, ts, []ids.ID{uvtx5.id}, [][]byte{{'d'}}, false)
	uvtx3 := newTestUniqueVertex(t, ts, []ids.ID{uvtx4.id}, [][]byte{{'c'}}, false)
	uvtx2 := newTestUniqueVertex(t, ts, []ids.ID{uvtx3.id}, [][]byte{{'b'}}, false)
	uvtx1 := newTestUniqueVertex(t, ts, []ids.ID{uvtx2.id}, [][]byte{{'a'}}, false)
	uvtx0 := newTestUniqueVertex(t, ts, []ids.ID{uvtx1.id}, nil, true)

	whitelist, err := uvtx0.Whitelist()
	if err != nil {
		t.Fatalf("failed to get whitelist %v", err)
	}

	expectedWhitelist := []ids.ID{
		txs[0].ID(),
		txs[1].ID(),
		txs[2].ID(),
		txs[3].ID(),
		uvtx0.ID(),
		uvtx1.ID(),
		uvtx2.ID(),
		uvtx3.ID(),
		uvtx4.ID(),
	}
	if !ids.UnsortedEquals(whitelist.List(), expectedWhitelist) {
		t.Fatalf("whitelist expected %v, got %v", expectedWhitelist, whitelist)
	}
}

func TestStopVertexVerifyUnexpectedDependencies(t *testing.T) {
	t.Parallel()

	txs, parseTx := generateTestTxs('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'x')
	ts := newTestSerializer(t, parseTx)

	//      (accepted)           (accepted)
	//        vtx_1                vtx_2
	//    [tx_a, tx_b]          [tx_c, tx_d]
	//          ⬆      ⬉     ⬈       ⬆
	//        vtx_3                vtx_4
	//    [tx_e, tx_f]          [tx_g, tx_h]
	//                               ⬆
	//                         stop_vertex_5
	//
	// [tx_a, tx_b] transitively referenced by "stop_vertex_5"
	// has the dependent transactions [tx_e, tx_f]
	// that are not transitively referenced by "stop_vertex_5"
	// in case "tx_g" depends on "tx_e" that is not in vtx4.
	// Thus "stop_vertex_5" is invalid!

	// "tx_g" depends on "tx_e"
	txEInf := txs[4]
	txGInf := txs[6]
	txG, ok := txGInf.(*snowstorm.TestTx)
	if !ok {
		t.Fatalf("unexpected type %T", txGInf)
	}
	txG.DependenciesV = []snowstorm.Tx{txEInf}

	uvtx1 := newTestUniqueVertex(t, ts, nil, [][]byte{{'a'}, {'b'}}, false)
	if err := uvtx1.Accept(); err != nil {
		t.Fatal(err)
	}
	uvtx2 := newTestUniqueVertex(t, ts, nil, [][]byte{{'c'}, {'d'}}, false)
	if err := uvtx2.Accept(); err != nil {
		t.Fatal(err)
	}

	uvtx3 := newTestUniqueVertex(t, ts, []ids.ID{uvtx1.id, uvtx2.id}, [][]byte{{'e'}, {'f'}}, false)
	uvtx4 := newTestUniqueVertex(t, ts, []ids.ID{uvtx1.id, uvtx2.id}, [][]byte{{'g'}, {'h'}}, false)

	svtx5 := newTestUniqueVertex(t, ts, []ids.ID{uvtx4.id}, nil, true)
	if verr := svtx5.Verify(); !errors.Is(verr, errUnexpectedDependencyStopVtx) {
		t.Fatalf("stop vertex 'Verify' expected %v, got %v", errUnexpectedDependencyStopVtx, verr)
	}

	// if "tx_e" that "tx_g" depends on were accepted,
	// transitive closure is reaching all accepted frontier
	txE, ok := txEInf.(*snowstorm.TestTx)
	if !ok {
		t.Fatalf("unexpected type %T", txEInf)
	}
	txE.StatusV = choices.Accepted
	svtx5 = newTestUniqueVertex(t, ts, []ids.ID{uvtx4.id}, nil, true)
	if verr := svtx5.Verify(); verr != nil {
		t.Fatalf("stop vertex 'Verify' expected nil, got %v", verr)
	}

	// valid stop vertex
	//
	//      (accepted)           (accepted)
	//        vtx_1                vtx_2
	//    [tx_a, tx_b]          [tx_c, tx_d]
	//          ⬆      ⬉     ⬈       ⬆
	//        vtx_3                vtx_4
	//    [tx_e, tx_f]          [tx_g, tx_h]
	//                    ⬉           ⬆
	//                         stop_vertex_5
	svtx5 = newTestUniqueVertex(t, ts, []ids.ID{uvtx3.id, uvtx4.id}, nil, true)
	if verr := svtx5.Verify(); verr != nil {
		t.Fatalf("stop vertex 'Verify' expected nil, got %v", verr)
	}
	if err := uvtx3.Accept(); err != nil {
		t.Fatal(err)
	}
	if err := uvtx4.Accept(); err != nil {
		t.Fatal(err)
	}
	if err := svtx5.Accept(); err != nil {
		t.Fatal(err)
	}
	// stop vertex cannot be issued twice
	if verr := svtx5.Verify(); !errors.Is(verr, errStopVertexAlreadyAccepted) {
		t.Fatalf("stop vertex 'Verify' expected %v, got %v", errStopVertexAlreadyAccepted, verr)
	}

	// no vertex should never be able to refer to a stop vertex in its transitive closure
	// regular vertex with stop vertex as a parent should fail!
	//
	//      (accepted)           (accepted)
	//        vtx_1                vtx_2
	//    [tx_a, tx_b]          [tx_c, tx_d]
	//          ⬆      ⬉     ⬈       ⬆
	//        vtx_3                vtx_4
	//    [tx_e, tx_f]          [tx_g, tx_h]
	//                    ⬉           ⬆
	//                         stop_vertex_5
	//                                ⬆
	//                              vtx_6
	//                              [tx_x]
	//                           (should fail)
	uvtx6 := newTestUniqueVertex(t, ts, []ids.ID{svtx5.id}, [][]byte{{'x'}}, false)
	if verr := uvtx6.Verify(); !errors.Is(verr, errStopVertexAlreadyAccepted) {
		t.Fatalf("stop vertex 'Verify' expected %v, got %v", errStopVertexAlreadyAccepted, verr)
	}
}

func TestStopVertexVerifyNotAllowedTimestamp(t *testing.T) {
	t.Parallel()

	_, parseTx := generateTestTxs('a')
	ts := newTestSerializer(t, parseTx)
	ts.XChainMigrationTime = version.XChainMigrationDefaultTime

	svtx := newTestUniqueVertex(t, ts, nil, nil, true)
	svtx.time = func() time.Time { return version.XChainMigrationDefaultTime.Add(-time.Second) }

	if verr := svtx.Verify(); !errors.Is(verr, errStopVertexNotAllowedTimestamp) {
		t.Fatalf("stop vertex 'Verify' expected %v, got %v", errStopVertexNotAllowedTimestamp, verr)
	}
}

func newTestUniqueVertex(
	t *testing.T,
	s *Serializer,
	parentIDs []ids.ID,
	txs [][]byte,
	stopVertex bool,
) *uniqueVertex {
	var (
		vtx vertex.StatelessVertex
		err error
	)
	if !stopVertex {
		vtx, err = vertex.Build(
			ids.ID{},
			uint64(1),
			parentIDs,
			txs,
		)
	} else {
		vtx, err = vertex.BuildStopVertex(
			ids.ID{},
			uint64(1),
			parentIDs,
		)
	}
	if err != nil {
		t.Fatal(err)
	}
	uvtx, err := newUniqueVertex(s, vtx.Bytes())
	if err != nil {
		t.Fatal(err)
	}
	return uvtx
}

func generateTestTxs(idSlice ...byte) ([]snowstorm.Tx, func(b []byte) (snowstorm.Tx, error)) {
	txs := make([]snowstorm.Tx, len(idSlice))
	bytesToTx := make(map[string]snowstorm.Tx, len(idSlice))
	for i, b := range idSlice {
		txs[i] = &snowstorm.TestTx{
			TestDecidable: choices.TestDecidable{
				IDV: ids.ID{b},
			},
			BytesV: []byte{b},
		}
		bytesToTx[string([]byte{b})] = txs[i]
	}
	parseTx := func(b []byte) (snowstorm.Tx, error) {
		tx, ok := bytesToTx[string(b)]
		if !ok {
			return nil, errors.New("unknown tx bytes")
		}
		return tx, nil
	}
	return txs, parseTx
}

```

avalanchego/snow/engine/avalanche/test_avalanche_engine.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package avalanche

import (
	"errors"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/consensus/avalanche"
	"github.com/ava-labs/avalanchego/snow/engine/common"
)

var (
	_ Engine = &EngineTest{}

	errGetVtx = errors.New("unexpectedly called GetVtx")
)

// EngineTest is a test engine
type EngineTest struct {
	common.EngineTest

	CantGetVtx bool
	GetVtxF    func(vtxID ids.ID) (avalanche.Vertex, error)
}

func (e *EngineTest) Default(cant bool) {
	e.EngineTest.Default(cant)
	e.CantGetVtx = false
}

func (e *EngineTest) GetVtx(vtxID ids.ID) (avalanche.Vertex, error) {
	if e.GetVtxF != nil {
		return e.GetVtxF(vtxID)
	}
	if e.CantGetVtx && e.T != nil {
		e.T.Fatalf("Unexpectedly called GetVtx")
	}
	return nil, errGetVtx
}

```

avalanchego/snow/engine/avalanche/transitive.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package avalanche

import (
	"fmt"
	"time"

	"go.uber.org/zap"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow"
	"github.com/ava-labs/avalanchego/snow/consensus/avalanche"
	"github.com/ava-labs/avalanchego/snow/consensus/avalanche/poll"
	"github.com/ava-labs/avalanchego/snow/consensus/snowstorm"
	"github.com/ava-labs/avalanchego/snow/engine/avalanche/vertex"
	"github.com/ava-labs/avalanchego/snow/engine/common"
	"github.com/ava-labs/avalanchego/snow/events"
	"github.com/ava-labs/avalanchego/utils/sampler"
	"github.com/ava-labs/avalanchego/utils/wrappers"
	"github.com/ava-labs/avalanchego/version"
)

var _ Engine = &Transitive{}

func New(config Config) (Engine, error) {
	return newTransitive(config)
}

// Transitive implements the Engine interface by attempting to fetch all
// transitive dependencies.
type Transitive struct {
	Config
	metrics

	// list of NoOpsHandler for messages dropped by engine
	common.StateSummaryFrontierHandler
	common.AcceptedStateSummaryHandler
	common.AcceptedFrontierHandler
	common.AcceptedHandler
	common.AncestorsHandler

	RequestID uint32

	polls poll.Set // track people I have asked for their preference

	// The set of vertices that have been requested in Get messages but not yet received
	outstandingVtxReqs common.Requests

	// missingTxs tracks transaction that are missing
	missingTxs ids.Set

	// IDs of vertices that are queued to be added to consensus but haven't yet been
	// because of missing dependencies
	pending ids.Set

	// vtxBlocked tracks operations that are blocked on vertices
	// txBlocked tracks operations that are blocked on transactions
	vtxBlocked, txBlocked events.Blocker

	// transactions that have been provided from the VM but that are pending to
	// be issued once the number of processing vertices has gone below the
	// optimal number.
	pendingTxs []snowstorm.Tx

	// A uniform sampler without replacement
	uniformSampler sampler.Uniform

	errs wrappers.Errs
}

func newTransitive(config Config) (*Transitive, error) {
	config.Ctx.Log.Info("initializing consensus engine")

	factory := poll.NewEarlyTermNoTraversalFactory(config.Params.Alpha)

	t := &Transitive{
		Config:                      config,
		StateSummaryFrontierHandler: common.NewNoOpStateSummaryFrontierHandler(config.Ctx.Log),
		AcceptedStateSummaryHandler: common.NewNoOpAcceptedStateSummaryHandler(config.Ctx.Log),
		AcceptedFrontierHandler:     common.NewNoOpAcceptedFrontierHandler(config.Ctx.Log),
		AcceptedHandler:             common.NewNoOpAcceptedHandler(config.Ctx.Log),
		AncestorsHandler:            common.NewNoOpAncestorsHandler(config.Ctx.Log),
		polls: poll.NewSet(factory,
			config.Ctx.Log,
			"",
			config.Ctx.Registerer,
		),
		uniformSampler: sampler.NewUniform(),
	}

	return t, t.metrics.Initialize("", config.Ctx.Registerer)
}

func (t *Transitive) Put(nodeID ids.NodeID, requestID uint32, vtxBytes []byte) error {
	t.Ctx.Log.Verbo("called Put",
		zap.Stringer("nodeID", nodeID),
		zap.Uint32("requestID", requestID),
	)
	vtx, err := t.Manager.ParseVtx(vtxBytes)
	if err != nil {
		t.Ctx.Log.Debug("failed to parse vertex",
			zap.Stringer("nodeID", nodeID),
			zap.Uint32("requestID", requestID),
			zap.Error(err),
		)
		t.Ctx.Log.Verbo("failed to parse vertex",
			zap.Stringer("nodeID", nodeID),
			zap.Uint32("requestID", requestID),
			zap.Binary("vertex", vtxBytes),
			zap.Error(err),
		)
		return t.GetFailed(nodeID, requestID)
	}

	actualVtxID := vtx.ID()
	expectedVtxID, ok := t.outstandingVtxReqs.Get(nodeID, requestID)
	// If the provided vertex is not the requested vertex, we need to explicitly
	// mark the request as failed to avoid having a dangling dependency.
	if ok && actualVtxID != expectedVtxID {
		t.Ctx.Log.Debug("incorrect vertex returned in Put",
			zap.Stringer("nodeID", nodeID),
			zap.Uint32("requestID", requestID),
			zap.Stringer("vtxID", actualVtxID),
			zap.Stringer("expectedVtxID", expectedVtxID),
		)
		// We assume that [vtx] is useless because it doesn't match what we
		// expected.
		return t.GetFailed(nodeID, requestID)
	}

	if t.Consensus.VertexIssued(vtx) || t.pending.Contains(actualVtxID) {
		t.metrics.numUselessPutBytes.Add(float64(len(vtxBytes)))
	}

	if _, err := t.issueFrom(nodeID, vtx); err != nil {
		return err
	}
	return t.attemptToIssueTxs()
}

func (t *Transitive) GetFailed(nodeID ids.NodeID, requestID uint32) error {
	vtxID, ok := t.outstandingVtxReqs.Remove(nodeID, requestID)
	if !ok {
		t.Ctx.Log.Debug("unexpected GetFailed",
			zap.Stringer("nodeID", nodeID),
			zap.Uint32("requestID", requestID),
		)
		return nil
	}

	t.vtxBlocked.Abandon(vtxID)

	if t.outstandingVtxReqs.Len() == 0 {
		for txID := range t.missingTxs {
			t.txBlocked.Abandon(txID)
		}
		t.missingTxs.Clear()
	}

	// Track performance statistics
	t.metrics.numVtxRequests.Set(float64(t.outstandingVtxReqs.Len()))
	t.metrics.numMissingTxs.Set(float64(t.missingTxs.Len()))
	t.metrics.blockerVtxs.Set(float64(t.vtxBlocked.Len()))
	t.metrics.blockerTxs.Set(float64(t.txBlocked.Len()))
	return t.attemptToIssueTxs()
}

func (t *Transitive) PullQuery(nodeID ids.NodeID, requestID uint32, vtxID ids.ID) error {
	// Immediately respond to the query with the current consensus preferences.
	t.Sender.SendChits(nodeID, requestID, t.Consensus.Preferences().List())

	// If we have [vtxID], attempt to put it into consensus, if we haven't
	// already. If we don't not have [vtxID], fetch it from [nodeID].
	if _, err := t.issueFromByID(nodeID, vtxID); err != nil {
		return err
	}

	return t.attemptToIssueTxs()
}

func (t *Transitive) PushQuery(nodeID ids.NodeID, requestID uint32, vtxBytes []byte) error {
	// Immediately respond to the query with the current consensus preferences.
	t.Sender.SendChits(nodeID, requestID, t.Consensus.Preferences().List())

	vtx, err := t.Manager.ParseVtx(vtxBytes)
	if err != nil {
		t.Ctx.Log.Debug("failed to parse vertex",
			zap.Stringer("nodeID", nodeID),
			zap.Uint32("requestID", requestID),
			zap.Error(err),
		)
		t.Ctx.Log.Verbo("failed to parse vertex",
			zap.Stringer("nodeID", nodeID),
			zap.Uint32("requestID", requestID),
			zap.Binary("vertex", vtxBytes),
			zap.Error(err),
		)
		return nil
	}

	if t.Consensus.VertexIssued(vtx) || t.pending.Contains(vtx.ID()) {
		t.metrics.numUselessPushQueryBytes.Add(float64(len(vtxBytes)))
	}

	if _, err := t.issueFrom(nodeID, vtx); err != nil {
		return err
	}

	return t.attemptToIssueTxs()
}

func (t *Transitive) Chits(nodeID ids.NodeID, requestID uint32, votes []ids.ID) error {
	v := &voter{
		t:         t,
		vdr:       nodeID,
		requestID: requestID,
		response:  votes,
	}
	for _, vote := range votes {
		if added, err := t.issueFromByID(nodeID, vote); err != nil {
			return err
		} else if !added {
			v.deps.Add(vote)
		}
	}

	t.vtxBlocked.Register(v)
	t.metrics.blockerVtxs.Set(float64(t.vtxBlocked.Len()))
	return t.attemptToIssueTxs()
}

func (t *Transitive) QueryFailed(nodeID ids.NodeID, requestID uint32) error {
	return t.Chits(nodeID, requestID, nil)
}

func (t *Transitive) AppRequest(nodeID ids.NodeID, requestID uint32, deadline time.Time, request []byte) error {
	// Notify the VM of this request
	return t.VM.AppRequest(nodeID, requestID, deadline, request)
}

func (t *Transitive) AppRequestFailed(nodeID ids.NodeID, requestID uint32) error {
	// Notify the VM that a request it made failed
	return t.VM.AppRequestFailed(nodeID, requestID)
}

func (t *Transitive) AppResponse(nodeID ids.NodeID, requestID uint32, response []byte) error {
	// Notify the VM of a response to its request
	return t.VM.AppResponse(nodeID, requestID, response)
}

func (t *Transitive) AppGossip(nodeID ids.NodeID, msg []byte) error {
	// Notify the VM of this message which has been gossiped to it
	return t.VM.AppGossip(nodeID, msg)
}

func (t *Transitive) Connected(nodeID ids.NodeID, nodeVersion *version.Application) error {
	return t.VM.Connected(nodeID, nodeVersion)
}

func (t *Transitive) Disconnected(nodeID ids.NodeID) error {
	return t.VM.Disconnected(nodeID)
}

func (t *Transitive) Timeout() error { return nil }

func (t *Transitive) Gossip() error {
	edge := t.Manager.Edge()
	if len(edge) == 0 {
		t.Ctx.Log.Verbo("dropping gossip request as no vertices have been accepted")
		return nil
	}

	if err := t.uniformSampler.Initialize(uint64(len(edge))); err != nil {
		return err // Should never happen
	}
	indices, err := t.uniformSampler.Sample(1)
	if err != nil {
		return err // Also should never really happen because the edge has positive length
	}
	vtxID := edge[int(indices[0])]
	vtx, err := t.Manager.GetVtx(vtxID)
	if err != nil {
		t.Ctx.Log.Warn("dropping gossip request",
			zap.String("reason", "couldn't load vertex"),
			zap.Stringer("vtxID", vtxID),
			zap.Error(err),
		)
		return nil
	}

	t.Ctx.Log.Verbo("gossiping accepted vertex to the network",
		zap.Stringer("vtxID", vtxID),
	)
	t.Sender.SendGossip(vtx.Bytes())
	return nil
}

func (t *Transitive) Halt() {}

func (t *Transitive) Shutdown() error {
	t.Ctx.Log.Info("shutting down consensus engine")
	return t.VM.Shutdown()
}

func (t *Transitive) Notify(msg common.Message) error {
	switch msg {
	case common.PendingTxs:
		t.pendingTxs = append(t.pendingTxs, t.VM.PendingTxs()...)
		t.metrics.pendingTxs.Set(float64(len(t.pendingTxs)))
		return t.attemptToIssueTxs()

	case common.StopVertex:
		// stop vertex doesn't have any txs, issue directly!
		return t.issueStopVtx()

	default:
		t.Ctx.Log.Warn("received an unexpected message from the VM",
			zap.Stringer("messageString", msg),
		)
		return nil
	}
}

func (t *Transitive) Context() *snow.ConsensusContext {
	return t.Ctx
}

func (t *Transitive) Start(startReqID uint32) error {
	t.RequestID = startReqID
	// Load the vertices that were last saved as the accepted frontier
	edge := t.Manager.Edge()
	frontier := make([]avalanche.Vertex, 0, len(edge))
	for _, vtxID := range edge {
		if vtx, err := t.Manager.GetVtx(vtxID); err == nil {
			frontier = append(frontier, vtx)
		} else {
			t.Ctx.Log.Error("failed to load vertex from the frontier",
				zap.Stringer("vtxID", vtxID),
				zap.Error(err),
			)
		}
	}

	t.Ctx.Log.Info("consensus starting",
		zap.Int("lenFrontier", len(frontier)),
	)
	t.metrics.bootstrapFinished.Set(1)

	t.Ctx.SetState(snow.NormalOp)
	if err := t.VM.SetState(snow.NormalOp); err != nil {
		return fmt.Errorf("failed to notify VM that consensus has started: %w",
			err)
	}
	return t.Consensus.Initialize(t.Ctx, t.Params, frontier)
}

func (t *Transitive) HealthCheck() (interface{}, error) {
	consensusIntf, consensusErr := t.Consensus.HealthCheck()
	vmIntf, vmErr := t.VM.HealthCheck()
	intf := map[string]interface{}{
		"consensus": consensusIntf,
		"vm":        vmIntf,
	}
	if consensusErr == nil {
		return intf, vmErr
	}
	if vmErr == nil {
		return intf, consensusErr
	}
	return intf, fmt.Errorf("vm: %s ; consensus: %s", vmErr, consensusErr)
}

func (t *Transitive) GetVM() common.VM {
	return t.VM
}

func (t *Transitive) GetVtx(vtxID ids.ID) (avalanche.Vertex, error) {
	// GetVtx returns a vertex by its ID.
	// Returns database.ErrNotFound if unknown.
	return t.Manager.GetVtx(vtxID)
}

func (t *Transitive) attemptToIssueTxs() error {
	err := t.errs.Err
	if err != nil {
		return err
	}

	t.pendingTxs, err = t.batch(t.pendingTxs, batchOption{limit: true})
	t.metrics.pendingTxs.Set(float64(len(t.pendingTxs)))
	return err
}

// If there are pending transactions from the VM, issue them.
// If we're not already at the limit for number of concurrent polls, issue a new
// query.
func (t *Transitive) repoll() {
	for i := t.polls.Len(); i < t.Params.ConcurrentRepolls && !t.errs.Errored(); i++ {
		t.issueRepoll()
	}
}

// issueFromByID issues the branch ending with vertex [vtxID] to consensus.
// Fetches [vtxID] if we don't have it locally.
// Returns true if [vtx] has been added to consensus (now or previously)
func (t *Transitive) issueFromByID(nodeID ids.NodeID, vtxID ids.ID) (bool, error) {
	vtx, err := t.Manager.GetVtx(vtxID)
	if err != nil {
		// We don't have [vtxID]. Request it.
		t.sendRequest(nodeID, vtxID)
		return false, nil
	}
	return t.issueFrom(nodeID, vtx)
}

// issueFrom issues the branch ending with [vtx] to consensus.
// Assumes we have [vtx] locally
// Returns true if [vtx] has been added to consensus (now or previously)
func (t *Transitive) issueFrom(nodeID ids.NodeID, vtx avalanche.Vertex) (bool, error) {
	issued := true
	// Before we issue [vtx] into consensus, we have to issue its ancestors.
	// Go through [vtx] and its ancestors. issue each ancestor that hasn't yet been issued.
	// If we find a missing ancestor, fetch it and note that we can't issue [vtx] yet.
	ancestry := vertex.NewHeap()
	ancestry.Push(vtx)
	for ancestry.Len() > 0 {
		vtx := ancestry.Pop()

		if t.Consensus.VertexIssued(vtx) {
			// This vertex has been issued --> its ancestors have been issued.
			// No need to try to issue it or its ancestors
			continue
		}
		if t.pending.Contains(vtx.ID()) {
			issued = false
			continue
		}

		parents, err := vtx.Parents()
		if err != nil {
			return false, err
		}
		// Ensure we have ancestors of this vertex
		for _, parent := range parents {
			if !parent.Status().Fetched() {
				// We don't have the parent. Request it.
				t.sendRequest(nodeID, parent.ID())
				// We're missing an ancestor so we can't have issued the vtx in this method's argument
				issued = false
			} else {
				// Come back to this vertex later to make sure it and its ancestors have been fetched/issued
				ancestry.Push(parent)
			}
		}

		// Queue up this vertex to be issued once its dependencies are met
		if err := t.issue(vtx); err != nil {
			return false, err
		}
	}
	return issued, nil
}

// issue queues [vtx] to be put into consensus after its dependencies are met.
// Assumes we have [vtx].
func (t *Transitive) issue(vtx avalanche.Vertex) error {
	vtxID := vtx.ID()

	// Add to set of vertices that have been queued up to be issued but haven't been yet
	t.pending.Add(vtxID)
	t.outstandingVtxReqs.RemoveAny(vtxID)

	// Will put [vtx] into consensus once dependencies are met
	i := &issuer{
		t:   t,
		vtx: vtx,
	}

	parents, err := vtx.Parents()
	if err != nil {
		return err
	}
	for _, parent := range parents {
		if !t.Consensus.VertexIssued(parent) {
			// This parent hasn't been issued yet. Add it as a dependency.
			i.vtxDeps.Add(parent.ID())
		}
	}

	txs, err := vtx.Txs()
	if err != nil {
		return err
	}
	txIDs := ids.NewSet(len(txs))
	for _, tx := range txs {
		txIDs.Add(tx.ID())
	}

	for _, tx := range txs {
		deps, err := tx.Dependencies()
		if err != nil {
			return err
		}
		for _, dep := range deps {
			depID := dep.ID()
			if !txIDs.Contains(depID) && !t.Consensus.TxIssued(dep) {
				// This transaction hasn't been issued yet. Add it as a dependency.
				t.missingTxs.Add(depID)
				i.txDeps.Add(depID)
			}
		}
	}

	t.Ctx.Log.Verbo("vertex is blocking",
		zap.Stringer("vtxID", vtxID),
		zap.Int("numVtxDeps", i.vtxDeps.Len()),
		zap.Int("numTxDeps", i.txDeps.Len()),
	)

	// Wait until all the parents of [vtx] are added to consensus before adding [vtx]
	t.vtxBlocked.Register(&vtxIssuer{i: i})
	// Wait until all the parents of [tx] are added to consensus before adding [vtx]
	t.txBlocked.Register(&txIssuer{i: i})

	if t.outstandingVtxReqs.Len() == 0 {
		// There are no outstanding vertex requests but we don't have these transactions, so we're not getting them.
		for txID := range t.missingTxs {
			t.txBlocked.Abandon(txID)
		}
		t.missingTxs.Clear()
	}

	// Track performance statistics
	t.metrics.numVtxRequests.Set(float64(t.outstandingVtxReqs.Len()))
	t.metrics.numMissingTxs.Set(float64(t.missingTxs.Len()))
	t.metrics.numPendingVts.Set(float64(len(t.pending)))
	t.metrics.blockerVtxs.Set(float64(t.vtxBlocked.Len()))
	t.metrics.blockerTxs.Set(float64(t.txBlocked.Len()))
	return t.errs.Err
}

type batchOption struct {
	// if [force], allow for a conflict to be issued, and force each tx to be issued
	// otherwise, some txs may not be put into vertices that are issued.
	force bool
	// if [limit], stop when "Params.OptimalProcessing <= Consensus.NumProcessing"
	limit bool
}

// Batchs [txs] into vertices and issue them.
func (t *Transitive) batch(txs []snowstorm.Tx, opt batchOption) ([]snowstorm.Tx, error) {
	if len(txs) == 0 {
		return nil, nil
	}
	if opt.limit && t.Params.OptimalProcessing <= t.Consensus.NumProcessing() {
		return txs, nil
	}
	issuedTxs := ids.Set{}
	consumed := ids.Set{}
	orphans := t.Consensus.Orphans()
	start := 0
	end := 0
	for end < len(txs) {
		tx := txs[end]
		inputs := ids.Set{}
		inputs.Add(tx.InputIDs()...)
		overlaps := consumed.Overlaps(inputs)
		if end-start >= t.Params.BatchSize || (opt.force && overlaps) {
			if err := t.issueBatch(txs[start:end]); err != nil {
				return nil, err
			}
			if opt.limit && t.Params.OptimalProcessing <= t.Consensus.NumProcessing() {
				return txs[end:], nil
			}
			start = end
			consumed.Clear()
			overlaps = false
		}

		if txID := tx.ID(); !overlaps && // should never allow conflicting txs in the same vertex
			!issuedTxs.Contains(txID) && // shouldn't issue duplicated transactions to the same vertex
			(opt.force || t.Consensus.IsVirtuous(tx)) && // force allows for a conflict to be issued
			(!t.Consensus.TxIssued(tx) || orphans.Contains(txID)) { // should only reissue orphaned txs
			end++
			issuedTxs.Add(txID)
			consumed.Union(inputs)
		} else {
			newLen := len(txs) - 1
			txs[end] = txs[newLen]
			txs[newLen] = nil
			txs = txs[:newLen]
		}
	}

	if end > start {
		return txs[end:], t.issueBatch(txs[start:end])
	}
	return txs[end:], nil
}

// Issues a new poll for a preferred vertex in order to move consensus along
func (t *Transitive) issueRepoll() {
	preferredIDs := t.Consensus.Preferences()
	if preferredIDs.Len() == 0 {
		t.Ctx.Log.Error("re-query attempt was dropped due to no pending vertices")
		return
	}

	vtxID := preferredIDs.CappedList(1)[0]
	vdrs, err := t.Validators.Sample(t.Params.K) // Validators to sample
	if err != nil {
		t.Ctx.Log.Error("dropped re-query",
			zap.String("reason", "insufficient number of validators"),
			zap.Stringer("vtxID", vtxID),
			zap.Error(err),
		)
		return
	}

	vdrBag := ids.NodeIDBag{} // IDs of validators to be sampled
	for _, vdr := range vdrs {
		vdrBag.Add(vdr.ID())
	}

	vdrList := vdrBag.List()
	vdrSet := ids.NewNodeIDSet(len(vdrList))
	vdrSet.Add(vdrList...)

	// Poll the network
	t.RequestID++
	if t.polls.Add(t.RequestID, vdrBag) {
		t.Sender.SendPullQuery(vdrSet, t.RequestID, vtxID)
	}
}

// Puts a batch of transactions into a vertex and issues it into consensus.
func (t *Transitive) issueBatch(txs []snowstorm.Tx) error {
	t.Ctx.Log.Verbo("batching transactions into a new vertex",
		zap.Int("numTxs", len(txs)),
	)

	// Randomly select parents of this vertex from among the virtuous set
	virtuousIDs := t.Consensus.Virtuous().CappedList(t.Params.Parents)
	numVirtuousIDs := len(virtuousIDs)
	if err := t.uniformSampler.Initialize(uint64(numVirtuousIDs)); err != nil {
		return err
	}

	indices, err := t.uniformSampler.Sample(numVirtuousIDs)
	if err != nil {
		return err
	}

	parentIDs := make([]ids.ID, len(indices))
	for i, index := range indices {
		parentIDs[i] = virtuousIDs[int(index)]
	}

	vtx, err := t.Manager.BuildVtx(parentIDs, txs)
	if err != nil {
		t.Ctx.Log.Warn("error building new vertex",
			zap.Int("numParents", len(parentIDs)),
			zap.Int("numTxs", len(txs)),
			zap.Error(err),
		)
		return nil
	}

	return t.issue(vtx)
}

// to be triggered via X-Chain API
func (t *Transitive) issueStopVtx() error {
	// use virtuous frontier (accepted) as parents
	virtuousSet := t.Consensus.Virtuous()
	vtx, err := t.Manager.BuildStopVtx(virtuousSet.List())
	if err != nil {
		t.Ctx.Log.Warn("error building new stop vertex",
			zap.Int("numParents", virtuousSet.Len()),
			zap.Error(err),
		)
		return nil
	}
	return t.issue(vtx)
}

// Send a request to [vdr] asking them to send us vertex [vtxID]
func (t *Transitive) sendRequest(nodeID ids.NodeID, vtxID ids.ID) {
	if t.outstandingVtxReqs.Contains(vtxID) {
		t.Ctx.Log.Debug("not sending request for vertex",
			zap.String("reason", "existing outstanding request"),
			zap.Stringer("vtxID", vtxID),
		)
		return
	}
	t.RequestID++
	t.outstandingVtxReqs.Add(nodeID, t.RequestID, vtxID) // Mark that there is an outstanding request for this vertex
	t.Sender.SendGet(nodeID, t.RequestID, vtxID)
	t.metrics.numVtxRequests.Set(float64(t.outstandingVtxReqs.Len())) // Tracks performance statistics
}

```

avalanchego/snow/engine/avalanche/transitive_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package avalanche

import (
	"bytes"
	"errors"
	"fmt"
	"testing"

	"github.com/prometheus/client_golang/prometheus"

	"github.com/stretchr/testify/require"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/choices"
	"github.com/ava-labs/avalanchego/snow/consensus/avalanche"
	"github.com/ava-labs/avalanchego/snow/consensus/snowball"
	"github.com/ava-labs/avalanchego/snow/consensus/snowstorm"
	"github.com/ava-labs/avalanchego/snow/engine/avalanche/bootstrap"
	"github.com/ava-labs/avalanchego/snow/engine/avalanche/vertex"
	"github.com/ava-labs/avalanchego/snow/engine/common"
	"github.com/ava-labs/avalanchego/snow/engine/common/tracker"
	"github.com/ava-labs/avalanchego/snow/validators"
	"github.com/ava-labs/avalanchego/utils"
	"github.com/ava-labs/avalanchego/utils/constants"
	"github.com/ava-labs/avalanchego/utils/wrappers"
	"github.com/ava-labs/avalanchego/version"

	avagetter "github.com/ava-labs/avalanchego/snow/engine/avalanche/getter"
)

var (
	errUnknownVertex = errors.New("unknown vertex")
	errFailedParsing = errors.New("failed parsing")
	errMissing       = errors.New("missing")
)

type dummyHandler struct {
	startEngineF func(startReqID uint32) error
}

func (dh *dummyHandler) onDoneBootstrapping(lastReqID uint32) error {
	lastReqID++
	return dh.startEngineF(lastReqID)
}

func TestEngineShutdown(t *testing.T) {
	_, _, engCfg := DefaultConfig()

	vmShutdownCalled := false
	vm := &vertex.TestVM{}
	vm.T = t
	vm.ShutdownF = func() error { vmShutdownCalled = true; return nil }
	engCfg.VM = vm

	transitive, err := newTransitive(engCfg)
	if err != nil {
		t.Fatal(err)
	}
	if err := transitive.Shutdown(); err != nil {
		t.Fatal(err)
	}
	if !vmShutdownCalled {
		t.Fatal("Shutting down the Transitive did not shutdown the VM")
	}
}

func TestEngineAdd(t *testing.T) {
	_, _, engCfg := DefaultConfig()

	vals := validators.NewSet()
	engCfg.Validators = vals

	vdr := ids.GenerateTestNodeID()
	if err := vals.AddWeight(vdr, 1); err != nil {
		t.Fatal(err)
	}

	sender := &common.SenderTest{T: t}
	engCfg.Sender = sender

	sender.Default(true)
	sender.CantSendGetAcceptedFrontier = false

	manager := vertex.NewTestManager(t)
	engCfg.Manager = manager

	manager.Default(true)

	manager.CantEdge = false

	te, err := newTransitive(engCfg)
	if err != nil {
		t.Fatal(err)
	}

	if err := te.Start(0); err != nil {
		t.Fatal(err)
	}

	if te.Ctx.ChainID != ids.Empty {
		t.Fatalf("Wrong chain ID")
	}

	vtx := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: []avalanche.Vertex{
			&avalanche.TestVertex{TestDecidable: choices.TestDecidable{
				IDV:     ids.GenerateTestID(),
				StatusV: choices.Unknown,
			}},
		},
		BytesV: []byte{1},
	}

	asked := new(bool)
	reqID := new(uint32)
	sender.SendGetF = func(inVdr ids.NodeID, requestID uint32, vtxID ids.ID) {
		*reqID = requestID
		if *asked {
			t.Fatalf("Asked multiple times")
		}
		*asked = true
		if vdr != inVdr {
			t.Fatalf("Asking wrong validator for vertex")
		}
		if vtx.ParentsV[0].ID() != vtxID {
			t.Fatalf("Asking for wrong vertex")
		}
	}

	manager.ParseVtxF = func(b []byte) (avalanche.Vertex, error) {
		if !bytes.Equal(b, vtx.Bytes()) {
			t.Fatalf("Wrong bytes")
		}
		return vtx, nil
	}

	if err := te.Put(vdr, 0, vtx.Bytes()); err != nil {
		t.Fatal(err)
	}

	manager.ParseVtxF = nil

	if !*asked {
		t.Fatalf("Didn't ask for a missing vertex")
	}

	if len(te.vtxBlocked) != 1 {
		t.Fatalf("Should have been blocking on request")
	}

	manager.ParseVtxF = func(b []byte) (avalanche.Vertex, error) { return nil, errFailedParsing }

	if err := te.Put(vdr, *reqID, nil); err != nil {
		t.Fatal(err)
	}

	manager.ParseVtxF = nil

	if len(te.vtxBlocked) != 0 {
		t.Fatalf("Should have finished blocking issue")
	}
}

func TestEngineQuery(t *testing.T) {
	_, _, engCfg := DefaultConfig()

	vals := validators.NewSet()
	engCfg.Validators = vals

	vdr := ids.GenerateTestNodeID()
	if err := vals.AddWeight(vdr, 1); err != nil {
		t.Fatal(err)
	}

	sender := &common.SenderTest{T: t}
	engCfg.Sender = sender

	sender.Default(true)
	sender.CantSendGetAcceptedFrontier = false

	manager := vertex.NewTestManager(t)
	engCfg.Manager = manager

	manager.Default(true)

	gVtx := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}
	mVtx := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}

	vts := []avalanche.Vertex{gVtx, mVtx}
	utxos := []ids.ID{ids.GenerateTestID()}

	tx0 := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Processing,
	}}
	tx0.InputIDsV = append(tx0.InputIDsV, utxos[0])

	vtx0 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx0},
		BytesV:   []byte{0, 1, 2, 3},
	}

	manager.EdgeF = func() []ids.ID { return []ids.ID{vts[0].ID(), vts[1].ID()} }
	manager.GetVtxF = func(id ids.ID) (avalanche.Vertex, error) {
		switch id {
		case gVtx.ID():
			return gVtx, nil
		case mVtx.ID():
			return mVtx, nil
		}

		t.Fatalf("Unknown vertex")
		panic("Should have errored")
	}

	te, err := newTransitive(engCfg)
	if err != nil {
		t.Fatal(err)
	}

	if err := te.Start(0); err != nil {
		t.Fatal(err)
	}

	vertexed := new(bool)
	manager.GetVtxF = func(vtxID ids.ID) (avalanche.Vertex, error) {
		if *vertexed {
			t.Fatalf("Sent multiple requests")
		}
		*vertexed = true
		if vtxID != vtx0.ID() {
			t.Fatalf("Wrong vertex requested")
		}
		return nil, errUnknownVertex
	}

	chitted := new(bool)
	sender.SendChitsF = func(inVdr ids.NodeID, _ uint32, prefs []ids.ID) {
		if *chitted {
			t.Fatalf("Sent multiple chits")
		}
		*chitted = true
		if len(prefs) != 2 {
			t.Fatalf("Wrong chits preferences")
		}
	}

	asked := new(bool)
	sender.SendGetF = func(inVdr ids.NodeID, _ uint32, vtxID ids.ID) {
		if *asked {
			t.Fatalf("Asked multiple times")
		}
		*asked = true
		if vdr != inVdr {
			t.Fatalf("Asking wrong validator for vertex")
		}
		if vtx0.ID() != vtxID {
			t.Fatalf("Asking for wrong vertex")
		}
	}

	// After receiving the pull query for [vtx0] we will first request [vtx0]
	// from the peer, because it is currently unknown to the engine.
	if err := te.PullQuery(vdr, 0, vtx0.ID()); err != nil {
		t.Fatal(err)
	}

	if !*vertexed {
		t.Fatalf("Didn't request vertex")
	}
	if !*asked {
		t.Fatalf("Didn't request vertex from validator")
	}

	queried := new(bool)
	queryRequestID := new(uint32)
	sender.SendPushQueryF = func(inVdrs ids.NodeIDSet, requestID uint32, vtx []byte) {
		if *queried {
			t.Fatalf("Asked multiple times")
		}
		*queried = true
		*queryRequestID = requestID
		vdrSet := ids.NodeIDSet{}
		vdrSet.Add(vdr)
		if !inVdrs.Equals(vdrSet) {
			t.Fatalf("Asking wrong validator for preference")
		}
		if !bytes.Equal(vtx0.Bytes(), vtx) {
			t.Fatalf("Asking for wrong vertex")
		}
	}

	manager.ParseVtxF = func(b []byte) (avalanche.Vertex, error) {
		if !bytes.Equal(b, vtx0.Bytes()) {
			t.Fatalf("Wrong bytes")
		}
		return vtx0, nil
	}

	// Once the peer returns [vtx0], we will respond to its query and then issue
	// our own push query for [vtx0].
	if err := te.Put(vdr, 0, vtx0.Bytes()); err != nil {
		t.Fatal(err)
	}
	manager.ParseVtxF = nil

	if !*queried {
		t.Fatalf("Didn't ask for preferences")
	}
	if !*chitted {
		t.Fatalf("Didn't provide preferences")
	}

	vtx1 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx0},
		BytesV:   []byte{5, 4, 3, 2, 1, 9},
	}

	manager.GetVtxF = func(vtxID ids.ID) (avalanche.Vertex, error) {
		if vtxID == vtx0.ID() {
			return &avalanche.TestVertex{
				TestDecidable: choices.TestDecidable{
					StatusV: choices.Unknown,
				},
			}, nil
		}
		if vtxID == vtx1.ID() {
			return nil, errUnknownVertex
		}
		t.Fatalf("Wrong vertex requested")
		panic("Should have failed")
	}

	*asked = false
	sender.SendGetF = func(inVdr ids.NodeID, _ uint32, vtxID ids.ID) {
		if *asked {
			t.Fatalf("Asked multiple times")
		}
		*asked = true
		if vdr != inVdr {
			t.Fatalf("Asking wrong validator for vertex")
		}
		if vtx1.ID() != vtxID {
			t.Fatalf("Asking for wrong vertex")
		}
	}

	// The peer returned [vtx1] from our query for [vtx0], which means we will
	// need to request the missing [vtx1].
	if err := te.Chits(vdr, *queryRequestID, []ids.ID{vtx1.ID()}); err != nil {
		t.Fatal(err)
	}

	*queried = false
	sender.SendPushQueryF = func(inVdrs ids.NodeIDSet, requestID uint32, vtx []byte) {
		if *queried {
			t.Fatalf("Asked multiple times")
		}
		*queried = true
		*queryRequestID = requestID
		vdrSet := ids.NodeIDSet{}
		vdrSet.Add(vdr)
		if !inVdrs.Equals(vdrSet) {
			t.Fatalf("Asking wrong validator for preference")
		}
		if !bytes.Equal(vtx1.Bytes(), vtx) {
			t.Fatalf("Asking for wrong vertex")
		}
	}

	manager.ParseVtxF = func(b []byte) (avalanche.Vertex, error) {
		if !bytes.Equal(b, vtx1.Bytes()) {
			t.Fatalf("Wrong bytes")
		}

		manager.GetVtxF = func(vtxID ids.ID) (avalanche.Vertex, error) {
			if vtxID == vtx0.ID() {
				return &avalanche.TestVertex{
					TestDecidable: choices.TestDecidable{
						StatusV: choices.Processing,
					},
				}, nil
			}
			if vtxID == vtx1.ID() {
				return vtx1, nil
			}
			t.Fatalf("Wrong vertex requested")
			panic("Should have failed")
		}

		return vtx1, nil
	}

	// Once the peer returns [vtx1], the poll that was issued for [vtx0] will be
	// able to terminate. Additionally the node will issue a push query with
	// [vtx1].
	if err := te.Put(vdr, 0, vtx1.Bytes()); err != nil {
		t.Fatal(err)
	}
	manager.ParseVtxF = nil

	// Because [vtx1] does not transitively reference [vtx0], the transaction
	// vertex for [vtx0] was never voted for. This results in [vtx0] still being
	// in processing.
	if vtx0.Status() != choices.Processing {
		t.Fatalf("Shouldn't have executed the vertex yet")
	}
	if vtx1.Status() != choices.Accepted {
		t.Fatalf("Should have executed the vertex")
	}
	if tx0.Status() != choices.Accepted {
		t.Fatalf("Should have executed the transaction")
	}

	// Make sure there is no memory leak for missing vertex tracking.
	if len(te.vtxBlocked) != 0 {
		t.Fatalf("Should have finished blocking")
	}

	sender.CantSendPullQuery = false

	// Abandon the query for [vtx1]. This will result in a re-query for [vtx0].
	if err := te.QueryFailed(vdr, *queryRequestID); err != nil {
		t.Fatal(err)
	}
	if len(te.vtxBlocked) != 0 {
		t.Fatalf("Should have finished blocking")
	}
}

func TestEngineMultipleQuery(t *testing.T) {
	_, _, engCfg := DefaultConfig()

	vals := validators.NewSet()
	engCfg.Validators = vals

	engCfg.Params = avalanche.Parameters{
		Parameters: snowball.Parameters{
			K:                       3,
			Alpha:                   2,
			BetaVirtuous:            1,
			BetaRogue:               2,
			ConcurrentRepolls:       1,
			OptimalProcessing:       100,
			MaxOutstandingItems:     1,
			MaxItemProcessingTime:   1,
			MixedQueryNumPushNonVdr: 3,
		},
		Parents:   2,
		BatchSize: 1,
	}

	vdr0 := ids.GenerateTestNodeID()
	vdr1 := ids.GenerateTestNodeID()
	vdr2 := ids.GenerateTestNodeID()

	errs := wrappers.Errs{}
	errs.Add(
		vals.AddWeight(vdr0, 1),
		vals.AddWeight(vdr1, 1),
		vals.AddWeight(vdr2, 1),
	)
	if errs.Errored() {
		t.Fatal(errs.Err)
	}

	sender := &common.SenderTest{T: t}
	engCfg.Sender = sender

	sender.Default(true)
	sender.CantSendGetAcceptedFrontier = false

	manager := vertex.NewTestManager(t)
	engCfg.Manager = manager

	gVtx := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}
	mVtx := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}

	vts := []avalanche.Vertex{gVtx, mVtx}
	utxos := []ids.ID{ids.GenerateTestID()}

	manager.EdgeF = func() []ids.ID { return []ids.ID{vts[0].ID(), vts[1].ID()} }
	manager.GetVtxF = func(id ids.ID) (avalanche.Vertex, error) {
		switch id {
		case gVtx.ID():
			return gVtx, nil
		case mVtx.ID():
			return mVtx, nil
		}
		t.Fatalf("Unknown vertex")
		panic("Should have errored")
	}

	tx0 := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Processing,
	}}
	tx0.InputIDsV = append(tx0.InputIDsV, utxos[0])

	vtx0 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx0},
	}

	te, err := newTransitive(engCfg)
	if err != nil {
		t.Fatal(err)
	}

	if err := te.Start(0); err != nil {
		t.Fatal(err)
	}

	queried := new(bool)
	queryRequestID := new(uint32)
	sender.SendPushQueryF = func(inVdrs ids.NodeIDSet, requestID uint32, vtx []byte) {
		if *queried {
			t.Fatalf("Asked multiple times")
		}
		*queried = true
		*queryRequestID = requestID
		vdrSet := ids.NodeIDSet{}
		vdrSet.Add(vdr0, vdr1, vdr2)
		if !inVdrs.Equals(vdrSet) {
			t.Fatalf("Asking wrong validator for preference")
		}
		if !bytes.Equal(vtx0.Bytes(), vtx) {
			t.Fatalf("Asking for wrong vertex")
		}
	}

	if err := te.issue(vtx0); err != nil {
		t.Fatal(err)
	}

	vtx1 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx0},
	}

	manager.GetVtxF = func(id ids.ID) (avalanche.Vertex, error) {
		switch id {
		case gVtx.ID():
			return gVtx, nil
		case mVtx.ID():
			return mVtx, nil
		case vtx0.ID():
			return vtx0, nil
		case vtx1.ID():
			return nil, errUnknownVertex
		}
		t.Fatalf("Unknown vertex")
		panic("Should have errored")
	}

	asked := new(bool)
	reqID := new(uint32)
	sender.SendGetF = func(inVdr ids.NodeID, requestID uint32, vtxID ids.ID) {
		*reqID = requestID
		if *asked {
			t.Fatalf("Asked multiple times")
		}
		*asked = true
		if vdr0 != inVdr {
			t.Fatalf("Asking wrong validator for vertex")
		}
		if vtx1.ID() != vtxID {
			t.Fatalf("Asking for wrong vertex")
		}
	}

	s0 := []ids.ID{vtx0.ID(), vtx1.ID()}

	s2 := []ids.ID{vtx0.ID()}

	if err := te.Chits(vdr0, *queryRequestID, s0); err != nil {
		t.Fatal(err)
	}
	if err := te.QueryFailed(vdr1, *queryRequestID); err != nil {
		t.Fatal(err)
	}
	if err := te.Chits(vdr2, *queryRequestID, s2); err != nil {
		t.Fatal(err)
	}

	// Should be dropped because the query was marked as failed
	if err := te.Chits(vdr1, *queryRequestID, s0); err != nil {
		t.Fatal(err)
	}

	if err := te.GetFailed(vdr0, *reqID); err != nil {
		t.Fatal(err)
	}

	if vtx0.Status() != choices.Accepted {
		t.Fatalf("Should have executed vertex")
	}
	if len(te.vtxBlocked) != 0 {
		t.Fatalf("Should have finished blocking")
	}
}

func TestEngineBlockedIssue(t *testing.T) {
	_, _, engCfg := DefaultConfig()

	vals := validators.NewSet()
	engCfg.Validators = vals

	vdr := ids.GenerateTestNodeID()
	if err := vals.AddWeight(vdr, 1); err != nil {
		t.Fatal(err)
	}

	manager := vertex.NewTestManager(t)
	engCfg.Manager = manager

	gVtx := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}
	mVtx := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}

	vts := []avalanche.Vertex{gVtx, mVtx}
	utxos := []ids.ID{ids.GenerateTestID()}

	tx0 := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Processing,
	}}
	tx0.InputIDsV = append(tx0.InputIDsV, utxos[0])

	vtx0 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx0},
	}

	vtx1 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: []avalanche.Vertex{
			&avalanche.TestVertex{TestDecidable: choices.TestDecidable{
				IDV:     vtx0.IDV,
				StatusV: choices.Unknown,
			}},
		},
		HeightV: 1,
		TxsV:    []snowstorm.Tx{tx0},
	}

	te, err := newTransitive(engCfg)
	if err != nil {
		t.Fatal(err)
	}

	if err := te.Start(0); err != nil {
		t.Fatal(err)
	}

	if err := te.issue(vtx1); err != nil {
		t.Fatal(err)
	}

	vtx1.ParentsV[0] = vtx0
	if err := te.issue(vtx0); err != nil {
		t.Fatal(err)
	}

	if prefs := te.Consensus.Preferences(); prefs.Len() != 1 || !prefs.Contains(vtx1.ID()) {
		t.Fatalf("Should have issued vtx1")
	}
}

func TestEngineAbandonResponse(t *testing.T) {
	_, _, engCfg := DefaultConfig()

	vals := validators.NewSet()
	engCfg.Validators = vals

	vdr := ids.GenerateTestNodeID()
	if err := vals.AddWeight(vdr, 1); err != nil {
		t.Fatal(err)
	}

	manager := vertex.NewTestManager(t)
	engCfg.Manager = manager

	sender := &common.SenderTest{T: t}
	engCfg.Sender = sender

	sender.Default(true)

	gVtx := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}
	mVtx := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}

	vts := []avalanche.Vertex{gVtx, mVtx}
	utxos := []ids.ID{ids.GenerateTestID()}

	tx0 := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Processing,
	}}
	tx0.InputIDsV = append(tx0.InputIDsV, utxos[0])

	vtx := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx0},
	}

	manager.GetVtxF = func(id ids.ID) (avalanche.Vertex, error) { return nil, errUnknownVertex }

	te, err := newTransitive(engCfg)
	if err != nil {
		t.Fatal(err)
	}

	if err := te.Start(0); err != nil {
		t.Fatal(err)
	}

	reqID := new(uint32)
	sender.SendGetF = func(vID ids.NodeID, requestID uint32, vtxID ids.ID) {
		*reqID = requestID
	}
	sender.CantSendChits = false

	if err := te.PullQuery(vdr, 0, vtx.ID()); err != nil {
		t.Fatal(err)
	}
	if err := te.GetFailed(vdr, *reqID); err != nil {
		t.Fatal(err)
	}

	if len(te.vtxBlocked) != 0 {
		t.Fatalf("Should have removed blocking event")
	}
}

func TestEngineScheduleRepoll(t *testing.T) {
	_, _, engCfg := DefaultConfig()

	vals := validators.NewSet()
	engCfg.Validators = vals

	vdr := ids.GenerateTestNodeID()
	if err := vals.AddWeight(vdr, 1); err != nil {
		t.Fatal(err)
	}

	gVtx := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}
	mVtx := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}

	vts := []avalanche.Vertex{gVtx, mVtx}
	utxos := []ids.ID{ids.GenerateTestID()}

	tx0 := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Processing,
	}}
	tx0.InputIDsV = append(tx0.InputIDsV, utxos[0])

	vtx := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx0},
	}

	manager := vertex.NewTestManager(t)
	engCfg.Manager = manager

	manager.Default(true)
	manager.CantEdge = false

	sender := &common.SenderTest{T: t}
	engCfg.Sender = sender

	sender.Default(true)
	sender.CantSendGetAcceptedFrontier = false

	te, err := newTransitive(engCfg)
	if err != nil {
		t.Fatal(err)
	}

	if err := te.Start(0); err != nil {
		t.Fatal(err)
	}

	requestID := new(uint32)
	sender.SendPushQueryF = func(_ ids.NodeIDSet, reqID uint32, _ []byte) {
		*requestID = reqID
	}

	if err := te.issue(vtx); err != nil {
		t.Fatal(err)
	}

	sender.SendPushQueryF = nil

	repolled := new(bool)
	sender.SendPullQueryF = func(_ ids.NodeIDSet, _ uint32, vtxID ids.ID) {
		*repolled = true
		if vtxID != vtx.ID() {
			t.Fatalf("Wrong vertex queried")
		}
	}

	if err := te.QueryFailed(vdr, *requestID); err != nil {
		t.Fatal(err)
	}

	if !*repolled {
		t.Fatalf("Should have issued a noop")
	}
}

func TestEngineRejectDoubleSpendTx(t *testing.T) {
	_, _, engCfg := DefaultConfig()

	engCfg.Params.BatchSize = 2

	sender := &common.SenderTest{T: t}
	engCfg.Sender = sender

	sender.Default(true)
	sender.CantSendGetAcceptedFrontier = false

	vals := validators.NewSet()
	engCfg.Validators = vals

	vdr := ids.GenerateTestNodeID()
	if err := vals.AddWeight(vdr, 1); err != nil {
		t.Fatal(err)
	}

	manager := vertex.NewTestManager(t)
	engCfg.Manager = manager
	manager.Default(true)

	vm := &vertex.TestVM{TestVM: common.TestVM{T: t}}
	engCfg.VM = vm
	vm.Default(true)

	gVtx := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}
	mVtx := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}

	gTx := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}

	utxos := []ids.ID{ids.GenerateTestID()}

	tx0 := &snowstorm.TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		DependenciesV: []snowstorm.Tx{gTx},
	}
	tx0.InputIDsV = append(tx0.InputIDsV, utxos[0])

	tx1 := &snowstorm.TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		DependenciesV: []snowstorm.Tx{gTx},
	}
	tx1.InputIDsV = append(tx1.InputIDsV, utxos[0])

	manager.EdgeF = func() []ids.ID { return []ids.ID{gVtx.ID(), mVtx.ID()} }
	manager.GetVtxF = func(id ids.ID) (avalanche.Vertex, error) {
		switch id {
		case gVtx.ID():
			return gVtx, nil
		case mVtx.ID():
			return mVtx, nil
		}
		t.Fatalf("Unknown vertex")
		panic("Should have errored")
	}
	manager.BuildVtxF = func(_ []ids.ID, txs []snowstorm.Tx) (avalanche.Vertex, error) {
		return &avalanche.TestVertex{
			TestDecidable: choices.TestDecidable{
				IDV:     ids.GenerateTestID(),
				StatusV: choices.Processing,
			},
			ParentsV: []avalanche.Vertex{gVtx, mVtx},
			HeightV:  1,
			TxsV:     txs,
			BytesV:   []byte{1},
		}, nil
	}

	vm.CantSetState = false
	te, err := newTransitive(engCfg)
	if err != nil {
		t.Fatal(err)
	}

	if err := te.Start(0); err != nil {
		t.Fatal(err)
	}

	vm.CantSetState = true
	sender.CantSendPushQuery = false
	vm.PendingTxsF = func() []snowstorm.Tx { return []snowstorm.Tx{tx0, tx1} }
	if err := te.Notify(common.PendingTxs); err != nil {
		t.Fatal(err)
	}
}

func TestEngineRejectDoubleSpendIssuedTx(t *testing.T) {
	_, _, engCfg := DefaultConfig()

	engCfg.Params.BatchSize = 2

	sender := &common.SenderTest{T: t}
	engCfg.Sender = sender
	sender.Default(true)
	sender.CantSendGetAcceptedFrontier = false

	vals := validators.NewSet()
	engCfg.Validators = vals

	vdr := ids.GenerateTestNodeID()
	if err := vals.AddWeight(vdr, 1); err != nil {
		t.Fatal(err)
	}

	manager := vertex.NewTestManager(t)
	engCfg.Manager = manager
	manager.Default(true)

	vm := &vertex.TestVM{TestVM: common.TestVM{T: t}}
	engCfg.VM = vm
	vm.Default(true)

	gVtx := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}
	mVtx := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}

	gTx := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}

	utxos := []ids.ID{ids.GenerateTestID()}

	tx0 := &snowstorm.TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		DependenciesV: []snowstorm.Tx{gTx},
	}
	tx0.InputIDsV = append(tx0.InputIDsV, utxos[0])

	tx1 := &snowstorm.TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		DependenciesV: []snowstorm.Tx{gTx},
	}
	tx1.InputIDsV = append(tx1.InputIDsV, utxos[0])

	manager.EdgeF = func() []ids.ID { return []ids.ID{gVtx.ID(), mVtx.ID()} }
	manager.GetVtxF = func(id ids.ID) (avalanche.Vertex, error) {
		switch id {
		case gVtx.ID():
			return gVtx, nil
		case mVtx.ID():
			return mVtx, nil
		}
		t.Fatalf("Unknown vertex")
		panic("Should have errored")
	}

	vm.CantSetState = false
	te, err := newTransitive(engCfg)
	if err != nil {
		t.Fatal(err)
	}

	if err := te.Start(0); err != nil {
		t.Fatal(err)
	}

	vm.CantSetState = true
	manager.BuildVtxF = func(_ []ids.ID, txs []snowstorm.Tx) (avalanche.Vertex, error) {
		return &avalanche.TestVertex{
			TestDecidable: choices.TestDecidable{
				IDV:     ids.GenerateTestID(),
				StatusV: choices.Processing,
			},
			ParentsV: []avalanche.Vertex{gVtx, mVtx},
			HeightV:  1,
			TxsV:     txs,
			BytesV:   []byte{1},
		}, nil
	}

	sender.CantSendPushQuery = false

	vm.PendingTxsF = func() []snowstorm.Tx { return []snowstorm.Tx{tx0} }
	if err := te.Notify(common.PendingTxs); err != nil {
		t.Fatal(err)
	}

	vm.PendingTxsF = func() []snowstorm.Tx { return []snowstorm.Tx{tx1} }
	if err := te.Notify(common.PendingTxs); err != nil {
		t.Fatal(err)
	}
}

func TestEngineIssueRepoll(t *testing.T) {
	_, _, engCfg := DefaultConfig()

	engCfg.Params.BatchSize = 2

	sender := &common.SenderTest{T: t}
	sender.Default(true)
	sender.CantSendGetAcceptedFrontier = false
	engCfg.Sender = sender

	vals := validators.NewSet()
	engCfg.Validators = vals

	vdr := ids.GenerateTestNodeID()
	if err := vals.AddWeight(vdr, 1); err != nil {
		t.Fatal(err)
	}

	manager := vertex.NewTestManager(t)
	manager.Default(true)
	engCfg.Manager = manager

	gVtx := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}
	mVtx := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}

	manager.EdgeF = func() []ids.ID { return []ids.ID{gVtx.ID(), mVtx.ID()} }
	manager.GetVtxF = func(id ids.ID) (avalanche.Vertex, error) {
		switch id {
		case gVtx.ID():
			return gVtx, nil
		case mVtx.ID():
			return mVtx, nil
		}
		t.Fatalf("Unknown vertex")
		panic("Should have errored")
	}

	te, err := newTransitive(engCfg)
	if err != nil {
		t.Fatal(err)
	}

	if err := te.Start(0); err != nil {
		t.Fatal(err)
	}

	sender.SendPullQueryF = func(vdrs ids.NodeIDSet, _ uint32, vtxID ids.ID) {
		vdrSet := ids.NodeIDSet{}
		vdrSet.Add(vdr)
		if !vdrs.Equals(vdrSet) {
			t.Fatalf("Wrong query recipients")
		}
		if vtxID != gVtx.ID() && vtxID != mVtx.ID() {
			t.Fatalf("Unknown re-query")
		}
	}

	te.repoll()
	if err := te.errs.Err; err != nil {
		t.Fatal(err)
	}
}

func TestEngineReissue(t *testing.T) {
	_, _, engCfg := DefaultConfig()

	engCfg.Params.BatchSize = 2
	engCfg.Params.BetaVirtuous = 5
	engCfg.Params.BetaRogue = 5

	sender := &common.SenderTest{T: t}
	sender.Default(true)
	sender.CantSendGetAcceptedFrontier = false
	engCfg.Sender = sender

	vals := validators.NewSet()
	engCfg.Validators = vals

	vdr := ids.GenerateTestNodeID()
	if err := vals.AddWeight(vdr, 1); err != nil {
		t.Fatal(err)
	}

	manager := vertex.NewTestManager(t)
	manager.Default(true)
	engCfg.Manager = manager

	vm := &vertex.TestVM{TestVM: common.TestVM{T: t}}
	vm.Default(true)
	engCfg.VM = vm

	gVtx := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}
	mVtx := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}

	gTx := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}

	utxos := []ids.ID{ids.GenerateTestID(), ids.GenerateTestID()}

	tx0 := &snowstorm.TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		DependenciesV: []snowstorm.Tx{gTx},
	}
	tx0.InputIDsV = append(tx0.InputIDsV, utxos[0])

	tx1 := &snowstorm.TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		DependenciesV: []snowstorm.Tx{gTx},
	}
	tx1.InputIDsV = append(tx1.InputIDsV, utxos[1])

	tx2 := &snowstorm.TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		DependenciesV: []snowstorm.Tx{gTx},
	}
	tx2.InputIDsV = append(tx2.InputIDsV, utxos[1])

	tx3 := &snowstorm.TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		DependenciesV: []snowstorm.Tx{gTx},
	}
	tx3.InputIDsV = append(tx3.InputIDsV, utxos[0])

	vtx := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: []avalanche.Vertex{gVtx, mVtx},
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx2},
		BytesV:   []byte{42},
	}

	manager.EdgeF = func() []ids.ID { return []ids.ID{gVtx.ID(), mVtx.ID()} }
	manager.GetVtxF = func(id ids.ID) (avalanche.Vertex, error) {
		switch id {
		case gVtx.ID():
			return gVtx, nil
		case mVtx.ID():
			return mVtx, nil
		case vtx.ID():
			return vtx, nil
		}
		t.Fatalf("Unknown vertex")
		panic("Should have errored")
	}

	vm.CantSetState = false
	te, err := newTransitive(engCfg)
	if err != nil {
		t.Fatal(err)
	}

	if err := te.Start(0); err != nil {
		t.Fatal(err)
	}

	vm.CantSetState = true
	lastVtx := new(avalanche.TestVertex)
	manager.BuildVtxF = func(_ []ids.ID, txs []snowstorm.Tx) (avalanche.Vertex, error) {
		lastVtx = &avalanche.TestVertex{
			TestDecidable: choices.TestDecidable{
				IDV:     ids.GenerateTestID(),
				StatusV: choices.Processing,
			},
			ParentsV: []avalanche.Vertex{gVtx, mVtx},
			HeightV:  1,
			TxsV:     txs,
			BytesV:   []byte{1},
		}
		return lastVtx, nil
	}

	vm.GetTxF = func(id ids.ID) (snowstorm.Tx, error) {
		if id != tx0.ID() {
			t.Fatalf("Wrong tx")
		}
		return tx0, nil
	}

	queryRequestID := new(uint32)
	sender.SendPushQueryF = func(_ ids.NodeIDSet, requestID uint32, _ []byte) {
		*queryRequestID = requestID
	}

	vm.PendingTxsF = func() []snowstorm.Tx { return []snowstorm.Tx{tx0, tx1} }
	if err := te.Notify(common.PendingTxs); err != nil {
		t.Fatal(err)
	}

	manager.ParseVtxF = func(b []byte) (avalanche.Vertex, error) {
		if !bytes.Equal(b, vtx.Bytes()) {
			t.Fatalf("Wrong bytes")
		}
		return vtx, nil
	}

	// must vote on the first poll for the second one to settle
	// *queryRequestID is 1
	if err := te.Chits(vdr, *queryRequestID, []ids.ID{vtx.ID()}); err != nil {
		t.Fatal(err)
	}

	if err := te.Put(vdr, 0, vtx.Bytes()); err != nil {
		t.Fatal(err)
	}
	manager.ParseVtxF = nil

	vm.PendingTxsF = func() []snowstorm.Tx { return []snowstorm.Tx{tx3} }
	if err := te.Notify(common.PendingTxs); err != nil {
		t.Fatal(err)
	}

	// vote on second poll, *queryRequestID is 2
	if err := te.Chits(vdr, *queryRequestID, []ids.ID{vtx.ID()}); err != nil {
		t.Fatal(err)
	}

	// all polls settled

	if len(lastVtx.TxsV) != 1 || lastVtx.TxsV[0].ID() != tx0.ID() {
		t.Fatalf("Should have re-issued the tx")
	}
}

func TestEngineLargeIssue(t *testing.T) {
	_, _, engCfg := DefaultConfig()
	engCfg.Params.BatchSize = 1
	engCfg.Params.BetaVirtuous = 5
	engCfg.Params.BetaRogue = 5

	sender := &common.SenderTest{T: t}
	sender.Default(true)
	sender.CantSendGetAcceptedFrontier = false
	engCfg.Sender = sender

	vals := validators.NewSet()
	engCfg.Validators = vals

	vdr := ids.GenerateTestNodeID()
	if err := vals.AddWeight(vdr, 1); err != nil {
		t.Fatal(err)
	}

	manager := vertex.NewTestManager(t)
	manager.Default(true)
	engCfg.Manager = manager

	vm := &vertex.TestVM{TestVM: common.TestVM{T: t}}
	vm.Default(true)
	engCfg.VM = vm

	gVtx := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}
	mVtx := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}

	gTx := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}

	utxos := []ids.ID{ids.GenerateTestID(), ids.GenerateTestID()}

	tx0 := &snowstorm.TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		DependenciesV: []snowstorm.Tx{gTx},
	}
	tx0.InputIDsV = append(tx0.InputIDsV, utxos[0])

	tx1 := &snowstorm.TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		DependenciesV: []snowstorm.Tx{gTx},
	}
	tx1.InputIDsV = append(tx1.InputIDsV, utxos[1])

	manager.EdgeF = func() []ids.ID { return []ids.ID{gVtx.ID(), mVtx.ID()} }
	manager.GetVtxF = func(id ids.ID) (avalanche.Vertex, error) {
		switch id {
		case gVtx.ID():
			return gVtx, nil
		case mVtx.ID():
			return mVtx, nil
		}
		t.Fatalf("Unknown vertex")
		panic("Should have errored")
	}

	vm.CantSetState = false
	te, err := newTransitive(engCfg)
	if err != nil {
		t.Fatal(err)
	}

	if err := te.Start(0); err != nil {
		t.Fatal(err)
	}

	vm.CantSetState = true
	lastVtx := new(avalanche.TestVertex)
	manager.BuildVtxF = func(_ []ids.ID, txs []snowstorm.Tx) (avalanche.Vertex, error) {
		lastVtx = &avalanche.TestVertex{
			TestDecidable: choices.TestDecidable{
				IDV:     ids.GenerateTestID(),
				StatusV: choices.Processing,
			},
			ParentsV: []avalanche.Vertex{gVtx, mVtx},
			HeightV:  1,
			TxsV:     txs,
			BytesV:   []byte{1},
		}
		return lastVtx, nil
	}

	sender.CantSendPushQuery = false

	vm.PendingTxsF = func() []snowstorm.Tx { return []snowstorm.Tx{tx0, tx1} }
	if err := te.Notify(common.PendingTxs); err != nil {
		t.Fatal(err)
	}

	if len(lastVtx.TxsV) != 1 || lastVtx.TxsV[0].ID() != tx1.ID() {
		t.Fatalf("Should have issued txs differently")
	}
}

func TestEngineGetVertex(t *testing.T) {
	commonCfg, _, engCfg := DefaultConfig()

	sender := &common.SenderTest{T: t}
	sender.Default(true)
	sender.CantSendGetAcceptedFrontier = false
	engCfg.Sender = sender

	vdr := validators.GenerateRandomValidator(1)

	manager := vertex.NewTestManager(t)
	manager.Default(true)
	engCfg.Manager = manager
	avaGetHandler, err := avagetter.New(manager, commonCfg)
	if err != nil {
		t.Fatal(err)
	}
	engCfg.AllGetsServer = avaGetHandler

	gVtx := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}
	mVtx := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}

	manager.EdgeF = func() []ids.ID { return []ids.ID{gVtx.ID(), mVtx.ID()} }
	manager.GetVtxF = func(id ids.ID) (avalanche.Vertex, error) {
		switch id {
		case gVtx.ID():
			return gVtx, nil
		case mVtx.ID():
			return mVtx, nil
		}
		t.Fatalf("Unknown vertex")
		panic("Should have errored")
	}

	te, err := newTransitive(engCfg)
	if err != nil {
		t.Fatal(err)
	}

	if err := te.Start(0); err != nil {
		t.Fatal(err)
	}

	sender.SendPutF = func(v ids.NodeID, _ uint32, vtx []byte) {
		if v != vdr.ID() {
			t.Fatalf("Wrong validator")
		}
		if !bytes.Equal(mVtx.Bytes(), vtx) {
			t.Fatalf("Wrong vertex")
		}
	}

	if err := te.Get(vdr.ID(), 0, mVtx.ID()); err != nil {
		t.Fatal(err)
	}
}

func TestEngineInsufficientValidators(t *testing.T) {
	_, _, engCfg := DefaultConfig()

	vals := validators.NewSet()
	engCfg.Validators = vals

	sender := &common.SenderTest{T: t}
	sender.Default(true)
	sender.CantSendGetAcceptedFrontier = false
	engCfg.Sender = sender

	manager := vertex.NewTestManager(t)
	manager.Default(true)
	engCfg.Manager = manager

	gVtx := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}
	mVtx := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}

	vts := []avalanche.Vertex{gVtx, mVtx}

	vtx := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		BytesV:   []byte{0, 1, 2, 3},
	}

	manager.EdgeF = func() []ids.ID { return []ids.ID{vts[0].ID(), vts[1].ID()} }
	manager.GetVtxF = func(id ids.ID) (avalanche.Vertex, error) {
		switch id {
		case gVtx.ID():
			return gVtx, nil
		case mVtx.ID():
			return mVtx, nil
		}
		t.Fatalf("Unknown vertex")
		panic("Should have errored")
	}

	te, err := newTransitive(engCfg)
	if err != nil {
		t.Fatal(err)
	}

	if err := te.Start(0); err != nil {
		t.Fatal(err)
	}

	queried := new(bool)
	sender.SendPushQueryF = func(ids.NodeIDSet, uint32, []byte) {
		*queried = true
	}

	if err := te.issue(vtx); err != nil {
		t.Fatal(err)
	}

	if *queried {
		t.Fatalf("Unknown query")
	}
}

func TestEnginePushGossip(t *testing.T) {
	_, _, engCfg := DefaultConfig()

	vals := validators.NewSet()
	engCfg.Validators = vals

	vdr := ids.GenerateTestNodeID()
	if err := vals.AddWeight(vdr, 1); err != nil {
		t.Fatal(err)
	}

	sender := &common.SenderTest{T: t}
	sender.Default(true)
	sender.CantSendGetAcceptedFrontier = false
	engCfg.Sender = sender

	manager := vertex.NewTestManager(t)
	manager.Default(true)
	engCfg.Manager = manager

	gVtx := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}
	mVtx := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}

	vts := []avalanche.Vertex{gVtx, mVtx}

	vtx := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		BytesV:   []byte{0, 1, 2, 3},
	}

	manager.EdgeF = func() []ids.ID { return []ids.ID{vts[0].ID(), vts[1].ID()} }
	manager.GetVtxF = func(id ids.ID) (avalanche.Vertex, error) {
		switch id {
		case gVtx.ID():
			return gVtx, nil
		case mVtx.ID():
			return mVtx, nil
		case vtx.ID():
			return vtx, nil
		}
		t.Fatalf("Unknown vertex")
		panic("Should have errored")
	}

	te, err := newTransitive(engCfg)
	if err != nil {
		t.Fatal(err)
	}

	if err := te.Start(0); err != nil {
		t.Fatal(err)
	}

	requested := new(bool)
	sender.SendGetF = func(vdr ids.NodeID, _ uint32, vtxID ids.ID) {
		*requested = true
	}

	manager.ParseVtxF = func(b []byte) (avalanche.Vertex, error) {
		if bytes.Equal(b, vtx.BytesV) {
			return vtx, nil
		}
		t.Fatalf("Unknown vertex bytes")
		panic("Should have errored")
	}

	sender.CantSendPushQuery = false
	sender.CantSendChits = false
	if err := te.PushQuery(vdr, 0, vtx.Bytes()); err != nil {
		t.Fatal(err)
	}

	if *requested {
		t.Fatalf("Shouldn't have requested the vertex")
	}
}

func TestEngineSingleQuery(t *testing.T) {
	_, _, engCfg := DefaultConfig()

	vals := validators.NewSet()
	engCfg.Validators = vals

	vdr := ids.GenerateTestNodeID()
	if err := vals.AddWeight(vdr, 1); err != nil {
		t.Fatal(err)
	}

	sender := &common.SenderTest{T: t}
	sender.Default(true)
	sender.CantSendGetAcceptedFrontier = false
	engCfg.Sender = sender

	manager := vertex.NewTestManager(t)
	manager.Default(true)
	engCfg.Manager = manager

	gVtx := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}
	mVtx := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}

	vts := []avalanche.Vertex{gVtx, mVtx}

	vtx := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		BytesV:   []byte{0, 1, 2, 3},
	}

	manager.EdgeF = func() []ids.ID { return []ids.ID{vts[0].ID(), vts[1].ID()} }
	manager.GetVtxF = func(id ids.ID) (avalanche.Vertex, error) {
		switch id {
		case gVtx.ID():
			return gVtx, nil
		case mVtx.ID():
			return mVtx, nil
		case vtx.ID():
			return vtx, nil
		}
		t.Fatalf("Unknown vertex")
		panic("Should have errored")
	}

	te, err := newTransitive(engCfg)
	if err != nil {
		t.Fatal(err)
	}

	if err := te.Start(0); err != nil {
		t.Fatal(err)
	}

	sender.CantSendPushQuery = false
	sender.CantSendPullQuery = false

	if err := te.issue(vtx); err != nil {
		t.Fatal(err)
	}
}

func TestEngineParentBlockingInsert(t *testing.T) {
	_, _, engCfg := DefaultConfig()

	vals := validators.NewSet()
	engCfg.Validators = vals

	vdr := ids.GenerateTestNodeID()
	if err := vals.AddWeight(vdr, 1); err != nil {
		t.Fatal(err)
	}

	sender := &common.SenderTest{T: t}
	sender.Default(true)
	sender.CantSendGetAcceptedFrontier = false
	engCfg.Sender = sender

	manager := vertex.NewTestManager(t)
	manager.Default(true)
	engCfg.Manager = manager

	gVtx := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}
	mVtx := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}

	vts := []avalanche.Vertex{gVtx, mVtx}

	missingVtx := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Unknown,
		},
		ParentsV: vts,
		HeightV:  1,
		BytesV:   []byte{0, 1, 2, 3},
	}

	parentVtx := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: []avalanche.Vertex{missingVtx},
		HeightV:  2,
		BytesV:   []byte{0, 1, 2, 3},
	}

	blockingVtx := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: []avalanche.Vertex{parentVtx},
		HeightV:  3,
		BytesV:   []byte{0, 1, 2, 3},
	}

	manager.EdgeF = func() []ids.ID { return []ids.ID{vts[0].ID(), vts[1].ID()} }
	manager.GetVtxF = func(id ids.ID) (avalanche.Vertex, error) {
		switch id {
		case gVtx.ID():
			return gVtx, nil
		case mVtx.ID():
			return mVtx, nil
		}
		t.Fatalf("Unknown vertex")
		panic("Should have errored")
	}

	te, err := newTransitive(engCfg)
	if err != nil {
		t.Fatal(err)
	}

	if err := te.Start(0); err != nil {
		t.Fatal(err)
	}

	if err := te.issue(parentVtx); err != nil {
		t.Fatal(err)
	}
	if err := te.issue(blockingVtx); err != nil {
		t.Fatal(err)
	}

	if len(te.vtxBlocked) != 2 {
		t.Fatalf("Both inserts should be blocking")
	}

	sender.CantSendPushQuery = false

	missingVtx.StatusV = choices.Processing
	if err := te.issue(missingVtx); err != nil {
		t.Fatal(err)
	}

	if len(te.vtxBlocked) != 0 {
		t.Fatalf("Both inserts should not longer be blocking")
	}
}

func TestEngineBlockingChitRequest(t *testing.T) {
	_, _, engCfg := DefaultConfig()

	vals := validators.NewSet()
	engCfg.Validators = vals

	vdr := ids.GenerateTestNodeID()
	if err := vals.AddWeight(vdr, 1); err != nil {
		t.Fatal(err)
	}

	sender := &common.SenderTest{T: t}
	sender.Default(true)
	sender.CantSendGetAcceptedFrontier = false
	engCfg.Sender = sender

	manager := vertex.NewTestManager(t)
	manager.Default(true)
	engCfg.Manager = manager

	gVtx := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}
	mVtx := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}

	vts := []avalanche.Vertex{gVtx, mVtx}

	missingVtx := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Unknown,
		},
		ParentsV: vts,
		HeightV:  1,
		BytesV:   []byte{0, 1, 2, 3},
	}

	parentVtx := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: []avalanche.Vertex{missingVtx},
		HeightV:  2,
		BytesV:   []byte{1, 1, 2, 3},
	}

	blockingVtx := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: []avalanche.Vertex{parentVtx},
		HeightV:  3,
		BytesV:   []byte{2, 1, 2, 3},
	}

	manager.EdgeF = func() []ids.ID { return []ids.ID{vts[0].ID(), vts[1].ID()} }
	manager.GetVtxF = func(id ids.ID) (avalanche.Vertex, error) {
		switch id {
		case gVtx.ID():
			return gVtx, nil
		case mVtx.ID():
			return mVtx, nil
		}
		t.Fatalf("Unknown vertex")
		panic("Should have errored")
	}

	te, err := newTransitive(engCfg)
	if err != nil {
		t.Fatal(err)
	}

	if err := te.Start(0); err != nil {
		t.Fatal(err)
	}

	if err := te.issue(parentVtx); err != nil {
		t.Fatal(err)
	}

	manager.GetVtxF = func(vtxID ids.ID) (avalanche.Vertex, error) {
		if vtxID == blockingVtx.ID() {
			return blockingVtx, nil
		}
		t.Fatalf("Unknown vertex")
		panic("Should have errored")
	}
	manager.ParseVtxF = func(b []byte) (avalanche.Vertex, error) {
		if bytes.Equal(b, blockingVtx.Bytes()) {
			return blockingVtx, nil
		}
		t.Fatalf("Unknown vertex")
		panic("Should have errored")
	}
	sender.CantSendChits = false

	if err := te.PushQuery(vdr, 0, blockingVtx.Bytes()); err != nil {
		t.Fatal(err)
	}

	if len(te.vtxBlocked) != 2 {
		t.Fatalf("Both inserts should be blocking")
	}

	sender.CantSendPushQuery = false

	missingVtx.StatusV = choices.Processing
	if err := te.issue(missingVtx); err != nil {
		t.Fatal(err)
	}

	if len(te.vtxBlocked) != 0 {
		t.Fatalf("Both inserts should not longer be blocking")
	}
}

func TestEngineBlockingChitResponse(t *testing.T) {
	_, _, engCfg := DefaultConfig()

	vals := validators.NewSet()
	engCfg.Validators = vals

	vdr := ids.GenerateTestNodeID()
	if err := vals.AddWeight(vdr, 1); err != nil {
		t.Fatal(err)
	}

	sender := &common.SenderTest{T: t}
	sender.Default(true)
	sender.CantSendGetAcceptedFrontier = false
	engCfg.Sender = sender

	manager := vertex.NewTestManager(t)
	manager.Default(true)
	engCfg.Manager = manager

	gVtx := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}
	mVtx := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}

	vts := []avalanche.Vertex{gVtx, mVtx}

	issuedVtx := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		BytesV:   []byte{0, 1, 2, 3},
	}

	missingVtx := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Unknown,
		},
		ParentsV: vts,
		HeightV:  1,
		BytesV:   []byte{1, 1, 2, 3},
	}

	blockingVtx := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: []avalanche.Vertex{missingVtx},
		HeightV:  2,
		BytesV:   []byte{2, 1, 2, 3},
	}

	manager.EdgeF = func() []ids.ID { return []ids.ID{vts[0].ID(), vts[1].ID()} }
	manager.GetVtxF = func(id ids.ID) (avalanche.Vertex, error) {
		switch id {
		case gVtx.ID():
			return gVtx, nil
		case mVtx.ID():
			return mVtx, nil
		}
		t.Fatalf("Unknown vertex")
		panic("Should have errored")
	}

	te, err := newTransitive(engCfg)
	if err != nil {
		t.Fatal(err)
	}

	if err := te.Start(0); err != nil {
		t.Fatal(err)
	}

	if err := te.issue(blockingVtx); err != nil {
		t.Fatal(err)
	}

	queryRequestID := new(uint32)
	sender.SendPushQueryF = func(inVdrs ids.NodeIDSet, requestID uint32, vtx []byte) {
		*queryRequestID = requestID
		vdrSet := ids.NodeIDSet{}
		vdrSet.Add(vdr)
		if !inVdrs.Equals(vdrSet) {
			t.Fatalf("Asking wrong validator for preference")
		}
		if !bytes.Equal(issuedVtx.Bytes(), vtx) {
			t.Fatalf("Asking for wrong vertex")
		}
	}

	if err := te.issue(issuedVtx); err != nil {
		t.Fatal(err)
	}

	manager.GetVtxF = func(id ids.ID) (avalanche.Vertex, error) {
		if id == blockingVtx.ID() {
			return blockingVtx, nil
		}
		t.Fatalf("Unknown vertex")
		panic("Should have errored")
	}

	if err := te.Chits(vdr, *queryRequestID, []ids.ID{blockingVtx.ID()}); err != nil {
		t.Fatal(err)
	}

	if len(te.vtxBlocked) != 2 {
		t.Fatalf("The insert should be blocking, as well as the chit response")
	}

	sender.SendPushQueryF = nil
	sender.CantSendPushQuery = false
	sender.CantSendChits = false

	missingVtx.StatusV = choices.Processing
	if err := te.issue(missingVtx); err != nil {
		t.Fatal(err)
	}

	if len(te.vtxBlocked) != 0 {
		t.Fatalf("Both inserts should not longer be blocking")
	}
}

func TestEngineMissingTx(t *testing.T) {
	_, _, engCfg := DefaultConfig()

	vals := validators.NewSet()
	engCfg.Validators = vals

	vdr := ids.GenerateTestNodeID()
	if err := vals.AddWeight(vdr, 1); err != nil {
		t.Fatal(err)
	}

	sender := &common.SenderTest{T: t}
	sender.Default(true)
	sender.CantSendGetAcceptedFrontier = false
	engCfg.Sender = sender

	manager := vertex.NewTestManager(t)
	manager.Default(true)
	engCfg.Manager = manager

	gVtx := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}
	mVtx := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}

	vts := []avalanche.Vertex{gVtx, mVtx}

	issuedVtx := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		BytesV:   []byte{0, 1, 2, 3},
	}

	missingVtx := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Unknown,
		},
		ParentsV: vts,
		HeightV:  1,
		BytesV:   []byte{1, 1, 2, 3},
	}

	blockingVtx := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: []avalanche.Vertex{missingVtx},
		HeightV:  2,
		BytesV:   []byte{2, 1, 2, 3},
	}

	manager.EdgeF = func() []ids.ID { return []ids.ID{vts[0].ID(), vts[1].ID()} }
	manager.GetVtxF = func(id ids.ID) (avalanche.Vertex, error) {
		switch id {
		case gVtx.ID():
			return gVtx, nil
		case mVtx.ID():
			return mVtx, nil
		}
		t.Fatalf("Unknown vertex")
		panic("Should have errored")
	}

	te, err := newTransitive(engCfg)
	if err != nil {
		t.Fatal(err)
	}

	if err := te.Start(0); err != nil {
		t.Fatal(err)
	}

	if err := te.issue(blockingVtx); err != nil {
		t.Fatal(err)
	}

	queryRequestID := new(uint32)
	sender.SendPushQueryF = func(inVdrs ids.NodeIDSet, requestID uint32, vtx []byte) {
		*queryRequestID = requestID
		vdrSet := ids.NodeIDSet{}
		vdrSet.Add(vdr)
		if !inVdrs.Equals(vdrSet) {
			t.Fatalf("Asking wrong validator for preference")
		}
		if !bytes.Equal(issuedVtx.Bytes(), vtx) {
			t.Fatalf("Asking for wrong vertex")
		}
	}

	if err := te.issue(issuedVtx); err != nil {
		t.Fatal(err)
	}

	manager.GetVtxF = func(id ids.ID) (avalanche.Vertex, error) {
		if id == blockingVtx.ID() {
			return blockingVtx, nil
		}
		t.Fatalf("Unknown vertex")
		panic("Should have errored")
	}

	if err := te.Chits(vdr, *queryRequestID, []ids.ID{blockingVtx.ID()}); err != nil {
		t.Fatal(err)
	}

	if len(te.vtxBlocked) != 2 {
		t.Fatalf("The insert should be blocking, as well as the chit response")
	}

	sender.SendPushQueryF = nil
	sender.CantSendPushQuery = false
	sender.CantSendChits = false

	missingVtx.StatusV = choices.Processing
	if err := te.issue(missingVtx); err != nil {
		t.Fatal(err)
	}

	if len(te.vtxBlocked) != 0 {
		t.Fatalf("Both inserts should not longer be blocking")
	}
}

func TestEngineIssueBlockingTx(t *testing.T) {
	_, _, engCfg := DefaultConfig()

	vals := validators.NewSet()
	engCfg.Validators = vals

	vdr := ids.GenerateTestNodeID()
	if err := vals.AddWeight(vdr, 1); err != nil {
		t.Fatal(err)
	}

	manager := vertex.NewTestManager(t)
	engCfg.Manager = manager

	gVtx := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}

	vts := []avalanche.Vertex{gVtx}
	utxos := []ids.ID{ids.GenerateTestID(), ids.GenerateTestID()}

	tx0 := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Processing,
	}}
	tx0.InputIDsV = append(tx0.InputIDsV, utxos[0])

	tx1 := &snowstorm.TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		DependenciesV: []snowstorm.Tx{tx0},
	}
	tx1.InputIDsV = append(tx1.InputIDsV, utxos[1])

	vtx := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx0, tx1},
	}

	te, err := newTransitive(engCfg)
	if err != nil {
		t.Fatal(err)
	}

	if err := te.Start(0); err != nil {
		t.Fatal(err)
	}

	if err := te.issue(vtx); err != nil {
		t.Fatal(err)
	}

	if prefs := te.Consensus.Preferences(); !prefs.Contains(vtx.ID()) {
		t.Fatalf("Vertex should be preferred")
	}
}

func TestEngineReissueAbortedVertex(t *testing.T) {
	_, _, engCfg := DefaultConfig()

	vals := validators.NewSet()
	engCfg.Validators = vals

	vdr := ids.GenerateTestNodeID()
	if err := vals.AddWeight(vdr, 1); err != nil {
		t.Fatal(err)
	}

	sender := &common.SenderTest{T: t}
	sender.Default(true)
	sender.CantSendGetAcceptedFrontier = false
	engCfg.Sender = sender

	manager := vertex.NewTestManager(t)
	manager.Default(true)
	engCfg.Manager = manager

	gVtx := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}

	vts := []avalanche.Vertex{gVtx}

	vtxID0 := ids.GenerateTestID()
	vtxID1 := ids.GenerateTestID()

	vtxBytes0 := []byte{0}
	vtxBytes1 := []byte{1}

	vtx0 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     vtxID0,
			StatusV: choices.Unknown,
		},
		ParentsV: vts,
		HeightV:  1,
		BytesV:   vtxBytes0,
	}
	vtx1 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     vtxID1,
			StatusV: choices.Processing,
		},
		ParentsV: []avalanche.Vertex{vtx0},
		HeightV:  2,
		BytesV:   vtxBytes1,
	}

	manager.EdgeF = func() []ids.ID {
		return []ids.ID{gVtx.ID()}
	}

	manager.GetVtxF = func(vtxID ids.ID) (avalanche.Vertex, error) {
		if vtxID == gVtx.ID() {
			return gVtx, nil
		}
		t.Fatalf("Unknown vertex requested")
		panic("Unknown vertex requested")
	}

	te, err := newTransitive(engCfg)
	if err != nil {
		t.Fatal(err)
	}

	if err := te.Start(0); err != nil {
		t.Fatal(err)
	}

	manager.EdgeF = nil
	manager.GetVtxF = nil

	requestID := new(uint32)
	sender.SendGetF = func(vID ids.NodeID, reqID uint32, vtxID ids.ID) {
		*requestID = reqID
	}
	sender.CantSendChits = false
	manager.ParseVtxF = func(b []byte) (avalanche.Vertex, error) {
		if bytes.Equal(b, vtxBytes1) {
			return vtx1, nil
		}
		t.Fatalf("Unknown bytes provided")
		panic("Unknown bytes provided")
	}
	manager.GetVtxF = func(vtxID ids.ID) (avalanche.Vertex, error) {
		if vtxID == vtxID1 {
			return vtx1, nil
		}
		t.Fatalf("Unknown bytes provided")
		panic("Unknown bytes provided")
	}

	if err := te.PushQuery(vdr, 0, vtx1.Bytes()); err != nil {
		t.Fatal(err)
	}

	sender.SendGetF = nil
	manager.ParseVtxF = nil

	if err := te.GetFailed(vdr, *requestID); err != nil {
		t.Fatal(err)
	}

	requested := new(bool)
	sender.SendGetF = func(_ ids.NodeID, _ uint32, vtxID ids.ID) {
		if vtxID == vtxID0 {
			*requested = true
		}
	}
	manager.GetVtxF = func(vtxID ids.ID) (avalanche.Vertex, error) {
		if vtxID == vtxID1 {
			return vtx1, nil
		}
		t.Fatalf("Unknown bytes provided")
		panic("Unknown bytes provided")
	}

	if err := te.PullQuery(vdr, 0, vtxID1); err != nil {
		t.Fatal(err)
	}

	if !*requested {
		t.Fatalf("Should have requested the missing vertex")
	}
}

func TestEngineBootstrappingIntoConsensus(t *testing.T) {
	_, bootCfg, engCfg := DefaultConfig()

	vals := validators.NewSet()
	vdr := ids.GenerateTestNodeID()
	if err := vals.AddWeight(vdr, 1); err != nil {
		t.Fatal(err)
	}

	peers := tracker.NewPeers()
	startup := tracker.NewStartup(peers, 0)
	vals.RegisterCallbackListener(startup)

	bootCfg.Beacons = vals
	bootCfg.Validators = vals
	bootCfg.StartupTracker = startup
	engCfg.Validators = vals

	bootCfg.SampleK = vals.Len()

	sender := &common.SenderTest{T: t}
	sender.Default(true)
	bootCfg.Sender = sender
	engCfg.Sender = sender

	manager := vertex.NewTestManager(t)
	manager.Default(true)
	bootCfg.Manager = manager
	engCfg.Manager = manager

	vm := &vertex.TestVM{TestVM: common.TestVM{T: t}}
	vm.Default(true)
	bootCfg.VM = vm
	engCfg.VM = vm

	vm.CantSetState = false
	vm.CantConnected = false

	utxos := []ids.ID{ids.GenerateTestID(), ids.GenerateTestID()}

	txID0 := ids.GenerateTestID()
	txID1 := ids.GenerateTestID()

	txBytes0 := []byte{0}
	txBytes1 := []byte{1}

	tx0 := &snowstorm.TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     txID0,
			StatusV: choices.Processing,
		},
		BytesV: txBytes0,
	}
	tx0.InputIDsV = append(tx0.InputIDsV, utxos[0])

	tx1 := &snowstorm.TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     txID1,
			StatusV: choices.Processing,
		},
		DependenciesV: []snowstorm.Tx{tx0},
		BytesV:        txBytes1,
	}
	tx1.InputIDsV = append(tx1.InputIDsV, utxos[1])

	vtxID0 := ids.GenerateTestID()
	vtxID1 := ids.GenerateTestID()

	vtxBytes0 := []byte{2}
	vtxBytes1 := []byte{3}

	vtx0 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     vtxID0,
			StatusV: choices.Processing,
		},
		HeightV: 1,
		TxsV:    []snowstorm.Tx{tx0},
		BytesV:  vtxBytes0,
	}
	vtx1 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     vtxID1,
			StatusV: choices.Processing,
		},
		ParentsV: []avalanche.Vertex{vtx0},
		HeightV:  2,
		TxsV:     []snowstorm.Tx{tx1},
		BytesV:   vtxBytes1,
	}

	requested := new(bool)
	requestID := new(uint32)
	sender.SendGetAcceptedFrontierF = func(vdrs ids.NodeIDSet, reqID uint32) {
		if vdrs.Len() != 1 {
			t.Fatalf("Should have requested from the validators")
		}
		if !vdrs.Contains(vdr) {
			t.Fatalf("Should have requested from %s", vdr)
		}
		*requested = true
		*requestID = reqID
	}

	dh := &dummyHandler{}
	bootstrapper, err := bootstrap.New(
		bootCfg,
		dh.onDoneBootstrapping,
	)
	if err != nil {
		t.Fatal(err)
	}

	te, err := newTransitive(engCfg)
	if err != nil {
		t.Fatal(err)
	}
	dh.startEngineF = te.Start

	if err := bootstrapper.Start(0); err != nil {
		t.Fatal(err)
	}

	if err := bootstrapper.Connected(vdr, version.CurrentApp); err != nil {
		t.Fatal(err)
	}

	sender.SendGetAcceptedFrontierF = nil

	if !*requested {
		t.Fatalf("Should have requested from the validators during Initialize")
	}

	acceptedFrontier := []ids.ID{vtxID0}

	*requested = false
	sender.SendGetAcceptedF = func(vdrs ids.NodeIDSet, reqID uint32, proposedAccepted []ids.ID) {
		if vdrs.Len() != 1 {
			t.Fatalf("Should have requested from the validators")
		}
		if !vdrs.Contains(vdr) {
			t.Fatalf("Should have requested from %s", vdr)
		}
		if !ids.Equals(acceptedFrontier, proposedAccepted) {
			t.Fatalf("Wrong proposedAccepted vertices.\nExpected: %s\nGot: %s", acceptedFrontier, proposedAccepted)
		}
		*requested = true
		*requestID = reqID
	}

	if err := bootstrapper.AcceptedFrontier(vdr, *requestID, acceptedFrontier); err != nil {
		t.Fatal(err)
	}

	if !*requested {
		t.Fatalf("Should have requested from the validators during AcceptedFrontier")
	}

	manager.GetVtxF = func(vtxID ids.ID) (avalanche.Vertex, error) {
		if vtxID == vtxID0 {
			return nil, errMissing
		}
		t.Fatalf("Unknown vertex requested")
		panic("Unknown vertex requested")
	}

	sender.SendGetAncestorsF = func(inVdr ids.NodeID, reqID uint32, vtxID ids.ID) {
		if vdr != inVdr {
			t.Fatalf("Asking wrong validator for vertex")
		}
		if vtx0.ID() != vtxID {
			t.Fatalf("Asking for wrong vertex")
		}
		*requestID = reqID
	}

	if err := bootstrapper.Accepted(vdr, *requestID, acceptedFrontier); err != nil {
		t.Fatal(err)
	}

	manager.GetVtxF = nil
	sender.SendGetF = nil

	vm.ParseTxF = func(b []byte) (snowstorm.Tx, error) {
		if bytes.Equal(b, txBytes0) {
			return tx0, nil
		}
		t.Fatalf("Unknown bytes provided")
		panic("Unknown bytes provided")
	}
	manager.ParseVtxF = func(b []byte) (avalanche.Vertex, error) {
		if bytes.Equal(b, vtxBytes0) {
			return vtx0, nil
		}
		t.Fatalf("Unknown bytes provided")
		panic("Unknown bytes provided")
	}
	manager.EdgeF = func() []ids.ID {
		return []ids.ID{vtxID0}
	}
	manager.GetVtxF = func(vtxID ids.ID) (avalanche.Vertex, error) {
		if vtxID == vtxID0 {
			return vtx0, nil
		}
		t.Fatalf("Unknown bytes provided")
		panic("Unknown bytes provided")
	}

	if err := bootstrapper.Ancestors(vdr, *requestID, [][]byte{vtxBytes0}); err != nil {
		t.Fatal(err)
	}

	vm.ParseTxF = nil
	manager.ParseVtxF = nil
	manager.EdgeF = nil
	manager.GetVtxF = nil

	if tx0.Status() != choices.Accepted {
		t.Fatalf("Should have accepted %s", txID0)
	}
	if vtx0.Status() != choices.Accepted {
		t.Fatalf("Should have accepted %s", vtxID0)
	}

	manager.ParseVtxF = func(b []byte) (avalanche.Vertex, error) {
		if bytes.Equal(b, vtxBytes1) {
			return vtx1, nil
		}
		t.Fatalf("Unknown bytes provided")
		panic("Unknown bytes provided")
	}
	sender.SendChitsF = func(inVdr ids.NodeID, _ uint32, chits []ids.ID) {
		if inVdr != vdr {
			t.Fatalf("Sent to the wrong validator")
		}

		expected := []ids.ID{vtxID0}

		if !ids.Equals(expected, chits) {
			t.Fatalf("Returned wrong chits")
		}
	}
	sender.SendPushQueryF = func(vdrs ids.NodeIDSet, _ uint32, vtx []byte) {
		if vdrs.Len() != 1 {
			t.Fatalf("Should have requested from the validators")
		}
		if !vdrs.Contains(vdr) {
			t.Fatalf("Should have requested from %s", vdr)
		}

		if !bytes.Equal(vtxBytes1, vtx) {
			t.Fatalf("Sent wrong query bytes")
		}
	}
	manager.GetVtxF = func(vtxID ids.ID) (avalanche.Vertex, error) {
		if vtxID == vtxID1 {
			return vtx1, nil
		}
		t.Fatalf("Unknown bytes provided")
		panic("Unknown bytes provided")
	}

	if err := te.PushQuery(vdr, 0, vtxBytes1); err != nil {
		t.Fatal(err)
	}

	manager.ParseVtxF = nil
	sender.SendChitsF = nil
	sender.SendPushQueryF = nil
	manager.GetVtxF = nil
}

func TestEngineReBootstrapFails(t *testing.T) {
	_, bootCfg, engCfg := DefaultConfig()
	bootCfg.Alpha = 1
	bootCfg.RetryBootstrap = true
	bootCfg.RetryBootstrapWarnFrequency = 4

	vals := validators.NewSet()
	vdr := ids.GenerateTestNodeID()
	if err := vals.AddWeight(vdr, 1); err != nil {
		t.Fatal(err)
	}

	peers := tracker.NewPeers()
	startup := tracker.NewStartup(peers, 0)
	vals.RegisterCallbackListener(startup)

	bootCfg.Beacons = vals
	bootCfg.Validators = vals
	bootCfg.StartupTracker = startup
	engCfg.Validators = vals

	bootCfg.SampleK = vals.Len()

	sender := &common.SenderTest{T: t}
	sender.Default(true)
	bootCfg.Sender = sender
	engCfg.Sender = sender

	manager := vertex.NewTestManager(t)
	manager.Default(true)
	bootCfg.Manager = manager
	engCfg.Manager = manager

	vm := &vertex.TestVM{TestVM: common.TestVM{T: t}}
	vm.Default(true)
	bootCfg.VM = vm
	engCfg.VM = vm

	vm.CantSetState = false

	utxos := []ids.ID{ids.GenerateTestID(), ids.GenerateTestID()}

	txID0 := ids.GenerateTestID()
	txID1 := ids.GenerateTestID()

	txBytes0 := []byte{0}
	txBytes1 := []byte{1}

	tx0 := &snowstorm.TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     txID0,
			StatusV: choices.Processing,
		},
		BytesV: txBytes0,
	}
	tx0.InputIDsV = append(tx0.InputIDsV, utxos[0])

	tx1 := &snowstorm.TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     txID1,
			StatusV: choices.Processing,
		},
		DependenciesV: []snowstorm.Tx{tx0},
		BytesV:        txBytes1,
	}
	tx1.InputIDsV = append(tx1.InputIDsV, utxos[1])

	requested := new(bool)
	requestID := new(uint32)
	sender.SendGetAcceptedFrontierF = func(vdrs ids.NodeIDSet, reqID uint32) {
		// instead of triggering the timeout here, we'll just invoke the GetAcceptedFrontierFailed func
		//
		// s.router.GetAcceptedFrontierFailed(vID, s.ctx.ChainID, requestID)
		// -> chain.GetAcceptedFrontierFailed(validatorID, requestID)
		// ---> h.sendReliableMsg(message{
		//			messageType: constants.GetAcceptedFrontierFailedMsg,
		//			validatorID: validatorID,
		//			requestID:   requestID,
		//		})
		// -----> h.engine.GetAcceptedFrontierFailed(msg.validatorID, msg.requestID)
		// -------> return b.AcceptedFrontier(validatorID, requestID, nil)

		// ensure the request is made to the correct validators
		if vdrs.Len() != 1 {
			t.Fatalf("Should have requested from the validators")
		}
		if !vdrs.Contains(vdr) {
			t.Fatalf("Should have requested from %s", vdr)
		}
		*requested = true
		*requestID = reqID
	}

	dh := &dummyHandler{}
	bootstrapper, err := bootstrap.New(
		bootCfg,
		dh.onDoneBootstrapping,
	)
	if err != nil {
		t.Fatal(err)
	}

	if err := bootstrapper.Start(0); err != nil {
		t.Fatal(err)
	}

	if !*requested {
		t.Fatalf("Should have requested from the validators during Initialize")
	}

	// reset requested
	*requested = false
	sender.SendGetAcceptedF = func(vdrs ids.NodeIDSet, reqID uint32, proposedAccepted []ids.ID) {
		if vdrs.Len() != 1 {
			t.Fatalf("Should have requested from the validators")
		}
		if !vdrs.Contains(vdr) {
			t.Fatalf("Should have requested from %s", vdr)
		}
		*requested = true
		*requestID = reqID
	}

	// mimic a GetAcceptedFrontierFailedMsg
	// only validator that was requested timed out on the request
	if err := bootstrapper.GetAcceptedFrontierFailed(vdr, *requestID); err != nil {
		t.Fatal(err)
	}

	// mimic a GetAcceptedFrontierFailedMsg
	// only validator that was requested timed out on the request
	if err := bootstrapper.GetAcceptedFrontierFailed(vdr, *requestID); err != nil {
		t.Fatal(err)
	}

	bootCfg.Ctx.Registerer = prometheus.NewRegistry()

	// re-register the Transitive
	bootstrapper2, err := bootstrap.New(
		bootCfg,
		dh.onDoneBootstrapping,
	)
	if err != nil {
		t.Fatal(err)
	}

	if err := bootstrapper2.Start(0); err != nil {
		t.Fatal(err)
	}

	if err := bootstrapper2.GetAcceptedFailed(vdr, *requestID); err != nil {
		t.Fatal(err)
	}

	if err := bootstrapper2.GetAcceptedFailed(vdr, *requestID); err != nil {
		t.Fatal(err)
	}

	if !*requested {
		t.Fatalf("Should have requested from the validators during AcceptedFrontier")
	}
}

func TestEngineReBootstrappingIntoConsensus(t *testing.T) {
	_, bootCfg, engCfg := DefaultConfig()
	bootCfg.Alpha = 1
	bootCfg.RetryBootstrap = true
	bootCfg.RetryBootstrapWarnFrequency = 4

	vals := validators.NewSet()
	vdr := ids.GenerateTestNodeID()
	if err := vals.AddWeight(vdr, 1); err != nil {
		t.Fatal(err)
	}

	peers := tracker.NewPeers()
	startup := tracker.NewStartup(peers, 0)
	vals.RegisterCallbackListener(startup)

	bootCfg.Beacons = vals
	bootCfg.Validators = vals
	bootCfg.StartupTracker = startup
	engCfg.Validators = vals

	bootCfg.SampleK = vals.Len()

	sender := &common.SenderTest{T: t}
	sender.Default(true)
	bootCfg.Sender = sender
	engCfg.Sender = sender

	manager := vertex.NewTestManager(t)
	manager.Default(true)
	bootCfg.Manager = manager
	engCfg.Manager = manager

	vm := &vertex.TestVM{TestVM: common.TestVM{T: t}}
	vm.Default(true)
	bootCfg.VM = vm
	engCfg.VM = vm

	vm.CantSetState = false
	vm.CantConnected = false

	utxos := []ids.ID{ids.GenerateTestID(), ids.GenerateTestID()}

	txID0 := ids.GenerateTestID()
	txID1 := ids.GenerateTestID()

	txBytes0 := []byte{0}
	txBytes1 := []byte{1}

	tx0 := &snowstorm.TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     txID0,
			StatusV: choices.Processing,
		},
		BytesV: txBytes0,
	}
	tx0.InputIDsV = append(tx0.InputIDsV, utxos[0])

	tx1 := &snowstorm.TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     txID1,
			StatusV: choices.Processing,
		},
		DependenciesV: []snowstorm.Tx{tx0},
		BytesV:        txBytes1,
	}
	tx1.InputIDsV = append(tx1.InputIDsV, utxos[1])

	vtxID0 := ids.GenerateTestID()
	vtxID1 := ids.GenerateTestID()

	vtxBytes0 := []byte{2}
	vtxBytes1 := []byte{3}

	vtx0 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     vtxID0,
			StatusV: choices.Processing,
		},
		HeightV: 1,
		TxsV:    []snowstorm.Tx{tx0},
		BytesV:  vtxBytes0,
	}
	vtx1 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     vtxID1,
			StatusV: choices.Processing,
		},
		ParentsV: []avalanche.Vertex{vtx0},
		HeightV:  2,
		TxsV:     []snowstorm.Tx{tx1},
		BytesV:   vtxBytes1,
	}

	requested := new(bool)
	requestID := new(uint32)
	sender.SendGetAcceptedFrontierF = func(vdrs ids.NodeIDSet, reqID uint32) {
		if vdrs.Len() != 1 {
			t.Fatalf("Should have requested from the validators")
		}
		if !vdrs.Contains(vdr) {
			t.Fatalf("Should have requested from %s", vdr)
		}
		*requested = true
		*requestID = reqID
	}

	dh := &dummyHandler{}
	bootstrapper, err := bootstrap.New(
		bootCfg,
		dh.onDoneBootstrapping,
	)
	if err != nil {
		t.Fatal(err)
	}

	te, err := newTransitive(engCfg)
	if err != nil {
		t.Fatal(err)
	}
	dh.startEngineF = te.Start

	if err := bootstrapper.Start(0); err != nil {
		t.Fatal(err)
	}

	if err := bootstrapper.Connected(vdr, version.CurrentApp); err != nil {
		t.Fatal(err)
	}

	// fail the AcceptedFrontier
	if err := bootstrapper.GetAcceptedFrontierFailed(vdr, *requestID); err != nil {
		t.Fatal(err)
	}

	// fail the GetAcceptedFailed
	if err := bootstrapper.GetAcceptedFailed(vdr, *requestID); err != nil {
		t.Fatal(err)
	}

	if !*requested {
		t.Fatalf("Should have requested from the validators during Initialize")
	}

	acceptedFrontier := []ids.ID{vtxID0}

	*requested = false
	sender.SendGetAcceptedF = func(vdrs ids.NodeIDSet, reqID uint32, proposedAccepted []ids.ID) {
		if vdrs.Len() != 1 {
			t.Fatalf("Should have requested from the validators")
		}
		if !vdrs.Contains(vdr) {
			t.Fatalf("Should have requested from %s", vdr)
		}
		if !ids.Equals(acceptedFrontier, proposedAccepted) {
			t.Fatalf("Wrong proposedAccepted vertices.\nExpected: %s\nGot: %s", acceptedFrontier, proposedAccepted)
		}
		*requested = true
		*requestID = reqID
	}

	if err := bootstrapper.AcceptedFrontier(vdr, *requestID, acceptedFrontier); err != nil {
		t.Fatal(err)
	}

	if !*requested {
		t.Fatalf("Should have requested from the validators during AcceptedFrontier")
	}

	manager.GetVtxF = func(vtxID ids.ID) (avalanche.Vertex, error) {
		if vtxID == vtxID0 {
			return nil, errMissing
		}
		t.Fatalf("Unknown vertex requested")
		panic("Unknown vertex requested")
	}

	sender.SendGetAncestorsF = func(inVdr ids.NodeID, reqID uint32, vtxID ids.ID) {
		if vdr != inVdr {
			t.Fatalf("Asking wrong validator for vertex")
		}
		if vtx0.ID() != vtxID {
			t.Fatalf("Asking for wrong vertex")
		}
		*requestID = reqID
	}

	if err := bootstrapper.Accepted(vdr, *requestID, acceptedFrontier); err != nil {
		t.Fatal(err)
	}

	manager.GetVtxF = nil

	vm.ParseTxF = func(b []byte) (snowstorm.Tx, error) {
		if bytes.Equal(b, txBytes0) {
			return tx0, nil
		}
		t.Fatalf("Unknown bytes provided")
		panic("Unknown bytes provided")
	}
	manager.ParseVtxF = func(b []byte) (avalanche.Vertex, error) {
		if bytes.Equal(b, vtxBytes0) {
			return vtx0, nil
		}
		t.Fatalf("Unknown bytes provided")
		panic("Unknown bytes provided")
	}
	manager.EdgeF = func() []ids.ID {
		return []ids.ID{vtxID0}
	}
	manager.GetVtxF = func(vtxID ids.ID) (avalanche.Vertex, error) {
		if vtxID == vtxID0 {
			return vtx0, nil
		}
		t.Fatalf("Unknown bytes provided")
		panic("Unknown bytes provided")
	}

	if err := bootstrapper.Ancestors(vdr, *requestID, [][]byte{vtxBytes0}); err != nil {
		t.Fatal(err)
	}

	sender.SendGetAcceptedFrontierF = nil
	sender.SendGetF = nil
	vm.ParseTxF = nil
	manager.ParseVtxF = nil
	manager.EdgeF = nil
	manager.GetVtxF = nil

	if tx0.Status() != choices.Accepted {
		t.Fatalf("Should have accepted %s", txID0)
	}
	if vtx0.Status() != choices.Accepted {
		t.Fatalf("Should have accepted %s", vtxID0)
	}

	manager.ParseVtxF = func(b []byte) (avalanche.Vertex, error) {
		if bytes.Equal(b, vtxBytes1) {
			return vtx1, nil
		}
		t.Fatalf("Unknown bytes provided")
		panic("Unknown bytes provided")
	}
	sender.SendChitsF = func(inVdr ids.NodeID, _ uint32, chits []ids.ID) {
		if inVdr != vdr {
			t.Fatalf("Sent to the wrong validator")
		}

		expected := []ids.ID{vtxID1}

		if !ids.Equals(expected, chits) {
			t.Fatalf("Returned wrong chits")
		}
	}
	sender.SendPushQueryF = func(vdrs ids.NodeIDSet, _ uint32, vtx []byte) {
		if vdrs.Len() != 1 {
			t.Fatalf("Should have requested from the validators")
		}
		if !vdrs.Contains(vdr) {
			t.Fatalf("Should have requested from %s", vdr)
		}

		if !bytes.Equal(vtxBytes1, vtx) {
			t.Fatalf("Sent wrong query bytes")
		}
	}
	manager.GetVtxF = func(vtxID ids.ID) (avalanche.Vertex, error) {
		if vtxID == vtxID1 {
			return vtx1, nil
		}
		t.Fatalf("Unknown bytes provided")
		panic("Unknown bytes provided")
	}

	if err := bootstrapper.PushQuery(vdr, 0, vtxBytes1); err != nil {
		t.Fatal(err)
	}

	manager.ParseVtxF = nil
	sender.SendChitsF = nil
	sender.SendPushQueryF = nil
	manager.GetVtxF = nil
}

func TestEngineUndeclaredDependencyDeadlock(t *testing.T) {
	_, _, engCfg := DefaultConfig()

	vals := validators.NewSet()
	engCfg.Validators = vals

	vdr := ids.GenerateTestNodeID()
	if err := vals.AddWeight(vdr, 1); err != nil {
		t.Fatal(err)
	}

	manager := vertex.NewTestManager(t)
	engCfg.Manager = manager

	gVtx := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}

	vts := []avalanche.Vertex{gVtx}
	utxos := []ids.ID{ids.GenerateTestID(), ids.GenerateTestID()}

	tx0 := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Processing,
	}}
	tx0.InputIDsV = append(tx0.InputIDsV, utxos[0])

	tx1 := &snowstorm.TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		VerifyV: errors.New(""),
	}
	tx1.InputIDsV = append(tx1.InputIDsV, utxos[1])

	vtx0 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx0},
	}
	vtx1 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: []avalanche.Vertex{vtx0},
		HeightV:  2,
		TxsV:     []snowstorm.Tx{tx1},
	}

	te, err := newTransitive(engCfg)
	if err != nil {
		t.Fatal(err)
	}

	if err := te.Start(0); err != nil {
		t.Fatal(err)
	}

	sender := &common.SenderTest{T: t}
	te.Sender = sender

	reqID := new(uint32)
	sender.SendPushQueryF = func(_ ids.NodeIDSet, requestID uint32, _ []byte) {
		*reqID = requestID
	}

	if err := te.issue(vtx0); err != nil {
		t.Fatal(err)
	}

	sender.SendPushQueryF = func(ids.NodeIDSet, uint32, []byte) {
		t.Fatalf("should have failed verification")
	}

	if err := te.issue(vtx1); err != nil {
		t.Fatal(err)
	}

	manager.GetVtxF = func(vtxID ids.ID) (avalanche.Vertex, error) {
		switch vtxID {
		case vtx0.ID():
			return vtx0, nil
		case vtx1.ID():
			return vtx1, nil
		}
		return nil, errors.New("Unknown vtx")
	}

	if err := te.Chits(vdr, *reqID, []ids.ID{vtx1.ID()}); err != nil {
		t.Fatal(err)
	}

	if status := vtx0.Status(); status != choices.Accepted {
		t.Fatalf("should have accepted the vertex due to transitive voting")
	}
}

func TestEnginePartiallyValidVertex(t *testing.T) {
	_, _, engCfg := DefaultConfig()

	vals := validators.NewSet()
	engCfg.Validators = vals

	vdr := ids.GenerateTestNodeID()
	if err := vals.AddWeight(vdr, 1); err != nil {
		t.Fatal(err)
	}

	manager := vertex.NewTestManager(t)
	engCfg.Manager = manager

	gVtx := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}

	vts := []avalanche.Vertex{gVtx}
	utxos := []ids.ID{ids.GenerateTestID(), ids.GenerateTestID()}

	tx0 := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Processing,
	}}
	tx0.InputIDsV = append(tx0.InputIDsV, utxos[0])

	tx1 := &snowstorm.TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		VerifyV: errors.New(""),
	}
	tx1.InputIDsV = append(tx1.InputIDsV, utxos[1])

	vtx := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx0, tx1},
	}

	te, err := newTransitive(engCfg)
	if err != nil {
		t.Fatal(err)
	}

	if err := te.Start(0); err != nil {
		t.Fatal(err)
	}

	expectedVtxBytes := []byte{1}
	manager.BuildVtxF = func(_ []ids.ID, txs []snowstorm.Tx) (avalanche.Vertex, error) {
		return &avalanche.TestVertex{
			TestDecidable: choices.TestDecidable{
				IDV:     ids.GenerateTestID(),
				StatusV: choices.Processing,
			},
			ParentsV: vts,
			HeightV:  1,
			TxsV:     txs,
			BytesV:   expectedVtxBytes,
		}, nil
	}

	sender := &common.SenderTest{T: t}
	te.Sender = sender

	sender.SendPushQueryF = func(_ ids.NodeIDSet, _ uint32, vtx []byte) {
		if !bytes.Equal(expectedVtxBytes, vtx) {
			t.Fatalf("wrong vertex queried")
		}
	}

	if err := te.issue(vtx); err != nil {
		t.Fatal(err)
	}
}

func TestEngineGossip(t *testing.T) {
	_, _, engCfg := DefaultConfig()

	sender := &common.SenderTest{T: t}
	sender.Default(true)
	engCfg.Sender = sender

	manager := vertex.NewTestManager(t)
	engCfg.Manager = manager

	gVtx := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}

	te, err := newTransitive(engCfg)
	if err != nil {
		t.Fatal(err)
	}

	if err := te.Start(0); err != nil {
		t.Fatal(err)
	}

	manager.EdgeF = func() []ids.ID { return []ids.ID{gVtx.ID()} }
	manager.GetVtxF = func(vtxID ids.ID) (avalanche.Vertex, error) {
		if vtxID == gVtx.ID() {
			return gVtx, nil
		}
		t.Fatal(errUnknownVertex)
		return nil, errUnknownVertex
	}

	called := new(bool)
	sender.SendGossipF = func(vtxBytes []byte) {
		*called = true
		if !bytes.Equal(vtxBytes, gVtx.Bytes()) {
			t.Fatal(errUnknownVertex)
		}
	}

	if err := te.Gossip(); err != nil {
		t.Fatal(err)
	}

	if !*called {
		t.Fatalf("Should have gossiped the vertex")
	}
}

func TestEngineInvalidVertexIgnoredFromUnexpectedPeer(t *testing.T) {
	_, _, engCfg := DefaultConfig()

	vals := validators.NewSet()
	engCfg.Validators = vals

	vdr := ids.GenerateTestNodeID()
	secondVdr := ids.GenerateTestNodeID()

	if err := vals.AddWeight(vdr, 1); err != nil {
		t.Fatal(err)
	}
	if err := vals.AddWeight(secondVdr, 1); err != nil {
		t.Fatal(err)
	}

	sender := &common.SenderTest{T: t}
	engCfg.Sender = sender

	manager := vertex.NewTestManager(t)
	engCfg.Manager = manager

	gVtx := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Accepted,
		},
		BytesV: []byte{0},
	}

	vts := []avalanche.Vertex{gVtx}
	utxos := []ids.ID{ids.GenerateTestID(), ids.GenerateTestID()}

	tx0 := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Processing,
	}}
	tx0.InputIDsV = append(tx0.InputIDsV, utxos[0])

	tx1 := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Processing,
	}}
	tx1.InputIDsV = append(tx1.InputIDsV, utxos[1])

	vtx0 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Unknown,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx0},
		BytesV:   []byte{1},
	}
	vtx1 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: []avalanche.Vertex{vtx0},
		HeightV:  2,
		TxsV:     []snowstorm.Tx{tx1},
		BytesV:   []byte{2},
	}

	te, err := newTransitive(engCfg)
	if err != nil {
		t.Fatal(err)
	}

	if err := te.Start(0); err != nil {
		t.Fatal(err)
	}

	parsed := new(bool)
	manager.ParseVtxF = func(b []byte) (avalanche.Vertex, error) {
		if bytes.Equal(b, vtx1.Bytes()) {
			*parsed = true
			return vtx1, nil
		}
		return nil, errUnknownVertex
	}

	manager.GetVtxF = func(vtxID ids.ID) (avalanche.Vertex, error) {
		if !*parsed {
			return nil, errUnknownVertex
		}

		if vtxID == vtx1.ID() {
			return vtx1, nil
		}
		return nil, errUnknownVertex
	}

	reqID := new(uint32)
	sender.SendGetF = func(reqVdr ids.NodeID, requestID uint32, vtxID ids.ID) {
		*reqID = requestID
		if reqVdr != vdr {
			t.Fatalf("Wrong validator requested")
		}
		if vtxID != vtx0.ID() {
			t.Fatalf("Wrong vertex requested")
		}
	}

	if err := te.PushQuery(vdr, 0, vtx1.Bytes()); err != nil {
		t.Fatal(err)
	}

	if err := te.Put(secondVdr, *reqID, []byte{3}); err != nil {
		t.Fatal(err)
	}

	*parsed = false
	manager.ParseVtxF = func(b []byte) (avalanche.Vertex, error) {
		if bytes.Equal(b, vtx0.Bytes()) {
			*parsed = true
			return vtx0, nil
		}
		return nil, errUnknownVertex
	}

	manager.GetVtxF = func(vtxID ids.ID) (avalanche.Vertex, error) {
		if !*parsed {
			return nil, errUnknownVertex
		}

		if vtxID == vtx0.ID() {
			return vtx0, nil
		}
		return nil, errUnknownVertex
	}
	sender.CantSendPushQuery = false
	sender.CantSendChits = false

	vtx0.StatusV = choices.Processing

	if err := te.Put(vdr, *reqID, vtx0.Bytes()); err != nil {
		t.Fatal(err)
	}

	prefs := te.Consensus.Preferences()
	if !prefs.Contains(vtx1.ID()) {
		t.Fatalf("Shouldn't have abandoned the pending vertex")
	}
}

func TestEnginePushQueryRequestIDConflict(t *testing.T) {
	_, _, engCfg := DefaultConfig()

	vals := validators.NewSet()
	engCfg.Validators = vals

	vdr := ids.GenerateTestNodeID()
	if err := vals.AddWeight(vdr, 1); err != nil {
		t.Fatal(err)
	}

	sender := &common.SenderTest{T: t}
	engCfg.Sender = sender

	manager := vertex.NewTestManager(t)
	engCfg.Manager = manager

	gVtx := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Accepted,
		},
		BytesV: []byte{0},
	}

	vts := []avalanche.Vertex{gVtx}
	utxos := []ids.ID{ids.GenerateTestID(), ids.GenerateTestID()}

	tx0 := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Processing,
	}}
	tx0.InputIDsV = append(tx0.InputIDsV, utxos[0])

	tx1 := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Processing,
	}}
	tx1.InputIDsV = append(tx1.InputIDsV, utxos[1])

	vtx0 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Unknown,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx0},
		BytesV:   []byte{1},
	}

	vtx1 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: []avalanche.Vertex{vtx0},
		HeightV:  2,
		TxsV:     []snowstorm.Tx{tx1},
		BytesV:   []byte{2},
	}

	te, err := newTransitive(engCfg)
	if err != nil {
		t.Fatal(err)
	}

	if err := te.Start(0); err != nil {
		t.Fatal(err)
	}

	parsed := new(bool)
	manager.ParseVtxF = func(b []byte) (avalanche.Vertex, error) {
		if bytes.Equal(b, vtx1.Bytes()) {
			*parsed = true
			return vtx1, nil
		}
		return nil, errUnknownVertex
	}

	manager.GetVtxF = func(vtxID ids.ID) (avalanche.Vertex, error) {
		if !*parsed {
			return nil, errUnknownVertex
		}

		if vtxID == vtx1.ID() {
			return vtx1, nil
		}
		return nil, errUnknownVertex
	}

	reqID := new(uint32)
	sender.SendGetF = func(reqVdr ids.NodeID, requestID uint32, vtxID ids.ID) {
		*reqID = requestID
		if reqVdr != vdr {
			t.Fatalf("Wrong validator requested")
		}
		if vtxID != vtx0.ID() {
			t.Fatalf("Wrong vertex requested")
		}
	}

	if err := te.PushQuery(vdr, 0, vtx1.Bytes()); err != nil {
		t.Fatal(err)
	}

	sender.SendGetF = nil
	sender.CantSendGet = false

	if err := te.PushQuery(vdr, *reqID, []byte{3}); err != nil {
		t.Fatal(err)
	}

	*parsed = false
	manager.ParseVtxF = func(b []byte) (avalanche.Vertex, error) {
		if bytes.Equal(b, vtx0.Bytes()) {
			*parsed = true
			return vtx0, nil
		}
		return nil, errUnknownVertex
	}

	manager.GetVtxF = func(vtxID ids.ID) (avalanche.Vertex, error) {
		if !*parsed {
			return nil, errUnknownVertex
		}

		if vtxID == vtx0.ID() {
			return vtx0, nil
		}
		return nil, errUnknownVertex
	}
	sender.CantSendPushQuery = false
	sender.CantSendChits = false

	vtx0.StatusV = choices.Processing

	if err := te.Put(vdr, *reqID, vtx0.Bytes()); err != nil {
		t.Fatal(err)
	}

	prefs := te.Consensus.Preferences()
	if !prefs.Contains(vtx1.ID()) {
		t.Fatalf("Shouldn't have abandoned the pending vertex")
	}
}

func TestEngineAggressivePolling(t *testing.T) {
	_, _, engCfg := DefaultConfig()

	engCfg.Params.ConcurrentRepolls = 3
	engCfg.Params.BetaRogue = 3

	vals := validators.NewSet()
	engCfg.Validators = vals

	vdr := ids.GenerateTestNodeID()
	if err := vals.AddWeight(vdr, 1); err != nil {
		t.Fatal(err)
	}

	sender := &common.SenderTest{T: t}
	engCfg.Sender = sender

	manager := vertex.NewTestManager(t)
	engCfg.Manager = manager

	vm := &vertex.TestVM{TestVM: common.TestVM{T: t}}
	vm.Default(true)
	engCfg.VM = vm

	gVtx := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Accepted,
		},
		BytesV: []byte{0},
	}

	vts := []avalanche.Vertex{gVtx}
	utxos := []ids.ID{ids.GenerateTestID(), ids.GenerateTestID()}

	tx0 := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Processing,
	}}
	tx0.InputIDsV = append(tx0.InputIDsV, utxos[0])

	tx1 := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Processing,
	}}
	tx1.InputIDsV = append(tx1.InputIDsV, utxos[1])

	vtx := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx0},
		BytesV:   []byte{1},
	}

	vm.CantSetState = false
	te, err := newTransitive(engCfg)
	if err != nil {
		t.Fatal(err)
	}

	if err := te.Start(0); err != nil {
		t.Fatal(err)
	}

	vm.CantSetState = true
	parsed := new(bool)
	manager.ParseVtxF = func(b []byte) (avalanche.Vertex, error) {
		if bytes.Equal(b, vtx.Bytes()) {
			*parsed = true
			return vtx, nil
		}
		return nil, errUnknownVertex
	}

	manager.GetVtxF = func(vtxID ids.ID) (avalanche.Vertex, error) {
		if !*parsed {
			return nil, errUnknownVertex
		}

		if vtxID == vtx.ID() {
			return vtx, nil
		}
		return nil, errUnknownVertex
	}

	numPushQueries := new(int)
	sender.SendPushQueryF = func(ids.NodeIDSet, uint32, []byte) { *numPushQueries++ }

	numPullQueries := new(int)
	sender.SendPullQueryF = func(ids.NodeIDSet, uint32, ids.ID) { *numPullQueries++ }

	vm.CantPendingTxs = false

	if err := te.Put(vdr, 0, vtx.Bytes()); err != nil {
		t.Fatal(err)
	}

	if *numPushQueries != 1 {
		t.Fatalf("should have issued one push query")
	}
	if *numPullQueries != 2 {
		t.Fatalf("should have issued two pull queries")
	}
}

func TestEngineDuplicatedIssuance(t *testing.T) {
	_, _, engCfg := DefaultConfig()
	engCfg.Params.BatchSize = 1
	engCfg.Params.BetaVirtuous = 5
	engCfg.Params.BetaRogue = 5

	sender := &common.SenderTest{T: t}
	sender.Default(true)
	sender.CantSendGetAcceptedFrontier = false
	engCfg.Sender = sender

	vals := validators.NewSet()
	engCfg.Validators = vals

	vdr := ids.GenerateTestNodeID()
	if err := vals.AddWeight(vdr, 1); err != nil {
		t.Fatal(err)
	}

	manager := vertex.NewTestManager(t)
	engCfg.Manager = manager

	manager.Default(true)

	vm := &vertex.TestVM{TestVM: common.TestVM{T: t}}
	vm.Default(true)
	engCfg.VM = vm

	gVtx := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}
	mVtx := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}

	gTx := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}

	utxos := []ids.ID{ids.GenerateTestID(), ids.GenerateTestID()}

	tx := &snowstorm.TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		DependenciesV: []snowstorm.Tx{gTx},
	}
	tx.InputIDsV = append(tx.InputIDsV, utxos[0])

	manager.EdgeF = func() []ids.ID { return []ids.ID{gVtx.ID(), mVtx.ID()} }
	manager.GetVtxF = func(id ids.ID) (avalanche.Vertex, error) {
		switch id {
		case gVtx.ID():
			return gVtx, nil
		case mVtx.ID():
			return mVtx, nil
		}
		t.Fatalf("Unknown vertex")
		panic("Should have errored")
	}

	vm.CantSetState = false
	te, err := newTransitive(engCfg)
	if err != nil {
		t.Fatal(err)
	}

	if err := te.Start(0); err != nil {
		t.Fatal(err)
	}

	vm.CantSetState = true
	lastVtx := new(avalanche.TestVertex)
	manager.BuildVtxF = func(_ []ids.ID, txs []snowstorm.Tx) (avalanche.Vertex, error) {
		lastVtx = &avalanche.TestVertex{
			TestDecidable: choices.TestDecidable{
				IDV:     ids.GenerateTestID(),
				StatusV: choices.Processing,
			},
			ParentsV: []avalanche.Vertex{gVtx, mVtx},
			HeightV:  1,
			TxsV:     txs,
			BytesV:   []byte{1},
		}
		return lastVtx, nil
	}

	sender.CantSendPushQuery = false

	vm.PendingTxsF = func() []snowstorm.Tx { return []snowstorm.Tx{tx} }
	if err := te.Notify(common.PendingTxs); err != nil {
		t.Fatal(err)
	}

	if len(lastVtx.TxsV) != 1 || lastVtx.TxsV[0].ID() != tx.ID() {
		t.Fatalf("Should have issued txs differently")
	}

	manager.BuildVtxF = func([]ids.ID, []snowstorm.Tx) (avalanche.Vertex, error) {
		t.Fatalf("shouldn't have attempted to issue a duplicated tx")
		return nil, nil
	}

	if err := te.Notify(common.PendingTxs); err != nil {
		t.Fatal(err)
	}
}

func TestEngineDoubleChit(t *testing.T) {
	_, _, engCfg := DefaultConfig()

	engCfg.Params.Alpha = 2
	engCfg.Params.K = 2
	engCfg.Params.MixedQueryNumPushNonVdr = 2

	vals := validators.NewSet()
	engCfg.Validators = vals

	vdr0 := ids.GenerateTestNodeID()
	vdr1 := ids.GenerateTestNodeID()

	if err := vals.AddWeight(vdr0, 1); err != nil {
		t.Fatal(err)
	}
	if err := vals.AddWeight(vdr1, 1); err != nil {
		t.Fatal(err)
	}

	sender := &common.SenderTest{T: t}
	sender.Default(true)
	sender.CantSendGetAcceptedFrontier = false
	engCfg.Sender = sender

	manager := vertex.NewTestManager(t)
	manager.Default(true)
	engCfg.Manager = manager

	gVtx := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}
	mVtx := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}

	vts := []avalanche.Vertex{gVtx, mVtx}
	utxos := []ids.ID{ids.GenerateTestID()}

	tx := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Processing,
	}}
	tx.InputIDsV = append(tx.InputIDsV, utxos[0])

	vtx := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: vts,
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx},
		BytesV:   []byte{1, 1, 2, 3},
	}

	manager.EdgeF = func() []ids.ID { return []ids.ID{vts[0].ID(), vts[1].ID()} }
	manager.GetVtxF = func(id ids.ID) (avalanche.Vertex, error) {
		switch id {
		case gVtx.ID():
			return gVtx, nil
		case mVtx.ID():
			return mVtx, nil
		}
		t.Fatalf("Unknown vertex")
		panic("Should have errored")
	}

	te, err := newTransitive(engCfg)
	if err != nil {
		t.Fatal(err)
	}

	if err := te.Start(0); err != nil {
		t.Fatal(err)
	}

	reqID := new(uint32)
	sender.SendPushQueryF = func(inVdrs ids.NodeIDSet, requestID uint32, vtxBytes []byte) {
		*reqID = requestID
		if inVdrs.Len() != 2 {
			t.Fatalf("Wrong number of validators")
		}
		if !bytes.Equal(vtx.Bytes(), vtxBytes) {
			t.Fatalf("Wrong vertex requested")
		}
	}
	manager.GetVtxF = func(id ids.ID) (avalanche.Vertex, error) {
		if id == vtx.ID() {
			return vtx, nil
		}
		t.Fatalf("Unknown vertex")
		panic("Should have errored")
	}

	if err := te.issue(vtx); err != nil {
		t.Fatal(err)
	}

	votes := []ids.ID{vtx.ID()}

	if status := tx.Status(); status != choices.Processing {
		t.Fatalf("Wrong tx status: %s ; expected: %s", status, choices.Processing)
	}

	if err := te.Chits(vdr0, *reqID, votes); err != nil {
		t.Fatal(err)
	}

	if status := tx.Status(); status != choices.Processing {
		t.Fatalf("Wrong tx status: %s ; expected: %s", status, choices.Processing)
	}

	if err := te.Chits(vdr0, *reqID, votes); err != nil {
		t.Fatal(err)
	}

	if status := tx.Status(); status != choices.Processing {
		t.Fatalf("Wrong tx status: %s ; expected: %s", status, choices.Processing)
	}

	if err := te.Chits(vdr1, *reqID, votes); err != nil {
		t.Fatal(err)
	}

	if status := tx.Status(); status != choices.Accepted {
		t.Fatalf("Wrong tx status: %s ; expected: %s", status, choices.Accepted)
	}
}

func TestEngineBubbleVotes(t *testing.T) {
	_, _, engCfg := DefaultConfig()

	vals := validators.NewSet()
	engCfg.Validators = vals

	vdr := ids.GenerateTestNodeID()
	err := vals.AddWeight(vdr, 1)
	require.NoError(t, err)

	sender := &common.SenderTest{T: t}
	sender.Default(true)
	sender.CantSendGetAcceptedFrontier = false
	engCfg.Sender = sender

	manager := vertex.NewTestManager(t)
	manager.Default(true)
	engCfg.Manager = manager

	utxos := []ids.ID{
		ids.GenerateTestID(),
		ids.GenerateTestID(),
		ids.GenerateTestID(),
	}

	tx0 := &snowstorm.TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		InputIDsV: utxos[:1],
	}
	tx1 := &snowstorm.TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		InputIDsV: utxos[1:2],
	}
	tx2 := &snowstorm.TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		InputIDsV: utxos[1:2],
	}

	vtx := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		HeightV: 0,
		TxsV:    []snowstorm.Tx{tx0},
		BytesV:  []byte{0},
	}

	missingVtx := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Unknown,
	}}

	pendingVtx0 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: []avalanche.Vertex{vtx, missingVtx},
		HeightV:  1,
		TxsV:     []snowstorm.Tx{tx1},
		BytesV:   []byte{1},
	}

	pendingVtx1 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentsV: []avalanche.Vertex{pendingVtx0},
		HeightV:  2,
		TxsV:     []snowstorm.Tx{tx2},
		BytesV:   []byte{2},
	}

	manager.EdgeF = func() []ids.ID { return nil }
	manager.GetVtxF = func(id ids.ID) (avalanche.Vertex, error) {
		switch id {
		case vtx.ID():
			return vtx, nil
		case missingVtx.ID():
			return nil, errMissing
		case pendingVtx0.ID():
			return pendingVtx0, nil
		case pendingVtx1.ID():
			return pendingVtx1, nil
		}
		require.FailNow(t, "unknown vertex", "vtxID: %s", id)
		panic("should have errored")
	}

	te, err := newTransitive(engCfg)
	if err != nil {
		t.Fatal(err)
	}

	if err := te.Start(0); err != nil {
		t.Fatal(err)
	}

	queryReqID := new(uint32)
	queried := new(bool)
	sender.SendPushQueryF = func(inVdrs ids.NodeIDSet, requestID uint32, vtxBytes []byte) {
		require.Len(t, inVdrs, 1, "wrong number of validators")
		*queryReqID = requestID
		require.Equal(t, vtx.Bytes(), vtxBytes, "wrong vertex requested")
		*queried = true
	}

	getReqID := new(uint32)
	fetched := new(bool)
	sender.SendGetF = func(inVdr ids.NodeID, requestID uint32, vtxID ids.ID) {
		require.Equal(t, vdr, inVdr, "wrong validator")
		*getReqID = requestID
		require.Equal(t, missingVtx.ID(), vtxID, "wrong vertex requested")
		*fetched = true
	}

	issued, err := te.issueFrom(vdr, pendingVtx1)
	require.NoError(t, err)
	require.False(t, issued, "shouldn't have been able to issue %s", pendingVtx1.ID())
	require.True(t, *queried, "should have queried for %s", vtx.ID())
	require.True(t, *fetched, "should have fetched %s", missingVtx.ID())

	// can't apply votes yet because pendingVtx0 isn't issued because missingVtx
	// is missing
	err = te.Chits(vdr, *queryReqID, []ids.ID{pendingVtx1.ID()})
	require.NoError(t, err)
	require.Equal(t, choices.Processing, tx0.Status(), "wrong tx status")
	require.Equal(t, choices.Processing, tx1.Status(), "wrong tx status")

	// vote for pendingVtx1 should be bubbled up to pendingVtx0 and then to vtx
	err = te.GetFailed(vdr, *getReqID)
	require.NoError(t, err)
	require.Equal(t, choices.Accepted, tx0.Status(), "wrong tx status")
	require.Equal(t, choices.Processing, tx1.Status(), "wrong tx status")
}

func TestEngineIssue(t *testing.T) {
	_, _, engCfg := DefaultConfig()
	engCfg.Params.BatchSize = 1
	engCfg.Params.BetaVirtuous = 1
	engCfg.Params.BetaRogue = 1
	engCfg.Params.OptimalProcessing = 1

	sender := &common.SenderTest{T: t}
	sender.Default(true)
	sender.CantSendGetAcceptedFrontier = false
	engCfg.Sender = sender

	vals := validators.NewSet()
	engCfg.Validators = vals

	vdr := ids.GenerateTestNodeID()
	if err := vals.AddWeight(vdr, 1); err != nil {
		t.Fatal(err)
	}

	manager := vertex.NewTestManager(t)
	manager.Default(true)
	engCfg.Manager = manager

	vm := &vertex.TestVM{TestVM: common.TestVM{T: t}}
	vm.Default(true)
	engCfg.VM = vm

	gVtx := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}
	mVtx := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}

	gTx := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}

	utxos := []ids.ID{ids.GenerateTestID(), ids.GenerateTestID()}

	tx0 := &snowstorm.TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		DependenciesV: []snowstorm.Tx{gTx},
		InputIDsV:     utxos[:1],
	}
	tx1 := &snowstorm.TestTx{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		DependenciesV: []snowstorm.Tx{gTx},
		InputIDsV:     utxos[1:],
	}

	manager.EdgeF = func() []ids.ID { return []ids.ID{gVtx.ID(), mVtx.ID()} }
	manager.GetVtxF = func(id ids.ID) (avalanche.Vertex, error) {
		switch id {
		case gVtx.ID():
			return gVtx, nil
		case mVtx.ID():
			return mVtx, nil
		}
		t.Fatalf("Unknown vertex")
		panic("Should have errored")
	}

	vm.CantSetState = false
	te, err := newTransitive(engCfg)
	if err != nil {
		t.Fatal(err)
	}

	if err := te.Start(0); err != nil {
		t.Fatal(err)
	}

	vm.CantSetState = true
	numBuilt := 0
	vtxID := ids.GenerateTestID()
	manager.BuildVtxF = func(_ []ids.ID, txs []snowstorm.Tx) (avalanche.Vertex, error) {
		numBuilt++
		vtx := &avalanche.TestVertex{
			TestDecidable: choices.TestDecidable{
				IDV:     vtxID,
				StatusV: choices.Processing,
			},
			ParentsV: []avalanche.Vertex{gVtx, mVtx},
			HeightV:  1,
			TxsV:     txs,
			BytesV:   []byte{1},
		}

		manager.GetVtxF = func(id ids.ID) (avalanche.Vertex, error) {
			switch id {
			case gVtx.ID():
				return gVtx, nil
			case mVtx.ID():
				return mVtx, nil
			case vtx.ID():
				return vtx, nil
			}
			t.Fatalf("Unknown vertex")
			panic("Should have errored")
		}

		return vtx, nil
	}

	var queryRequestID uint32
	sender.SendPushQueryF = func(_ ids.NodeIDSet, requestID uint32, _ []byte) {
		queryRequestID = requestID
	}

	vm.PendingTxsF = func() []snowstorm.Tx { return []snowstorm.Tx{tx0, tx1} }
	if err := te.Notify(common.PendingTxs); err != nil {
		t.Fatal(err)
	}

	if numBuilt != 1 {
		t.Fatalf("Should have issued txs differently")
	}

	if err := te.Chits(vdr, queryRequestID, []ids.ID{vtxID}); err != nil {
		t.Fatal(err)
	}

	if numBuilt != 2 {
		t.Fatalf("Should have issued txs differently")
	}
}

// Test that a transaction is abandoned if a dependency fails verification,
// even if there are outstanding requests for vertices when the
// dependency fails verification.
func TestAbandonTx(t *testing.T) {
	require := require.New(t)
	_, _, engCfg := DefaultConfig()
	engCfg.Params.BatchSize = 1
	engCfg.Params.BetaVirtuous = 1
	engCfg.Params.BetaRogue = 1
	engCfg.Params.OptimalProcessing = 1

	sender := &common.SenderTest{
		T:                           t,
		CantSendGetAcceptedFrontier: false,
	}
	sender.Default(true)
	engCfg.Sender = sender

	engCfg.Validators = validators.NewSet()
	vdr := ids.GenerateTestNodeID()
	if err := engCfg.Validators.AddWeight(vdr, 1); err != nil {
		t.Fatal(err)
	}

	manager := vertex.NewTestManager(t)
	manager.Default(true)
	manager.CantEdge = false
	manager.CantGetVtx = false
	engCfg.Manager = manager

	vm := &vertex.TestVM{TestVM: common.TestVM{T: t}}
	vm.Default(true)
	vm.CantSetState = false

	engCfg.VM = vm

	te, err := newTransitive(engCfg)
	if err != nil {
		t.Fatal(err)
	}

	if err := te.Start(0); err != nil {
		t.Fatal(err)
	}

	gVtx := &avalanche.TestVertex{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}

	gTx := &snowstorm.TestTx{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}

	tx0 := &snowstorm.TestTx{ // Fails verification
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		DependenciesV: []snowstorm.Tx{gTx},
		InputIDsV:     []ids.ID{gTx.ID()},
		BytesV:        utils.RandomBytes(32),
		VerifyV:       errors.New(""),
	}

	tx1 := &snowstorm.TestTx{ // Depends on tx0
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		DependenciesV: []snowstorm.Tx{tx0},
		InputIDsV:     []ids.ID{gTx.ID()},
		BytesV:        utils.RandomBytes(32),
	}

	vtx0 := &avalanche.TestVertex{ // Contains tx0, which will fail verification
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Unknown,
		},
		ParentsV: []avalanche.Vertex{gVtx},
		HeightV:  gVtx.HeightV + 1,
		TxsV:     []snowstorm.Tx{tx0},
	}

	// Contains tx1, which depends on tx0.
	// vtx0 and vtx1 are siblings.
	vtx1 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Unknown,
		},
		ParentsV: []avalanche.Vertex{gVtx},
		HeightV:  gVtx.HeightV + 1,
		TxsV:     []snowstorm.Tx{tx1},
	}

	// Cause the engine to send a Get request for vtx1, vtx0, and some other vtx that doesn't exist
	sender.CantSendGet = false
	sender.CantSendChits = false
	err = te.PullQuery(vdr, 0, vtx1.ID())
	require.NoError(err)
	err = te.PullQuery(vdr, 0, vtx0.ID())
	require.NoError(err)
	err = te.PullQuery(vdr, 0, ids.GenerateTestID())
	require.NoError(err)

	// Give the engine vtx1. It should wait to issue vtx1
	// until tx0 is issued, because tx1 depends on tx0.
	manager.ParseVtxF = func(b []byte) (avalanche.Vertex, error) {
		if bytes.Equal(b, vtx1.BytesV) {
			vtx1.StatusV = choices.Processing
			return vtx1, nil
		}
		require.FailNow("should have asked to parse vtx1")
		return nil, errors.New("should have asked to parse vtx1")
	}
	err = te.Put(vdr, 0, vtx1.Bytes())
	require.NoError(err)

	// Verify that vtx1 is waiting to be issued.
	require.True(te.pending.Contains(vtx1.ID()))

	// Give the engine vtx0. It should try to issue vtx0
	// but then abandon it because tx0 fails verification.
	manager.ParseVtxF = func(b []byte) (avalanche.Vertex, error) {
		if bytes.Equal(b, vtx0.BytesV) {
			vtx0.StatusV = choices.Processing
			return vtx0, nil
		}
		require.FailNow("should have asked to parse vtx0")
		return nil, errors.New("should have asked to parse vtx0")
	}
	err = te.Put(vdr, 0, vtx0.Bytes())
	require.NoError(err)

	// Despite the fact that there is still an outstanding vertex request,
	// vtx1 should have been abandoned because tx0 failed verification
	require.False(te.pending.Contains(vtx1.ID()))
	// sanity check that there is indeed an outstanding vertex request
	require.True(te.outstandingVtxReqs.Len() == 1)
}

func TestSendMixedQuery(t *testing.T) {
	type test struct {
		isVdr bool
	}
	tests := []test{
		{isVdr: true},
		{isVdr: false},
	}
	for _, tt := range tests {
		t.Run(
			fmt.Sprintf("is validator: %v", tt.isVdr),
			func(t *testing.T) {
				_, _, engCfg := DefaultConfig()
				sender := &common.SenderTest{T: t}
				engCfg.Sender = sender
				sender.Default(true)
				vdrSet := engCfg.Validators
				manager := vertex.NewTestManager(t)
				engCfg.Manager = manager
				// Override the parameters k, MixedQueryNumPushVdr, MixedQueryNumPushNonVdr,
				// and update the validator set to have k validators.
				engCfg.Params.K = 20
				engCfg.Params.Alpha = 12
				engCfg.Params.MixedQueryNumPushVdr = 12
				engCfg.Params.MixedQueryNumPushNonVdr = 11
				te, err := newTransitive(engCfg)
				if err != nil {
					t.Fatal(err)
				}
				startReqID := uint32(0)
				if err := te.Start(startReqID); err != nil {
					t.Fatal(err)
				}

				vdrsList := []validators.Validator{}
				vdrs := ids.NodeIDSet{}
				for i := 0; i < te.Config.Params.K; i++ {
					vdr := ids.GenerateTestNodeID()
					vdrs.Add(vdr)
					vdrsList = append(vdrsList, validators.NewValidator(vdr, 1))
				}
				if tt.isVdr {
					vdrs.Add(te.Ctx.NodeID)
					vdrsList = append(vdrsList, validators.NewValidator(te.Ctx.NodeID, 1))
				}
				if err := vdrSet.Set(vdrsList); err != nil {
					t.Fatal(err)
				}

				// [blk1] is a child of [gBlk] and passes verification
				vtx1 := &avalanche.TestVertex{
					TestDecidable: choices.TestDecidable{
						IDV:     ids.GenerateTestID(),
						StatusV: choices.Processing,
					},
					ParentsV: []avalanche.Vertex{
						&avalanche.TestVertex{TestDecidable: choices.TestDecidable{
							IDV:     ids.GenerateTestID(),
							StatusV: choices.Accepted,
						}},
					},
					BytesV: []byte{1},
				}

				manager.ParseVtxF = func(b []byte) (avalanche.Vertex, error) {
					switch {
					case bytes.Equal(b, vtx1.Bytes()):
						return vtx1, nil
					default:
						t.Fatalf("Unknown block bytes")
						return nil, nil
					}
				}

				pullQuerySent := new(bool)
				pullQueryReqID := new(uint32)
				pullQueriedVdrs := ids.NodeIDSet{}
				sender.SendPullQueryF = func(inVdrs ids.NodeIDSet, requestID uint32, vtxID ids.ID) {
					switch {
					case *pullQuerySent:
						t.Fatalf("Asked multiple times")
					case vtxID != vtx1.ID():
						t.Fatalf("Expected engine to request vtx1")
					}
					pullQueriedVdrs.Union(inVdrs)
					*pullQuerySent = true
					*pullQueryReqID = requestID
				}

				pushQuerySent := new(bool)
				pushQueryReqID := new(uint32)
				pushQueriedVdrs := ids.NodeIDSet{}
				sender.SendPushQueryF = func(inVdrs ids.NodeIDSet, requestID uint32, vtx []byte) {
					switch {
					case *pushQuerySent:
						t.Fatal("Asked multiple times")
					case !bytes.Equal(vtx, vtx1.Bytes()):
						t.Fatal("got unexpected block bytes instead of blk1")
					}
					*pushQuerySent = true
					*pushQueryReqID = requestID
					pushQueriedVdrs.Union(inVdrs)
				}

				// Give the engine vtx1. It should insert it into consensus and send a mixed query
				// consisting of 12 pull queries and 8 push queries.
				if err := te.Put(vdrSet.List()[0].ID(), constants.GossipMsgRequestID, vtx1.Bytes()); err != nil {
					t.Fatal(err)
				}

				switch {
				case !*pullQuerySent:
					t.Fatal("expected us to send pull queries")
				case !*pushQuerySent:
					t.Fatal("expected us to send push queries")
				case *pushQueryReqID != *pullQueryReqID:
					t.Fatalf("expected equal push query (%v) and pull query (%v) req IDs", *pushQueryReqID, *pullQueryReqID)
				case pushQueriedVdrs.Len()+pullQueriedVdrs.Len() != te.Config.Params.K:
					t.Fatalf("expected num push queried (%d) + num pull queried (%d) to be %d", pushQueriedVdrs.Len(), pullQueriedVdrs.Len(), te.Config.Params.K)
				case tt.isVdr && pushQueriedVdrs.Len() != te.Params.MixedQueryNumPushVdr:
					t.Fatalf("expected num push queried (%d) to be %d", pullQueriedVdrs.Len(), te.Params.MixedQueryNumPushVdr)
				case !tt.isVdr && pushQueriedVdrs.Len() != te.Params.MixedQueryNumPushNonVdr:
					t.Fatalf("expected num push queried (%d) to be %d", pullQueriedVdrs.Len(), te.Params.MixedQueryNumPushNonVdr)
				}

				pullQueriedVdrs.Union(pushQueriedVdrs) // Now this holds all queried validators (push and pull)
				for vdr := range pullQueriedVdrs {
					if !vdrs.Contains(vdr) {
						t.Fatalf("got unexpected vdr %v", vdr)
					}
				}
			})
	}
}

```

avalanchego/snow/engine/avalanche/vertex/builder.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package vertex

import (
	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/consensus/avalanche"
	"github.com/ava-labs/avalanchego/snow/consensus/snowstorm"
	"github.com/ava-labs/avalanchego/utils/hashing"
)

// Builder builds a vertex given a set of parentIDs and transactions.
type Builder interface {
	// Build a new vertex from the contents of a vertex
	BuildVtx(parentIDs []ids.ID, txs []snowstorm.Tx) (avalanche.Vertex, error)
	// Build a new stop vertex from the parents
	BuildStopVtx(parentIDs []ids.ID) (avalanche.Vertex, error)
}

// Build a new stateless vertex from the contents of a vertex
func Build(
	chainID ids.ID,
	height uint64,
	parentIDs []ids.ID,
	txs [][]byte,
) (StatelessVertex, error) {
	return buildVtx(
		chainID,
		height,
		parentIDs,
		txs,
		func(vtx innerStatelessVertex) error {
			return vtx.verify()
		},
		false,
	)
}

// Build a new stateless vertex from the contents of a vertex
func BuildStopVertex(chainID ids.ID, height uint64, parentIDs []ids.ID) (StatelessVertex, error) {
	return buildVtx(
		chainID,
		height,
		parentIDs,
		nil,
		func(vtx innerStatelessVertex) error {
			return vtx.verifyStopVertex()
		},
		true,
	)
}

func buildVtx(
	chainID ids.ID,
	height uint64,
	parentIDs []ids.ID,
	txs [][]byte,
	verifyFunc func(innerStatelessVertex) error,
	stopVertex bool,
) (StatelessVertex, error) {
	ids.SortIDs(parentIDs)
	SortHashOf(txs)

	codecVer := codecVersion
	if stopVertex {
		// use new codec version for the "StopVertex"
		codecVer = codecVersionWithStopVtx
	}

	innerVtx := innerStatelessVertex{
		Version:   codecVer,
		ChainID:   chainID,
		Height:    height,
		Epoch:     0,
		ParentIDs: parentIDs,
		Txs:       txs,
	}
	if err := verifyFunc(innerVtx); err != nil {
		return nil, err
	}

	vtxBytes, err := c.Marshal(innerVtx.Version, innerVtx)
	vtx := statelessVertex{
		innerStatelessVertex: innerVtx,
		id:                   hashing.ComputeHash256Array(vtxBytes),
		bytes:                vtxBytes,
	}
	return vtx, err
}

```

avalanchego/snow/engine/avalanche/vertex/builder_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package vertex

import (
	"testing"

	"github.com/stretchr/testify/require"

	"github.com/ava-labs/avalanchego/ids"
)

func TestBuildInvalid(t *testing.T) {
	chainID := ids.ID{1}
	height := uint64(2)
	parentIDs := []ids.ID{{4}, {5}}
	txs := [][]byte{{6}, {6}}
	_, err := Build(
		chainID,
		height,
		parentIDs,
		txs,
	)
	require.Error(t, err, "build should have errored because restrictions were provided in epoch 0")
}

func TestBuildValid(t *testing.T) {
	chainID := ids.ID{1}
	height := uint64(2)
	parentIDs := []ids.ID{{4}, {5}}
	txs := [][]byte{{7}, {6}}
	vtx, err := Build(
		chainID,
		height,
		parentIDs,
		txs,
	)
	require.NoError(t, err)
	require.Equal(t, chainID, vtx.ChainID())
	require.Equal(t, height, vtx.Height())
	require.Equal(t, parentIDs, vtx.ParentIDs())
	require.Equal(t, txs, vtx.Txs())
}

```

avalanchego/snow/engine/avalanche/vertex/codec.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package vertex

import (
	"github.com/ava-labs/avalanchego/codec"
	"github.com/ava-labs/avalanchego/codec/linearcodec"
	"github.com/ava-labs/avalanchego/codec/reflectcodec"
	"github.com/ava-labs/avalanchego/utils/units"
)

const (
	// maxSize is the maximum allowed vertex size. It is necessary to deter DoS
	maxSize = units.MiB

	codecVersion            uint16 = 0
	codecVersionWithStopVtx uint16 = 1
)

var c codec.Manager

func init() {
	lc := linearcodec.New([]string{reflectcodec.DefaultTagName + "V0"}, maxSize)
	lc2 := linearcodec.New([]string{reflectcodec.DefaultTagName + "V1"}, maxSize)

	c = codec.NewManager(maxSize)
	// for backward compatibility, still register the initial codec version
	if err := c.RegisterCodec(codecVersion, lc); err != nil {
		panic(err)
	}
	if err := c.RegisterCodec(codecVersionWithStopVtx, lc2); err != nil {
		panic(err)
	}
}

```

avalanchego/snow/engine/avalanche/vertex/heap.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package vertex

import (
	"container/heap"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/consensus/avalanche"
)

var (
	_ Heap           = &maxHeightVertexHeap{}
	_ heap.Interface = &priorityQueue{}
)

type priorityQueue []avalanche.Vertex

func (pq priorityQueue) Len() int { return len(pq) }

// Returns true if the vertex at index i has greater height than the vertex at
// index j.
func (pq priorityQueue) Less(i, j int) bool {
	statusI := pq[i].Status()
	statusJ := pq[j].Status()

	// Put unknown vertices at the front of the heap to ensure once we have made
	// it below a certain height in DAG traversal we do not need to reset
	if !statusI.Fetched() {
		return true
	}
	if !statusJ.Fetched() {
		return false
	}

	// Treat errors on retrieving the height as if the vertex is not fetched
	heightI, errI := pq[i].Height()
	if errI != nil {
		return true
	}
	heightJ, errJ := pq[j].Height()
	if errJ != nil {
		return false
	}
	return heightI > heightJ
}

func (pq priorityQueue) Swap(i, j int) {
	pq[i], pq[j] = pq[j], pq[i]
}

// Push adds an item to this priority queue. x must have type *vertexItem
func (pq *priorityQueue) Push(x interface{}) {
	item := x.(avalanche.Vertex)
	*pq = append(*pq, item)
}

// Pop returns the last item in this priorityQueue
func (pq *priorityQueue) Pop() interface{} {
	old := *pq
	n := len(old)
	item := old[n-1]
	old[n-1] = nil
	*pq = old[0 : n-1]
	return item
}

// Heap defines the functionality of a heap of vertices with unique VertexIDs
// ordered by height
type Heap interface {
	// Empty the heap.
	Clear()

	// Add the provided vertex to the heap. Vertices are de-duplicated, returns
	// true if the vertex was added, false if it was dropped.
	Push(avalanche.Vertex) bool

	// Remove the top vertex. Assumes that there is at least one element.
	Pop() avalanche.Vertex

	// Returns if a vertex with the provided ID is currently in the heap.
	Contains(ids.ID) bool

	// Returns the number of vertices in the heap.
	Len() int
}

// NewHeap returns an empty Heap
func NewHeap() Heap { return &maxHeightVertexHeap{} }

type maxHeightVertexHeap struct {
	heap       priorityQueue
	elementIDs ids.Set
}

func (vh *maxHeightVertexHeap) Clear() {
	vh.heap = priorityQueue{}
	vh.elementIDs.Clear()
}

// Push adds an element to this heap. Returns true if the element was added.
// Returns false if it was already in the heap.
func (vh *maxHeightVertexHeap) Push(vtx avalanche.Vertex) bool {
	vtxID := vtx.ID()
	if vh.elementIDs.Contains(vtxID) {
		return false
	}

	vh.elementIDs.Add(vtxID)
	heap.Push(&vh.heap, vtx)
	return true
}

// If there are any vertices in this heap with status Unknown, removes one such
// vertex and returns it. Otherwise, removes and returns the vertex in this heap
// with the greatest height.
func (vh *maxHeightVertexHeap) Pop() avalanche.Vertex {
	vtx := heap.Pop(&vh.heap).(avalanche.Vertex)
	vh.elementIDs.Remove(vtx.ID())
	return vtx
}

func (vh *maxHeightVertexHeap) Len() int { return vh.heap.Len() }

func (vh *maxHeightVertexHeap) Contains(vtxID ids.ID) bool { return vh.elementIDs.Contains(vtxID) }

```

avalanchego/snow/engine/avalanche/vertex/heap_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package vertex

import (
	"testing"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/choices"
	"github.com/ava-labs/avalanchego/snow/consensus/avalanche"
)

// This example inserts several ints into an IntHeap, checks the minimum,
// and removes them in order of priority.
func TestUniqueVertexHeapReturnsOrdered(t *testing.T) {
	h := NewHeap()

	vtx0 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		HeightV: 0,
	}
	vtx1 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		HeightV: 1,
	}
	vtx2 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		HeightV: 1,
	}
	vtx3 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		HeightV: 3,
	}
	vtx4 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Unknown,
		},
		HeightV: 0,
	}

	vts := []avalanche.Vertex{vtx0, vtx1, vtx2, vtx3, vtx4}

	for _, vtx := range vts {
		h.Push(vtx)
	}

	vtxZ := h.Pop()
	if vtxZ.ID() != vtx4.ID() {
		t.Fatalf("Heap did not pop unknown element first")
	}

	vtxA := h.Pop()
	if height, err := vtxA.Height(); err != nil || height != 3 {
		t.Fatalf("First height from heap was incorrect")
	} else if vtxA.ID() != vtx3.ID() {
		t.Fatalf("Incorrect ID on vertex popped from heap")
	}

	vtxB := h.Pop()
	if height, err := vtxB.Height(); err != nil || height != 1 {
		t.Fatalf("First height from heap was incorrect")
	} else if vtxB.ID() != vtx1.ID() && vtxB.ID() != vtx2.ID() {
		t.Fatalf("Incorrect ID on vertex popped from heap")
	}

	vtxC := h.Pop()
	if height, err := vtxC.Height(); err != nil || height != 1 {
		t.Fatalf("First height from heap was incorrect")
	} else if vtxC.ID() != vtx1.ID() && vtxC.ID() != vtx2.ID() {
		t.Fatalf("Incorrect ID on vertex popped from heap")
	}

	if vtxB.ID() == vtxC.ID() {
		t.Fatalf("Heap returned same element more than once")
	}

	vtxD := h.Pop()
	if height, err := vtxD.Height(); err != nil || height != 0 {
		t.Fatalf("Last height returned was incorrect")
	} else if vtxD.ID() != vtx0.ID() {
		t.Fatalf("Last item from heap had incorrect ID")
	}

	if h.Len() != 0 {
		t.Fatalf("Heap was not empty after popping all of its elements")
	}
}

func TestUniqueVertexHeapRemainsUnique(t *testing.T) {
	h := NewHeap()

	vtx0 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		HeightV: 0,
	}
	vtx1 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		HeightV: 1,
	}

	sharedID := ids.GenerateTestID()
	vtx2 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     sharedID,
			StatusV: choices.Processing,
		},
		HeightV: 1,
	}
	vtx3 := &avalanche.TestVertex{
		TestDecidable: choices.TestDecidable{
			IDV:     sharedID,
			StatusV: choices.Processing,
		},
		HeightV: 2,
	}

	pushed1 := h.Push(vtx0)
	pushed2 := h.Push(vtx1)
	pushed3 := h.Push(vtx2)
	pushed4 := h.Push(vtx3)
	switch {
	case h.Len() != 3:
		t.Fatalf("Unique Vertex Heap has incorrect length: %d", h.Len())
	case !(pushed1 && pushed2 && pushed3):
		t.Fatalf("Failed to push a new unique element")
	case pushed4:
		t.Fatalf("Pushed non-unique element to the unique vertex heap")
	}
}

```

avalanchego/snow/engine/avalanche/vertex/manager.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package vertex

// Manager defines all the vertex related functionality that is required by the
// consensus engine.
type Manager interface {
	Builder
	Parser
	Storage
}

```

avalanchego/snow/engine/avalanche/vertex/mocks/dag_vm.go:
```
// Code generated by mockery v2.9.4. DO NOT EDIT.

package mocks

import (
	ids "github.com/ava-labs/avalanchego/ids"
	common "github.com/ava-labs/avalanchego/snow/engine/common"

	manager "github.com/ava-labs/avalanchego/database/manager"

	mock "github.com/stretchr/testify/mock"

	snow "github.com/ava-labs/avalanchego/snow"

	snowstorm "github.com/ava-labs/avalanchego/snow/consensus/snowstorm"

	time "time"

	version "github.com/ava-labs/avalanchego/version"
)

// DAGVM is an autogenerated mock type for the DAGVM type
type DAGVM struct {
	mock.Mock
}

// AppGossip provides a mock function with given fields: nodeID, msg
func (_m *DAGVM) AppGossip(nodeID ids.NodeID, msg []byte) error {
	ret := _m.Called(nodeID, msg)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, []byte) error); ok {
		r0 = rf(nodeID, msg)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// AppRequest provides a mock function with given fields: nodeID, requestID, deadline, request
func (_m *DAGVM) AppRequest(nodeID ids.NodeID, requestID uint32, deadline time.Time, request []byte) error {
	ret := _m.Called(nodeID, requestID, deadline, request)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32, time.Time, []byte) error); ok {
		r0 = rf(nodeID, requestID, deadline, request)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// AppRequestFailed provides a mock function with given fields: nodeID, requestID
func (_m *DAGVM) AppRequestFailed(nodeID ids.NodeID, requestID uint32) error {
	ret := _m.Called(nodeID, requestID)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32) error); ok {
		r0 = rf(nodeID, requestID)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// AppResponse provides a mock function with given fields: nodeID, requestID, response
func (_m *DAGVM) AppResponse(nodeID ids.NodeID, requestID uint32, response []byte) error {
	ret := _m.Called(nodeID, requestID, response)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32, []byte) error); ok {
		r0 = rf(nodeID, requestID, response)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// Connected provides a mock function with given fields: id, nodeVersion
func (_m *DAGVM) Connected(id ids.NodeID, nodeVersion version.Application) error {
	ret := _m.Called(id, nodeVersion)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, version.Application) error); ok {
		r0 = rf(id, nodeVersion)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// CreateHandlers provides a mock function with given fields:
func (_m *DAGVM) CreateHandlers() (map[string]*common.HTTPHandler, error) {
	ret := _m.Called()

	var r0 map[string]*common.HTTPHandler
	if rf, ok := ret.Get(0).(func() map[string]*common.HTTPHandler); ok {
		r0 = rf()
	} else {
		if ret.Get(0) != nil {
			r0 = ret.Get(0).(map[string]*common.HTTPHandler)
		}
	}

	var r1 error
	if rf, ok := ret.Get(1).(func() error); ok {
		r1 = rf()
	} else {
		r1 = ret.Error(1)
	}

	return r0, r1
}

// CreateStaticHandlers provides a mock function with given fields:
func (_m *DAGVM) CreateStaticHandlers() (map[string]*common.HTTPHandler, error) {
	ret := _m.Called()

	var r0 map[string]*common.HTTPHandler
	if rf, ok := ret.Get(0).(func() map[string]*common.HTTPHandler); ok {
		r0 = rf()
	} else {
		if ret.Get(0) != nil {
			r0 = ret.Get(0).(map[string]*common.HTTPHandler)
		}
	}

	var r1 error
	if rf, ok := ret.Get(1).(func() error); ok {
		r1 = rf()
	} else {
		r1 = ret.Error(1)
	}

	return r0, r1
}

// Disconnected provides a mock function with given fields: id
func (_m *DAGVM) Disconnected(id ids.NodeID) error {
	ret := _m.Called(id)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID) error); ok {
		r0 = rf(id)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// GetTx provides a mock function with given fields: _a0
func (_m *DAGVM) GetTx(_a0 ids.ID) (snowstorm.Tx, error) {
	ret := _m.Called(_a0)

	var r0 snowstorm.Tx
	if rf, ok := ret.Get(0).(func(ids.ID) snowstorm.Tx); ok {
		r0 = rf(_a0)
	} else {
		if ret.Get(0) != nil {
			r0 = ret.Get(0).(snowstorm.Tx)
		}
	}

	var r1 error
	if rf, ok := ret.Get(1).(func(ids.ID) error); ok {
		r1 = rf(_a0)
	} else {
		r1 = ret.Error(1)
	}

	return r0, r1
}

// HealthCheck provides a mock function with given fields:
func (_m *DAGVM) HealthCheck() (interface{}, error) {
	ret := _m.Called()

	var r0 interface{}
	if rf, ok := ret.Get(0).(func() interface{}); ok {
		r0 = rf()
	} else {
		if ret.Get(0) != nil {
			r0 = ret.Get(0).(interface{})
		}
	}

	var r1 error
	if rf, ok := ret.Get(1).(func() error); ok {
		r1 = rf()
	} else {
		r1 = ret.Error(1)
	}

	return r0, r1
}

// Initialize provides a mock function with given fields: ctx, dbManager, genesisBytes, upgradeBytes, configBytes, toEngine, fxs, appSender
func (_m *DAGVM) Initialize(ctx *snow.Context, dbManager manager.Manager, genesisBytes []byte, upgradeBytes []byte, configBytes []byte, toEngine chan<- common.Message, fxs []*common.Fx, appSender common.AppSender) error {
	ret := _m.Called(ctx, dbManager, genesisBytes, upgradeBytes, configBytes, toEngine, fxs, appSender)

	var r0 error
	if rf, ok := ret.Get(0).(func(*snow.Context, manager.Manager, []byte, []byte, []byte, chan<- common.Message, []*common.Fx, common.AppSender) error); ok {
		r0 = rf(ctx, dbManager, genesisBytes, upgradeBytes, configBytes, toEngine, fxs, appSender)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// ParseTx provides a mock function with given fields: tx
func (_m *DAGVM) ParseTx(tx []byte) (snowstorm.Tx, error) {
	ret := _m.Called(tx)

	var r0 snowstorm.Tx
	if rf, ok := ret.Get(0).(func([]byte) snowstorm.Tx); ok {
		r0 = rf(tx)
	} else {
		if ret.Get(0) != nil {
			r0 = ret.Get(0).(snowstorm.Tx)
		}
	}

	var r1 error
	if rf, ok := ret.Get(1).(func([]byte) error); ok {
		r1 = rf(tx)
	} else {
		r1 = ret.Error(1)
	}

	return r0, r1
}

// PendingTxs provides a mock function with given fields:
func (_m *DAGVM) PendingTxs() []snowstorm.Tx {
	ret := _m.Called()

	var r0 []snowstorm.Tx
	if rf, ok := ret.Get(0).(func() []snowstorm.Tx); ok {
		r0 = rf()
	} else {
		if ret.Get(0) != nil {
			r0 = ret.Get(0).([]snowstorm.Tx)
		}
	}

	return r0
}

// SetState provides a mock function with given fields: state
func (_m *DAGVM) SetState(state snow.State) error {
	ret := _m.Called(state)

	var r0 error
	if rf, ok := ret.Get(0).(func(snow.State) error); ok {
		r0 = rf(state)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// Shutdown provides a mock function with given fields:
func (_m *DAGVM) Shutdown() error {
	ret := _m.Called()

	var r0 error
	if rf, ok := ret.Get(0).(func() error); ok {
		r0 = rf()
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// Version provides a mock function with given fields:
func (_m *DAGVM) Version() (string, error) {
	ret := _m.Called()

	var r0 string
	if rf, ok := ret.Get(0).(func() string); ok {
		r0 = rf()
	} else {
		r0 = ret.Get(0).(string)
	}

	var r1 error
	if rf, ok := ret.Get(1).(func() error); ok {
		r1 = rf()
	} else {
		r1 = ret.Error(1)
	}

	return r0, r1
}

```

avalanchego/snow/engine/avalanche/vertex/parser.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package vertex

import (
	"github.com/ava-labs/avalanchego/snow/consensus/avalanche"
	"github.com/ava-labs/avalanchego/utils/hashing"
)

// Parser parses bytes into a vertex.
type Parser interface {
	// Parse a vertex from a slice of bytes
	ParseVtx(vertex []byte) (avalanche.Vertex, error)
}

// Parse parses the provided vertex bytes into a stateless vertex
func Parse(bytes []byte) (StatelessVertex, error) {
	vtx := innerStatelessVertex{}
	version, err := c.Unmarshal(bytes, &vtx)
	if err != nil {
		return nil, err
	}
	vtx.Version = version

	return statelessVertex{
		innerStatelessVertex: vtx,
		id:                   hashing.ComputeHash256Array(bytes),
		bytes:                bytes,
	}, nil
}

```

avalanchego/snow/engine/avalanche/vertex/parser_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package vertex

import (
	"testing"

	"github.com/stretchr/testify/require"

	"github.com/ava-labs/avalanchego/ids"
)

func TestParseInvalid(t *testing.T) {
	vtxBytes := []byte{}
	_, err := Parse(vtxBytes)
	require.Error(t, err, "parse on an invalid vertex should have errored")
}

func TestParseValid(t *testing.T) {
	chainID := ids.ID{1}
	height := uint64(2)
	parentIDs := []ids.ID{{4}, {5}}
	txs := [][]byte{{6}, {7}}
	vtx, err := Build(
		chainID,
		height,
		parentIDs,
		txs,
	)
	require.NoError(t, err)

	vtxBytes := vtx.Bytes()
	parsedVtx, err := Parse(vtxBytes)
	require.NoError(t, err)
	require.Equal(t, vtx, parsedVtx)
}

```

avalanchego/snow/engine/avalanche/vertex/sorting.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package vertex

import (
	"bytes"
	"sort"

	"github.com/ava-labs/avalanchego/utils"
	"github.com/ava-labs/avalanchego/utils/hashing"
)

type sortHashOfData [][]byte

func (d sortHashOfData) Less(i, j int) bool {
	return bytes.Compare(
		hashing.ComputeHash256(d[i]),
		hashing.ComputeHash256(d[j]),
	) == -1
}
func (d sortHashOfData) Len() int      { return len(d) }
func (d sortHashOfData) Swap(i, j int) { d[j], d[i] = d[i], d[j] }

func SortHashOf(bytesSlice [][]byte) { sort.Sort(sortHashOfData(bytesSlice)) }
func IsSortedAndUniqueHashOf(bytesSlice [][]byte) bool {
	return utils.IsSortedAndUnique(sortHashOfData(bytesSlice))
}

```

avalanchego/snow/engine/avalanche/vertex/stateless_vertex.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package vertex

import (
	"errors"
	"fmt"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/vms/components/verify"
)

const (
	// maxNumParents is the max number of parents a vertex may have
	maxNumParents = 128

	// maxTxsPerVtx is the max number of transactions a vertex may have
	maxTxsPerVtx = 128
)

var (
	errBadVersion       = errors.New("invalid version")
	errBadEpoch         = errors.New("invalid epoch")
	errTooManyparentIDs = fmt.Errorf("vertex contains more than %d parentIDs", maxNumParents)
	errNoOperations     = errors.New("vertex contains no operations")
	errTooManyTxs       = fmt.Errorf("vertex contains more than %d transactions", maxTxsPerVtx)
	errInvalidParents   = errors.New("vertex contains non-sorted or duplicated parentIDs")
	errInvalidTxs       = errors.New("vertex contains non-sorted or duplicated transactions")

	_ StatelessVertex = statelessVertex{}
)

type StatelessVertex interface {
	verify.Verifiable
	ID() ids.ID
	Bytes() []byte
	Version() uint16
	ChainID() ids.ID
	StopVertex() bool
	Height() uint64
	Epoch() uint32
	ParentIDs() []ids.ID
	Txs() [][]byte
}

type statelessVertex struct {
	// This wrapper exists so that the function calls aren't ambiguous
	innerStatelessVertex

	// cache the ID of this vertex
	id ids.ID

	// cache the binary format of this vertex
	bytes []byte
}

func (v statelessVertex) ID() ids.ID      { return v.id }
func (v statelessVertex) Bytes() []byte   { return v.bytes }
func (v statelessVertex) Version() uint16 { return v.innerStatelessVertex.Version }
func (v statelessVertex) ChainID() ids.ID { return v.innerStatelessVertex.ChainID }
func (v statelessVertex) StopVertex() bool {
	return v.innerStatelessVertex.Version == codecVersionWithStopVtx
}
func (v statelessVertex) Height() uint64      { return v.innerStatelessVertex.Height }
func (v statelessVertex) Epoch() uint32       { return v.innerStatelessVertex.Epoch }
func (v statelessVertex) ParentIDs() []ids.ID { return v.innerStatelessVertex.ParentIDs }
func (v statelessVertex) Txs() [][]byte       { return v.innerStatelessVertex.Txs }

type innerStatelessVertex struct {
	Version   uint16   `json:"version"`
	ChainID   ids.ID   `serializeV0:"true" serializeV1:"true" json:"chainID"`
	Height    uint64   `serializeV0:"true" serializeV1:"true" json:"height"`
	Epoch     uint32   `serializeV0:"true" json:"epoch"`
	ParentIDs []ids.ID `serializeV0:"true" serializeV1:"true" len:"128" json:"parentIDs"`
	Txs       [][]byte `serializeV0:"true" len:"128" json:"txs"`
}

func (v innerStatelessVertex) Verify() error {
	if v.Version == codecVersionWithStopVtx {
		return v.verifyStopVertex()
	}
	return v.verify()
}

func (v innerStatelessVertex) verify() error {
	switch {
	case v.Version != codecVersion:
		return errBadVersion
	case v.Epoch != 0:
		return errBadEpoch
	case len(v.ParentIDs) > maxNumParents:
		return errTooManyparentIDs
	case len(v.Txs) == 0:
		return errNoOperations
	case len(v.Txs) > maxTxsPerVtx:
		return errTooManyTxs
	case !ids.IsSortedAndUniqueIDs(v.ParentIDs):
		return errInvalidParents
	case !IsSortedAndUniqueHashOf(v.Txs):
		return errInvalidTxs
	default:
		return nil
	}
}

func (v innerStatelessVertex) verifyStopVertex() error {
	switch {
	case v.Version != codecVersionWithStopVtx:
		return errBadVersion
	case v.Epoch != 0:
		return errBadEpoch
	case len(v.ParentIDs) > maxNumParents:
		return errTooManyparentIDs
	case len(v.Txs) != 0:
		return errTooManyTxs
	case !ids.IsSortedAndUniqueIDs(v.ParentIDs):
		return errInvalidParents
	default:
		return nil
	}
}

```

avalanchego/snow/engine/avalanche/vertex/stateless_vertex_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package vertex

import (
	"testing"

	"github.com/ava-labs/avalanchego/ids"
)

func TestVertexVerify(t *testing.T) {
	tooManyParents := make([]ids.ID, maxNumParents+1)
	for i := range tooManyParents {
		tooManyParents[i][0] = byte(i)
	}
	tooManyTxs := make([][]byte, maxTxsPerVtx+1)
	for i := range tooManyTxs {
		tooManyTxs[i] = []byte{byte(i)}
	}
	tooManyRestrictions := make([]ids.ID, maxTxsPerVtx+1)
	for i := range tooManyRestrictions {
		tooManyRestrictions[i][0] = byte(i)
	}

	tests := []struct {
		name      string
		vertex    StatelessVertex
		shouldErr bool
	}{
		{
			name:      "zero vertex",
			vertex:    statelessVertex{innerStatelessVertex: innerStatelessVertex{}},
			shouldErr: true,
		},
		{
			name: "valid vertex",
			vertex: statelessVertex{innerStatelessVertex: innerStatelessVertex{
				Version:   0,
				ChainID:   ids.ID{},
				Height:    0,
				Epoch:     0,
				ParentIDs: []ids.ID{},
				Txs:       [][]byte{{}},
			}},
			shouldErr: false,
		},
		{
			name: "invalid vertex epoch",
			vertex: statelessVertex{innerStatelessVertex: innerStatelessVertex{
				Version:   0,
				ChainID:   ids.ID{},
				Height:    0,
				Epoch:     1,
				ParentIDs: []ids.ID{},
				Txs:       [][]byte{{}},
			}},
			shouldErr: true,
		},
		{
			name: "too many vertex parents",
			vertex: statelessVertex{innerStatelessVertex: innerStatelessVertex{
				Version:   0,
				ChainID:   ids.ID{},
				Height:    0,
				Epoch:     0,
				ParentIDs: tooManyParents,
				Txs:       [][]byte{{}},
			}},
			shouldErr: true,
		},
		{
			name: "no vertex txs",
			vertex: statelessVertex{innerStatelessVertex: innerStatelessVertex{
				Version:   0,
				ChainID:   ids.ID{},
				Height:    0,
				Epoch:     0,
				ParentIDs: []ids.ID{},
				Txs:       [][]byte{},
			}},
			shouldErr: true,
		},
		{
			name: "too many vertex txs",
			vertex: statelessVertex{innerStatelessVertex: innerStatelessVertex{
				Version:   0,
				ChainID:   ids.ID{},
				Height:    0,
				Epoch:     0,
				ParentIDs: []ids.ID{},
				Txs:       tooManyTxs,
			}},
			shouldErr: true,
		},
		{
			name: "unsorted vertex parents",
			vertex: statelessVertex{innerStatelessVertex: innerStatelessVertex{
				Version:   0,
				ChainID:   ids.ID{},
				Height:    0,
				Epoch:     0,
				ParentIDs: []ids.ID{{1}, {0}},
				Txs:       [][]byte{{}},
			}},
			shouldErr: true,
		},
		{
			name: "unsorted vertex txs",
			vertex: statelessVertex{innerStatelessVertex: innerStatelessVertex{
				Version:   0,
				ChainID:   ids.ID{},
				Height:    0,
				Epoch:     0,
				ParentIDs: []ids.ID{},
				Txs:       [][]byte{{0}, {1}}, // note that txs are sorted by their hashes
			}},
			shouldErr: true,
		},
		{
			name: "duplicate vertex parents",
			vertex: statelessVertex{innerStatelessVertex: innerStatelessVertex{
				Version:   0,
				ChainID:   ids.ID{},
				Height:    0,
				Epoch:     0,
				ParentIDs: []ids.ID{{0}, {0}},
				Txs:       [][]byte{{}},
			}},
			shouldErr: true,
		},
		{
			name: "duplicate vertex txs",
			vertex: statelessVertex{innerStatelessVertex: innerStatelessVertex{
				Version:   0,
				ChainID:   ids.ID{},
				Height:    0,
				Epoch:     0,
				ParentIDs: []ids.ID{},
				Txs:       [][]byte{{0}, {0}}, // note that txs are sorted by their hashes
			}},
			shouldErr: true,
		},
	}
	for _, test := range tests {
		t.Run(test.name, func(t *testing.T) {
			err := test.vertex.Verify()
			if test.shouldErr && err == nil {
				t.Fatal("expected verify to return an error but it didn't")
			} else if !test.shouldErr && err != nil {
				t.Fatalf("expected verify to pass but it returned: %s", err)
			}
		})
	}
}

```

avalanchego/snow/engine/avalanche/vertex/storage.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package vertex

import (
	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/consensus/avalanche"
)

// Storage defines the persistent storage that is required by the consensus
// engine.
type Storage interface {
	// Get a vertex by its hash from storage.
	GetVtx(vtxID ids.ID) (avalanche.Vertex, error)
	// Edge returns a list of accepted vertex IDs with no accepted children.
	Edge() (vtxIDs []ids.ID)
	// Returns "true" if accepted frontier ("Edge") is stop vertex.
	StopVertexAccepted() (bool, error)
}

```

avalanchego/snow/engine/avalanche/vertex/test_builder.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package vertex

import (
	"errors"
	"testing"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/consensus/avalanche"
	"github.com/ava-labs/avalanchego/snow/consensus/snowstorm"
)

var (
	errBuild = errors.New("unexpectedly called Build")

	_ Builder = &TestBuilder{}
)

type TestBuilder struct {
	T             *testing.T
	CantBuildVtx  bool
	BuildVtxF     func(parentIDs []ids.ID, txs []snowstorm.Tx) (avalanche.Vertex, error)
	BuildStopVtxF func(parentIDs []ids.ID) (avalanche.Vertex, error)
}

func (b *TestBuilder) Default(cant bool) { b.CantBuildVtx = cant }

func (b *TestBuilder) BuildVtx(parentIDs []ids.ID, txs []snowstorm.Tx) (avalanche.Vertex, error) {
	if b.BuildVtxF != nil {
		return b.BuildVtxF(parentIDs, txs)
	}
	if b.CantBuildVtx && b.T != nil {
		b.T.Fatal(errBuild)
	}
	return nil, errBuild
}

func (b *TestBuilder) BuildStopVtx(parentIDs []ids.ID) (avalanche.Vertex, error) {
	if b.BuildStopVtxF != nil {
		return b.BuildStopVtxF(parentIDs)
	}
	if b.CantBuildVtx && b.T != nil {
		b.T.Fatal(errBuild)
	}
	return nil, errBuild
}

```

avalanchego/snow/engine/avalanche/vertex/test_manager.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package vertex

import "testing"

var _ Manager = &TestManager{}

type TestManager struct {
	TestBuilder
	TestParser
	TestStorage
}

func NewTestManager(t *testing.T) *TestManager {
	return &TestManager{
		TestBuilder: TestBuilder{T: t},
		TestParser:  TestParser{T: t},
		TestStorage: TestStorage{T: t},
	}
}

func (m *TestManager) Default(cant bool) {
	m.TestBuilder.Default(cant)
	m.TestParser.Default(cant)
	m.TestStorage.Default(cant)
}

```

avalanchego/snow/engine/avalanche/vertex/test_parser.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package vertex

import (
	"errors"
	"testing"

	"github.com/ava-labs/avalanchego/snow/consensus/avalanche"
)

var (
	errParse = errors.New("unexpectedly called Parse")

	_ Parser = &TestParser{}
)

type TestParser struct {
	T            *testing.T
	CantParseVtx bool
	ParseVtxF    func([]byte) (avalanche.Vertex, error)
}

func (p *TestParser) Default(cant bool) { p.CantParseVtx = cant }

func (p *TestParser) ParseVtx(b []byte) (avalanche.Vertex, error) {
	if p.ParseVtxF != nil {
		return p.ParseVtxF(b)
	}
	if p.CantParseVtx && p.T != nil {
		p.T.Fatal(errParse)
	}
	return nil, errParse
}

```

avalanchego/snow/engine/avalanche/vertex/test_storage.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package vertex

import (
	"errors"
	"testing"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/consensus/avalanche"
)

var (
	errGet                = errors.New("unexpectedly called Get")
	errEdge               = errors.New("unexpectedly called Edge")
	errStopVertexAccepted = errors.New("unexpectedly called StopVertexAccepted")

	_ Storage = &TestStorage{}
)

type TestStorage struct {
	T                                            *testing.T
	CantGetVtx, CantEdge, CantStopVertexAccepted bool
	GetVtxF                                      func(ids.ID) (avalanche.Vertex, error)
	EdgeF                                        func() []ids.ID
	StopVertexAcceptedF                          func() (bool, error)
}

func (s *TestStorage) Default(cant bool) {
	s.CantGetVtx = cant
	s.CantEdge = cant
}

func (s *TestStorage) GetVtx(id ids.ID) (avalanche.Vertex, error) {
	if s.GetVtxF != nil {
		return s.GetVtxF(id)
	}
	if s.CantGetVtx && s.T != nil {
		s.T.Fatal(errGet)
	}
	return nil, errGet
}

func (s *TestStorage) Edge() []ids.ID {
	if s.EdgeF != nil {
		return s.EdgeF()
	}
	if s.CantEdge && s.T != nil {
		s.T.Fatal(errEdge)
	}
	return nil
}

func (s *TestStorage) StopVertexAccepted() (bool, error) {
	if s.StopVertexAcceptedF != nil {
		return s.StopVertexAcceptedF()
	}
	if s.CantStopVertexAccepted && s.T != nil {
		s.T.Fatal(errStopVertexAccepted)
	}
	return false, nil
}

```

avalanchego/snow/engine/avalanche/vertex/test_vm.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package vertex

import (
	"errors"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/consensus/snowstorm"
	"github.com/ava-labs/avalanchego/snow/engine/common"
)

var (
	errPending = errors.New("unexpectedly called Pending")

	_ DAGVM = &TestVM{}
)

type TestVM struct {
	common.TestVM

	CantPendingTxs, CantParse, CantGet bool

	PendingTxsF func() []snowstorm.Tx
	ParseTxF    func([]byte) (snowstorm.Tx, error)
	GetTxF      func(ids.ID) (snowstorm.Tx, error)
}

func (vm *TestVM) Default(cant bool) {
	vm.TestVM.Default(cant)

	vm.CantPendingTxs = cant
	vm.CantParse = cant
	vm.CantGet = cant
}

func (vm *TestVM) PendingTxs() []snowstorm.Tx {
	if vm.PendingTxsF != nil {
		return vm.PendingTxsF()
	}
	if vm.CantPendingTxs && vm.T != nil {
		vm.T.Fatal(errPending)
	}
	return nil
}

func (vm *TestVM) ParseTx(b []byte) (snowstorm.Tx, error) {
	if vm.ParseTxF != nil {
		return vm.ParseTxF(b)
	}
	if vm.CantParse && vm.T != nil {
		vm.T.Fatal(errParse)
	}
	return nil, errParse
}

func (vm *TestVM) GetTx(txID ids.ID) (snowstorm.Tx, error) {
	if vm.GetTxF != nil {
		return vm.GetTxF(txID)
	}
	if vm.CantGet && vm.T != nil {
		vm.T.Fatal(errGet)
	}
	return nil, errGet
}

```

avalanchego/snow/engine/avalanche/vertex/vm.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package vertex

import (
	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/consensus/snowstorm"
	"github.com/ava-labs/avalanchego/snow/engine/common"
)

// DAGVM defines the minimum functionality that an avalanche VM must
// implement
type DAGVM interface {
	common.VM
	Getter

	// Return any transactions that have not been sent to consensus yet
	PendingTxs() []snowstorm.Tx

	// Convert a stream of bytes to a transaction or return an error
	ParseTx(tx []byte) (snowstorm.Tx, error)
}

// Getter defines the functionality for fetching a tx/block by its ID.
type Getter interface {
	// Retrieve a transaction that was submitted previously
	GetTx(ids.ID) (snowstorm.Tx, error)
}

```

avalanchego/snow/engine/avalanche/voter.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package avalanche

import (
	"go.uber.org/zap"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/consensus/snowstorm"
	"github.com/ava-labs/avalanchego/snow/engine/avalanche/vertex"
)

// Voter records chits received from [vdr] once its dependencies are met.
type voter struct {
	t         *Transitive
	vdr       ids.NodeID
	requestID uint32
	response  []ids.ID
	deps      ids.Set
}

func (v *voter) Dependencies() ids.Set { return v.deps }

// Mark that a dependency has been met.
func (v *voter) Fulfill(id ids.ID) {
	v.deps.Remove(id)
	v.Update()
}

// Abandon this attempt to record chits.
func (v *voter) Abandon(id ids.ID) { v.Fulfill(id) }

func (v *voter) Update() {
	if v.deps.Len() != 0 || v.t.errs.Errored() {
		return
	}

	results := v.t.polls.Vote(v.requestID, v.vdr, v.response)
	if len(results) == 0 {
		return
	}
	for _, result := range results {
		_, err := v.bubbleVotes(result)
		if err != nil {
			v.t.errs.Add(err)
			return
		}
	}

	for _, result := range results {
		result := result

		v.t.Ctx.Log.Debug("finishing poll",
			zap.Stringer("result", &result),
		)
		if err := v.t.Consensus.RecordPoll(result); err != nil {
			v.t.errs.Add(err)
			return
		}
	}

	orphans := v.t.Consensus.Orphans()
	txs := make([]snowstorm.Tx, 0, orphans.Len())
	for orphanID := range orphans {
		if tx, err := v.t.VM.GetTx(orphanID); err == nil {
			txs = append(txs, tx)
		} else {
			v.t.Ctx.Log.Warn("failed to fetch tx during attempted re-issuance",
				zap.Stringer("txID", orphanID),
				zap.Error(err),
			)
		}
	}
	if len(txs) > 0 {
		v.t.Ctx.Log.Debug("re-issuing transactions",
			zap.Int("numTxs", len(txs)),
		)
	}
	if _, err := v.t.batch(txs, batchOption{force: true}); err != nil {
		v.t.errs.Add(err)
		return
	}

	if v.t.Consensus.Quiesce() {
		v.t.Ctx.Log.Debug("avalanche engine can quiesce")
		return
	}

	v.t.Ctx.Log.Debug("avalanche engine can't quiesce")
	v.t.repoll()
}

func (v *voter) bubbleVotes(votes ids.UniqueBag) (ids.UniqueBag, error) {
	vertexHeap := vertex.NewHeap()
	for vote, set := range votes {
		vtx, err := v.t.Manager.GetVtx(vote)
		if err != nil {
			v.t.Ctx.Log.Debug("dropping vote(s)",
				zap.String("reason", "failed to fetch vertex"),
				zap.Stringer("voteID", vote),
				zap.Int("numVotes", set.Len()),
				zap.Error(err),
			)
			votes.RemoveSet(vote)
			continue
		}
		vertexHeap.Push(vtx)
	}

	for vertexHeap.Len() > 0 {
		vtx := vertexHeap.Pop()
		vtxID := vtx.ID()
		set := votes.GetSet(vtxID)
		status := vtx.Status()

		if !status.Fetched() {
			v.t.Ctx.Log.Debug("dropping vote(s)",
				zap.String("reason", "vertex unknown"),
				zap.Int("numVotes", set.Len()),
				zap.Stringer("vtxID", vtxID),
			)
			votes.RemoveSet(vtxID)
			continue
		}

		if status.Decided() {
			v.t.Ctx.Log.Verbo("dropping vote(s)",
				zap.String("reason", "vertex already decided"),
				zap.Int("numVotes", set.Len()),
				zap.Stringer("vtxID", vtxID),
				zap.Stringer("status", status),
			)

			votes.RemoveSet(vtxID)
			continue
		}

		if !v.t.Consensus.VertexIssued(vtx) {
			v.t.Ctx.Log.Verbo("bubbling vote(s)",
				zap.String("reason", "vertex not issued"),
				zap.Int("numVotes", set.Len()),
				zap.Stringer("vtxID", vtxID),
			)
			votes.RemoveSet(vtxID) // Remove votes for this vertex because it hasn't been issued

			parents, err := vtx.Parents()
			if err != nil {
				return votes, err
			}
			for _, parentVtx := range parents {
				votes.UnionSet(parentVtx.ID(), set)
				vertexHeap.Push(parentVtx)
			}
		}
	}

	return votes, nil
}

```

avalanchego/snow/engine/avalanche/voter_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package avalanche

import (
	"testing"

	"github.com/stretchr/testify/require"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/engine/avalanche/vertex"
)

func TestVotingFinishesWithAbandonedDep(t *testing.T) {
	_, _, engCfg := DefaultConfig()
	mngr := vertex.NewTestManager(t)
	engCfg.Manager = mngr
	transitive, err := newTransitive(engCfg)
	require.NoError(t, err)
	require.NoError(t, transitive.Start( /*startReqID*/ 0))

	// prepare 3 validators
	vdr1 := ids.NodeID{1}
	vdr2 := ids.NodeID{2}
	vdr3 := ids.NodeID{3}

	vdrs := ids.NodeIDBag{}
	vdrs.Add(
		vdr1,
		vdr2,
	)

	// add poll for request 1
	transitive.polls.Add(1, vdrs)

	vdrs = ids.NodeIDBag{}
	vdrs.Add(
		vdr1,
		vdr3,
	)

	// add poll for request 2
	transitive.polls.Add(2, vdrs)

	// expect 2 pending polls
	require.Equal(t, 2, transitive.polls.Len())

	// vote on request 2 first
	vote1 := ids.GenerateTestID()
	vote2 := ids.GenerateTestID()

	voter1 := &voter{
		t:         transitive,
		requestID: 2,
		response:  []ids.ID{vote2},
		deps:      ids.NewSet(0),
		vdr:       vdr1,
	}

	voter3 := &voter{
		t:         transitive,
		requestID: 2,
		response:  []ids.ID{vote2},
		deps:      ids.NewSet(0),
		vdr:       vdr3,
	}

	voter1.Update()
	voter3.Update()

	// still expect 2 pending polls since request 1 voting is still pending
	require.Equal(t, 2, transitive.polls.Len())

	// vote on request 1
	// add dependency to voter1's vote which has to be fulfilled prior to finishing
	voter1Dep := ids.GenerateTestID()
	voter1DepSet := ids.NewSet(1)
	voter1DepSet.Add(voter1Dep)

	voter1 = &voter{
		t:         transitive,
		requestID: 1,
		response:  []ids.ID{vote1},
		deps:      voter1DepSet,
		vdr:       vdr1,
	}

	voter2 := &voter{
		t:         transitive,
		requestID: 1,
		response:  []ids.ID{vote1},
		deps:      ids.NewSet(0),
		vdr:       vdr2,
	}

	voter1.Update() // does nothing because the dependency is still pending
	voter2.Update() // voter1 is still remaining with the pending dependency

	voter1.Abandon(voter1Dep) // voter1 abandons dep1

	// expect all polls to have finished
	require.Equal(t, 0, transitive.polls.Len())
}

func TestVotingFinishesWithAbandonDepMiddleRequest(t *testing.T) {
	_, _, engCfg := DefaultConfig()
	mngr := vertex.NewTestManager(t)
	engCfg.Manager = mngr
	transitive, err := newTransitive(engCfg)
	require.NoError(t, err)
	require.NoError(t, transitive.Start( /*startReqID*/ 0))

	// prepare 3 validators
	vdr1 := ids.NodeID{1}
	vdr2 := ids.NodeID{2}
	vdr3 := ids.NodeID{3}

	vdrs := ids.NodeIDBag{}
	vdrs.Add(
		vdr1,
		vdr2,
	)

	// add poll for request 1
	transitive.polls.Add(1, vdrs)

	vdrs = ids.NodeIDBag{}
	vdrs.Add(
		vdr1,
		vdr3,
	)

	// add poll for request 2
	transitive.polls.Add(2, vdrs)

	vdrs = ids.NodeIDBag{}
	vdrs.Add(
		vdr2,
		vdr3,
	)

	// add poll for request 3
	transitive.polls.Add(3, vdrs)

	// expect 3 pending polls
	require.Equal(t, 3, transitive.polls.Len())

	vote1 := ids.GenerateTestID()
	vote2 := ids.GenerateTestID()
	vote3 := ids.GenerateTestID()

	// vote on request 3 first
	req3Voter1 := &voter{
		t:         transitive,
		requestID: 3,
		response:  []ids.ID{vote3},
		deps:      ids.NewSet(0),
		vdr:       vdr3,
	}

	req3Voter2 := &voter{
		t:         transitive,
		requestID: 3,
		response:  []ids.ID{vote3},
		deps:      ids.NewSet(0),
		vdr:       vdr2,
	}

	req3Voter1.Update()
	req3Voter2.Update()

	// expect 3 pending polls since 2 and 1 are still pending
	require.Equal(t, 3, transitive.polls.Len())

	// vote on request 2
	// add dependency to req2/voter3's vote which has to be fulfilled prior to finishing
	req2Voter2Dep := ids.GenerateTestID()
	req2Voter2DepSet := ids.NewSet(1)
	req2Voter2DepSet.Add(req2Voter2Dep)

	req2Voter1 := &voter{
		t:         transitive,
		requestID: 2,
		response:  []ids.ID{vote2},
		deps:      ids.NewSet(0),
		vdr:       vdr1,
	}

	req2Voter2 := &voter{
		t:         transitive,
		requestID: 2,
		response:  []ids.ID{vote2},
		deps:      req2Voter2DepSet,
		vdr:       vdr3,
	}

	req2Voter1.Update() // does nothing because dep is unfulfilled
	req2Voter2.Update()

	// still expect 3 pending polls since request 1 voting is still pending
	require.Equal(t, 3, transitive.polls.Len())

	// vote on request 1
	// add dependency to voter1's vote which has to be fulfilled prior to finishing
	req1Voter1Dep := ids.GenerateTestID()
	req1Voter1DepSet := ids.NewSet(1)
	req1Voter1DepSet.Add(req1Voter1Dep)
	req1Voter1 := &voter{
		t:         transitive,
		requestID: 1,
		response:  []ids.ID{vote1},
		deps:      req1Voter1DepSet,
		vdr:       vdr1,
	}

	req1Voter2 := &voter{
		t:         transitive,
		requestID: 1,
		response:  []ids.ID{vote1},
		deps:      ids.NewSet(0),
		vdr:       vdr2,
	}

	req1Voter1.Update() // does nothing because the req2/voter1 dependency is still pending
	req1Voter2.Update() // voter1 is still remaining with the pending dependency

	// abandon dep on voter3
	req2Voter2.Abandon(req2Voter2Dep) // voter3 abandons dep1

	// expect polls to be pending as req1/voter1's dep is still unfulfilled
	require.Equal(t, 3, transitive.polls.Len())

	req1Voter1.Abandon(req1Voter1Dep)

	// expect all polls to have finished
	require.Equal(t, 0, transitive.polls.Len())
}

func TestSharedDependency(t *testing.T) {
	_, _, engCfg := DefaultConfig()
	mngr := vertex.NewTestManager(t)
	engCfg.Manager = mngr
	transitive, err := newTransitive(engCfg)
	require.NoError(t, err)
	require.NoError(t, transitive.Start( /*startReqID*/ 0))

	// prepare 3 validators
	vdr1 := ids.NodeID{1}
	vdr2 := ids.NodeID{2}
	vdr3 := ids.NodeID{3}

	vdrs := ids.NodeIDBag{}
	vdrs.Add(
		vdr1,
		vdr2,
	)

	// add poll for request 1
	transitive.polls.Add(1, vdrs)

	vdrs = ids.NodeIDBag{}
	vdrs.Add(
		vdr1,
		vdr3,
	)

	// add poll for request 2
	transitive.polls.Add(2, vdrs)

	vdrs = ids.NodeIDBag{}
	vdrs.Add(
		vdr2,
		vdr3,
	)

	// add poll for request 3
	transitive.polls.Add(3, vdrs)

	// expect 3 pending polls
	require.Equal(t, 3, transitive.polls.Len())

	vote1 := ids.GenerateTestID()
	vote2 := ids.GenerateTestID()
	vote3 := ids.GenerateTestID()

	// req3 voters all vote

	req3Voter1 := &voter{
		t:         transitive,
		requestID: 3,
		response:  []ids.ID{vote3},
		deps:      ids.NewSet(0),
		vdr:       vdr3,
	}

	req3Voter1.Update()

	req3Voter2 := &voter{
		t:         transitive,
		requestID: 3,
		response:  []ids.ID{vote3},
		deps:      ids.NewSet(0),
		vdr:       vdr2,
	}

	req3Voter2.Update()

	// 3 polls pending because req 2 and 1 have not voted
	require.Equal(t, 3, transitive.polls.Len())

	// setup common dependency
	dep := ids.GenerateTestID()
	depSet := ids.NewSet(1)
	depSet.Add(dep)

	req2Voter1 := &voter{
		t:         transitive,
		requestID: 2,
		response:  []ids.ID{vote2},
		deps:      depSet,
		vdr:       vdr1,
	}

	// does nothing because dependency is unfulfilled
	req2Voter1.Update()

	req2Voter2 := &voter{
		t:         transitive,
		requestID: 2,
		response:  []ids.ID{vote2},
		deps:      ids.NewSet(0),
		vdr:       vdr3,
	}

	req2Voter2.Update()

	// 3 polls pending as req 2 dependency is unfulfilled and 1 has not voted
	require.Equal(t, 3, transitive.polls.Len())

	req1Voter1 := &voter{
		t:         transitive,
		requestID: 1,
		response:  []ids.ID{vote1},
		deps:      depSet,
		vdr:       vdr1,
	}

	// does nothing because dependency is unfulfilled
	req1Voter1.Update()

	req1Voter2 := &voter{
		t:         transitive,
		requestID: 1,
		response:  []ids.ID{vote1},
		deps:      ids.NewSet(0),
		vdr:       vdr2,
	}

	req1Voter2.Update()

	// 3 polls pending as req2 and req 1 dependencies are unfulfilled
	require.Equal(t, 3, transitive.polls.Len())

	// abandon dependency
	req1Voter1.Abandon(dep)
	req2Voter1.Abandon(dep)

	// expect no pending polls
	require.Equal(t, 0, transitive.polls.Len())
}

```

avalanchego/snow/engine/common/appsender/appsender_client.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package appsender

import (
	"context"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/engine/common"

	appsenderpb "github.com/ava-labs/avalanchego/proto/pb/appsender"
)

var _ common.AppSender = &Client{}

type Client struct {
	client appsenderpb.AppSenderClient
}

// NewClient returns a client that is connected to a remote AppSender.
func NewClient(client appsenderpb.AppSenderClient) *Client {
	return &Client{client: client}
}

func (c *Client) SendAppRequest(nodeIDs ids.NodeIDSet, requestID uint32, request []byte) error {
	nodeIDsBytes := make([][]byte, nodeIDs.Len())
	i := 0
	for nodeID := range nodeIDs {
		nodeID := nodeID // Prevent overwrite in next iteration
		nodeIDsBytes[i] = nodeID[:]
		i++
	}
	_, err := c.client.SendAppRequest(
		context.Background(),
		&appsenderpb.SendAppRequestMsg{
			NodeIds:   nodeIDsBytes,
			RequestId: requestID,
			Request:   request,
		},
	)
	return err
}

func (c *Client) SendAppResponse(nodeID ids.NodeID, requestID uint32, response []byte) error {
	_, err := c.client.SendAppResponse(
		context.Background(),
		&appsenderpb.SendAppResponseMsg{
			NodeId:    nodeID[:],
			RequestId: requestID,
			Response:  response,
		},
	)
	return err
}

func (c *Client) SendAppGossip(msg []byte) error {
	_, err := c.client.SendAppGossip(
		context.Background(),
		&appsenderpb.SendAppGossipMsg{
			Msg: msg,
		},
	)
	return err
}

func (c *Client) SendAppGossipSpecific(nodeIDs ids.NodeIDSet, msg []byte) error {
	nodeIDsBytes := make([][]byte, nodeIDs.Len())
	i := 0
	for nodeID := range nodeIDs {
		nodeID := nodeID // Prevent overwrite in next iteration
		nodeIDsBytes[i] = nodeID[:]
		i++
	}
	_, err := c.client.SendAppGossipSpecific(
		context.Background(),
		&appsenderpb.SendAppGossipSpecificMsg{
			NodeIds: nodeIDsBytes,
			Msg:     msg,
		},
	)
	return err
}

```

avalanchego/snow/engine/common/appsender/appsender_server.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package appsender

import (
	"context"

	"google.golang.org/protobuf/types/known/emptypb"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/engine/common"

	appsenderpb "github.com/ava-labs/avalanchego/proto/pb/appsender"
)

var _ appsenderpb.AppSenderServer = &Server{}

type Server struct {
	appsenderpb.UnsafeAppSenderServer
	appSender common.AppSender
}

// NewServer returns a messenger connected to a remote channel
func NewServer(appSender common.AppSender) *Server {
	return &Server{appSender: appSender}
}

func (s *Server) SendAppRequest(_ context.Context, req *appsenderpb.SendAppRequestMsg) (*emptypb.Empty, error) {
	nodeIDs := ids.NewNodeIDSet(len(req.NodeIds))
	for _, nodeIDBytes := range req.NodeIds {
		nodeID, err := ids.ToNodeID(nodeIDBytes)
		if err != nil {
			return nil, err
		}
		nodeIDs.Add(nodeID)
	}
	err := s.appSender.SendAppRequest(nodeIDs, req.RequestId, req.Request)
	return &emptypb.Empty{}, err
}

func (s *Server) SendAppResponse(_ context.Context, req *appsenderpb.SendAppResponseMsg) (*emptypb.Empty, error) {
	nodeID, err := ids.ToNodeID(req.NodeId)
	if err != nil {
		return nil, err
	}
	err = s.appSender.SendAppResponse(nodeID, req.RequestId, req.Response)
	return &emptypb.Empty{}, err
}

func (s *Server) SendAppGossip(_ context.Context, req *appsenderpb.SendAppGossipMsg) (*emptypb.Empty, error) {
	err := s.appSender.SendAppGossip(req.Msg)
	return &emptypb.Empty{}, err
}

func (s *Server) SendAppGossipSpecific(_ context.Context, req *appsenderpb.SendAppGossipSpecificMsg) (*emptypb.Empty, error) {
	nodeIDs := ids.NewNodeIDSet(len(req.NodeIds))
	for _, nodeIDBytes := range req.NodeIds {
		nodeID, err := ids.ToNodeID(nodeIDBytes)
		if err != nil {
			return nil, err
		}
		nodeIDs.Add(nodeID)
	}
	err := s.appSender.SendAppGossipSpecific(nodeIDs, req.Msg)
	return &emptypb.Empty{}, err
}

```

avalanchego/snow/engine/common/bootstrapable.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package common

import (
	"github.com/ava-labs/avalanchego/ids"
)

type BootstrapableEngine interface {
	Bootstrapable
	Engine
}

// Bootstrapable defines the functionality required to support bootstrapping
type Bootstrapable interface {
	// Force the provided containers to be accepted. Only returns fatal errors
	// if they occur.
	ForceAccepted(acceptedContainerIDs []ids.ID) error

	// Clear removes all containers to be processed upon bootstrapping
	Clear() error
}

```

avalanchego/snow/engine/common/bootstrapper.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package common

import (
	stdmath "math"

	"go.uber.org/zap"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/validators"
	"github.com/ava-labs/avalanchego/utils/math"
)

const (
	// StatusUpdateFrequency is how many containers should be processed between
	// logs
	StatusUpdateFrequency = 5000

	// MaxOutstandingGetAncestorsRequests is the maximum number of GetAncestors
	// sent but not responded to/failed
	MaxOutstandingGetAncestorsRequests = 10

	// MaxOutstandingBroadcastRequests is the maximum number of requests to have
	// outstanding when broadcasting.
	MaxOutstandingBroadcastRequests = 50
)

var _ Bootstrapper = &bootstrapper{}

type Bootstrapper interface {
	AcceptedFrontierHandler
	AcceptedHandler
	Haltable
	Startup() error
	Restart(reset bool) error
}

// It collects mechanisms common to both snowman and avalanche bootstrappers
type bootstrapper struct {
	Config
	Halter

	// Holds the beacons that were sampled for the accepted frontier
	sampledBeacons validators.Set
	// IDs of validators we should request an accepted frontier from
	pendingSendAcceptedFrontier ids.NodeIDSet
	// IDs of validators we requested an accepted frontier from but haven't
	// received a reply yet
	pendingReceiveAcceptedFrontier ids.NodeIDSet
	// IDs of validators that failed to respond with their accepted frontier
	failedAcceptedFrontier ids.NodeIDSet
	// IDs of all the returned accepted frontiers
	acceptedFrontierSet ids.Set

	// IDs of validators we should request filtering the accepted frontier from
	pendingSendAccepted ids.NodeIDSet
	// IDs of validators we requested filtering the accepted frontier from but
	// haven't received a reply yet
	pendingReceiveAccepted ids.NodeIDSet
	// IDs of validators that failed to respond with their filtered accepted
	// frontier
	failedAccepted ids.NodeIDSet
	// IDs of the returned accepted containers and the stake weight that has
	// marked them as accepted
	acceptedVotes    map[ids.ID]uint64
	acceptedFrontier []ids.ID

	// number of times the bootstrap has been attempted
	bootstrapAttempts int
}

func NewCommonBootstrapper(config Config) Bootstrapper {
	return &bootstrapper{
		Config: config,
	}
}

func (b *bootstrapper) AcceptedFrontier(nodeID ids.NodeID, requestID uint32, containerIDs []ids.ID) error {
	// ignores any late responses
	if requestID != b.Config.SharedCfg.RequestID {
		b.Ctx.Log.Debug("received out-of-sync AcceptedFrontier message",
			zap.Stringer("nodeID", nodeID),
			zap.Uint32("expectedRequestID", b.Config.SharedCfg.RequestID),
			zap.Uint32("requestID", requestID),
		)
		return nil
	}

	if !b.pendingReceiveAcceptedFrontier.Contains(nodeID) {
		b.Ctx.Log.Debug("received unexpected AcceptedFrontier message",
			zap.Stringer("nodeID", nodeID),
		)
		return nil
	}

	// Mark that we received a response from [nodeID]
	b.pendingReceiveAcceptedFrontier.Remove(nodeID)

	// Union the reported accepted frontier from [nodeID] with the accepted
	// frontier we got from others
	b.acceptedFrontierSet.Add(containerIDs...)

	b.sendGetAcceptedFrontiers()

	// still waiting on requests
	if b.pendingReceiveAcceptedFrontier.Len() != 0 {
		return nil
	}

	// We've received the accepted frontier from every bootstrap validator
	// Ask each bootstrap validator to filter the list of containers that we were
	// told are on the accepted frontier such that the list only contains containers
	// they think are accepted
	var err error

	// Create a newAlpha taking using the sampled beacon
	// Keep the proportion of b.Alpha in the newAlpha
	// newAlpha := totalSampledWeight * b.Alpha / totalWeight

	newAlpha := float64(b.sampledBeacons.Weight()*b.Alpha) / float64(b.Beacons.Weight())

	failedBeaconWeight, err := b.Beacons.SubsetWeight(b.failedAcceptedFrontier)
	if err != nil {
		return err
	}

	// fail the bootstrap if the weight is not enough to bootstrap
	if float64(b.sampledBeacons.Weight())-newAlpha < float64(failedBeaconWeight) {
		if b.Config.RetryBootstrap {
			b.Ctx.Log.Debug("restarting bootstrap",
				zap.String("reason", "not enough frontiers received"),
				zap.Int("numBeacons", b.Beacons.Len()),
				zap.Int("numFailedBootstrappers", b.failedAcceptedFrontier.Len()),
				zap.Int("numBootstrapAttemps", b.bootstrapAttempts),
			)
			return b.Restart(false)
		}

		b.Ctx.Log.Debug("didn't receive enough frontiers",
			zap.Int("numFailedValidators", b.failedAcceptedFrontier.Len()),
			zap.Int("numBootstrapAttempts", b.bootstrapAttempts),
		)
	}

	b.Config.SharedCfg.RequestID++
	b.acceptedFrontier = b.acceptedFrontierSet.List()

	b.sendGetAccepted()
	return nil
}

func (b *bootstrapper) GetAcceptedFrontierFailed(nodeID ids.NodeID, requestID uint32) error {
	// ignores any late responses
	if requestID != b.Config.SharedCfg.RequestID {
		b.Ctx.Log.Debug("received out-of-sync GetAcceptedFrontierFailed message",
			zap.Stringer("nodeID", nodeID),
			zap.Uint32("expectedRequestID", b.Config.SharedCfg.RequestID),
			zap.Uint32("requestID", requestID),
		)
		return nil
	}

	// If we can't get a response from [nodeID], act as though they said their
	// accepted frontier is empty and we add the validator to the failed list
	b.failedAcceptedFrontier.Add(nodeID)
	return b.AcceptedFrontier(nodeID, requestID, nil)
}

func (b *bootstrapper) Accepted(nodeID ids.NodeID, requestID uint32, containerIDs []ids.ID) error {
	// ignores any late responses
	if requestID != b.Config.SharedCfg.RequestID {
		b.Ctx.Log.Debug("received out-of-sync Accepted message",
			zap.Stringer("nodeID", nodeID),
			zap.Uint32("expectedRequestID", b.Config.SharedCfg.RequestID),
			zap.Uint32("requestID", requestID),
		)
		return nil
	}

	if !b.pendingReceiveAccepted.Contains(nodeID) {
		b.Ctx.Log.Debug("received unexpected Accepted message",
			zap.Stringer("nodeID", nodeID),
		)
		return nil
	}
	// Mark that we received a response from [nodeID]
	b.pendingReceiveAccepted.Remove(nodeID)

	weight := uint64(0)
	if w, ok := b.Beacons.GetWeight(nodeID); ok {
		weight = w
	}

	for _, containerID := range containerIDs {
		previousWeight := b.acceptedVotes[containerID]
		newWeight, err := math.Add64(weight, previousWeight)
		if err != nil {
			b.Ctx.Log.Error("failed calculating the Accepted votes",
				zap.Uint64("weight", weight),
				zap.Uint64("previousWeight", previousWeight),
				zap.Error(err),
			)
			newWeight = stdmath.MaxUint64
		}
		b.acceptedVotes[containerID] = newWeight
	}

	b.sendGetAccepted()

	// wait on pending responses
	if b.pendingReceiveAccepted.Len() != 0 {
		return nil
	}

	// We've received the filtered accepted frontier from every bootstrap validator
	// Accept all containers that have a sufficient weight behind them
	accepted := make([]ids.ID, 0, len(b.acceptedVotes))
	for containerID, weight := range b.acceptedVotes {
		if weight >= b.Alpha {
			accepted = append(accepted, containerID)
		}
	}

	// if we don't have enough weight for the bootstrap to be accepted then retry or fail the bootstrap
	size := len(accepted)
	if size == 0 && b.Beacons.Len() > 0 {
		// retry the bootstrap if the weight is not enough to bootstrap
		failedBeaconWeight, err := b.Beacons.SubsetWeight(b.failedAccepted)
		if err != nil {
			return err
		}

		// in a zero network there will be no accepted votes but the voting weight will be greater than the failed weight
		if b.Config.RetryBootstrap && b.Beacons.Weight()-b.Alpha < failedBeaconWeight {
			b.Ctx.Log.Debug("restarting bootstrap",
				zap.String("reason", "not enough votes received"),
				zap.Int("numBeacons", b.Beacons.Len()),
				zap.Int("numFailedBootstrappers", b.failedAccepted.Len()),
				zap.Int("numBootstrapAttempts", b.bootstrapAttempts),
			)
			return b.Restart(false)
		}
	}

	if !b.Config.SharedCfg.Restarted {
		b.Ctx.Log.Info("bootstrapping started syncing",
			zap.Int("numVerticesInFrontier", size),
		)
	} else {
		b.Ctx.Log.Debug("bootstrapping started syncing",
			zap.Int("numVerticesInFrontier", size),
		)
	}

	return b.Bootstrapable.ForceAccepted(accepted)
}

func (b *bootstrapper) GetAcceptedFailed(nodeID ids.NodeID, requestID uint32) error {
	// ignores any late responses
	if requestID != b.Config.SharedCfg.RequestID {
		b.Ctx.Log.Debug("received out-of-sync GetAcceptedFailed message",
			zap.Stringer("nodeID", nodeID),
			zap.Uint32("expectedRequestID", b.Config.SharedCfg.RequestID),
			zap.Uint32("requestID", requestID),
		)
		return nil
	}

	// If we can't get a response from [nodeID], act as though they said that
	// they think none of the containers we sent them in GetAccepted are
	// accepted
	b.failedAccepted.Add(nodeID)
	return b.Accepted(nodeID, requestID, nil)
}

func (b *bootstrapper) Startup() error {
	beacons, err := b.Beacons.Sample(b.Config.SampleK)
	if err != nil {
		return err
	}

	b.sampledBeacons = validators.NewSet()
	err = b.sampledBeacons.Set(beacons)
	if err != nil {
		return err
	}

	b.pendingSendAcceptedFrontier.Clear()
	for _, vdr := range beacons {
		vdrID := vdr.ID()
		b.pendingSendAcceptedFrontier.Add(vdrID)
	}

	b.pendingReceiveAcceptedFrontier.Clear()
	b.failedAcceptedFrontier.Clear()
	b.acceptedFrontierSet.Clear()

	b.pendingSendAccepted.Clear()
	for _, vdr := range b.Beacons.List() {
		vdrID := vdr.ID()
		b.pendingSendAccepted.Add(vdrID)
	}

	b.pendingReceiveAccepted.Clear()
	b.failedAccepted.Clear()
	b.acceptedVotes = make(map[ids.ID]uint64)

	b.bootstrapAttempts++
	if b.pendingSendAcceptedFrontier.Len() == 0 {
		b.Ctx.Log.Info("bootstrapping skipped",
			zap.String("reason", "no provided bootstraps"),
		)
		return b.Bootstrapable.ForceAccepted(nil)
	}

	b.Config.SharedCfg.RequestID++
	b.sendGetAcceptedFrontiers()
	return nil
}

func (b *bootstrapper) Restart(reset bool) error {
	// resets the attempts when we're pulling blocks/vertices we don't want to
	// fail the bootstrap at that stage
	if reset {
		b.Ctx.Log.Debug("Checking for new frontiers")

		b.Config.SharedCfg.Restarted = true
		b.bootstrapAttempts = 0
	}

	if b.bootstrapAttempts > 0 && b.bootstrapAttempts%b.RetryBootstrapWarnFrequency == 0 {
		b.Ctx.Log.Debug("check internet connection",
			zap.Int("numBootstrapAttempts", b.bootstrapAttempts),
		)
	}

	return b.Startup()
}

// Ask up to [MaxOutstandingBroadcastRequests] bootstrap validators to send
// their accepted frontier with the current accepted frontier
func (b *bootstrapper) sendGetAcceptedFrontiers() {
	vdrs := ids.NewNodeIDSet(1)
	for b.pendingSendAcceptedFrontier.Len() > 0 && b.pendingReceiveAcceptedFrontier.Len() < MaxOutstandingBroadcastRequests {
		vdr, _ := b.pendingSendAcceptedFrontier.Pop()
		// Add the validator to the set to send the messages to
		vdrs.Add(vdr)
		// Add the validator to send pending receipt set
		b.pendingReceiveAcceptedFrontier.Add(vdr)
	}

	if vdrs.Len() > 0 {
		b.Sender.SendGetAcceptedFrontier(vdrs, b.Config.SharedCfg.RequestID)
	}
}

// Ask up to [MaxOutstandingBroadcastRequests] bootstrap validators to send
// their filtered accepted frontier
func (b *bootstrapper) sendGetAccepted() {
	vdrs := ids.NewNodeIDSet(1)
	for b.pendingSendAccepted.Len() > 0 && b.pendingReceiveAccepted.Len() < MaxOutstandingBroadcastRequests {
		vdr, _ := b.pendingSendAccepted.Pop()
		// Add the validator to the set to send the messages to
		vdrs.Add(vdr)
		// Add the validator to send pending receipt set
		b.pendingReceiveAccepted.Add(vdr)
	}

	if vdrs.Len() > 0 {
		b.Ctx.Log.Debug("sent GetAccepted messages",
			zap.Int("numSent", vdrs.Len()),
			zap.Int("numPending", b.pendingSendAccepted.Len()),
		)
		b.Sender.SendGetAccepted(vdrs, b.Config.SharedCfg.RequestID, b.acceptedFrontier)
	}
}

```

avalanchego/snow/engine/common/config.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package common

import (
	"time"

	"github.com/ava-labs/avalanchego/snow"
	"github.com/ava-labs/avalanchego/snow/engine/common/tracker"
	"github.com/ava-labs/avalanchego/snow/validators"
)

// Config wraps the common configurations that are needed by a Snow consensus
// engine
type Config struct {
	Ctx        *snow.ConsensusContext
	Validators validators.Set
	Beacons    validators.Set

	SampleK        int
	Alpha          uint64
	StartupTracker tracker.Startup
	Sender         Sender
	Bootstrapable  Bootstrapable
	Subnet         Subnet
	Timer          Timer

	// Should Bootstrap be retried
	RetryBootstrap bool

	// Max number of times to retry bootstrap before warning the node operator
	RetryBootstrapWarnFrequency int

	// Max time to spend fetching a container and its ancestors when responding
	// to a GetAncestors
	MaxTimeGetAncestors time.Duration

	// Max number of containers in an ancestors message sent by this node.
	AncestorsMaxContainersSent int

	// This node will only consider the first [AncestorsMaxContainersReceived]
	// containers in an ancestors message it receives.
	AncestorsMaxContainersReceived int

	SharedCfg *SharedConfig
}

func (c *Config) Context() *snow.ConsensusContext { return c.Ctx }

// IsBootstrapped returns true iff this chain is done bootstrapping
func (c *Config) IsBootstrapped() bool { return c.Ctx.GetState() == snow.NormalOp }

// Shared among common.bootstrapper and snowman/avalanche bootstrapper
type SharedConfig struct {
	// Tracks the last requestID that was used in a request
	RequestID uint32

	// True if RestartBootstrap has been called at least once
	Restarted bool
}

```

avalanchego/snow/engine/common/engine.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package common

import (
	"time"

	"github.com/ava-labs/avalanchego/api/health"
	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow"
	"github.com/ava-labs/avalanchego/snow/validators"
)

// Engine describes the standard interface of a consensus engine
type Engine interface {
	Handler

	// Return the context of the chain this engine is working on
	Context() *snow.ConsensusContext

	// Start engine operations from given request ID
	Start(startReqID uint32) error

	// Returns nil if the engine is healthy.
	// Periodically called and reported through the health API
	health.Checker

	// GetVM returns this engine's VM
	GetVM() VM
}

type Handler interface {
	AllGetsServer
	StateSummaryFrontierHandler
	AcceptedStateSummaryHandler
	AcceptedFrontierHandler
	AcceptedHandler
	AncestorsHandler
	PutHandler
	QueryHandler
	ChitsHandler
	AppHandler
	InternalHandler
}

type AllGetsServer interface {
	GetStateSummaryFrontierHandler
	GetAcceptedStateSummaryHandler
	GetAcceptedFrontierHandler
	GetAcceptedHandler
	GetAncestorsHandler
	GetHandler
}

// GetStateSummaryFrontierHandler defines how a consensus engine reacts to a get
// state summary frontier message from another validator. Functions only return
// fatal errors.
type GetStateSummaryFrontierHandler interface {
	// Notify this engine of a request for the frontier of state summaries.
	//
	// The accepted frontier is the last state summary available locally.
	//
	// This function can be called by any validator. It is not safe to assume
	// this message is utilizing a unique requestID.
	//
	// This engine should respond with an StateSummaryFrontier message with the
	// same requestID, and the engine's current state summary frontier.
	GetStateSummaryFrontier(validatorID ids.NodeID, requestID uint32) error
}

// StateSummaryFrontierHandler defines how a consensus engine reacts to a state
// summary frontier message from other validators. Functions only return fatal
// errors.
type StateSummaryFrontierHandler interface {
	// Notify this engine of a state summary frontier.
	//
	// This function can be called by any validator. It is not safe to assume
	// this message is in response to a GetStateSummaryFrontier message, is
	// utilizing a unique requestID, or that the summary bytes are from a valid
	// state summary.
	StateSummaryFrontier(validatorID ids.NodeID, requestID uint32, summary []byte) error

	// Notify this engine that a get state summary frontier request it issued
	// has failed.
	//
	// This function will be called if the engine sent a GetStateSummaryFrontier
	// message that is not anticipated to be responded to. This could be because
	// the recipient of the message is unknown or if the message request has
	// timed out.
	//
	// The validatorID, and requestID, are assumed to be the same as those sent
	// in the GetStateSummaryFrontier message.
	GetStateSummaryFrontierFailed(validatorID ids.NodeID, requestID uint32) error
}

// GetAcceptedStateSummaryHandler defines how a consensus engine reacts to a get
// accepted state summary message from another validator. Functions only return
// fatal errors.
type GetAcceptedStateSummaryHandler interface {
	// Notify this engine of a request to return state summary IDs referenced by
	// the provided keys.
	//
	// This function can be called by any validator. It is not safe to assume
	// this message is utilizing a unique requestID. However, the validatorID is
	// assumed to be authenticated.
	//
	// This engine should respond with an AcceptedStateSummary message with the
	// same requestID, and the subset of the state summaries that this node has
	// locally available.
	GetAcceptedStateSummary(validatorID ids.NodeID, requestID uint32, keys []uint64) error
}

// AcceptedStateSummaryHandler defines how a consensus engine reacts to an
// accepted state summary message from another validator. Functions only return
// fatal errors.
type AcceptedStateSummaryHandler interface {
	// Notify this engine of a set of state summaries.
	//
	// This function can be called by any validator. It is not safe to assume
	// this message is in response to a GetAcceptedStateSummary message,
	// is utilizing a unique requestID, or that the summaryIDs are a subset of the
	// state summaries requested by key from a GetAcceptedStateSummary message.
	AcceptedStateSummary(validatorID ids.NodeID, requestID uint32, summaryIDs []ids.ID) error

	// Notify this engine that a get accepted state summary request it issued has
	// failed.
	//
	// This function will be called if the engine sent a GetAcceptedStateSummary
	// message that is not anticipated to be responded to. This could be because
	// the recipient of the message is unknown or if the message request has
	// timed out.
	//
	// The validatorID, and requestID, are assumed to be the same as those sent
	// in the GetAcceptedStateSummary message.
	GetAcceptedStateSummaryFailed(validatorID ids.NodeID, requestID uint32) error
}

// GetAcceptedFrontierHandler defines how a consensus engine reacts to a get
// accepted frontier message from another validator. Functions only return fatal
// errors.
type GetAcceptedFrontierHandler interface {
	// Notify this engine of a request for the accepted frontier of vertices.
	//
	// The accepted frontier is the set of accepted vertices that do not have
	// any accepted descendants.
	//
	// This function can be called by any validator. It is not safe to assume
	// this message is utilizing a unique requestID.
	//
	// This engine should respond with an AcceptedFrontier message with the same
	// requestID, and the engine's current accepted frontier.
	GetAcceptedFrontier(validatorID ids.NodeID, requestID uint32) error
}

// AcceptedFrontierHandler defines how a consensus engine reacts to accepted
// frontier messages from other validators. Functions only return fatal errors.
type AcceptedFrontierHandler interface {
	// Notify this engine of an accepted frontier.
	//
	// This function can be called by any validator. It is not safe to assume
	// this message is in response to a GetAcceptedFrontier message, is
	// utilizing a unique requestID, or that the containerIDs from a valid
	// frontier.
	AcceptedFrontier(
		validatorID ids.NodeID,
		requestID uint32,
		containerIDs []ids.ID,
	) error

	// Notify this engine that a get accepted frontier request it issued has
	// failed.
	//
	// This function will be called if the engine sent a GetAcceptedFrontier
	// message that is not anticipated to be responded to. This could be because
	// the recipient of the message is unknown or if the message request has
	// timed out.
	//
	// The validatorID, and requestID, are assumed to be the same as those sent
	// in the GetAcceptedFrontier message.
	GetAcceptedFrontierFailed(validatorID ids.NodeID, requestID uint32) error
}

// GetAcceptedHandler defines how a consensus engine reacts to a get accepted
// message from another validator. Functions only return fatal errors.
type GetAcceptedHandler interface {
	// Notify this engine of a request to filter non-accepted vertices.
	//
	// This function can be called by any validator. It is not safe to assume
	// this message is utilizing a unique requestID. However, the validatorID is
	// assumed to be authenticated.
	//
	// This engine should respond with an Accepted message with the same
	// requestID, and the subset of the containerIDs that this node has decided
	// are accepted.
	GetAccepted(
		validatorID ids.NodeID,
		requestID uint32,
		containerIDs []ids.ID,
	) error
}

// AcceptedHandler defines how a consensus engine reacts to accepted messages
// from other validators. Functions only return fatal
// errors.
type AcceptedHandler interface {
	// Notify this engine of a set of accepted vertices.
	//
	// This function can be called by any validator. It is not safe to assume
	// this message is in response to a GetAccepted message, is utilizing a
	// unique requestID, or that the containerIDs are a subset of the
	// containerIDs from a GetAccepted message.
	Accepted(
		validatorID ids.NodeID,
		requestID uint32,
		containerIDs []ids.ID,
	) error

	// Notify this engine that a get accepted request it issued has failed.
	//
	// This function will be called if the engine sent a GetAccepted message
	// that is not anticipated to be responded to. This could be because the
	// recipient of the message is unknown or if the message request has timed
	// out.
	//
	// The validatorID, and requestID, are assumed to be the same as those sent
	// in the GetAccepted message.
	GetAcceptedFailed(validatorID ids.NodeID, requestID uint32) error
}

// GetAncestorsHandler defines how a consensus engine reacts to a get ancestors
// message from another validator. Functions only return fatal errors.
type GetAncestorsHandler interface {
	// Notify this engine of a request for a container and its ancestors.
	//
	// The request is from validator [validatorID]. The requested container is
	// [containerID].
	//
	// This function can be called by any validator. It is not safe to assume
	// this message is utilizing a unique requestID. It is also not safe to
	// assume the requested containerID exists.
	//
	// This engine should respond with an Ancestors message with the same
	// requestID, which contains [containerID] as well as its ancestors. See
	// Ancestors's documentation.
	//
	// If this engine doesn't have some ancestors, it should reply with its best
	// effort attempt at getting them. If this engine doesn't have [containerID]
	// it can ignore this message.
	GetAncestors(validatorID ids.NodeID, requestID uint32, containerID ids.ID) error
}

// AncestorsHandler defines how a consensus engine reacts to bootstrapping
// retrieval messages from other validators. Functions only return fatal errors.
type AncestorsHandler interface {
	// Notify this engine of multiple containers.
	//
	// Each element of [containers] is the byte representation of a container.
	//
	// This should only be called during bootstrapping, in response to a
	// GetAncestors message to [validatorID] with request ID [requestID].
	//
	// This call should contain the container requested in that message, along
	// with ancestors. The containers should be in BFS order (ie the first
	// container must be the container requested in the GetAncestors message and
	// further back ancestors are later in [containers]
	//
	// It is not safe to assume this message is in response to a GetAncestor
	// message, that this message has a unique requestID or that any of the
	// containers in [containers] are valid.
	Ancestors(
		validatorID ids.NodeID,
		requestID uint32,
		containers [][]byte,
	) error

	// Notify this engine that a GetAncestors request it issued has failed.
	//
	// This function will be called if the engine sent a GetAncestors message
	// that is not anticipated to be responded to. This could be because the
	// recipient of the message is unknown or if the message request has timed
	// out.
	//
	// The validatorID and requestID are assumed to be the same as those sent in
	// the GetAncestors message.
	GetAncestorsFailed(validatorID ids.NodeID, requestID uint32) error
}

// GetHandler defines how a consensus engine reacts to get message from another
// validator. Functions only return fatal errors.
type GetHandler interface {
	// Notify this engine of a request for a container.
	//
	// This function can be called by any validator. It is not safe to assume
	// this message is utilizing a unique requestID. It is also not safe to
	// assume the requested containerID exists.
	//
	// There should never be a situation where a virtuous node sends a Get
	// request to another virtuous node that does not have the requested
	// container.
	//
	// This engine should respond with a Put message with the same requestID if
	// the container was locally available. Otherwise, the message can be safely
	// dropped.
	Get(validatorID ids.NodeID, requestID uint32, containerID ids.ID) error
}

// PutHandler defines how a consensus engine reacts to put messages from other
// validators. Functions only return fatal errors.
type PutHandler interface {
	// Notify this engine of a container.
	//
	// This function can be called by any validator. It is not safe to assume
	// this message is utilizing a unique requestID.
	Put(
		validatorID ids.NodeID,
		requestID uint32,
		container []byte,
	) error

	// Notify this engine that a get request it issued has failed.
	//
	// This function will be called if the engine sent a Get message that is not
	// anticipated to be responded to. This could be because the recipient of
	// the message is unknown or if the message request has timed out.
	//
	// The validatorID and requestID are assumed to be the same as those sent in
	// the Get message.
	GetFailed(validatorID ids.NodeID, requestID uint32) error
}

// QueryHandler defines how a consensus engine reacts to query messages from
// other validators. Functions only return fatal errors.
type QueryHandler interface {
	// Notify this engine of a request for our preferences.
	//
	// This function can be called by any validator. It is not safe to assume
	// this message is utilizing a unique requestID. However, the validatorID is
	// assumed to be authenticated.
	//
	// If the container or its ancestry is incomplete, this engine is expected
	// to request the missing containers from the validator. Once the ancestry
	// is complete, this engine should send this validator the current
	// preferences in a Chits message. The Chits message should have the same
	// requestID that was passed in here.
	PullQuery(
		validatorID ids.NodeID,
		requestID uint32,
		containerID ids.ID,
	) error

	// Notify this engine of a request for our preferences.
	//
	// This function can be called by any validator. It is not safe to assume
	// this message is utilizing a unique requestID.
	//
	// This function is meant to behave the same way as PullQuery, except the
	// container is optimistically provided to potentially remove the need for
	// a series of Get/Put messages.
	//
	// If the ancestry of the container is incomplete, this engine is expected
	// to request the ancestry from the validator. Once the ancestry is
	// complete, this engine should send this validator the current preferences
	// in a Chits message. The Chits message should have the same requestID that
	// was passed in here.
	PushQuery(
		validatorID ids.NodeID,
		requestID uint32,
		container []byte,
	) error
}

// ChitsHandler defines how a consensus engine reacts to query response messages
// from other validators. Functions only return fatal errors.
type ChitsHandler interface {
	// Notify this engine of the specified validators preferences.
	//
	// This function can be called by any validator. It is not safe to assume
	// this message is in response to a PullQuery or a PushQuery message.
	// However, the validatorID is assumed to be authenticated.
	Chits(validatorID ids.NodeID, requestID uint32, containerIDs []ids.ID) error

	// Notify this engine that a query it issued has failed.
	//
	// This function will be called if the engine sent a PullQuery or PushQuery
	// message that is not anticipated to be responded to. This could be because
	// the recipient of the message is unknown or if the message request has
	// timed out.
	//
	// The validatorID and the requestID are assumed to be the same as those
	// sent in the Query message.
	QueryFailed(validatorID ids.NodeID, requestID uint32) error
}

// AppHandler defines how a consensus engine reacts to app specific messages.
// Functions only return fatal errors.
type AppHandler interface {
	// Notify this engine of a request for data from [nodeID].
	//
	// The meaning of [request], and what should be sent in response to it, is
	// application (VM) specific.
	//
	// It is not guaranteed that:
	// * [request] is well-formed/valid.
	//
	// This node should typically send an AppResponse to [nodeID] in response to
	// a valid message using the same request ID before the deadline. However,
	// the VM may arbitrarily choose to not send a response to this request.
	AppRequest(nodeID ids.NodeID, requestID uint32, deadline time.Time, request []byte) error

	// Notify this engine that an AppRequest message it sent to [nodeID] with
	// request ID [requestID] failed.
	//
	// This may be because the request timed out or because the message couldn't
	// be sent to [nodeID].
	//
	// It is guaranteed that:
	// * This engine sent a request to [nodeID] with ID [requestID].
	// * AppRequestFailed([nodeID], [requestID]) has not already been called.
	// * AppResponse([nodeID], [requestID]) has not already been called.
	AppRequestFailed(nodeID ids.NodeID, requestID uint32) error

	// Notify this engine of a response to the AppRequest message it sent to
	// [nodeID] with request ID [requestID].
	//
	// The meaning of [response] is application (VM) specifc.
	//
	// It is guaranteed that:
	// * This engine sent a request to [nodeID] with ID [requestID].
	// * AppRequestFailed([nodeID], [requestID]) has not already been called.
	// * AppResponse([nodeID], [requestID]) has not already been called.
	//
	// It is not guaranteed that:
	// * [response] contains the expected response
	// * [response] is well-formed/valid.
	//
	// If [response] is invalid or not the expected response, the VM chooses how
	// to react. For example, the VM may send another AppRequest, or it may give
	// up trying to get the requested information.
	AppResponse(nodeID ids.NodeID, requestID uint32, response []byte) error

	// Notify this engine of a gossip message from [nodeID].
	//
	// The meaning of [msg] is application (VM) specific, and the VM defines how
	// to react to this message.
	//
	// This message is not expected in response to any event, and it does not
	// need to be responded to.
	//
	// A node may gossip the same message multiple times. That is,
	// AppGossip([nodeID], [msg]) may be called multiple times.
	AppGossip(nodeID ids.NodeID, msg []byte) error
}

// InternalHandler defines how this consensus engine reacts to messages from
// other components of this validator. Functions only return fatal errors if
// they occur.
type InternalHandler interface {
	// Notify this engine of peer changes.
	validators.Connector

	// Notify this engine that a registered timeout has fired.
	Timeout() error

	// Gossip to the network a container on the accepted frontier
	Gossip() error

	// Halt this engine.
	//
	// This function will be called before the environment starts exiting. This
	// function is slightly special, in that it does not expect the chain's
	// context lock to be held before calling this function.
	Halt()

	// Shutdown this engine.
	//
	// This function will be called when the environment is exiting.
	Shutdown() error

	// Notify this engine of a message from the virtual machine.
	Notify(Message) error
}

```

avalanchego/snow/engine/common/fetcher.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package common

type Fetcher struct {
	// tracks which validators were asked for which containers in which requests
	OutstandingRequests Requests

	// Called when bootstrapping is done on a specific chain
	OnFinished func(lastReqID uint32) error
}

```

avalanchego/snow/engine/common/fx.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package common

import (
	"github.com/ava-labs/avalanchego/ids"
)

// Fx wraps an instance of a feature extension
type Fx struct {
	ID ids.ID
	Fx interface{}
}

```

avalanchego/snow/engine/common/halter.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package common

import (
	"sync/atomic"
)

var _ Haltable = &Halter{}

type Haltable interface {
	Halt()
	Halted() bool
}

type Halter struct {
	halted uint32
}

func (h *Halter) Halt() {
	atomic.StoreUint32(&h.halted, 1)
}

func (h *Halter) Halted() bool {
	return atomic.LoadUint32(&h.halted) == 1
}

```

avalanchego/snow/engine/common/http_handler.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package common

import (
	"net/http"
)

// LockOption allows the vm to specify their lock option based on their endpoint
type LockOption uint32

// List of all allowed options
const (
	WriteLock = iota
	ReadLock
	NoLock
)

type HTTPHandler struct {
	LockOptions LockOption
	Handler     http.Handler
}

```

avalanchego/snow/engine/common/message.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package common

import (
	"fmt"
)

// TODO: Consider renaming Message to, say, VMMessage

// Message is an enum of the message types that vms can send to consensus
type Message uint32

const (
	// PendingTxs notifies a consensus engine that
	// its VM has pending transactions
	// (i.e. it would like to add a new block/vertex to consensus)
	PendingTxs Message = iota

	// StateSyncDone notifies the state syncer engine that the VM has finishing
	// syncing the requested state summary.
	StateSyncDone

	// StopVertex notifies a consensus that it has a pending stop vertex
	StopVertex
)

func (msg Message) String() string {
	switch msg {
	case PendingTxs:
		return "Pending Transactions"
	case StateSyncDone:
		return "State Sync Done"
	case StopVertex:
		return "Pending Stop Vertex"
	default:
		return fmt.Sprintf("Unknown Message: %d", msg)
	}
}

```

avalanchego/snow/engine/common/mixed_query.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package common

import "github.com/ava-labs/avalanchego/ids"

// Send a query composed partially of push queries and partially of pull queries.
// The validators in [vdrs] will be queried.
// This function sends at most [numPushTo] push queries. The rest are pull queries.
// If [numPushTo] > len(vdrs), len(vdrs) push queries are sent.
// [containerID] and [container] are the ID and body of the container being queried.
// [sender] is used to actually send the queries.
func SendMixedQuery(
	sender Sender,
	vdrs []ids.NodeID,
	numPushTo int,
	reqID uint32,
	containerID ids.ID,
	container []byte,
) {
	if numPushTo > len(vdrs) {
		numPushTo = len(vdrs)
	}
	if numPushTo > 0 {
		sendPushQueryTo := ids.NewNodeIDSet(numPushTo)
		sendPushQueryTo.Add(vdrs[:numPushTo]...)
		sender.SendPushQuery(sendPushQueryTo, reqID, container)
	}
	if numPullTo := len(vdrs) - numPushTo; numPullTo > 0 {
		sendPullQueryTo := ids.NewNodeIDSet(numPullTo)
		sendPullQueryTo.Add(vdrs[numPushTo:]...)
		sender.SendPullQuery(sendPullQueryTo, reqID, containerID)
	}
}

```

avalanchego/snow/engine/common/mixed_query_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package common

import (
	"fmt"
	"testing"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/golang/mock/gomock"
)

func TestSendMixedQuery(t *testing.T) {
	type test struct {
		senderF   func() *MockSender
		vdrs      []ids.NodeID
		numPushTo int
	}
	reqID := uint32(1337)
	containerID := ids.GenerateTestID()
	containerBytes := []byte{'y', 'e', 'e', 't'}
	ctrl := gomock.NewController(t)
	defer ctrl.Finish()

	vdr1, vdr2, vdr3 := ids.GenerateTestNodeID(), ids.GenerateTestNodeID(), ids.GenerateTestNodeID()
	tests := []test{
		{
			senderF: func() *MockSender {
				s := NewMockSender(ctrl)
				s.EXPECT().SendPushQuery(
					ids.NodeIDSet{vdr1: struct{}{}, vdr2: struct{}{}, vdr3: struct{}{}},
					reqID,
					containerBytes,
				).Times(1)
				s.EXPECT().SendPullQuery(
					gomock.Any(),
					gomock.Any(),
					gomock.Any(),
				).Times(0)
				return s
			},
			vdrs:      []ids.NodeID{vdr1, vdr2, vdr3},
			numPushTo: 3,
		},
		{
			senderF: func() *MockSender {
				s := NewMockSender(ctrl)
				s.EXPECT().SendPushQuery(
					ids.NodeIDSet{vdr1: struct{}{}},
					reqID,
					containerBytes,
				).Times(1)
				s.EXPECT().SendPullQuery(
					ids.NodeIDSet{vdr2: struct{}{}, vdr3: struct{}{}},
					reqID,
					containerID,
				).Times(1)
				return s
			},
			vdrs:      []ids.NodeID{vdr1, vdr2, vdr3},
			numPushTo: 1,
		},
		{
			senderF: func() *MockSender {
				s := NewMockSender(ctrl)
				s.EXPECT().SendPushQuery(
					ids.NodeIDSet{vdr1: struct{}{}, vdr2: struct{}{}},
					reqID,
					containerBytes,
				).Times(1)
				s.EXPECT().SendPullQuery(
					gomock.Any(),
					gomock.Any(),
					gomock.Any(),
				).Times(0)
				return s
			},
			vdrs:      []ids.NodeID{vdr1, vdr2},
			numPushTo: 2,
		},
		{
			senderF: func() *MockSender {
				s := NewMockSender(ctrl)
				s.EXPECT().SendPushQuery(
					gomock.Any(),
					reqID,
					containerBytes,
				).Times(0)
				s.EXPECT().SendPullQuery(
					ids.NodeIDSet{vdr1: struct{}{}},
					reqID,
					containerID,
				).Times(1)
				return s
			},
			vdrs:      []ids.NodeID{vdr1},
			numPushTo: 0,
		},
		{
			senderF: func() *MockSender {
				s := NewMockSender(ctrl)
				s.EXPECT().SendPushQuery(
					ids.NodeIDSet{vdr1: struct{}{}, vdr2: struct{}{}},
					reqID,
					containerBytes,
				).Times(1)
				s.EXPECT().SendPullQuery(
					gomock.Any(),
					gomock.Any(),
					gomock.Any(),
				).Times(0)
				return s
			},
			vdrs:      []ids.NodeID{vdr1, vdr2},
			numPushTo: 4,
		},
	}

	for _, tt := range tests {
		t.Run(
			fmt.Sprintf("numPushTo: %d, numVdrs: %d", tt.numPushTo, len(tt.vdrs)),
			func(t *testing.T) {
				sender := tt.senderF()
				SendMixedQuery(
					sender,
					tt.vdrs,
					tt.numPushTo,
					reqID,
					containerID,
					containerBytes,
				)
			},
		)
	}
}

```

avalanchego/snow/engine/common/mock_sender.go:
```
// Code generated by MockGen. DO NOT EDIT.
// Source: snow/engine/common/sender.go

package common

import (
	reflect "reflect"

	ids "github.com/ava-labs/avalanchego/ids"
	snow "github.com/ava-labs/avalanchego/snow"
	gomock "github.com/golang/mock/gomock"
)

// MockSender is a mock of Sender interface.
type MockSender struct {
	ctrl     *gomock.Controller
	recorder *MockSenderMockRecorder
}

// MockSenderMockRecorder is the mock recorder for MockSender.
type MockSenderMockRecorder struct {
	mock *MockSender
}

// NewMockSender creates a new mock instance.
func NewMockSender(ctrl *gomock.Controller) *MockSender {
	mock := &MockSender{ctrl: ctrl}
	mock.recorder = &MockSenderMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockSender) EXPECT() *MockSenderMockRecorder {
	return m.recorder
}

// Accept mocks base method.
func (m *MockSender) Accept(ctx *snow.ConsensusContext, containerID ids.ID, container []byte) error {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "Accept", ctx, containerID, container)
	ret0, _ := ret[0].(error)
	return ret0
}

// Accept indicates an expected call of Accept.
func (mr *MockSenderMockRecorder) Accept(ctx, containerID, container interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Accept", reflect.TypeOf((*MockSender)(nil).Accept), ctx, containerID, container)
}

// SendAccepted mocks base method.
func (m *MockSender) SendAccepted(nodeID ids.NodeID, requestID uint32, containerIDs []ids.ID) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "SendAccepted", nodeID, requestID, containerIDs)
}

// SendAccepted indicates an expected call of SendAccepted.
func (mr *MockSenderMockRecorder) SendAccepted(nodeID, requestID, containerIDs interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "SendAccepted", reflect.TypeOf((*MockSender)(nil).SendAccepted), nodeID, requestID, containerIDs)
}

// SendAcceptedFrontier mocks base method.
func (m *MockSender) SendAcceptedFrontier(nodeID ids.NodeID, requestID uint32, containerIDs []ids.ID) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "SendAcceptedFrontier", nodeID, requestID, containerIDs)
}

// SendAcceptedFrontier indicates an expected call of SendAcceptedFrontier.
func (mr *MockSenderMockRecorder) SendAcceptedFrontier(nodeID, requestID, containerIDs interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "SendAcceptedFrontier", reflect.TypeOf((*MockSender)(nil).SendAcceptedFrontier), nodeID, requestID, containerIDs)
}

// SendAcceptedStateSummary mocks base method.
func (m *MockSender) SendAcceptedStateSummary(nodeID ids.NodeID, requestID uint32, summaryIDs []ids.ID) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "SendAcceptedStateSummary", nodeID, requestID, summaryIDs)
}

// SendAcceptedStateSummary indicates an expected call of SendAcceptedStateSummary.
func (mr *MockSenderMockRecorder) SendAcceptedStateSummary(nodeID, requestID, summaryIDs interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "SendAcceptedStateSummary", reflect.TypeOf((*MockSender)(nil).SendAcceptedStateSummary), nodeID, requestID, summaryIDs)
}

// SendAncestors mocks base method.
func (m *MockSender) SendAncestors(nodeID ids.NodeID, requestID uint32, containers [][]byte) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "SendAncestors", nodeID, requestID, containers)
}

// SendAncestors indicates an expected call of SendAncestors.
func (mr *MockSenderMockRecorder) SendAncestors(nodeID, requestID, containers interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "SendAncestors", reflect.TypeOf((*MockSender)(nil).SendAncestors), nodeID, requestID, containers)
}

// SendAppGossip mocks base method.
func (m *MockSender) SendAppGossip(appGossipBytes []byte) error {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "SendAppGossip", appGossipBytes)
	ret0, _ := ret[0].(error)
	return ret0
}

// SendAppGossip indicates an expected call of SendAppGossip.
func (mr *MockSenderMockRecorder) SendAppGossip(appGossipBytes interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "SendAppGossip", reflect.TypeOf((*MockSender)(nil).SendAppGossip), appGossipBytes)
}

// SendAppGossipSpecific mocks base method.
func (m *MockSender) SendAppGossipSpecific(nodeIDs ids.NodeIDSet, appGossipBytes []byte) error {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "SendAppGossipSpecific", nodeIDs, appGossipBytes)
	ret0, _ := ret[0].(error)
	return ret0
}

// SendAppGossipSpecific indicates an expected call of SendAppGossipSpecific.
func (mr *MockSenderMockRecorder) SendAppGossipSpecific(nodeIDs, appGossipBytes interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "SendAppGossipSpecific", reflect.TypeOf((*MockSender)(nil).SendAppGossipSpecific), nodeIDs, appGossipBytes)
}

// SendAppRequest mocks base method.
func (m *MockSender) SendAppRequest(nodeIDs ids.NodeIDSet, requestID uint32, appRequestBytes []byte) error {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "SendAppRequest", nodeIDs, requestID, appRequestBytes)
	ret0, _ := ret[0].(error)
	return ret0
}

// SendAppRequest indicates an expected call of SendAppRequest.
func (mr *MockSenderMockRecorder) SendAppRequest(nodeIDs, requestID, appRequestBytes interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "SendAppRequest", reflect.TypeOf((*MockSender)(nil).SendAppRequest), nodeIDs, requestID, appRequestBytes)
}

// SendAppResponse mocks base method.
func (m *MockSender) SendAppResponse(nodeID ids.NodeID, requestID uint32, appResponseBytes []byte) error {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "SendAppResponse", nodeID, requestID, appResponseBytes)
	ret0, _ := ret[0].(error)
	return ret0
}

// SendAppResponse indicates an expected call of SendAppResponse.
func (mr *MockSenderMockRecorder) SendAppResponse(nodeID, requestID, appResponseBytes interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "SendAppResponse", reflect.TypeOf((*MockSender)(nil).SendAppResponse), nodeID, requestID, appResponseBytes)
}

// SendChits mocks base method.
func (m *MockSender) SendChits(nodeID ids.NodeID, requestID uint32, votes []ids.ID) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "SendChits", nodeID, requestID, votes)
}

// SendChits indicates an expected call of SendChits.
func (mr *MockSenderMockRecorder) SendChits(nodeID, requestID, votes interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "SendChits", reflect.TypeOf((*MockSender)(nil).SendChits), nodeID, requestID, votes)
}

// SendGet mocks base method.
func (m *MockSender) SendGet(nodeID ids.NodeID, requestID uint32, containerID ids.ID) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "SendGet", nodeID, requestID, containerID)
}

// SendGet indicates an expected call of SendGet.
func (mr *MockSenderMockRecorder) SendGet(nodeID, requestID, containerID interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "SendGet", reflect.TypeOf((*MockSender)(nil).SendGet), nodeID, requestID, containerID)
}

// SendGetAccepted mocks base method.
func (m *MockSender) SendGetAccepted(nodeIDs ids.NodeIDSet, requestID uint32, containerIDs []ids.ID) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "SendGetAccepted", nodeIDs, requestID, containerIDs)
}

// SendGetAccepted indicates an expected call of SendGetAccepted.
func (mr *MockSenderMockRecorder) SendGetAccepted(nodeIDs, requestID, containerIDs interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "SendGetAccepted", reflect.TypeOf((*MockSender)(nil).SendGetAccepted), nodeIDs, requestID, containerIDs)
}

// SendGetAcceptedFrontier mocks base method.
func (m *MockSender) SendGetAcceptedFrontier(nodeIDs ids.NodeIDSet, requestID uint32) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "SendGetAcceptedFrontier", nodeIDs, requestID)
}

// SendGetAcceptedFrontier indicates an expected call of SendGetAcceptedFrontier.
func (mr *MockSenderMockRecorder) SendGetAcceptedFrontier(nodeIDs, requestID interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "SendGetAcceptedFrontier", reflect.TypeOf((*MockSender)(nil).SendGetAcceptedFrontier), nodeIDs, requestID)
}

// SendGetAcceptedStateSummary mocks base method.
func (m *MockSender) SendGetAcceptedStateSummary(nodeIDs ids.NodeIDSet, requestID uint32, heights []uint64) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "SendGetAcceptedStateSummary", nodeIDs, requestID, heights)
}

// SendGetAcceptedStateSummary indicates an expected call of SendGetAcceptedStateSummary.
func (mr *MockSenderMockRecorder) SendGetAcceptedStateSummary(nodeIDs, requestID, heights interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "SendGetAcceptedStateSummary", reflect.TypeOf((*MockSender)(nil).SendGetAcceptedStateSummary), nodeIDs, requestID, heights)
}

// SendGetAncestors mocks base method.
func (m *MockSender) SendGetAncestors(nodeID ids.NodeID, requestID uint32, containerID ids.ID) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "SendGetAncestors", nodeID, requestID, containerID)
}

// SendGetAncestors indicates an expected call of SendGetAncestors.
func (mr *MockSenderMockRecorder) SendGetAncestors(nodeID, requestID, containerID interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "SendGetAncestors", reflect.TypeOf((*MockSender)(nil).SendGetAncestors), nodeID, requestID, containerID)
}

// SendGetStateSummaryFrontier mocks base method.
func (m *MockSender) SendGetStateSummaryFrontier(nodeIDs ids.NodeIDSet, requestID uint32) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "SendGetStateSummaryFrontier", nodeIDs, requestID)
}

// SendGetStateSummaryFrontier indicates an expected call of SendGetStateSummaryFrontier.
func (mr *MockSenderMockRecorder) SendGetStateSummaryFrontier(nodeIDs, requestID interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "SendGetStateSummaryFrontier", reflect.TypeOf((*MockSender)(nil).SendGetStateSummaryFrontier), nodeIDs, requestID)
}

// SendGossip mocks base method.
func (m *MockSender) SendGossip(container []byte) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "SendGossip", container)
}

// SendGossip indicates an expected call of SendGossip.
func (mr *MockSenderMockRecorder) SendGossip(container interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "SendGossip", reflect.TypeOf((*MockSender)(nil).SendGossip), container)
}

// SendPullQuery mocks base method.
func (m *MockSender) SendPullQuery(nodeIDs ids.NodeIDSet, requestID uint32, containerID ids.ID) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "SendPullQuery", nodeIDs, requestID, containerID)
}

// SendPullQuery indicates an expected call of SendPullQuery.
func (mr *MockSenderMockRecorder) SendPullQuery(nodeIDs, requestID, containerID interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "SendPullQuery", reflect.TypeOf((*MockSender)(nil).SendPullQuery), nodeIDs, requestID, containerID)
}

// SendPushQuery mocks base method.
func (m *MockSender) SendPushQuery(nodeIDs ids.NodeIDSet, requestID uint32, container []byte) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "SendPushQuery", nodeIDs, requestID, container)
}

// SendPushQuery indicates an expected call of SendPushQuery.
func (mr *MockSenderMockRecorder) SendPushQuery(nodeIDs, requestID, container interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "SendPushQuery", reflect.TypeOf((*MockSender)(nil).SendPushQuery), nodeIDs, requestID, container)
}

// SendPut mocks base method.
func (m *MockSender) SendPut(nodeID ids.NodeID, requestID uint32, container []byte) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "SendPut", nodeID, requestID, container)
}

// SendPut indicates an expected call of SendPut.
func (mr *MockSenderMockRecorder) SendPut(nodeID, requestID, container interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "SendPut", reflect.TypeOf((*MockSender)(nil).SendPut), nodeID, requestID, container)
}

// SendStateSummaryFrontier mocks base method.
func (m *MockSender) SendStateSummaryFrontier(nodeID ids.NodeID, requestID uint32, summary []byte) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "SendStateSummaryFrontier", nodeID, requestID, summary)
}

// SendStateSummaryFrontier indicates an expected call of SendStateSummaryFrontier.
func (mr *MockSenderMockRecorder) SendStateSummaryFrontier(nodeID, requestID, summary interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "SendStateSummaryFrontier", reflect.TypeOf((*MockSender)(nil).SendStateSummaryFrontier), nodeID, requestID, summary)
}

// MockStateSummarySender is a mock of StateSummarySender interface.
type MockStateSummarySender struct {
	ctrl     *gomock.Controller
	recorder *MockStateSummarySenderMockRecorder
}

// MockStateSummarySenderMockRecorder is the mock recorder for MockStateSummarySender.
type MockStateSummarySenderMockRecorder struct {
	mock *MockStateSummarySender
}

// NewMockStateSummarySender creates a new mock instance.
func NewMockStateSummarySender(ctrl *gomock.Controller) *MockStateSummarySender {
	mock := &MockStateSummarySender{ctrl: ctrl}
	mock.recorder = &MockStateSummarySenderMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockStateSummarySender) EXPECT() *MockStateSummarySenderMockRecorder {
	return m.recorder
}

// SendGetStateSummaryFrontier mocks base method.
func (m *MockStateSummarySender) SendGetStateSummaryFrontier(nodeIDs ids.NodeIDSet, requestID uint32) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "SendGetStateSummaryFrontier", nodeIDs, requestID)
}

// SendGetStateSummaryFrontier indicates an expected call of SendGetStateSummaryFrontier.
func (mr *MockStateSummarySenderMockRecorder) SendGetStateSummaryFrontier(nodeIDs, requestID interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "SendGetStateSummaryFrontier", reflect.TypeOf((*MockStateSummarySender)(nil).SendGetStateSummaryFrontier), nodeIDs, requestID)
}

// SendStateSummaryFrontier mocks base method.
func (m *MockStateSummarySender) SendStateSummaryFrontier(nodeID ids.NodeID, requestID uint32, summary []byte) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "SendStateSummaryFrontier", nodeID, requestID, summary)
}

// SendStateSummaryFrontier indicates an expected call of SendStateSummaryFrontier.
func (mr *MockStateSummarySenderMockRecorder) SendStateSummaryFrontier(nodeID, requestID, summary interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "SendStateSummaryFrontier", reflect.TypeOf((*MockStateSummarySender)(nil).SendStateSummaryFrontier), nodeID, requestID, summary)
}

// MockAcceptedStateSummarySender is a mock of AcceptedStateSummarySender interface.
type MockAcceptedStateSummarySender struct {
	ctrl     *gomock.Controller
	recorder *MockAcceptedStateSummarySenderMockRecorder
}

// MockAcceptedStateSummarySenderMockRecorder is the mock recorder for MockAcceptedStateSummarySender.
type MockAcceptedStateSummarySenderMockRecorder struct {
	mock *MockAcceptedStateSummarySender
}

// NewMockAcceptedStateSummarySender creates a new mock instance.
func NewMockAcceptedStateSummarySender(ctrl *gomock.Controller) *MockAcceptedStateSummarySender {
	mock := &MockAcceptedStateSummarySender{ctrl: ctrl}
	mock.recorder = &MockAcceptedStateSummarySenderMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockAcceptedStateSummarySender) EXPECT() *MockAcceptedStateSummarySenderMockRecorder {
	return m.recorder
}

// SendAcceptedStateSummary mocks base method.
func (m *MockAcceptedStateSummarySender) SendAcceptedStateSummary(nodeID ids.NodeID, requestID uint32, summaryIDs []ids.ID) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "SendAcceptedStateSummary", nodeID, requestID, summaryIDs)
}

// SendAcceptedStateSummary indicates an expected call of SendAcceptedStateSummary.
func (mr *MockAcceptedStateSummarySenderMockRecorder) SendAcceptedStateSummary(nodeID, requestID, summaryIDs interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "SendAcceptedStateSummary", reflect.TypeOf((*MockAcceptedStateSummarySender)(nil).SendAcceptedStateSummary), nodeID, requestID, summaryIDs)
}

// SendGetAcceptedStateSummary mocks base method.
func (m *MockAcceptedStateSummarySender) SendGetAcceptedStateSummary(nodeIDs ids.NodeIDSet, requestID uint32, heights []uint64) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "SendGetAcceptedStateSummary", nodeIDs, requestID, heights)
}

// SendGetAcceptedStateSummary indicates an expected call of SendGetAcceptedStateSummary.
func (mr *MockAcceptedStateSummarySenderMockRecorder) SendGetAcceptedStateSummary(nodeIDs, requestID, heights interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "SendGetAcceptedStateSummary", reflect.TypeOf((*MockAcceptedStateSummarySender)(nil).SendGetAcceptedStateSummary), nodeIDs, requestID, heights)
}

// MockFrontierSender is a mock of FrontierSender interface.
type MockFrontierSender struct {
	ctrl     *gomock.Controller
	recorder *MockFrontierSenderMockRecorder
}

// MockFrontierSenderMockRecorder is the mock recorder for MockFrontierSender.
type MockFrontierSenderMockRecorder struct {
	mock *MockFrontierSender
}

// NewMockFrontierSender creates a new mock instance.
func NewMockFrontierSender(ctrl *gomock.Controller) *MockFrontierSender {
	mock := &MockFrontierSender{ctrl: ctrl}
	mock.recorder = &MockFrontierSenderMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockFrontierSender) EXPECT() *MockFrontierSenderMockRecorder {
	return m.recorder
}

// SendAcceptedFrontier mocks base method.
func (m *MockFrontierSender) SendAcceptedFrontier(nodeID ids.NodeID, requestID uint32, containerIDs []ids.ID) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "SendAcceptedFrontier", nodeID, requestID, containerIDs)
}

// SendAcceptedFrontier indicates an expected call of SendAcceptedFrontier.
func (mr *MockFrontierSenderMockRecorder) SendAcceptedFrontier(nodeID, requestID, containerIDs interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "SendAcceptedFrontier", reflect.TypeOf((*MockFrontierSender)(nil).SendAcceptedFrontier), nodeID, requestID, containerIDs)
}

// SendGetAcceptedFrontier mocks base method.
func (m *MockFrontierSender) SendGetAcceptedFrontier(nodeIDs ids.NodeIDSet, requestID uint32) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "SendGetAcceptedFrontier", nodeIDs, requestID)
}

// SendGetAcceptedFrontier indicates an expected call of SendGetAcceptedFrontier.
func (mr *MockFrontierSenderMockRecorder) SendGetAcceptedFrontier(nodeIDs, requestID interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "SendGetAcceptedFrontier", reflect.TypeOf((*MockFrontierSender)(nil).SendGetAcceptedFrontier), nodeIDs, requestID)
}

// MockAcceptedSender is a mock of AcceptedSender interface.
type MockAcceptedSender struct {
	ctrl     *gomock.Controller
	recorder *MockAcceptedSenderMockRecorder
}

// MockAcceptedSenderMockRecorder is the mock recorder for MockAcceptedSender.
type MockAcceptedSenderMockRecorder struct {
	mock *MockAcceptedSender
}

// NewMockAcceptedSender creates a new mock instance.
func NewMockAcceptedSender(ctrl *gomock.Controller) *MockAcceptedSender {
	mock := &MockAcceptedSender{ctrl: ctrl}
	mock.recorder = &MockAcceptedSenderMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockAcceptedSender) EXPECT() *MockAcceptedSenderMockRecorder {
	return m.recorder
}

// SendAccepted mocks base method.
func (m *MockAcceptedSender) SendAccepted(nodeID ids.NodeID, requestID uint32, containerIDs []ids.ID) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "SendAccepted", nodeID, requestID, containerIDs)
}

// SendAccepted indicates an expected call of SendAccepted.
func (mr *MockAcceptedSenderMockRecorder) SendAccepted(nodeID, requestID, containerIDs interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "SendAccepted", reflect.TypeOf((*MockAcceptedSender)(nil).SendAccepted), nodeID, requestID, containerIDs)
}

// SendGetAccepted mocks base method.
func (m *MockAcceptedSender) SendGetAccepted(nodeIDs ids.NodeIDSet, requestID uint32, containerIDs []ids.ID) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "SendGetAccepted", nodeIDs, requestID, containerIDs)
}

// SendGetAccepted indicates an expected call of SendGetAccepted.
func (mr *MockAcceptedSenderMockRecorder) SendGetAccepted(nodeIDs, requestID, containerIDs interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "SendGetAccepted", reflect.TypeOf((*MockAcceptedSender)(nil).SendGetAccepted), nodeIDs, requestID, containerIDs)
}

// MockFetchSender is a mock of FetchSender interface.
type MockFetchSender struct {
	ctrl     *gomock.Controller
	recorder *MockFetchSenderMockRecorder
}

// MockFetchSenderMockRecorder is the mock recorder for MockFetchSender.
type MockFetchSenderMockRecorder struct {
	mock *MockFetchSender
}

// NewMockFetchSender creates a new mock instance.
func NewMockFetchSender(ctrl *gomock.Controller) *MockFetchSender {
	mock := &MockFetchSender{ctrl: ctrl}
	mock.recorder = &MockFetchSenderMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockFetchSender) EXPECT() *MockFetchSenderMockRecorder {
	return m.recorder
}

// SendAncestors mocks base method.
func (m *MockFetchSender) SendAncestors(nodeID ids.NodeID, requestID uint32, containers [][]byte) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "SendAncestors", nodeID, requestID, containers)
}

// SendAncestors indicates an expected call of SendAncestors.
func (mr *MockFetchSenderMockRecorder) SendAncestors(nodeID, requestID, containers interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "SendAncestors", reflect.TypeOf((*MockFetchSender)(nil).SendAncestors), nodeID, requestID, containers)
}

// SendGet mocks base method.
func (m *MockFetchSender) SendGet(nodeID ids.NodeID, requestID uint32, containerID ids.ID) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "SendGet", nodeID, requestID, containerID)
}

// SendGet indicates an expected call of SendGet.
func (mr *MockFetchSenderMockRecorder) SendGet(nodeID, requestID, containerID interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "SendGet", reflect.TypeOf((*MockFetchSender)(nil).SendGet), nodeID, requestID, containerID)
}

// SendGetAncestors mocks base method.
func (m *MockFetchSender) SendGetAncestors(nodeID ids.NodeID, requestID uint32, containerID ids.ID) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "SendGetAncestors", nodeID, requestID, containerID)
}

// SendGetAncestors indicates an expected call of SendGetAncestors.
func (mr *MockFetchSenderMockRecorder) SendGetAncestors(nodeID, requestID, containerID interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "SendGetAncestors", reflect.TypeOf((*MockFetchSender)(nil).SendGetAncestors), nodeID, requestID, containerID)
}

// SendPut mocks base method.
func (m *MockFetchSender) SendPut(nodeID ids.NodeID, requestID uint32, container []byte) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "SendPut", nodeID, requestID, container)
}

// SendPut indicates an expected call of SendPut.
func (mr *MockFetchSenderMockRecorder) SendPut(nodeID, requestID, container interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "SendPut", reflect.TypeOf((*MockFetchSender)(nil).SendPut), nodeID, requestID, container)
}

// MockQuerySender is a mock of QuerySender interface.
type MockQuerySender struct {
	ctrl     *gomock.Controller
	recorder *MockQuerySenderMockRecorder
}

// MockQuerySenderMockRecorder is the mock recorder for MockQuerySender.
type MockQuerySenderMockRecorder struct {
	mock *MockQuerySender
}

// NewMockQuerySender creates a new mock instance.
func NewMockQuerySender(ctrl *gomock.Controller) *MockQuerySender {
	mock := &MockQuerySender{ctrl: ctrl}
	mock.recorder = &MockQuerySenderMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockQuerySender) EXPECT() *MockQuerySenderMockRecorder {
	return m.recorder
}

// SendChits mocks base method.
func (m *MockQuerySender) SendChits(nodeID ids.NodeID, requestID uint32, votes []ids.ID) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "SendChits", nodeID, requestID, votes)
}

// SendChits indicates an expected call of SendChits.
func (mr *MockQuerySenderMockRecorder) SendChits(nodeID, requestID, votes interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "SendChits", reflect.TypeOf((*MockQuerySender)(nil).SendChits), nodeID, requestID, votes)
}

// SendPullQuery mocks base method.
func (m *MockQuerySender) SendPullQuery(nodeIDs ids.NodeIDSet, requestID uint32, containerID ids.ID) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "SendPullQuery", nodeIDs, requestID, containerID)
}

// SendPullQuery indicates an expected call of SendPullQuery.
func (mr *MockQuerySenderMockRecorder) SendPullQuery(nodeIDs, requestID, containerID interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "SendPullQuery", reflect.TypeOf((*MockQuerySender)(nil).SendPullQuery), nodeIDs, requestID, containerID)
}

// SendPushQuery mocks base method.
func (m *MockQuerySender) SendPushQuery(nodeIDs ids.NodeIDSet, requestID uint32, container []byte) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "SendPushQuery", nodeIDs, requestID, container)
}

// SendPushQuery indicates an expected call of SendPushQuery.
func (mr *MockQuerySenderMockRecorder) SendPushQuery(nodeIDs, requestID, container interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "SendPushQuery", reflect.TypeOf((*MockQuerySender)(nil).SendPushQuery), nodeIDs, requestID, container)
}

// MockGossiper is a mock of Gossiper interface.
type MockGossiper struct {
	ctrl     *gomock.Controller
	recorder *MockGossiperMockRecorder
}

// MockGossiperMockRecorder is the mock recorder for MockGossiper.
type MockGossiperMockRecorder struct {
	mock *MockGossiper
}

// NewMockGossiper creates a new mock instance.
func NewMockGossiper(ctrl *gomock.Controller) *MockGossiper {
	mock := &MockGossiper{ctrl: ctrl}
	mock.recorder = &MockGossiperMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockGossiper) EXPECT() *MockGossiperMockRecorder {
	return m.recorder
}

// SendGossip mocks base method.
func (m *MockGossiper) SendGossip(container []byte) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "SendGossip", container)
}

// SendGossip indicates an expected call of SendGossip.
func (mr *MockGossiperMockRecorder) SendGossip(container interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "SendGossip", reflect.TypeOf((*MockGossiper)(nil).SendGossip), container)
}

// MockAppSender is a mock of AppSender interface.
type MockAppSender struct {
	ctrl     *gomock.Controller
	recorder *MockAppSenderMockRecorder
}

// MockAppSenderMockRecorder is the mock recorder for MockAppSender.
type MockAppSenderMockRecorder struct {
	mock *MockAppSender
}

// NewMockAppSender creates a new mock instance.
func NewMockAppSender(ctrl *gomock.Controller) *MockAppSender {
	mock := &MockAppSender{ctrl: ctrl}
	mock.recorder = &MockAppSenderMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockAppSender) EXPECT() *MockAppSenderMockRecorder {
	return m.recorder
}

// SendAppGossip mocks base method.
func (m *MockAppSender) SendAppGossip(appGossipBytes []byte) error {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "SendAppGossip", appGossipBytes)
	ret0, _ := ret[0].(error)
	return ret0
}

// SendAppGossip indicates an expected call of SendAppGossip.
func (mr *MockAppSenderMockRecorder) SendAppGossip(appGossipBytes interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "SendAppGossip", reflect.TypeOf((*MockAppSender)(nil).SendAppGossip), appGossipBytes)
}

// SendAppGossipSpecific mocks base method.
func (m *MockAppSender) SendAppGossipSpecific(nodeIDs ids.NodeIDSet, appGossipBytes []byte) error {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "SendAppGossipSpecific", nodeIDs, appGossipBytes)
	ret0, _ := ret[0].(error)
	return ret0
}

// SendAppGossipSpecific indicates an expected call of SendAppGossipSpecific.
func (mr *MockAppSenderMockRecorder) SendAppGossipSpecific(nodeIDs, appGossipBytes interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "SendAppGossipSpecific", reflect.TypeOf((*MockAppSender)(nil).SendAppGossipSpecific), nodeIDs, appGossipBytes)
}

// SendAppRequest mocks base method.
func (m *MockAppSender) SendAppRequest(nodeIDs ids.NodeIDSet, requestID uint32, appRequestBytes []byte) error {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "SendAppRequest", nodeIDs, requestID, appRequestBytes)
	ret0, _ := ret[0].(error)
	return ret0
}

// SendAppRequest indicates an expected call of SendAppRequest.
func (mr *MockAppSenderMockRecorder) SendAppRequest(nodeIDs, requestID, appRequestBytes interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "SendAppRequest", reflect.TypeOf((*MockAppSender)(nil).SendAppRequest), nodeIDs, requestID, appRequestBytes)
}

// SendAppResponse mocks base method.
func (m *MockAppSender) SendAppResponse(nodeID ids.NodeID, requestID uint32, appResponseBytes []byte) error {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "SendAppResponse", nodeID, requestID, appResponseBytes)
	ret0, _ := ret[0].(error)
	return ret0
}

// SendAppResponse indicates an expected call of SendAppResponse.
func (mr *MockAppSenderMockRecorder) SendAppResponse(nodeID, requestID, appResponseBytes interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "SendAppResponse", reflect.TypeOf((*MockAppSender)(nil).SendAppResponse), nodeID, requestID, appResponseBytes)
}

```

avalanchego/snow/engine/common/no_ops_handlers.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package common

import (
	"time"

	"go.uber.org/zap"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/message"
	"github.com/ava-labs/avalanchego/utils/constants"
	"github.com/ava-labs/avalanchego/utils/logging"
)

var (
	_ StateSummaryFrontierHandler = &noOpStateSummaryFrontierHandler{}
	_ AcceptedStateSummaryHandler = &noOpAcceptedStateSummaryHandler{}
	_ AcceptedFrontierHandler     = &noOpAcceptedFrontierHandler{}
	_ AcceptedHandler             = &noOpAcceptedHandler{}
	_ AncestorsHandler            = &noOpAncestorsHandler{}
	_ PutHandler                  = &noOpPutHandler{}
	_ QueryHandler                = &noOpQueryHandler{}
	_ ChitsHandler                = &noOpChitsHandler{}
	_ AppHandler                  = &noOpAppHandler{}
)

type noOpStateSummaryFrontierHandler struct {
	log logging.Logger
}

func NewNoOpStateSummaryFrontierHandler(log logging.Logger) StateSummaryFrontierHandler {
	return &noOpStateSummaryFrontierHandler{log: log}
}

func (nop *noOpStateSummaryFrontierHandler) StateSummaryFrontier(nodeID ids.NodeID, requestID uint32, _ []byte) error {
	nop.log.Debug("dropping request",
		zap.String("reason", "unhandled by this gear"),
		zap.Stringer("messageOp", message.StateSummaryFrontier),
		zap.Stringer("nodeID", nodeID),
		zap.Uint32("requestID", requestID),
	)
	return nil
}

func (nop *noOpStateSummaryFrontierHandler) GetStateSummaryFrontierFailed(nodeID ids.NodeID, requestID uint32) error {
	nop.log.Debug("dropping request",
		zap.String("reason", "unhandled by this gear"),
		zap.Stringer("messageOp", message.GetStateSummaryFrontierFailed),
		zap.Stringer("nodeID", nodeID),
		zap.Uint32("requestID", requestID),
	)
	return nil
}

type noOpAcceptedStateSummaryHandler struct {
	log logging.Logger
}

func NewNoOpAcceptedStateSummaryHandler(log logging.Logger) AcceptedStateSummaryHandler {
	return &noOpAcceptedStateSummaryHandler{log: log}
}

func (nop *noOpAcceptedStateSummaryHandler) AcceptedStateSummary(nodeID ids.NodeID, requestID uint32, _ []ids.ID) error {
	nop.log.Debug("dropping request",
		zap.String("reason", "unhandled by this gear"),
		zap.Stringer("messageOp", message.AcceptedStateSummary),
		zap.Stringer("nodeID", nodeID),
		zap.Uint32("requestID", requestID),
	)
	return nil
}

func (nop *noOpAcceptedStateSummaryHandler) GetAcceptedStateSummaryFailed(nodeID ids.NodeID, requestID uint32) error {
	nop.log.Debug("dropping request",
		zap.String("reason", "unhandled by this gear"),
		zap.Stringer("messageOp", message.GetAcceptedStateSummaryFailed),
		zap.Stringer("nodeID", nodeID),
		zap.Uint32("requestID", requestID),
	)
	return nil
}

type noOpAcceptedFrontierHandler struct {
	log logging.Logger
}

func NewNoOpAcceptedFrontierHandler(log logging.Logger) AcceptedFrontierHandler {
	return &noOpAcceptedFrontierHandler{log: log}
}

func (nop *noOpAcceptedFrontierHandler) AcceptedFrontier(nodeID ids.NodeID, requestID uint32, _ []ids.ID) error {
	nop.log.Debug("dropping request",
		zap.String("reason", "unhandled by this gear"),
		zap.Stringer("messageOp", message.AcceptedFrontier),
		zap.Stringer("nodeID", nodeID),
		zap.Uint32("requestID", requestID),
	)
	return nil
}

func (nop *noOpAcceptedFrontierHandler) GetAcceptedFrontierFailed(nodeID ids.NodeID, requestID uint32) error {
	nop.log.Debug("dropping request",
		zap.String("reason", "unhandled by this gear"),
		zap.Stringer("messageOp", message.GetAcceptedFrontierFailed),
		zap.Stringer("nodeID", nodeID),
		zap.Uint32("requestID", requestID),
	)
	return nil
}

type noOpAcceptedHandler struct {
	log logging.Logger
}

func NewNoOpAcceptedHandler(log logging.Logger) AcceptedHandler {
	return &noOpAcceptedHandler{log: log}
}

func (nop *noOpAcceptedHandler) Accepted(nodeID ids.NodeID, requestID uint32, _ []ids.ID) error {
	nop.log.Debug("dropping request",
		zap.String("reason", "unhandled by this gear"),
		zap.Stringer("messageOp", message.Accepted),
		zap.Stringer("nodeID", nodeID),
		zap.Uint32("requestID", requestID),
	)
	return nil
}

func (nop *noOpAcceptedHandler) GetAcceptedFailed(nodeID ids.NodeID, requestID uint32) error {
	nop.log.Debug("dropping request",
		zap.String("reason", "unhandled by this gear"),
		zap.Stringer("messageOp", message.GetAcceptedFailed),
		zap.Stringer("nodeID", nodeID),
		zap.Uint32("requestID", requestID),
	)
	return nil
}

type noOpAncestorsHandler struct {
	log logging.Logger
}

func NewNoOpAncestorsHandler(log logging.Logger) AncestorsHandler {
	return &noOpAncestorsHandler{log: log}
}

func (nop *noOpAncestorsHandler) Ancestors(nodeID ids.NodeID, requestID uint32, _ [][]byte) error {
	nop.log.Debug("dropping request",
		zap.String("reason", "unhandled by this gear"),
		zap.Stringer("messageOp", message.Ancestors),
		zap.Stringer("nodeID", nodeID),
		zap.Uint32("requestID", requestID),
	)
	return nil
}

func (nop *noOpAncestorsHandler) GetAncestorsFailed(nodeID ids.NodeID, requestID uint32) error {
	nop.log.Debug("dropping request",
		zap.String("reason", "unhandled by this gear"),
		zap.Stringer("messageOp", message.GetAncestorsFailed),
		zap.Stringer("nodeID", nodeID),
		zap.Uint32("requestID", requestID),
	)
	return nil
}

type noOpPutHandler struct {
	log logging.Logger
}

func NewNoOpPutHandler(log logging.Logger) PutHandler {
	return &noOpPutHandler{log: log}
}

func (nop *noOpPutHandler) Put(nodeID ids.NodeID, requestID uint32, _ []byte) error {
	if requestID == constants.GossipMsgRequestID {
		nop.log.Verbo("dropping request",
			zap.String("reason", "unhandled by this gear"),
			zap.Stringer("messageOp", message.Put),
			zap.Stringer("nodeID", nodeID),
			zap.Uint32("requestID", requestID),
		)
	} else {
		nop.log.Debug("dropping request",
			zap.String("reason", "unhandled by this gear"),
			zap.Stringer("messageOp", message.Put),
			zap.Stringer("nodeID", nodeID),
			zap.Uint32("requestID", requestID),
		)
	}
	return nil
}

func (nop *noOpPutHandler) GetFailed(nodeID ids.NodeID, requestID uint32) error {
	nop.log.Debug("dropping request",
		zap.String("reason", "unhandled by this gear"),
		zap.Stringer("messageOp", message.GetFailed),
		zap.Stringer("nodeID", nodeID),
		zap.Uint32("requestID", requestID),
	)
	return nil
}

type noOpQueryHandler struct {
	log logging.Logger
}

func NewNoOpQueryHandler(log logging.Logger) QueryHandler {
	return &noOpQueryHandler{log: log}
}

func (nop *noOpQueryHandler) PullQuery(nodeID ids.NodeID, requestID uint32, _ ids.ID) error {
	nop.log.Debug("dropping request",
		zap.String("reason", "unhandled by this gear"),
		zap.Stringer("messageOp", message.PullQuery),
		zap.Stringer("nodeID", nodeID),
		zap.Uint32("requestID", requestID),
	)
	return nil
}

func (nop *noOpQueryHandler) PushQuery(nodeID ids.NodeID, requestID uint32, _ []byte) error {
	nop.log.Debug("dropping request",
		zap.String("reason", "unhandled by this gear"),
		zap.Stringer("messageOp", message.PushQuery),
		zap.Stringer("nodeID", nodeID),
		zap.Uint32("requestID", requestID),
	)
	return nil
}

type noOpChitsHandler struct {
	log logging.Logger
}

func NewNoOpChitsHandler(log logging.Logger) ChitsHandler {
	return &noOpChitsHandler{log: log}
}

func (nop *noOpChitsHandler) Chits(nodeID ids.NodeID, requestID uint32, _ []ids.ID) error {
	nop.log.Debug("dropping request",
		zap.String("reason", "unhandled by this gear"),
		zap.Stringer("messageOp", message.Chits),
		zap.Stringer("nodeID", nodeID),
		zap.Uint32("requestID", requestID),
	)
	return nil
}

func (nop *noOpChitsHandler) QueryFailed(nodeID ids.NodeID, requestID uint32) error {
	nop.log.Debug("dropping request",
		zap.String("reason", "unhandled by this gear"),
		zap.Stringer("messageOp", message.QueryFailed),
		zap.Stringer("nodeID", nodeID),
		zap.Uint32("requestID", requestID),
	)
	return nil
}

type noOpAppHandler struct {
	log logging.Logger
}

func NewNoOpAppHandler(log logging.Logger) AppHandler {
	return &noOpAppHandler{log: log}
}

func (nop *noOpAppHandler) AppRequest(nodeID ids.NodeID, requestID uint32, deadline time.Time, _ []byte) error {
	nop.log.Debug("dropping request",
		zap.String("reason", "unhandled by this gear"),
		zap.Stringer("messageOp", message.AppRequest),
		zap.Stringer("nodeID", nodeID),
		zap.Uint32("requestID", requestID),
	)
	return nil
}

func (nop *noOpAppHandler) AppRequestFailed(nodeID ids.NodeID, requestID uint32) error {
	nop.log.Debug("dropping request",
		zap.String("reason", "unhandled by this gear"),
		zap.Stringer("messageOp", message.AppRequestFailed),
		zap.Stringer("nodeID", nodeID),
		zap.Uint32("requestID", requestID),
	)
	return nil
}

func (nop *noOpAppHandler) AppResponse(nodeID ids.NodeID, requestID uint32, _ []byte) error {
	nop.log.Debug("dropping request",
		zap.String("reason", "unhandled by this gear"),
		zap.Stringer("messageOp", message.AppResponse),
		zap.Stringer("nodeID", nodeID),
		zap.Uint32("requestID", requestID),
	)
	return nil
}

func (nop *noOpAppHandler) AppGossip(nodeID ids.NodeID, _ []byte) error {
	nop.log.Debug("dropping request",
		zap.String("reason", "unhandled by this gear"),
		zap.Stringer("messageOp", message.AppGossip),
		zap.Stringer("nodeID", nodeID),
	)
	return nil
}

```

avalanchego/snow/engine/common/queue/job.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package queue

import (
	"github.com/ava-labs/avalanchego/ids"
)

// Job defines the interface required to be placed on the job queue.
type Job interface {
	ID() ids.ID
	MissingDependencies() (ids.Set, error)
	// Returns true if this job has at least 1 missing dependency
	HasMissingDependencies() (bool, error)
	Execute() error
	Bytes() []byte
}

```

avalanchego/snow/engine/common/queue/jobs.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package queue

import (
	"fmt"
	"time"

	"github.com/prometheus/client_golang/prometheus"

	"go.uber.org/zap"

	"github.com/ava-labs/avalanchego/database"
	"github.com/ava-labs/avalanchego/database/versiondb"
	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow"
	"github.com/ava-labs/avalanchego/snow/engine/common"
	"github.com/ava-labs/avalanchego/utils/timer"
	"github.com/ava-labs/avalanchego/utils/wrappers"
)

const (
	// StatusUpdateFrequency is how many containers should be processed between
	// logs
	StatusUpdateFrequency = 2500
)

// Jobs tracks a series of jobs that form a DAG of dependencies.
type Jobs struct {
	// db ensures that database updates are atomically updated.
	db *versiondb.Database
	// state writes the job queue to [db].
	state *state
	// Measures the ETA until bootstrapping finishes in nanoseconds.
	etaMetric prometheus.Gauge
}

// New attempts to create a new job queue from the provided database.
func New(
	db database.Database,
	metricsNamespace string,
	metricsRegisterer prometheus.Registerer,
) (*Jobs, error) {
	vdb := versiondb.New(db)
	state, err := newState(vdb, metricsNamespace, metricsRegisterer)
	if err != nil {
		return nil, fmt.Errorf("couldn't create new jobs state: %w", err)
	}

	etaMetric := prometheus.NewGauge(prometheus.GaugeOpts{
		Namespace: metricsNamespace,
		Name:      "eta_execution_complete",
		Help:      "ETA in nanoseconds until execution phase of bootstrapping finishes",
	})

	return &Jobs{
		db:        vdb,
		state:     state,
		etaMetric: etaMetric,
	}, metricsRegisterer.Register(etaMetric)
}

// SetParser tells this job queue how to parse jobs from the database.
func (j *Jobs) SetParser(parser Parser) error { j.state.parser = parser; return nil }

func (j *Jobs) Has(jobID ids.ID) (bool, error) { return j.state.HasJob(jobID) }

// Returns how many pending jobs are waiting in the queue.
func (j *Jobs) PendingJobs() uint64 { return j.state.numJobs }

// Push adds a new job to the queue. Returns true if [job] was added to the queue and false
// if [job] was already in the queue.
func (j *Jobs) Push(job Job) (bool, error) {
	jobID := job.ID()
	if has, err := j.state.HasJob(jobID); err != nil {
		return false, fmt.Errorf("failed to check for existing job %s due to %w", jobID, err)
	} else if has {
		return false, nil
	}

	deps, err := job.MissingDependencies()
	if err != nil {
		return false, err
	}
	// Store this job into the database.
	if err := j.state.PutJob(job); err != nil {
		return false, fmt.Errorf("failed to write job due to %w", err)
	}

	if deps.Len() != 0 {
		// This job needs to block on a set of dependencies.
		for depID := range deps {
			if err := j.state.AddDependency(depID, jobID); err != nil {
				return false, fmt.Errorf("failed to add blocking for depID %s, jobID %s", depID, jobID)
			}
		}
		return true, nil
	}
	// This job doesn't have any dependencies, so it should be placed onto the
	// executable stack.
	if err := j.state.AddRunnableJob(jobID); err != nil {
		return false, fmt.Errorf("failed to add %s as a runnable job due to %w", jobID, err)
	}
	return true, nil
}

func (j *Jobs) ExecuteAll(ctx *snow.ConsensusContext, halter common.Haltable, restarted bool, acceptors ...snow.Acceptor) (int, error) {
	ctx.Executing(true)
	defer ctx.Executing(false)

	numExecuted := 0
	numToExecute := j.state.numJobs
	startTime := time.Now()

	// Disable and clear state caches to prevent us from attempting to execute
	// a vertex that was previously parsed, but not saved to the VM. Some VMs
	// may only persist containers when they are accepted. This is a stop-gap
	// measure to ensure the job will be re-parsed before executing until the VM
	// provides a more explicit interface for freeing parsed blocks.
	// TODO remove DisableCaching when VM provides better interface for freeing
	// blocks.
	j.state.DisableCaching()
	for {
		if halter.Halted() {
			ctx.Log.Info("interrupted execution",
				zap.Int("numExecuted", numExecuted),
			)
			return numExecuted, nil
		}

		job, err := j.state.RemoveRunnableJob()
		if err == database.ErrNotFound {
			break
		}
		if err != nil {
			return 0, fmt.Errorf("failed to removing runnable job with %w", err)
		}

		jobID := job.ID()
		ctx.Log.Debug("executing",
			zap.Stringer("jobID", jobID),
		)
		jobBytes := job.Bytes()
		// Note that acceptor.Accept must be called before executing [job] to
		// honor Acceptor.Accept's invariant.
		for _, acceptor := range acceptors {
			if err := acceptor.Accept(ctx, jobID, jobBytes); err != nil {
				return numExecuted, err
			}
		}
		if err := job.Execute(); err != nil {
			return 0, fmt.Errorf("failed to execute job %s due to %w", jobID, err)
		}

		dependentIDs, err := j.state.RemoveDependencies(jobID)
		if err != nil {
			return 0, fmt.Errorf("failed to remove blocking jobs for %s due to %w", jobID, err)
		}

		for _, dependentID := range dependentIDs {
			job, err := j.state.GetJob(dependentID)
			if err != nil {
				return 0, fmt.Errorf("failed to get job %s from blocking jobs due to %w", dependentID, err)
			}
			hasMissingDeps, err := job.HasMissingDependencies()
			if err != nil {
				return 0, fmt.Errorf("failed to get missing dependencies for %s due to %w", dependentID, err)
			}
			if hasMissingDeps {
				continue
			}
			if err := j.state.AddRunnableJob(dependentID); err != nil {
				return 0, fmt.Errorf("failed to add %s as a runnable job due to %w", dependentID, err)
			}
		}
		if err := j.Commit(); err != nil {
			return 0, err
		}

		numExecuted++
		if numExecuted%StatusUpdateFrequency == 0 { // Periodically print progress
			eta := timer.EstimateETA(
				startTime,
				uint64(numExecuted),
				numToExecute,
			)
			j.etaMetric.Set(float64(eta))

			if !restarted {
				ctx.Log.Info("executing operations",
					zap.Int("numExecuted", numExecuted),
					zap.Uint64("numToExecute", numToExecute),
					zap.Duration("eta", eta),
				)
			} else {
				ctx.Log.Debug("executing operations",
					zap.Int("numExecuted", numExecuted),
					zap.Uint64("numToExecute", numToExecute),
					zap.Duration("eta", eta),
				)
			}
		}
	}

	// Now that executing has finished, zero out the ETA.
	j.etaMetric.Set(0)

	if !restarted {
		ctx.Log.Info("executed operations",
			zap.Int("numExecuted", numExecuted),
		)
	} else {
		ctx.Log.Debug("executed operations",
			zap.Int("numExecuted", numExecuted),
		)
	}
	return numExecuted, nil
}

func (j *Jobs) Clear() error {
	return j.state.Clear()
}

// Commit the versionDB to the underlying database.
func (j *Jobs) Commit() error {
	return j.db.Commit()
}

type JobsWithMissing struct {
	*Jobs

	// keep the missing ID set in memory to avoid unnecessary database reads and
	// writes.
	missingIDs                            ids.Set
	removeFromMissingIDs, addToMissingIDs ids.Set
}

func NewWithMissing(
	db database.Database,
	metricsNamespace string,
	metricsRegisterer prometheus.Registerer,
) (*JobsWithMissing, error) {
	innerJobs, err := New(db, metricsNamespace, metricsRegisterer)
	if err != nil {
		return nil, err
	}

	jobs := &JobsWithMissing{
		Jobs: innerJobs,
	}

	missingIDs, err := jobs.state.MissingJobIDs()
	jobs.missingIDs.Add(missingIDs...)
	return jobs, err
}

// SetParser tells this job queue how to parse jobs from the database.
func (jm *JobsWithMissing) SetParser(parser Parser) error {
	jm.state.parser = parser
	return jm.cleanRunnableStack()
}

func (jm *JobsWithMissing) Clear() error {
	if err := jm.state.RemoveMissingJobIDs(jm.missingIDs); err != nil {
		return err
	}

	jm.missingIDs.Clear()
	jm.addToMissingIDs.Clear()
	jm.removeFromMissingIDs.Clear()

	return jm.Jobs.Clear()
}

func (jm *JobsWithMissing) Has(jobID ids.ID) (bool, error) {
	if jm.missingIDs.Contains(jobID) {
		return false, nil
	}

	return jm.Jobs.Has(jobID)
}

// Push adds a new job to the queue. Returns true if [job] was added to the queue and false
// if [job] was already in the queue.
func (jm *JobsWithMissing) Push(job Job) (bool, error) {
	jobID := job.ID()
	if has, err := jm.Has(jobID); err != nil {
		return false, fmt.Errorf("failed to check for existing job %s due to %w", jobID, err)
	} else if has {
		return false, nil
	}

	deps, err := job.MissingDependencies()
	if err != nil {
		return false, err
	}
	// Store this job into the database.
	if err := jm.state.PutJob(job); err != nil {
		return false, fmt.Errorf("failed to write job due to %w", err)
	}

	if deps.Len() != 0 {
		// This job needs to block on a set of dependencies.
		for depID := range deps {
			if err := jm.state.AddDependency(depID, jobID); err != nil {
				return false, fmt.Errorf("failed to add blocking for depID %s, jobID %s", depID, jobID)
			}
		}
		return true, nil
	}
	// This job doesn't have any dependencies, so it should be placed onto the
	// executable stack.
	if err := jm.state.AddRunnableJob(jobID); err != nil {
		return false, fmt.Errorf("failed to add %s as a runnable job due to %w", jobID, err)
	}
	return true, nil
}

// AddMissingID adds [jobID] to missingIDs
func (jm *JobsWithMissing) AddMissingID(jobIDs ...ids.ID) {
	for _, jobID := range jobIDs {
		if !jm.missingIDs.Contains(jobID) {
			jm.missingIDs.Add(jobID)
			jm.addToMissingIDs.Add(jobID)
			jm.removeFromMissingIDs.Remove(jobID)
		}
	}
}

// RemoveMissingID removes [jobID] from missingIDs
func (jm *JobsWithMissing) RemoveMissingID(jobIDs ...ids.ID) {
	for _, jobID := range jobIDs {
		if jm.missingIDs.Contains(jobID) {
			jm.missingIDs.Remove(jobID)
			jm.addToMissingIDs.Remove(jobID)
			jm.removeFromMissingIDs.Add(jobID)
		}
	}
}

func (jm *JobsWithMissing) MissingIDs() []ids.ID { return jm.missingIDs.List() }

func (jm *JobsWithMissing) NumMissingIDs() int { return jm.missingIDs.Len() }

// Commit the versionDB to the underlying database.
func (jm *JobsWithMissing) Commit() error {
	if jm.addToMissingIDs.Len() != 0 {
		if err := jm.state.AddMissingJobIDs(jm.addToMissingIDs); err != nil {
			return err
		}
		jm.addToMissingIDs.Clear()
	}
	if jm.removeFromMissingIDs.Len() != 0 {
		if err := jm.state.RemoveMissingJobIDs(jm.removeFromMissingIDs); err != nil {
			return err
		}
		jm.removeFromMissingIDs.Clear()
	}
	return jm.Jobs.Commit()
}

// cleanRunnableStack iterates over the jobs on the runnable stack and resets any job
// that has missing dependencies to block on those dependencies.
// Note: the jobs queue ensures that no job with missing dependencies will be placed
// on the runnable stack in the first place.
// However, for specific VM implementations blocks may be committed via a two stage commit
// (ex. platformvm Proposal and Commit/Abort blocks). This can cause an issue where if the first stage
// is executed immediately before the node dies, it will be removed from the runnable stack
// without writing the state transition to the VM's database. When the node restarts, the
// VM will not have marked the first block (the proposal block as accepted), but it could
// have already been removed from the jobs queue. cleanRunnableStack handles this case.
func (jm *JobsWithMissing) cleanRunnableStack() error {
	runnableJobsIter := jm.state.runnableJobIDs.NewIterator()
	defer runnableJobsIter.Release()

	for runnableJobsIter.Next() {
		jobIDBytes := runnableJobsIter.Key()
		jobID, err := ids.ToID(jobIDBytes)
		if err != nil {
			return fmt.Errorf("failed to convert jobID bytes into ID due to: %w", err)
		}

		job, err := jm.state.GetJob(jobID)
		if err != nil {
			return fmt.Errorf("failed to retrieve job on runnnable stack due to: %w", err)
		}
		deps, err := job.MissingDependencies()
		if err != nil {
			return fmt.Errorf("failed to retrieve missing dependencies of job on runnable stack due to: %w", err)
		}
		if deps.Len() == 0 {
			continue
		}

		// If the job has missing dependencies, remove it from the runnable stack
		if err := jm.state.runnableJobIDs.Delete(jobIDBytes); err != nil {
			return fmt.Errorf("failed to delete jobID from runnable stack due to: %w", err)
		}

		// Add the missing dependencies to the set that needs to be fetched.
		jm.AddMissingID(deps.List()...)
		for depID := range deps {
			if err := jm.state.AddDependency(depID, jobID); err != nil {
				return fmt.Errorf("failed to add blocking for depID %s, jobID %s while cleaning the runnable stack", depID, jobID)
			}
		}
	}

	errs := wrappers.Errs{}
	errs.Add(
		runnableJobsIter.Error(),
		jm.Commit(),
	)
	return errs.Err
}

```

avalanchego/snow/engine/common/queue/jobs_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package queue

import (
	"bytes"
	"testing"

	"github.com/prometheus/client_golang/prometheus"

	"github.com/stretchr/testify/require"

	"github.com/ava-labs/avalanchego/database"
	"github.com/ava-labs/avalanchego/database/memdb"
	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow"
	"github.com/ava-labs/avalanchego/snow/engine/common"
)

// Magic value that comes from the size in bytes of a serialized key-value bootstrap checkpoint in a database +
// the overhead of the key-value storage.
const bootstrapProgressCheckpointSize = 55

func testJob(t *testing.T, jobID ids.ID, executed *bool, parentID ids.ID, parentExecuted *bool) *TestJob {
	return &TestJob{
		T:   t,
		IDF: func() ids.ID { return jobID },
		MissingDependenciesF: func() (ids.Set, error) {
			if parentID != ids.Empty && !*parentExecuted {
				return ids.Set{parentID: struct{}{}}, nil
			}
			return ids.Set{}, nil
		},
		HasMissingDependenciesF: func() (bool, error) {
			if parentID != ids.Empty && !*parentExecuted {
				return true, nil
			}
			return false, nil
		},
		ExecuteF: func() error {
			if executed != nil {
				*executed = true
			}
			return nil
		},
		BytesF: func() []byte { return []byte{0} },
	}
}

// Test that creating a new queue can be created and that it is initially empty.
func TestNew(t *testing.T) {
	require := require.New(t)

	parser := &TestParser{T: t}
	db := memdb.New()

	jobs, err := New(db, "", prometheus.NewRegistry())
	if err != nil {
		t.Fatal(err)
	}
	if err := jobs.SetParser(parser); err != nil {
		t.Fatal(err)
	}

	dbSize, err := database.Size(db)
	require.NoError(err)
	require.Zero(dbSize)
}

// Test that a job can be added to a queue, and then the job can be executed
// from the queue after a shutdown.
func TestPushAndExecute(t *testing.T) {
	require := require.New(t)

	parser := &TestParser{T: t}
	db := memdb.New()

	jobs, err := New(db, "", prometheus.NewRegistry())
	if err != nil {
		t.Fatal(err)
	}
	if err := jobs.SetParser(parser); err != nil {
		t.Fatal(err)
	}

	jobID := ids.GenerateTestID()
	job := testJob(t, jobID, nil, ids.Empty, nil)
	has, err := jobs.Has(jobID)
	require.NoError(err)
	require.False(has)

	pushed, err := jobs.Push(job)
	require.True(pushed)
	require.NoError(err)

	has, err = jobs.Has(jobID)
	require.NoError(err)
	require.True(has)

	err = jobs.Commit()
	require.NoError(err)

	jobs, err = New(db, "", prometheus.NewRegistry())
	require.NoError(err)
	if err := jobs.SetParser(parser); err != nil {
		t.Fatal(err)
	}

	has, err = jobs.Has(jobID)
	require.NoError(err)
	require.True(has)

	hasNext, err := jobs.state.HasRunnableJob()
	require.NoError(err)
	require.True(hasNext)

	parser.ParseF = func(b []byte) (Job, error) {
		require.Equal([]byte{0}, b)
		return job, nil
	}

	count, err := jobs.ExecuteAll(snow.DefaultConsensusContextTest(), &common.Halter{}, false)
	require.NoError(err)
	require.Equal(1, count)

	has, err = jobs.Has(jobID)
	require.NoError(err)
	require.False(has)

	hasNext, err = jobs.state.HasRunnableJob()
	require.NoError(err)
	require.False(hasNext)

	dbSize, err := database.Size(db)
	require.NoError(err)
	require.Equal(bootstrapProgressCheckpointSize, dbSize)
}

// Test that executing a job will cause a dependent job to be placed on to the
// ready queue
func TestRemoveDependency(t *testing.T) {
	require := require.New(t)

	parser := &TestParser{T: t}
	db := memdb.New()

	jobs, err := New(db, "", prometheus.NewRegistry())
	if err != nil {
		t.Fatal(err)
	}
	if err := jobs.SetParser(parser); err != nil {
		t.Fatal(err)
	}

	job0ID, executed0 := ids.GenerateTestID(), false
	job1ID, executed1 := ids.GenerateTestID(), false

	job0 := testJob(t, job0ID, &executed0, ids.Empty, nil)
	job1 := testJob(t, job1ID, &executed1, job0ID, &executed0)
	job1.BytesF = func() []byte { return []byte{1} }

	pushed, err := jobs.Push(job1)
	require.True(pushed)
	require.NoError(err)

	hasNext, err := jobs.state.HasRunnableJob()
	require.NoError(err)
	require.False(hasNext)

	pushed, err = jobs.Push(job0)
	require.True(pushed)
	require.NoError(err)

	hasNext, err = jobs.state.HasRunnableJob()
	require.NoError(err)
	require.True(hasNext)

	parser.ParseF = func(b []byte) (Job, error) {
		switch {
		case bytes.Equal(b, []byte{0}):
			return job0, nil
		case bytes.Equal(b, []byte{1}):
			return job1, nil
		default:
			require.FailNow("Unknown job")
			return nil, nil
		}
	}

	count, err := jobs.ExecuteAll(snow.DefaultConsensusContextTest(), &common.Halter{}, false)
	require.NoError(err)
	require.Equal(2, count)
	require.True(executed0)
	require.True(executed1)

	hasNext, err = jobs.state.HasRunnableJob()
	require.NoError(err)
	require.False(hasNext)

	dbSize, err := database.Size(db)
	require.NoError(err)
	require.Equal(bootstrapProgressCheckpointSize, dbSize)
}

// Test that a job that is ready to be executed can only be added once
func TestDuplicatedExecutablePush(t *testing.T) {
	require := require.New(t)

	db := memdb.New()

	jobs, err := New(db, "", prometheus.NewRegistry())
	if err != nil {
		t.Fatal(err)
	}

	jobID := ids.GenerateTestID()
	job := testJob(t, jobID, nil, ids.Empty, nil)

	pushed, err := jobs.Push(job)
	require.True(pushed)
	require.NoError(err)

	pushed, err = jobs.Push(job)
	require.False(pushed)
	require.NoError(err)

	err = jobs.Commit()
	require.NoError(err)

	jobs, err = New(db, "", prometheus.NewRegistry())
	require.NoError(err)

	pushed, err = jobs.Push(job)
	require.False(pushed)
	require.NoError(err)
}

// Test that a job that isn't ready to be executed can only be added once
func TestDuplicatedNotExecutablePush(t *testing.T) {
	require := require.New(t)

	db := memdb.New()

	jobs, err := New(db, "", prometheus.NewRegistry())
	if err != nil {
		t.Fatal(err)
	}

	job0ID, executed0 := ids.GenerateTestID(), false
	job1ID := ids.GenerateTestID()
	job1 := testJob(t, job1ID, nil, job0ID, &executed0)

	pushed, err := jobs.Push(job1)
	require.True(pushed)
	require.NoError(err)

	pushed, err = jobs.Push(job1)
	require.False(pushed)
	require.NoError(err)

	err = jobs.Commit()
	require.NoError(err)

	jobs, err = New(db, "", prometheus.NewRegistry())
	require.NoError(err)

	pushed, err = jobs.Push(job1)
	require.False(pushed)
	require.NoError(err)
}

func TestMissingJobs(t *testing.T) {
	require := require.New(t)

	parser := &TestParser{T: t}
	db := memdb.New()

	jobs, err := NewWithMissing(db, "", prometheus.NewRegistry())
	require.NoError(err)
	if err := jobs.SetParser(parser); err != nil {
		t.Fatal(err)
	}

	job0ID := ids.GenerateTestID()
	job1ID := ids.GenerateTestID()

	jobs.AddMissingID(job0ID)
	jobs.AddMissingID(job1ID)

	err = jobs.Commit()
	require.NoError(err)

	numMissingIDs := jobs.NumMissingIDs()
	require.Equal(2, numMissingIDs)

	missingIDSet := ids.Set{}
	missingIDSet.Add(jobs.MissingIDs()...)

	containsJob0ID := missingIDSet.Contains(job0ID)
	require.True(containsJob0ID)

	containsJob1ID := missingIDSet.Contains(job1ID)
	require.True(containsJob1ID)

	jobs.RemoveMissingID(job1ID)

	err = jobs.Commit()
	require.NoError(err)

	jobs, err = NewWithMissing(db, "", prometheus.NewRegistry())
	require.NoError(err)
	if err := jobs.SetParser(parser); err != nil {
		t.Fatal(err)
	}

	missingIDSet = ids.Set{}
	missingIDSet.Add(jobs.MissingIDs()...)

	containsJob0ID = missingIDSet.Contains(job0ID)
	require.True(containsJob0ID)

	containsJob1ID = missingIDSet.Contains(job1ID)
	require.False(containsJob1ID)
}

func TestHandleJobWithMissingDependencyOnRunnableStack(t *testing.T) {
	require := require.New(t)

	parser := &TestParser{T: t}
	db := memdb.New()

	jobs, err := NewWithMissing(db, "", prometheus.NewRegistry())
	if err != nil {
		t.Fatal(err)
	}
	if err := jobs.SetParser(parser); err != nil {
		t.Fatal(err)
	}

	job0ID, executed0 := ids.GenerateTestID(), false
	job1ID, executed1 := ids.GenerateTestID(), false
	job0 := testJob(t, job0ID, &executed0, ids.Empty, nil)
	job1 := testJob(t, job1ID, &executed1, job0ID, &executed0)
	job1.ExecuteF = func() error { return database.ErrClosed } // job1 fails to execute the first time due to a closed database
	job1.BytesF = func() []byte { return []byte{1} }

	pushed, err := jobs.Push(job1)
	require.True(pushed)
	require.NoError(err)

	hasNext, err := jobs.state.HasRunnableJob()
	require.NoError(err)
	require.False(hasNext)

	pushed, err = jobs.Push(job0)
	require.True(pushed)
	require.NoError(err)

	hasNext, err = jobs.state.HasRunnableJob()
	require.NoError(err)
	require.True(hasNext)

	parser.ParseF = func(b []byte) (Job, error) {
		switch {
		case bytes.Equal(b, []byte{0}):
			return job0, nil
		case bytes.Equal(b, []byte{1}):
			return job1, nil
		default:
			require.FailNow("Unknown job")
			return nil, nil
		}
	}

	_, err = jobs.ExecuteAll(snow.DefaultConsensusContextTest(), &common.Halter{}, false)
	// Assert that the database closed error on job1 causes ExecuteAll
	// to fail in the middle of execution.
	require.Error(err)
	require.True(executed0)
	require.False(executed1)

	executed0 = false
	job1.ExecuteF = func() error { executed1 = true; return nil } // job1 succeeds the second time

	// Create jobs queue from the same database and ensure that the jobs queue
	// recovers correctly.
	jobs, err = NewWithMissing(db, "", prometheus.NewRegistry())
	if err != nil {
		t.Fatal(err)
	}
	if err := jobs.SetParser(parser); err != nil {
		t.Fatal(err)
	}

	missingIDs := jobs.MissingIDs()
	require.Equal(1, len(missingIDs))

	require.Equal(missingIDs[0], job0.ID())

	pushed, err = jobs.Push(job0)
	require.NoError(err)
	require.True(pushed)

	hasNext, err = jobs.state.HasRunnableJob()
	require.NoError(err)
	require.True(hasNext)

	count, err := jobs.ExecuteAll(snow.DefaultConsensusContextTest(), &common.Halter{}, false)
	require.NoError(err)
	require.Equal(2, count)
	require.True(executed1)
}

func TestInitializeNumJobs(t *testing.T) {
	require := require.New(t)

	parser := &TestParser{T: t}
	db := memdb.New()

	jobs, err := NewWithMissing(db, "", prometheus.NewRegistry())
	if err != nil {
		t.Fatal(err)
	}
	if err := jobs.SetParser(parser); err != nil {
		t.Fatal(err)
	}

	job0ID := ids.GenerateTestID()
	job1ID := ids.GenerateTestID()

	job0 := &TestJob{
		T: t,

		IDF:                     func() ids.ID { return job0ID },
		MissingDependenciesF:    func() (ids.Set, error) { return nil, nil },
		HasMissingDependenciesF: func() (bool, error) { return false, nil },
		BytesF:                  func() []byte { return []byte{0} },
	}
	job1 := &TestJob{
		T: t,

		IDF:                     func() ids.ID { return job1ID },
		MissingDependenciesF:    func() (ids.Set, error) { return nil, nil },
		HasMissingDependenciesF: func() (bool, error) { return false, nil },
		BytesF:                  func() []byte { return []byte{1} },
	}

	pushed, err := jobs.Push(job0)
	require.True(pushed)
	require.NoError(err)
	require.EqualValues(1, jobs.state.numJobs)

	pushed, err = jobs.Push(job1)
	require.True(pushed)
	require.NoError(err)
	require.EqualValues(2, jobs.state.numJobs)

	err = jobs.Commit()
	require.NoError(err)

	err = database.Clear(jobs.state.metadataDB, jobs.state.metadataDB)
	require.NoError(err)

	err = jobs.Commit()
	require.NoError(err)

	jobs, err = NewWithMissing(db, "", prometheus.NewRegistry())
	if err != nil {
		t.Fatal(err)
	}
	require.EqualValues(2, jobs.state.numJobs)
}

func TestClearAll(t *testing.T) {
	require := require.New(t)

	parser := &TestParser{T: t}
	db := memdb.New()

	jobs, err := NewWithMissing(db, "", prometheus.NewRegistry())
	if err != nil {
		t.Fatal(err)
	}
	if err := jobs.SetParser(parser); err != nil {
		t.Fatal(err)
	}
	job0ID, executed0 := ids.GenerateTestID(), false
	job1ID, executed1 := ids.GenerateTestID(), false
	job0 := testJob(t, job0ID, &executed0, ids.Empty, nil)
	job1 := testJob(t, job1ID, &executed1, job0ID, &executed0)
	job1.BytesF = func() []byte { return []byte{1} }

	pushed, err := jobs.Push(job0)
	require.NoError(err)
	require.True(pushed)

	pushed, err = jobs.Push(job1)
	require.True(pushed)
	require.NoError(err)

	parser.ParseF = func(b []byte) (Job, error) {
		switch {
		case bytes.Equal(b, []byte{0}):
			return job0, nil
		case bytes.Equal(b, []byte{1}):
			return job1, nil
		default:
			require.FailNow("Unknown job")
			return nil, nil
		}
	}

	require.NoError(jobs.Clear())
	hasJob0, err := jobs.Has(job0.ID())
	require.NoError(err)
	require.False(hasJob0)
	hasJob1, err := jobs.Has(job1.ID())
	require.NoError(err)
	require.False(hasJob1)
}

```

avalanchego/snow/engine/common/queue/parser.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package queue

// Parser allows parsing a job from bytes.
type Parser interface {
	Parse([]byte) (Job, error)
}

```

avalanchego/snow/engine/common/queue/state.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package queue

import (
	"fmt"

	"github.com/prometheus/client_golang/prometheus"

	"github.com/ava-labs/avalanchego/cache"
	"github.com/ava-labs/avalanchego/cache/metercacher"
	"github.com/ava-labs/avalanchego/database"
	"github.com/ava-labs/avalanchego/database/linkeddb"
	"github.com/ava-labs/avalanchego/database/prefixdb"
	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/utils/wrappers"
)

const (
	dependentsCacheSize = 1024
	jobsCacheSize       = 2048
)

var (
	runnableJobIDsPrefix = []byte("runnable")
	jobsPrefix           = []byte("jobs")
	dependenciesPrefix   = []byte("dependencies")
	missingJobIDsPrefix  = []byte("missing job IDs")
	metadataPrefix       = []byte("metadata")
	numJobsKey           = []byte("numJobs")
)

type state struct {
	parser         Parser
	runnableJobIDs linkeddb.LinkedDB
	cachingEnabled bool
	jobsCache      cache.Cacher
	jobsDB         database.Database
	// Should be prefixed with the jobID that we are attempting to find the
	// dependencies of. This prefixdb.Database should then be wrapped in a
	// linkeddb.LinkedDB to read the dependencies.
	dependenciesDB database.Database
	// This is a cache that tracks LinkedDB iterators that have recently been
	// made.
	dependentsCache cache.Cacher
	missingJobIDs   linkeddb.LinkedDB
	// This tracks the summary values of this state. Currently, this only
	// contains the last known checkpoint of how many jobs are currently in the
	// queue to execute.
	metadataDB database.Database
	// This caches the number of jobs that are currently in the queue to
	// execute.
	numJobs uint64
}

func newState(
	db database.Database,
	metricsNamespace string,
	metricsRegisterer prometheus.Registerer,
) (*state, error) {
	jobsCacheMetricsNamespace := fmt.Sprintf("%s_jobs_cache", metricsNamespace)
	jobsCache, err := metercacher.New(jobsCacheMetricsNamespace, metricsRegisterer, &cache.LRU{Size: jobsCacheSize})
	if err != nil {
		return nil, fmt.Errorf("couldn't create metered cache: %w", err)
	}

	metadataDB := prefixdb.New(metadataPrefix, db)
	jobs := prefixdb.New(jobsPrefix, db)
	numJobs, err := getNumJobs(metadataDB, jobs)
	if err != nil {
		return nil, fmt.Errorf("couldn't initialize pending jobs: %w", err)
	}
	return &state{
		runnableJobIDs:  linkeddb.NewDefault(prefixdb.New(runnableJobIDsPrefix, db)),
		cachingEnabled:  true,
		jobsCache:       jobsCache,
		jobsDB:          jobs,
		dependenciesDB:  prefixdb.New(dependenciesPrefix, db),
		dependentsCache: &cache.LRU{Size: dependentsCacheSize},
		missingJobIDs:   linkeddb.NewDefault(prefixdb.New(missingJobIDsPrefix, db)),
		metadataDB:      metadataDB,
		numJobs:         numJobs,
	}, nil
}

func getNumJobs(d database.Database, jobs database.Iteratee) (uint64, error) {
	numJobs, err := database.GetUInt64(d, numJobsKey)
	if err == database.ErrNotFound {
		// If we don't have a checkpoint, we need to initialize it.
		count, err := database.Count(jobs)
		return uint64(count), err
	}
	return numJobs, err
}

func (s *state) Clear() error {
	var (
		runJobsIter  = s.runnableJobIDs.NewIterator()
		jobsIter     = s.jobsDB.NewIterator()
		depsIter     = s.dependenciesDB.NewIterator()
		missJobsIter = s.missingJobIDs.NewIterator()
	)
	defer func() {
		runJobsIter.Release()
		jobsIter.Release()
		depsIter.Release()
		missJobsIter.Release()
	}()

	// clear runnableJobIDs
	for runJobsIter.Next() {
		if err := s.runnableJobIDs.Delete(runJobsIter.Key()); err != nil {
			return err
		}
	}

	// clear jobs
	s.jobsCache.Flush()
	for jobsIter.Next() {
		if err := s.jobsDB.Delete(jobsIter.Key()); err != nil {
			return err
		}
	}

	// clear dependencies
	s.dependentsCache.Flush()
	for depsIter.Next() {
		if err := s.dependenciesDB.Delete(depsIter.Key()); err != nil {
			return err
		}
	}

	// clear missing jobs IDs
	for missJobsIter.Next() {
		if err := s.missingJobIDs.Delete(missJobsIter.Key()); err != nil {
			return err
		}
	}

	// clear number of pending jobs
	s.numJobs = 0
	if err := database.PutUInt64(s.metadataDB, numJobsKey, s.numJobs); err != nil {
		return err
	}

	errs := wrappers.Errs{}
	errs.Add(
		runJobsIter.Error(),
		jobsIter.Error(),
		depsIter.Error(),
		missJobsIter.Error(),
	)
	return errs.Err
}

// AddRunnableJob adds [jobID] to the runnable queue
func (s *state) AddRunnableJob(jobID ids.ID) error {
	return s.runnableJobIDs.Put(jobID[:], nil)
}

// HasRunnableJob returns true if there is a job that can be run on the queue
func (s *state) HasRunnableJob() (bool, error) {
	isEmpty, err := s.runnableJobIDs.IsEmpty()
	return !isEmpty, err
}

// RemoveRunnableJob fetches and deletes the next job from the runnable queue
func (s *state) RemoveRunnableJob() (Job, error) {
	jobIDBytes, err := s.runnableJobIDs.HeadKey()
	if err != nil {
		return nil, err
	}
	if err := s.runnableJobIDs.Delete(jobIDBytes); err != nil {
		return nil, err
	}

	jobID, err := ids.ToID(jobIDBytes)
	if err != nil {
		return nil, fmt.Errorf("couldn't convert job ID bytes to job ID: %w", err)
	}
	job, err := s.GetJob(jobID)
	if err != nil {
		return nil, err
	}

	if err := s.jobsDB.Delete(jobIDBytes); err != nil {
		return job, err
	}

	// Guard rail to make sure we don't underflow.
	if s.numJobs == 0 {
		return job, nil
	}
	s.numJobs--

	return job, database.PutUInt64(s.metadataDB, numJobsKey, s.numJobs)
}

// PutJob adds the job to the queue
func (s *state) PutJob(job Job) error {
	id := job.ID()
	if s.cachingEnabled {
		s.jobsCache.Put(id, job)
	}

	if err := s.jobsDB.Put(id[:], job.Bytes()); err != nil {
		return err
	}

	s.numJobs++
	return database.PutUInt64(s.metadataDB, numJobsKey, s.numJobs)
}

// HasJob returns true if the job [id] is in the queue
func (s *state) HasJob(id ids.ID) (bool, error) {
	if s.cachingEnabled {
		if _, exists := s.jobsCache.Get(id); exists {
			return true, nil
		}
	}
	return s.jobsDB.Has(id[:])
}

// GetJob returns the job [id]
func (s *state) GetJob(id ids.ID) (Job, error) {
	if s.cachingEnabled {
		if job, exists := s.jobsCache.Get(id); exists {
			return job.(Job), nil
		}
	}
	jobBytes, err := s.jobsDB.Get(id[:])
	if err != nil {
		return nil, err
	}
	job, err := s.parser.Parse(jobBytes)
	if err == nil && s.cachingEnabled {
		s.jobsCache.Put(id, job)
	}
	return job, err
}

// AddDependency adds [dependent] as blocking on [dependency] being completed
func (s *state) AddDependency(dependency, dependent ids.ID) error {
	dependentsDB := s.getDependentsDB(dependency)
	return dependentsDB.Put(dependent[:], nil)
}

// RemoveDependencies removes the set of IDs that are blocking on the completion
// of [dependency] from the database and returns them.
func (s *state) RemoveDependencies(dependency ids.ID) ([]ids.ID, error) {
	dependentsDB := s.getDependentsDB(dependency)
	iterator := dependentsDB.NewIterator()
	defer iterator.Release()

	dependents := []ids.ID(nil)
	for iterator.Next() {
		dependentKey := iterator.Key()
		if err := dependentsDB.Delete(dependentKey); err != nil {
			return nil, err
		}
		dependent, err := ids.ToID(dependentKey)
		if err != nil {
			return nil, err
		}
		dependents = append(dependents, dependent)
	}
	return dependents, iterator.Error()
}

func (s *state) DisableCaching() {
	s.dependentsCache.Flush()
	s.jobsCache.Flush()
	s.cachingEnabled = false
}

func (s *state) AddMissingJobIDs(missingIDs ids.Set) error {
	for missingID := range missingIDs {
		missingID := missingID
		if err := s.missingJobIDs.Put(missingID[:], nil); err != nil {
			return err
		}
	}
	return nil
}

func (s *state) RemoveMissingJobIDs(missingIDs ids.Set) error {
	for missingID := range missingIDs {
		missingID := missingID
		if err := s.missingJobIDs.Delete(missingID[:]); err != nil {
			return err
		}
	}
	return nil
}

func (s *state) MissingJobIDs() ([]ids.ID, error) {
	iterator := s.missingJobIDs.NewIterator()
	defer iterator.Release()

	missingIDs := []ids.ID(nil)
	for iterator.Next() {
		missingID, err := ids.ToID(iterator.Key())
		if err != nil {
			return nil, err
		}
		missingIDs = append(missingIDs, missingID)
	}
	return missingIDs, iterator.Error()
}

func (s *state) getDependentsDB(dependency ids.ID) linkeddb.LinkedDB {
	if s.cachingEnabled {
		if dependentsDBIntf, ok := s.dependentsCache.Get(dependency); ok {
			return dependentsDBIntf.(linkeddb.LinkedDB)
		}
	}
	dependencyDB := prefixdb.New(dependency[:], s.dependenciesDB)
	dependentsDB := linkeddb.NewDefault(dependencyDB)
	if s.cachingEnabled {
		s.dependentsCache.Put(dependency, dependentsDB)
	}
	return dependentsDB
}

```

avalanchego/snow/engine/common/queue/test_job.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package queue

import (
	"errors"
	"testing"

	"github.com/ava-labs/avalanchego/ids"
)

var (
	errExecute                = errors.New("unexpectedly called Execute")
	errHasMissingDependencies = errors.New("unexpectedly called HasMissingDependencies")
)

// TestJob is a test Job
type TestJob struct {
	T *testing.T

	CantID,
	CantMissingDependencies,
	CantExecute,
	CantBytes,
	CantHasMissingDependencies bool

	IDF                     func() ids.ID
	MissingDependenciesF    func() (ids.Set, error)
	ExecuteF                func() error
	BytesF                  func() []byte
	HasMissingDependenciesF func() (bool, error)
}

func (j *TestJob) Default(cant bool) {
	j.CantID = cant
	j.CantMissingDependencies = cant
	j.CantExecute = cant
	j.CantBytes = cant
	j.CantHasMissingDependencies = cant
}

func (j *TestJob) ID() ids.ID {
	if j.IDF != nil {
		return j.IDF()
	}
	if j.CantID && j.T != nil {
		j.T.Fatalf("Unexpectedly called ID")
	}
	return ids.ID{}
}

func (j *TestJob) MissingDependencies() (ids.Set, error) {
	if j.MissingDependenciesF != nil {
		return j.MissingDependenciesF()
	}
	if j.CantMissingDependencies && j.T != nil {
		j.T.Fatalf("Unexpectedly called MissingDependencies")
	}
	return ids.Set{}, nil
}

func (j *TestJob) Execute() error {
	if j.ExecuteF != nil {
		return j.ExecuteF()
	}
	if j.CantExecute && j.T != nil {
		j.T.Fatal(errExecute)
	}
	return errExecute
}

func (j *TestJob) Bytes() []byte {
	if j.BytesF != nil {
		return j.BytesF()
	}
	if j.CantBytes && j.T != nil {
		j.T.Fatalf("Unexpectedly called Bytes")
	}
	return nil
}

func (j *TestJob) HasMissingDependencies() (bool, error) {
	if j.HasMissingDependenciesF != nil {
		return j.HasMissingDependenciesF()
	}
	if j.CantHasMissingDependencies && j.T != nil {
		j.T.Fatal(errHasMissingDependencies)
	}
	return false, errHasMissingDependencies
}

```

avalanchego/snow/engine/common/queue/test_parser.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package queue

import (
	"errors"
	"testing"
)

var errParse = errors.New("unexpectedly called Parse")

// TestParser is a test Parser
type TestParser struct {
	T *testing.T

	CantParse bool

	ParseF func([]byte) (Job, error)
}

func (p *TestParser) Default(cant bool) { p.CantParse = cant }

func (p *TestParser) Parse(b []byte) (Job, error) {
	if p.ParseF != nil {
		return p.ParseF(b)
	}
	if p.CantParse && p.T != nil {
		p.T.Fatal(errParse)
	}
	return nil, errParse
}

```

avalanchego/snow/engine/common/requests.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package common

import (
	"fmt"
	"strings"

	"github.com/ava-labs/avalanchego/ids"
)

const (
	minRequestsSize = 32
)

type req struct {
	vdr ids.NodeID
	id  uint32
}

// Requests tracks pending container messages from a peer.
type Requests struct {
	reqsToID map[ids.NodeID]map[uint32]ids.ID
	idToReq  map[ids.ID]req
}

// Add a request. Assumes that requestIDs are unique. Assumes that containerIDs
// are only in one request at a time.
func (r *Requests) Add(vdr ids.NodeID, requestID uint32, containerID ids.ID) {
	if r.reqsToID == nil {
		r.reqsToID = make(map[ids.NodeID]map[uint32]ids.ID, minRequestsSize)
	}
	vdrReqs, ok := r.reqsToID[vdr]
	if !ok {
		vdrReqs = make(map[uint32]ids.ID)
		r.reqsToID[vdr] = vdrReqs
	}
	vdrReqs[requestID] = containerID

	if r.idToReq == nil {
		r.idToReq = make(map[ids.ID]req, minRequestsSize)
	}
	r.idToReq[containerID] = req{
		vdr: vdr,
		id:  requestID,
	}
}

// Get the containerID the request is expecting and if the request exists.
func (r *Requests) Get(vdr ids.NodeID, requestID uint32) (ids.ID, bool) {
	containerID, ok := r.reqsToID[vdr][requestID]
	return containerID, ok
}

// Remove attempts to abandon a requestID sent to a validator. If the request is
// currently outstanding, the requested ID will be returned along with true. If
// the request isn't currently outstanding, false will be returned.
func (r *Requests) Remove(vdr ids.NodeID, requestID uint32) (ids.ID, bool) {
	vdrReqs := r.reqsToID[vdr]
	containerID, ok := vdrReqs[requestID]
	if !ok {
		return ids.ID{}, false
	}

	if len(vdrReqs) == 1 {
		delete(r.reqsToID, vdr)
	} else {
		delete(vdrReqs, requestID)
	}

	delete(r.idToReq, containerID)
	return containerID, true
}

// RemoveAny outstanding requests for the container ID. True is returned if the
// container ID had an outstanding request.
func (r *Requests) RemoveAny(containerID ids.ID) bool {
	req, ok := r.idToReq[containerID]
	if !ok {
		return false
	}

	r.Remove(req.vdr, req.id)
	return true
}

// Len returns the total number of outstanding requests.
func (r *Requests) Len() int { return len(r.idToReq) }

// Contains returns true if there is an outstanding request for the container
// ID.
func (r *Requests) Contains(containerID ids.ID) bool {
	_, ok := r.idToReq[containerID]
	return ok
}

func (r Requests) String() string {
	sb := strings.Builder{}
	sb.WriteString(fmt.Sprintf("Requests: (Num Validators = %d)", len(r.reqsToID)))
	for vdr, reqs := range r.reqsToID {
		sb.WriteString(fmt.Sprintf("\n  VDR[%s]: (Outstanding Requests %d)", vdr, len(reqs)))
		for reqID, containerID := range reqs {
			sb.WriteString(fmt.Sprintf("\n    Request[%d]: %s", reqID, containerID))
		}
	}
	return sb.String()
}

```

avalanchego/snow/engine/common/requests_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package common

import (
	"testing"

	"github.com/stretchr/testify/require"

	"github.com/ava-labs/avalanchego/ids"
)

func TestRequests(t *testing.T) {
	req := Requests{}

	length := req.Len()
	require.Equal(t, 0, length, "should have had no outstanding requests")

	_, removed := req.Remove(ids.EmptyNodeID, 0)
	require.False(t, removed, "shouldn't have removed the request")

	removed = req.RemoveAny(ids.Empty)
	require.False(t, removed, "shouldn't have removed the request")

	constains := req.Contains(ids.Empty)
	require.False(t, constains, "shouldn't contain this request")

	req.Add(ids.EmptyNodeID, 0, ids.Empty)

	length = req.Len()
	require.Equal(t, 1, length, "should have had one outstanding request")

	_, removed = req.Remove(ids.EmptyNodeID, 1)
	require.False(t, removed, "shouldn't have removed the request")

	_, removed = req.Remove(ids.NodeID{1}, 0)
	require.False(t, removed, "shouldn't have removed the request")

	constains = req.Contains(ids.Empty)
	require.True(t, constains, "should contain this request")

	length = req.Len()
	require.Equal(t, 1, length, "should have had one outstanding request")

	req.Add(ids.EmptyNodeID, 10, ids.Empty.Prefix(0))

	length = req.Len()
	require.Equal(t, 2, length, "should have had two outstanding requests")

	_, removed = req.Remove(ids.EmptyNodeID, 1)
	require.False(t, removed, "shouldn't have removed the request")

	_, removed = req.Remove(ids.NodeID{1}, 0)
	require.False(t, removed, "shouldn't have removed the request")

	constains = req.Contains(ids.Empty)
	require.True(t, constains, "should contain this request")

	length = req.Len()
	require.Equal(t, 2, length, "should have had two outstanding requests")

	removedID, removed := req.Remove(ids.EmptyNodeID, 0)
	require.Equal(t, ids.Empty, removedID, "should have removed the requested ID")
	require.True(t, removed, "should have removed the request")

	removedID, removed = req.Remove(ids.EmptyNodeID, 10)
	require.Equal(t, ids.Empty.Prefix(0), removedID, "should have removed the requested ID")
	require.True(t, removed, "should have removed the request")

	length = req.Len()
	require.Equal(t, 0, length, "should have had no outstanding requests")

	req.Add(ids.EmptyNodeID, 0, ids.Empty)

	length = req.Len()
	require.Equal(t, 1, length, "should have had one outstanding request")

	removed = req.RemoveAny(ids.Empty)
	require.True(t, removed, "should have removed the request")

	length = req.Len()
	require.Equal(t, 0, length, "should have had no outstanding requests")

	removed = req.RemoveAny(ids.Empty)
	require.False(t, removed, "shouldn't have removed the request")

	length = req.Len()
	require.Equal(t, 0, length, "should have had no outstanding requests")
}

```

avalanchego/snow/engine/common/sender.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package common

import (
	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow"
)

// Sender defines how a consensus engine sends messages and requests to other
// validators
type Sender interface {
	snow.Acceptor

	StateSummarySender
	AcceptedStateSummarySender
	FrontierSender
	AcceptedSender
	FetchSender
	QuerySender
	Gossiper
	AppSender
}

// StateSummarySender defines how a consensus engine sends state sync messages to
// other nodes.
type StateSummarySender interface {
	// SendGetStateSummaryFrontier requests that every node in [nodeIDs] sends a
	// StateSummaryFrontier message.
	SendGetStateSummaryFrontier(nodeIDs ids.NodeIDSet, requestID uint32)

	// SendStateSummaryFrontier responds to a StateSummaryFrontier message with this
	// engine's current state summary frontier.
	SendStateSummaryFrontier(nodeID ids.NodeID, requestID uint32, summary []byte)
}

type AcceptedStateSummarySender interface {
	// SendGetAcceptedStateSummary requests that every node in [nodeIDs] sends an
	// AcceptedStateSummary message with all the state summary IDs referenced by [heights]
	// that the node thinks are accepted.
	SendGetAcceptedStateSummary(nodeIDs ids.NodeIDSet, requestID uint32, heights []uint64)

	// SendAcceptedStateSummary responds to a AcceptedStateSummary message with a
	// set of summary ids that are accepted.
	SendAcceptedStateSummary(nodeID ids.NodeID, requestID uint32, summaryIDs []ids.ID)
}

// FrontierSender defines how a consensus engine sends frontier messages to
// other nodes.
type FrontierSender interface {
	// SendGetAcceptedFrontier requests that every node in [nodeIDs] sends an
	// AcceptedFrontier message.
	SendGetAcceptedFrontier(nodeIDs ids.NodeIDSet, requestID uint32)

	// SendAcceptedFrontier responds to a AcceptedFrontier message with this
	// engine's current accepted frontier.
	SendAcceptedFrontier(
		nodeID ids.NodeID,
		requestID uint32,
		containerIDs []ids.ID,
	)
}

// AcceptedSender defines how a consensus engine sends messages pertaining to
// accepted containers
type AcceptedSender interface {
	// SendGetAccepted requests that every node in [nodeIDs] sends an Accepted
	// message with all the IDs in [containerIDs] that the node thinks are
	// accepted.
	SendGetAccepted(
		nodeIDs ids.NodeIDSet,
		requestID uint32,
		containerIDs []ids.ID,
	)

	// SendAccepted responds to a GetAccepted message with a set of IDs of
	// containers that are accepted.
	SendAccepted(nodeID ids.NodeID, requestID uint32, containerIDs []ids.ID)
}

// FetchSender defines how a consensus engine sends retrieval messages to other
// nodes.
type FetchSender interface {
	// Request that the specified node send the specified container to this
	// node.
	SendGet(nodeID ids.NodeID, requestID uint32, containerID ids.ID)

	// SendGetAncestors requests that node [nodeID] send container [containerID]
	// and its ancestors.
	SendGetAncestors(nodeID ids.NodeID, requestID uint32, containerID ids.ID)

	// Tell the specified node about [container].
	SendPut(nodeID ids.NodeID, requestID uint32, container []byte)

	// Give the specified node several containers at once. Should be in response
	// to a GetAncestors message with request ID [requestID] from the node.
	SendAncestors(nodeID ids.NodeID, requestID uint32, containers [][]byte)
}

// QuerySender defines how a consensus engine sends query messages to other
// nodes.
type QuerySender interface {
	// Request from the specified nodes their preferred frontier, given the
	// existence of the specified container.
	// This is the same as PullQuery, except that this message includes the body
	// of the container rather than its ID.
	SendPushQuery(nodeIDs ids.NodeIDSet, requestID uint32, container []byte)

	// Request from the specified nodes their preferred frontier, given the
	// existence of the specified container.
	SendPullQuery(nodeIDs ids.NodeIDSet, requestID uint32, containerID ids.ID)

	// Send chits to the specified node
	SendChits(nodeID ids.NodeID, requestID uint32, votes []ids.ID)
}

// Gossiper defines how a consensus engine gossips a container on the accepted
// frontier to other nodes
type Gossiper interface {
	// Gossip the provided container throughout the network
	SendGossip(container []byte)
}

// AppSender sends application (VM) level messages.
// See also common.AppHandler.
type AppSender interface {
	// Send an application-level request.
	// A nil return value guarantees that for each nodeID in [nodeIDs],
	// the VM corresponding to this AppSender eventually receives either:
	// * An AppResponse from nodeID with ID [requestID]
	// * An AppRequestFailed from nodeID with ID [requestID]
	// Exactly one of the above messages will eventually be received per nodeID.
	// A non-nil error should be considered fatal.
	SendAppRequest(nodeIDs ids.NodeIDSet, requestID uint32, appRequestBytes []byte) error
	// Send an application-level response to a request.
	// This response must be in response to an AppRequest that the VM corresponding
	// to this AppSender received from [nodeID] with ID [requestID].
	// A non-nil error should be considered fatal.
	SendAppResponse(nodeID ids.NodeID, requestID uint32, appResponseBytes []byte) error
	// Gossip an application-level message.
	// A non-nil error should be considered fatal.
	SendAppGossip(appGossipBytes []byte) error
	SendAppGossipSpecific(nodeIDs ids.NodeIDSet, appGossipBytes []byte) error
}

```

avalanchego/snow/engine/common/state_syncer.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package common

// StateSyncer controls the selection and verification of state summaries
// to drive VM state syncing. It collects the latest state summaries and elicit
// votes on them, making sure that a qualified majority of nodes support the
// selected state summary.
type StateSyncer interface {
	Engine

	// IsEnabled returns true if the underlying VM wants to perform state sync.
	// Any returned error will be considered fatal.
	IsEnabled() (bool, error)
}

```

avalanchego/snow/engine/common/subnet.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package common

import (
	"github.com/ava-labs/avalanchego/ids"
)

// Subnet describes the standard interface of a subnet description
type Subnet interface {
	// Returns true iff the subnet is done bootstrapping
	IsBootstrapped() bool

	// Bootstrapped marks the named chain as being bootstrapped
	Bootstrapped(chainID ids.ID)
}

```

avalanchego/snow/engine/common/subnet_tracker.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package common

import (
	"github.com/ava-labs/avalanchego/ids"
)

// SubnetTracker describes the interface for checking if a node is tracking a
// subnet, namely if a node has whitelisted a subnet.
type SubnetTracker interface {
	// TracksSubnet returns true if [nodeID] tracks [subnetID]
	TracksSubnet(nodeID ids.NodeID, subnetID ids.ID) bool
}

```

avalanchego/snow/engine/common/test_bootstrapable.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package common

import (
	"errors"
	"testing"

	"github.com/ava-labs/avalanchego/ids"
)

var (
	_ Bootstrapable = &BootstrapableTest{}

	errForceAccepted = errors.New("unexpectedly called ForceAccepted")
	errClear         = errors.New("unexpectedly called Clear")
)

// BootstrapableTest is a test engine that supports bootstrapping
type BootstrapableTest struct {
	T *testing.T

	CantForceAccepted, CantClear bool

	ClearF         func() error
	ForceAcceptedF func(acceptedContainerIDs []ids.ID) error
}

// Default sets the default on call handling
func (b *BootstrapableTest) Default(cant bool) {
	b.CantForceAccepted = cant
}

func (b *BootstrapableTest) Clear() error {
	if b.ClearF != nil {
		return b.ClearF()
	} else if b.CantClear {
		if b.T != nil {
			b.T.Fatalf("Unexpectedly called Clear")
		}
		return errClear
	}
	return nil
}

func (b *BootstrapableTest) ForceAccepted(containerIDs []ids.ID) error {
	if b.ForceAcceptedF != nil {
		return b.ForceAcceptedF(containerIDs)
	} else if b.CantForceAccepted {
		if b.T != nil {
			b.T.Fatalf("Unexpectedly called ForceAccepted")
		}
		return errForceAccepted
	}
	return nil
}

```

avalanchego/snow/engine/common/test_bootstrapper.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package common

var (
	_ Engine        = &BootstrapperTest{}
	_ Bootstrapable = &BootstrapperTest{}
)

// EngineTest is a test engine
type BootstrapperTest struct {
	BootstrapableTest
	EngineTest
}

func (b *BootstrapperTest) Default(cant bool) {
	b.BootstrapableTest.Default(cant)
	b.EngineTest.Default(cant)
}

```

avalanchego/snow/engine/common/test_config.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package common

import (
	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow"
	"github.com/ava-labs/avalanchego/snow/engine/common/tracker"
	"github.com/ava-labs/avalanchego/snow/validators"
)

// DefaultConfigTest returns a test configuration
func DefaultConfigTest() Config {
	isBootstrapped := false
	subnet := &SubnetTest{
		IsBootstrappedF: func() bool { return isBootstrapped },
		BootstrappedF:   func(ids.ID) { isBootstrapped = true },
	}

	beacons := validators.NewSet()

	connectedPeers := tracker.NewPeers()
	startupTracker := tracker.NewStartup(connectedPeers, 0)
	beacons.RegisterCallbackListener(startupTracker)

	return Config{
		Ctx:                            snow.DefaultConsensusContextTest(),
		Validators:                     validators.NewSet(),
		Beacons:                        beacons,
		StartupTracker:                 startupTracker,
		Sender:                         &SenderTest{},
		Bootstrapable:                  &BootstrapableTest{},
		Subnet:                         subnet,
		Timer:                          &TimerTest{},
		AncestorsMaxContainersSent:     2000,
		AncestorsMaxContainersReceived: 2000,
		SharedCfg:                      &SharedConfig{},
	}
}

```

avalanchego/snow/engine/common/test_engine.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package common

import (
	"errors"
	"testing"
	"time"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow"
	"github.com/ava-labs/avalanchego/version"
)

var (
	errTimeout                       = errors.New("unexpectedly called Timeout")
	errGossip                        = errors.New("unexpectedly called Gossip")
	errNotify                        = errors.New("unexpectedly called Notify")
	errGetStateSummaryFrontier       = errors.New("unexpectedly called GetStateSummaryFrontier")
	errGetStateSummaryFrontierFailed = errors.New("unexpectedly called GetStateSummaryFrontierFailed")
	errStateSummaryFrontier          = errors.New("unexpectedly called StateSummaryFrontier")
	errGetAcceptedStateSummary       = errors.New("unexpectedly called GetAcceptedStateSummary")
	errGetAcceptedStateSummaryFailed = errors.New("unexpectedly called GetAcceptedStateSummaryFailed")
	errAcceptedStateSummary          = errors.New("unexpectedly called AcceptedStateSummary")
	errGetAcceptedFrontier           = errors.New("unexpectedly called GetAcceptedFrontier")
	errGetAcceptedFrontierFailed     = errors.New("unexpectedly called GetAcceptedFrontierFailed")
	errAcceptedFrontier              = errors.New("unexpectedly called AcceptedFrontier")
	errGetAccepted                   = errors.New("unexpectedly called GetAccepted")
	errGetAcceptedFailed             = errors.New("unexpectedly called GetAcceptedFailed")
	errAccepted                      = errors.New("unexpectedly called Accepted")
	errGet                           = errors.New("unexpectedly called Get")
	errGetAncestors                  = errors.New("unexpectedly called GetAncestors")
	errGetFailed                     = errors.New("unexpectedly called GetFailed")
	errGetAncestorsFailed            = errors.New("unexpectedly called GetAncestorsFailed")
	errPut                           = errors.New("unexpectedly called Put")
	errAncestors                     = errors.New("unexpectedly called Ancestors")
	errPushQuery                     = errors.New("unexpectedly called PushQuery")
	errPullQuery                     = errors.New("unexpectedly called PullQuery")
	errQueryFailed                   = errors.New("unexpectedly called QueryFailed")
	errChits                         = errors.New("unexpectedly called Chits")
	errStart                         = errors.New("unexpectedly called Start")

	_ Engine = &EngineTest{}
)

// EngineTest is a test engine
type EngineTest struct {
	T *testing.T

	CantStart,

	CantIsBootstrapped,
	CantTimeout,
	CantGossip,
	CantHalt,
	CantShutdown,

	CantContext,

	CantNotify,

	CantGetStateSummaryFrontier,
	CantGetStateSummaryFrontierFailed,
	CantStateSummaryFrontier,

	CantGetAcceptedStateSummary,
	CantGetAcceptedStateSummaryFailed,
	CantAcceptedStateSummary,

	CantGetAcceptedFrontier,
	CantGetAcceptedFrontierFailed,
	CantAcceptedFrontier,

	CantGetAccepted,
	CantGetAcceptedFailed,
	CantAccepted,

	CantGet,
	CantGetAncestors,
	CantGetFailed,
	CantGetAncestorsFailed,
	CantPut,
	CantAncestors,

	CantPushQuery,
	CantPullQuery,
	CantQueryFailed,
	CantChits,

	CantConnected,
	CantDisconnected,

	CantHealth,

	CantAppRequest,
	CantAppResponse,
	CantAppGossip,
	CantAppRequestFailed,

	CantGetVM bool

	StartF                                             func(startReqID uint32) error
	IsBootstrappedF                                    func() bool
	ContextF                                           func() *snow.ConsensusContext
	HaltF                                              func()
	TimeoutF, GossipF, ShutdownF                       func() error
	NotifyF                                            func(Message) error
	GetF, GetAncestorsF, PullQueryF                    func(nodeID ids.NodeID, requestID uint32, containerID ids.ID) error
	PutF, PushQueryF                                   func(nodeID ids.NodeID, requestID uint32, container []byte) error
	AncestorsF                                         func(nodeID ids.NodeID, requestID uint32, containers [][]byte) error
	AcceptedFrontierF, GetAcceptedF, AcceptedF, ChitsF func(nodeID ids.NodeID, requestID uint32, containerIDs []ids.ID) error
	GetStateSummaryFrontierF, GetStateSummaryFrontierFailedF, GetAcceptedStateSummaryFailedF,
	GetAcceptedFrontierF, GetFailedF, GetAncestorsFailedF,
	QueryFailedF, GetAcceptedFrontierFailedF, GetAcceptedFailedF, AppRequestFailedF func(nodeID ids.NodeID, requestID uint32) error
	StateSummaryFrontierF     func(nodeID ids.NodeID, requestID uint32, summary []byte) error
	GetAcceptedStateSummaryF  func(nodeID ids.NodeID, requestID uint32, keys []uint64) error
	AcceptedStateSummaryF     func(nodeID ids.NodeID, requestID uint32, summaryIDs []ids.ID) error
	ConnectedF                func(nodeID ids.NodeID, nodeVersion *version.Application) error
	DisconnectedF             func(nodeID ids.NodeID) error
	HealthF                   func() (interface{}, error)
	GetVMF                    func() VM
	AppRequestF, AppResponseF func(nodeID ids.NodeID, requestID uint32, msg []byte) error
	AppGossipF                func(nodeID ids.NodeID, msg []byte) error
}

func (e *EngineTest) Default(cant bool) {
	e.CantStart = cant
	e.CantIsBootstrapped = cant
	e.CantTimeout = cant
	e.CantGossip = cant
	e.CantHalt = cant
	e.CantShutdown = cant
	e.CantContext = cant
	e.CantNotify = cant
	e.CantGetStateSummaryFrontier = cant
	e.CantGetStateSummaryFrontierFailed = cant
	e.CantStateSummaryFrontier = cant
	e.CantGetAcceptedStateSummary = cant
	e.CantGetAcceptedStateSummaryFailed = cant
	e.CantAcceptedStateSummary = cant
	e.CantGetAcceptedFrontier = cant
	e.CantGetAcceptedFrontierFailed = cant
	e.CantAcceptedFrontier = cant
	e.CantGetAccepted = cant
	e.CantGetAcceptedFailed = cant
	e.CantAccepted = cant
	e.CantGet = cant
	e.CantGetAncestors = cant
	e.CantGetAncestorsFailed = cant
	e.CantGetFailed = cant
	e.CantPut = cant
	e.CantAncestors = cant
	e.CantPushQuery = cant
	e.CantPullQuery = cant
	e.CantQueryFailed = cant
	e.CantChits = cant
	e.CantConnected = cant
	e.CantDisconnected = cant
	e.CantHealth = cant
	e.CantAppRequest = cant
	e.CantAppRequestFailed = cant
	e.CantAppResponse = cant
	e.CantAppGossip = cant
	e.CantGetVM = cant
}

func (e *EngineTest) Start(startReqID uint32) error {
	if e.StartF != nil {
		return e.StartF(startReqID)
	}
	if e.CantStart && e.T != nil {
		e.T.Fatalf("Unexpectedly called Start")
	}
	return errStart
}

func (e *EngineTest) Context() *snow.ConsensusContext {
	if e.ContextF != nil {
		return e.ContextF()
	}
	if e.CantContext && e.T != nil {
		e.T.Fatalf("Unexpectedly called Context")
	}
	return nil
}

func (e *EngineTest) Timeout() error {
	if e.TimeoutF != nil {
		return e.TimeoutF()
	}
	if !e.CantTimeout {
		return nil
	}
	if e.T != nil {
		e.T.Fatal(errTimeout)
	}
	return errTimeout
}

func (e *EngineTest) Gossip() error {
	if e.GossipF != nil {
		return e.GossipF()
	}
	if !e.CantGossip {
		return nil
	}
	if e.T != nil {
		e.T.Fatal(errGossip)
	}
	return errGossip
}

func (e *EngineTest) Halt() {
	if e.HaltF != nil {
		e.HaltF()
	} else if e.CantHalt && e.T != nil {
		e.T.Fatalf("Unexpectedly called Halt")
	}
}

func (e *EngineTest) Shutdown() error {
	if e.ShutdownF != nil {
		return e.ShutdownF()
	}
	if !e.CantShutdown {
		return nil
	}
	if e.T != nil {
		e.T.Fatal(errShutdown)
	}
	return errShutdown
}

func (e *EngineTest) Notify(msg Message) error {
	if e.NotifyF != nil {
		return e.NotifyF(msg)
	}
	if !e.CantNotify {
		return nil
	}
	if e.T != nil {
		e.T.Fatal(errNotify)
	}
	return errNotify
}

func (e *EngineTest) GetStateSummaryFrontier(validatorID ids.NodeID, requestID uint32) error {
	if e.GetStateSummaryFrontierF != nil {
		return e.GetStateSummaryFrontierF(validatorID, requestID)
	}
	if e.CantGetStateSummaryFrontier && e.T != nil {
		e.T.Fatalf("Unexpectedly called GetStateSummaryFrontier")
	}
	return errGetStateSummaryFrontier
}

func (e *EngineTest) StateSummaryFrontier(validatorID ids.NodeID, requestID uint32, summary []byte) error {
	if e.StateSummaryFrontierF != nil {
		return e.StateSummaryFrontierF(validatorID, requestID, summary)
	}
	if e.CantGetStateSummaryFrontier && e.T != nil {
		e.T.Fatalf("Unexpectedly called CantStateSummaryFrontier")
	}
	return errStateSummaryFrontier
}

func (e *EngineTest) GetStateSummaryFrontierFailed(validatorID ids.NodeID, requestID uint32) error {
	if e.GetStateSummaryFrontierFailedF != nil {
		return e.GetStateSummaryFrontierFailedF(validatorID, requestID)
	}
	if e.CantGetStateSummaryFrontierFailed && e.T != nil {
		e.T.Fatalf("Unexpectedly called GetStateSummaryFrontierFailed")
	}
	return errGetStateSummaryFrontierFailed
}

func (e *EngineTest) GetAcceptedStateSummary(validatorID ids.NodeID, requestID uint32, keys []uint64) error {
	if e.GetAcceptedStateSummaryF != nil {
		return e.GetAcceptedStateSummaryF(validatorID, requestID, keys)
	}
	if e.CantGetAcceptedStateSummary && e.T != nil {
		e.T.Fatalf("Unexpectedly called GetAcceptedStateSummary")
	}
	return errGetAcceptedStateSummary
}

func (e *EngineTest) AcceptedStateSummary(validatorID ids.NodeID, requestID uint32, summaryIDs []ids.ID) error {
	if e.AcceptedStateSummaryF != nil {
		return e.AcceptedStateSummaryF(validatorID, requestID, summaryIDs)
	}
	if e.CantAcceptedStateSummary && e.T != nil {
		e.T.Fatalf("Unexpectedly called AcceptedStateSummary")
	}
	return errAcceptedStateSummary
}

func (e *EngineTest) GetAcceptedStateSummaryFailed(validatorID ids.NodeID, requestID uint32) error {
	if e.GetAcceptedStateSummaryFailedF != nil {
		return e.GetAcceptedStateSummaryFailedF(validatorID, requestID)
	}
	if e.CantGetAcceptedStateSummaryFailed && e.T != nil {
		e.T.Fatalf("Unexpectedly called GetAcceptedStateSummaryFailed")
	}
	return errGetAcceptedStateSummaryFailed
}

func (e *EngineTest) GetAcceptedFrontier(nodeID ids.NodeID, requestID uint32) error {
	if e.GetAcceptedFrontierF != nil {
		return e.GetAcceptedFrontierF(nodeID, requestID)
	}
	if !e.CantGetAcceptedFrontier {
		return nil
	}
	if e.T != nil {
		e.T.Fatal(errGetAcceptedFrontier)
	}
	return errGetAcceptedFrontier
}

func (e *EngineTest) GetAcceptedFrontierFailed(nodeID ids.NodeID, requestID uint32) error {
	if e.GetAcceptedFrontierFailedF != nil {
		return e.GetAcceptedFrontierFailedF(nodeID, requestID)
	}
	if !e.CantGetAcceptedFrontierFailed {
		return nil
	}
	if e.T != nil {
		e.T.Fatal(errGetAcceptedFrontierFailed)
	}
	return errGetAcceptedFrontierFailed
}

func (e *EngineTest) AcceptedFrontier(nodeID ids.NodeID, requestID uint32, containerIDs []ids.ID) error {
	if e.AcceptedFrontierF != nil {
		return e.AcceptedFrontierF(nodeID, requestID, containerIDs)
	}
	if !e.CantAcceptedFrontier {
		return nil
	}
	if e.T != nil {
		e.T.Fatal(errAcceptedFrontier)
	}
	return errAcceptedFrontier
}

func (e *EngineTest) GetAccepted(nodeID ids.NodeID, requestID uint32, containerIDs []ids.ID) error {
	if e.GetAcceptedF != nil {
		return e.GetAcceptedF(nodeID, requestID, containerIDs)
	}
	if !e.CantGetAccepted {
		return nil
	}
	if e.T != nil {
		e.T.Fatal(errGetAccepted)
	}
	return errGetAccepted
}

func (e *EngineTest) GetAcceptedFailed(nodeID ids.NodeID, requestID uint32) error {
	if e.GetAcceptedFailedF != nil {
		return e.GetAcceptedFailedF(nodeID, requestID)
	}
	if !e.CantGetAcceptedFailed {
		return nil
	}
	if e.T != nil {
		e.T.Fatal(errGetAcceptedFailed)
	}
	return errGetAcceptedFailed
}

func (e *EngineTest) Accepted(nodeID ids.NodeID, requestID uint32, containerIDs []ids.ID) error {
	if e.AcceptedF != nil {
		return e.AcceptedF(nodeID, requestID, containerIDs)
	}
	if !e.CantAccepted {
		return nil
	}
	if e.T != nil {
		e.T.Fatal(errAccepted)
	}
	return errAccepted
}

func (e *EngineTest) Get(nodeID ids.NodeID, requestID uint32, containerID ids.ID) error {
	if e.GetF != nil {
		return e.GetF(nodeID, requestID, containerID)
	}
	if !e.CantGet {
		return nil
	}
	if e.T != nil {
		e.T.Fatal(errGet)
	}
	return errGet
}

func (e *EngineTest) GetAncestors(nodeID ids.NodeID, requestID uint32, containerID ids.ID) error {
	if e.GetAncestorsF != nil {
		return e.GetAncestorsF(nodeID, requestID, containerID)
	}
	if !e.CantGetAncestors {
		return nil
	}
	if e.T != nil {
		e.T.Fatal(errGetAncestors)
	}
	return errGetAncestors
}

func (e *EngineTest) GetFailed(nodeID ids.NodeID, requestID uint32) error {
	if e.GetFailedF != nil {
		return e.GetFailedF(nodeID, requestID)
	}
	if !e.CantGetFailed {
		return nil
	}
	if e.T != nil {
		e.T.Fatal(errGetFailed)
	}
	return errGetFailed
}

func (e *EngineTest) GetAncestorsFailed(nodeID ids.NodeID, requestID uint32) error {
	if e.GetAncestorsFailedF != nil {
		return e.GetAncestorsFailedF(nodeID, requestID)
	}
	if e.CantGetAncestorsFailed {
		return nil
	}
	if e.T != nil {
		e.T.Fatal(errGetAncestorsFailed)
	}
	return errGetAncestorsFailed
}

func (e *EngineTest) Put(nodeID ids.NodeID, requestID uint32, container []byte) error {
	if e.PutF != nil {
		return e.PutF(nodeID, requestID, container)
	}
	if !e.CantPut {
		return nil
	}
	if e.T != nil {
		e.T.Fatal(errPut)
	}
	return errPut
}

func (e *EngineTest) Ancestors(nodeID ids.NodeID, requestID uint32, containers [][]byte) error {
	if e.AncestorsF != nil {
		return e.AncestorsF(nodeID, requestID, containers)
	}
	if !e.CantAncestors {
		return nil
	}
	if e.T != nil {
		e.T.Fatal(errAncestors)
	}
	return errAncestors
}

func (e *EngineTest) PushQuery(nodeID ids.NodeID, requestID uint32, container []byte) error {
	if e.PushQueryF != nil {
		return e.PushQueryF(nodeID, requestID, container)
	}
	if !e.CantPushQuery {
		return nil
	}
	if e.T != nil {
		e.T.Fatal(errPushQuery)
	}
	return errPushQuery
}

func (e *EngineTest) PullQuery(nodeID ids.NodeID, requestID uint32, containerID ids.ID) error {
	if e.PullQueryF != nil {
		return e.PullQueryF(nodeID, requestID, containerID)
	}
	if !e.CantPullQuery {
		return nil
	}
	if e.T != nil {
		e.T.Fatal(errPullQuery)
	}
	return errPullQuery
}

func (e *EngineTest) QueryFailed(nodeID ids.NodeID, requestID uint32) error {
	if e.QueryFailedF != nil {
		return e.QueryFailedF(nodeID, requestID)
	}
	if !e.CantQueryFailed {
		return nil
	}
	if e.T != nil {
		e.T.Fatal(errQueryFailed)
	}
	return errQueryFailed
}

func (e *EngineTest) AppRequest(nodeID ids.NodeID, requestID uint32, deadline time.Time, request []byte) error {
	if e.AppRequestF != nil {
		return e.AppRequestF(nodeID, requestID, request)
	}
	if !e.CantAppRequest {
		return nil
	}
	if e.T != nil {
		e.T.Fatal(errAppRequest)
	}
	return errAppRequest
}

func (e *EngineTest) AppResponse(nodeID ids.NodeID, requestID uint32, response []byte) error {
	if e.AppResponseF != nil {
		return e.AppResponseF(nodeID, requestID, response)
	}
	if !e.CantAppResponse {
		return nil
	}
	if e.T != nil {
		e.T.Fatal(errAppResponse)
	}
	return errAppResponse
}

func (e *EngineTest) AppRequestFailed(nodeID ids.NodeID, requestID uint32) error {
	if e.AppRequestFailedF != nil {
		return e.AppRequestFailedF(nodeID, requestID)
	}
	if !e.CantAppRequestFailed {
		return nil
	}
	if e.T != nil {
		e.T.Fatal(errAppRequestFailed)
	}
	return errAppRequestFailed
}

func (e *EngineTest) AppGossip(nodeID ids.NodeID, msg []byte) error {
	if e.AppGossipF != nil {
		return e.AppGossipF(nodeID, msg)
	}
	if !e.CantAppGossip {
		return nil
	}
	if e.T != nil {
		e.T.Fatal(errAppGossip)
	}
	return errAppGossip
}

func (e *EngineTest) Chits(nodeID ids.NodeID, requestID uint32, containerIDs []ids.ID) error {
	if e.ChitsF != nil {
		return e.ChitsF(nodeID, requestID, containerIDs)
	}
	if !e.CantChits {
		return nil
	}
	if e.T != nil {
		e.T.Fatal(errChits)
	}
	return errChits
}

func (e *EngineTest) Connected(nodeID ids.NodeID, nodeVersion *version.Application) error {
	if e.ConnectedF != nil {
		return e.ConnectedF(nodeID, nodeVersion)
	}
	if !e.CantConnected {
		return nil
	}
	if e.T != nil {
		e.T.Fatal(errConnected)
	}
	return errConnected
}

func (e *EngineTest) Disconnected(nodeID ids.NodeID) error {
	if e.DisconnectedF != nil {
		return e.DisconnectedF(nodeID)
	}
	if !e.CantDisconnected {
		return nil
	}
	if e.T != nil {
		e.T.Fatal(errDisconnected)
	}
	return errDisconnected
}

func (e *EngineTest) HealthCheck() (interface{}, error) {
	if e.HealthF != nil {
		return e.HealthF()
	}
	if e.CantHealth && e.T != nil {
		e.T.Fatal(errHealthCheck)
	}
	return nil, errHealthCheck
}

func (e *EngineTest) GetVM() VM {
	if e.GetVMF != nil {
		return e.GetVMF()
	}
	if e.CantGetVM && e.T != nil {
		e.T.Fatalf("Unexpectedly called GetVM")
	}
	return nil
}

```

avalanchego/snow/engine/common/test_sender.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package common

import (
	"errors"
	"testing"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow"
)

var (
	_ Sender = &SenderTest{}

	errAccept                = errors.New("unexpectedly called Accept")
	errSendAppRequest        = errors.New("unexpectedly called SendAppRequest")
	errSendAppResponse       = errors.New("unexpectedly called SendAppResponse")
	errSendAppGossip         = errors.New("unexpectedly called SendAppGossip")
	errSendAppGossipSpecific = errors.New("unexpectedly called SendAppGossipSpecific")
)

// SenderTest is a test sender
type SenderTest struct {
	T *testing.T

	CantAccept,
	CantSendGetStateSummaryFrontier, CantSendStateSummaryFrontier,
	CantSendGetAcceptedStateSummary, CantSendAcceptedStateSummary,
	CantSendGetAcceptedFrontier, CantSendAcceptedFrontier,
	CantSendGetAccepted, CantSendAccepted,
	CantSendGet, CantSendGetAncestors, CantSendPut, CantSendAncestors,
	CantSendPullQuery, CantSendPushQuery, CantSendChits,
	CantSendGossip,
	CantSendAppRequest, CantSendAppResponse, CantSendAppGossip, CantSendAppGossipSpecific bool

	AcceptF                      func(*snow.ConsensusContext, ids.ID, []byte) error
	SendGetStateSummaryFrontierF func(ids.NodeIDSet, uint32)
	SendStateSummaryFrontierF    func(ids.NodeID, uint32, []byte)
	SendGetAcceptedStateSummaryF func(ids.NodeIDSet, uint32, []uint64)
	SendAcceptedStateSummaryF    func(ids.NodeID, uint32, []ids.ID)
	SendGetAcceptedFrontierF     func(ids.NodeIDSet, uint32)
	SendAcceptedFrontierF        func(ids.NodeID, uint32, []ids.ID)
	SendGetAcceptedF             func(ids.NodeIDSet, uint32, []ids.ID)
	SendAcceptedF                func(ids.NodeID, uint32, []ids.ID)
	SendGetF                     func(ids.NodeID, uint32, ids.ID)
	SendGetAncestorsF            func(ids.NodeID, uint32, ids.ID)
	SendPutF                     func(ids.NodeID, uint32, []byte)
	SendAncestorsF               func(ids.NodeID, uint32, [][]byte)
	SendPushQueryF               func(ids.NodeIDSet, uint32, []byte)
	SendPullQueryF               func(ids.NodeIDSet, uint32, ids.ID)
	SendChitsF                   func(ids.NodeID, uint32, []ids.ID)
	SendGossipF                  func([]byte)
	SendAppRequestF              func(ids.NodeIDSet, uint32, []byte) error
	SendAppResponseF             func(ids.NodeID, uint32, []byte) error
	SendAppGossipF               func([]byte) error
	SendAppGossipSpecificF       func(ids.NodeIDSet, []byte) error
}

// Default set the default callable value to [cant]
func (s *SenderTest) Default(cant bool) {
	s.CantAccept = cant
	s.CantSendGetStateSummaryFrontier = cant
	s.CantSendStateSummaryFrontier = cant
	s.CantSendGetAcceptedStateSummary = cant
	s.CantSendAcceptedStateSummary = cant
	s.CantSendGetAcceptedFrontier = cant
	s.CantSendAcceptedFrontier = cant
	s.CantSendGetAccepted = cant
	s.CantSendAccepted = cant
	s.CantSendGet = cant
	s.CantSendGetAccepted = cant
	s.CantSendPut = cant
	s.CantSendAncestors = cant
	s.CantSendPullQuery = cant
	s.CantSendPushQuery = cant
	s.CantSendChits = cant
	s.CantSendGossip = cant
	s.CantSendAppRequest = cant
	s.CantSendAppResponse = cant
	s.CantSendAppGossip = cant
	s.CantSendAppGossipSpecific = cant
}

// SendGetStateSummaryFrontier calls SendGetStateSummaryFrontierF if it was initialized. If it
// wasn't initialized and this function shouldn't be called and testing was
// initialized, then testing will fail.
func (s *SenderTest) Accept(ctx *snow.ConsensusContext, containerID ids.ID, container []byte) error {
	if s.AcceptF != nil {
		return s.AcceptF(ctx, containerID, container)
	}
	if !s.CantAccept {
		return nil
	}
	if s.T != nil {
		s.T.Fatal(errAccept)
	}
	return errAccept
}

// SendGetStateSummaryFrontier calls SendGetStateSummaryFrontierF if it was initialized. If it
// wasn't initialized and this function shouldn't be called and testing was
// initialized, then testing will fail.
func (s *SenderTest) SendGetStateSummaryFrontier(validatorIDs ids.NodeIDSet, requestID uint32) {
	if s.SendGetStateSummaryFrontierF != nil {
		s.SendGetStateSummaryFrontierF(validatorIDs, requestID)
	} else if s.CantSendGetStateSummaryFrontier && s.T != nil {
		s.T.Fatalf("Unexpectedly called SendGetStateSummaryFrontier")
	}
}

// SendAcceptedFrontier calls SendAcceptedFrontierF if it was initialized. If it
// wasn't initialized and this function shouldn't be called and testing was
// initialized, then testing will fail.
func (s *SenderTest) SendStateSummaryFrontier(validatorID ids.NodeID, requestID uint32, summary []byte) {
	if s.SendStateSummaryFrontierF != nil {
		s.SendStateSummaryFrontierF(validatorID, requestID, summary)
	} else if s.CantSendStateSummaryFrontier && s.T != nil {
		s.T.Fatalf("Unexpectedly called SendStateSummaryFrontier")
	}
}

// SendGetAcceptedStateSummary calls SendGetAcceptedStateSummaryF if it was initialized. If it wasn't
// initialized and this function shouldn't be called and testing was
// initialized, then testing will fail.
func (s *SenderTest) SendGetAcceptedStateSummary(nodeIDs ids.NodeIDSet, requestID uint32, heights []uint64) {
	if s.SendGetAcceptedStateSummaryF != nil {
		s.SendGetAcceptedStateSummaryF(nodeIDs, requestID, heights)
	} else if s.CantSendGetAcceptedStateSummary && s.T != nil {
		s.T.Fatalf("Unexpectedly called SendGetAcceptedStateSummaryF")
	}
}

// SendAcceptedStateSummary calls SendAcceptedStateSummaryF if it was initialized. If it wasn't
// initialized and this function shouldn't be called and testing was
// initialized, then testing will fail.
func (s *SenderTest) SendAcceptedStateSummary(validatorID ids.NodeID, requestID uint32, summaryIDs []ids.ID) {
	if s.SendAcceptedStateSummaryF != nil {
		s.SendAcceptedStateSummaryF(validatorID, requestID, summaryIDs)
	} else if s.CantSendAcceptedStateSummary && s.T != nil {
		s.T.Fatalf("Unexpectedly called SendAcceptedStateSummary")
	}
}

// SendGetAcceptedFrontier calls SendGetAcceptedFrontierF if it was initialized.
// If it wasn't initialized and this function shouldn't be called and testing
// was initialized, then testing will fail.
func (s *SenderTest) SendGetAcceptedFrontier(validatorIDs ids.NodeIDSet, requestID uint32) {
	if s.SendGetAcceptedFrontierF != nil {
		s.SendGetAcceptedFrontierF(validatorIDs, requestID)
	} else if s.CantSendGetAcceptedFrontier && s.T != nil {
		s.T.Fatalf("Unexpectedly called SendGetAcceptedFrontier")
	}
}

// SendAcceptedFrontier calls SendAcceptedFrontierF if it was initialized. If it
// wasn't initialized and this function shouldn't be called and testing was
// initialized, then testing will fail.
func (s *SenderTest) SendAcceptedFrontier(validatorID ids.NodeID, requestID uint32, containerIDs []ids.ID) {
	if s.SendAcceptedFrontierF != nil {
		s.SendAcceptedFrontierF(validatorID, requestID, containerIDs)
	} else if s.CantSendAcceptedFrontier && s.T != nil {
		s.T.Fatalf("Unexpectedly called SendAcceptedFrontier")
	}
}

// SendGetAccepted calls SendGetAcceptedF if it was initialized. If it wasn't
// initialized and this function shouldn't be called and testing was
// initialized, then testing will fail.
func (s *SenderTest) SendGetAccepted(nodeIDs ids.NodeIDSet, requestID uint32, containerIDs []ids.ID) {
	if s.SendGetAcceptedF != nil {
		s.SendGetAcceptedF(nodeIDs, requestID, containerIDs)
	} else if s.CantSendGetAccepted && s.T != nil {
		s.T.Fatalf("Unexpectedly called SendGetAccepted")
	}
}

// SendAccepted calls SendAcceptedF if it was initialized. If it wasn't
// initialized and this function shouldn't be called and testing was
// initialized, then testing will fail.
func (s *SenderTest) SendAccepted(validatorID ids.NodeID, requestID uint32, containerIDs []ids.ID) {
	if s.SendAcceptedF != nil {
		s.SendAcceptedF(validatorID, requestID, containerIDs)
	} else if s.CantSendAccepted && s.T != nil {
		s.T.Fatalf("Unexpectedly called SendAccepted")
	}
}

// SendGet calls SendGetF if it was initialized. If it wasn't initialized and
// this function shouldn't be called and testing was initialized, then testing
// will fail.
func (s *SenderTest) SendGet(vdr ids.NodeID, requestID uint32, vtxID ids.ID) {
	if s.SendGetF != nil {
		s.SendGetF(vdr, requestID, vtxID)
	} else if s.CantSendGet && s.T != nil {
		s.T.Fatalf("Unexpectedly called SendGet")
	}
}

// SendGetAncestors calls SendGetAncestorsF if it was initialized. If it wasn't
// initialized and this function shouldn't be called and testing was
// initialized, then testing will fail.
func (s *SenderTest) SendGetAncestors(validatorID ids.NodeID, requestID uint32, vtxID ids.ID) {
	if s.SendGetAncestorsF != nil {
		s.SendGetAncestorsF(validatorID, requestID, vtxID)
	} else if s.CantSendGetAncestors && s.T != nil {
		s.T.Fatalf("Unexpectedly called SendCantSendGetAncestors")
	}
}

// SendPut calls SendPutF if it was initialized. If it wasn't initialized and
// this function shouldn't be called and testing was initialized, then testing
// will fail.
func (s *SenderTest) SendPut(vdr ids.NodeID, requestID uint32, vtx []byte) {
	if s.SendPutF != nil {
		s.SendPutF(vdr, requestID, vtx)
	} else if s.CantSendPut && s.T != nil {
		s.T.Fatalf("Unexpectedly called SendPut")
	}
}

// SendAncestors calls SendAncestorsF if it was initialized. If it wasn't
// initialized and this function shouldn't be called and testing was
// initialized, then testing will fail.
func (s *SenderTest) SendAncestors(vdr ids.NodeID, requestID uint32, vtxs [][]byte) {
	if s.SendAncestorsF != nil {
		s.SendAncestorsF(vdr, requestID, vtxs)
	} else if s.CantSendAncestors && s.T != nil {
		s.T.Fatalf("Unexpectedly called SendAncestors")
	}
}

// SendPushQuery calls SendPushQueryF if it was initialized. If it wasn't
// initialized and this function shouldn't be called and testing was
// initialized, then testing will fail.
func (s *SenderTest) SendPushQuery(vdrs ids.NodeIDSet, requestID uint32, vtx []byte) {
	if s.SendPushQueryF != nil {
		s.SendPushQueryF(vdrs, requestID, vtx)
	} else if s.CantSendPushQuery && s.T != nil {
		s.T.Fatalf("Unexpectedly called SendPushQuery")
	}
}

// SendPullQuery calls SendPullQueryF if it was initialized. If it wasn't
// initialized and this function shouldn't be called and testing was
// initialized, then testing will fail.
func (s *SenderTest) SendPullQuery(vdrs ids.NodeIDSet, requestID uint32, vtxID ids.ID) {
	if s.SendPullQueryF != nil {
		s.SendPullQueryF(vdrs, requestID, vtxID)
	} else if s.CantSendPullQuery && s.T != nil {
		s.T.Fatalf("Unexpectedly called SendPullQuery")
	}
}

// SendChits calls SendChitsF if it was initialized. If it wasn't initialized
// and this function shouldn't be called and testing was initialized, then
// testing will fail.
func (s *SenderTest) SendChits(vdr ids.NodeID, requestID uint32, votes []ids.ID) {
	if s.SendChitsF != nil {
		s.SendChitsF(vdr, requestID, votes)
	} else if s.CantSendChits && s.T != nil {
		s.T.Fatalf("Unexpectedly called SendChits")
	}
}

// SendGossip calls SendGossipF if it was initialized. If it wasn't initialized
// and this function shouldn't be called and testing was initialized, then
// testing will fail.
func (s *SenderTest) SendGossip(container []byte) {
	if s.SendGossipF != nil {
		s.SendGossipF(container)
	} else if s.CantSendGossip && s.T != nil {
		s.T.Fatalf("Unexpectedly called SendGossip")
	}
}

// SendAppRequest calls SendAppRequestF if it was initialized. If it wasn't
// initialized and this function shouldn't be called and testing was
// initialized, then testing will fail.
func (s *SenderTest) SendAppRequest(nodeIDs ids.NodeIDSet, requestID uint32, appRequestBytes []byte) error {
	switch {
	case s.SendAppRequestF != nil:
		return s.SendAppRequestF(nodeIDs, requestID, appRequestBytes)
	case s.CantSendAppRequest && s.T != nil:
		s.T.Fatal(errSendAppRequest)
	}
	return errSendAppRequest
}

// SendAppResponse calls SendAppResponseF if it was initialized. If it wasn't
// initialized and this function shouldn't be called and testing was
// initialized, then testing will fail.
func (s *SenderTest) SendAppResponse(nodeID ids.NodeID, requestID uint32, appResponseBytes []byte) error {
	switch {
	case s.SendAppResponseF != nil:
		return s.SendAppResponseF(nodeID, requestID, appResponseBytes)
	case s.CantSendAppResponse && s.T != nil:
		s.T.Fatal(errSendAppResponse)
	}
	return errSendAppResponse
}

// SendAppGossip calls SendAppGossipF if it was initialized. If it wasn't
// initialized and this function shouldn't be called and testing was
// initialized, then testing will fail.
func (s *SenderTest) SendAppGossip(appGossipBytes []byte) error {
	switch {
	case s.SendAppGossipF != nil:
		return s.SendAppGossipF(appGossipBytes)
	case s.CantSendAppGossip && s.T != nil:
		s.T.Fatal(errSendAppGossip)
	}
	return errSendAppGossip
}

// SendAppGossipSpecific calls SendAppGossipSpecificF if it was initialized. If it wasn't
// initialized and this function shouldn't be called and testing was
// initialized, then testing will fail.
func (s *SenderTest) SendAppGossipSpecific(nodeIDs ids.NodeIDSet, appGossipBytes []byte) error {
	switch {
	case s.SendAppGossipSpecificF != nil:
		return s.SendAppGossipSpecificF(nodeIDs, appGossipBytes)
	case s.CantSendAppGossipSpecific && s.T != nil:
		s.T.Fatal(errSendAppGossipSpecific)
	}
	return errSendAppGossipSpecific
}

```

avalanchego/snow/engine/common/test_subnet.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package common

import (
	"testing"

	"github.com/ava-labs/avalanchego/ids"
)

// SubnetTest is a test subnet
type SubnetTest struct {
	T *testing.T

	CantIsBootstrapped, CantBootstrapped bool

	IsBootstrappedF func() bool
	BootstrappedF   func(ids.ID)
}

// Default set the default callable value to [cant]
func (s *SubnetTest) Default(cant bool) {
	s.CantIsBootstrapped = cant
	s.CantBootstrapped = cant
}

// IsBootstrapped calls IsBootstrappedF if it was initialized. If it wasn't
// initialized and this function shouldn't be called and testing was
// initialized, then testing will fail. Defaults to returning false.
func (s *SubnetTest) IsBootstrapped() bool {
	if s.IsBootstrappedF != nil {
		return s.IsBootstrappedF()
	}
	if s.CantIsBootstrapped && s.T != nil {
		s.T.Fatalf("Unexpectedly called IsBootstrapped")
	}
	return false
}

// Bootstrapped calls BootstrappedF if it was initialized. If it wasn't
// initialized and this function shouldn't be called and testing was
// initialized, then testing will fail.
func (s *SubnetTest) Bootstrapped(chainID ids.ID) {
	if s.BootstrappedF != nil {
		s.BootstrappedF(chainID)
	} else if s.CantBootstrapped && s.T != nil {
		s.T.Fatalf("Unexpectedly called Bootstrapped")
	}
}

```

avalanchego/snow/engine/common/test_timer.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package common

import (
	"testing"
	"time"
)

var _ Timer = &TimerTest{}

// TimerTest is a test timer
type TimerTest struct {
	T *testing.T

	CantRegisterTimout bool

	RegisterTimeoutF func(time.Duration)
}

// Default set the default callable value to [cant]
func (t *TimerTest) Default(cant bool) {
	t.CantRegisterTimout = cant
}

func (t *TimerTest) RegisterTimeout(delay time.Duration) {
	if t.RegisterTimeoutF != nil {
		t.RegisterTimeoutF(delay)
	} else if t.CantRegisterTimout && t.T != nil {
		t.T.Fatalf("Unexpectedly called RegisterTimeout")
	}
}

```

avalanchego/snow/engine/common/test_vm.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package common

import (
	"errors"
	"testing"
	"time"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/version"

	"github.com/ava-labs/avalanchego/database/manager"
	"github.com/ava-labs/avalanchego/snow"
)

var (
	errInitialize           = errors.New("unexpectedly called Initialize")
	errSetState             = errors.New("unexpectedly called SetState")
	errShutdown             = errors.New("unexpectedly called Shutdown")
	errCreateHandlers       = errors.New("unexpectedly called CreateHandlers")
	errCreateStaticHandlers = errors.New("unexpectedly called CreateStaticHandlers")
	errHealthCheck          = errors.New("unexpectedly called HealthCheck")
	errConnected            = errors.New("unexpectedly called Connected")
	errDisconnected         = errors.New("unexpectedly called Disconnected")
	errVersion              = errors.New("unexpectedly called Version")
	errAppRequest           = errors.New("unexpectedly called AppRequest")
	errAppResponse          = errors.New("unexpectedly called AppResponse")
	errAppRequestFailed     = errors.New("unexpectedly called AppRequestFailed")
	errAppGossip            = errors.New("unexpectedly called AppGossip")

	_ VM = &TestVM{}
)

// TestVM is a test vm
type TestVM struct {
	T *testing.T

	CantInitialize, CantSetState,
	CantShutdown, CantCreateHandlers, CantCreateStaticHandlers,
	CantHealthCheck, CantConnected, CantDisconnected, CantVersion,
	CantAppRequest, CantAppResponse, CantAppGossip, CantAppRequestFailed bool

	InitializeF           func(*snow.Context, manager.Manager, []byte, []byte, []byte, chan<- Message, []*Fx, AppSender) error
	SetStateF             func(snow.State) error
	ShutdownF             func() error
	CreateHandlersF       func() (map[string]*HTTPHandler, error)
	CreateStaticHandlersF func() (map[string]*HTTPHandler, error)
	ConnectedF            func(nodeID ids.NodeID, nodeVersion *version.Application) error
	DisconnectedF         func(nodeID ids.NodeID) error
	HealthCheckF          func() (interface{}, error)
	AppRequestF           func(nodeID ids.NodeID, requestID uint32, deadline time.Time, msg []byte) error
	AppResponseF          func(nodeID ids.NodeID, requestID uint32, msg []byte) error
	AppGossipF            func(nodeID ids.NodeID, msg []byte) error
	AppRequestFailedF     func(nodeID ids.NodeID, requestID uint32) error
	VersionF              func() (string, error)
}

func (vm *TestVM) Default(cant bool) {
	vm.CantInitialize = cant
	vm.CantSetState = cant
	vm.CantShutdown = cant
	vm.CantCreateHandlers = cant
	vm.CantCreateStaticHandlers = cant
	vm.CantHealthCheck = cant
	vm.CantAppRequest = cant
	vm.CantAppRequestFailed = cant
	vm.CantAppResponse = cant
	vm.CantAppGossip = cant
	vm.CantVersion = cant
	vm.CantConnected = cant
	vm.CantDisconnected = cant
}

func (vm *TestVM) Initialize(ctx *snow.Context, db manager.Manager, genesisBytes, upgradeBytes, configBytes []byte, msgChan chan<- Message, fxs []*Fx, appSender AppSender) error {
	if vm.InitializeF != nil {
		return vm.InitializeF(ctx, db, genesisBytes, upgradeBytes, configBytes, msgChan, fxs, appSender)
	}
	if vm.CantInitialize && vm.T != nil {
		vm.T.Fatal(errInitialize)
	}
	return errInitialize
}

func (vm *TestVM) SetState(state snow.State) error {
	if vm.SetStateF != nil {
		return vm.SetStateF(state)
	}
	if vm.CantSetState {
		if vm.T != nil {
			vm.T.Fatal(errSetState)
		}
		return errSetState
	}
	return nil
}

func (vm *TestVM) Shutdown() error {
	if vm.ShutdownF != nil {
		return vm.ShutdownF()
	}
	if vm.CantShutdown {
		if vm.T != nil {
			vm.T.Fatal(errShutdown)
		}
		return errShutdown
	}
	return nil
}

func (vm *TestVM) CreateHandlers() (map[string]*HTTPHandler, error) {
	if vm.CreateHandlersF != nil {
		return vm.CreateHandlersF()
	}
	if vm.CantCreateHandlers && vm.T != nil {
		vm.T.Fatal(errCreateHandlers)
	}
	return nil, nil
}

func (vm *TestVM) CreateStaticHandlers() (map[string]*HTTPHandler, error) {
	if vm.CreateStaticHandlersF != nil {
		return vm.CreateStaticHandlersF()
	}
	if vm.CantCreateStaticHandlers && vm.T != nil {
		vm.T.Fatal(errCreateStaticHandlers)
	}
	return nil, nil
}

func (vm *TestVM) HealthCheck() (interface{}, error) {
	if vm.HealthCheckF != nil {
		return vm.HealthCheckF()
	}
	if vm.CantHealthCheck && vm.T != nil {
		vm.T.Fatal(errHealthCheck)
	}
	return nil, errHealthCheck
}

func (vm *TestVM) AppRequestFailed(nodeID ids.NodeID, requestID uint32) error {
	if vm.AppRequestFailedF != nil {
		return vm.AppRequestFailedF(nodeID, requestID)
	}
	if !vm.CantAppRequestFailed {
		return nil
	}
	if vm.T != nil {
		vm.T.Fatal(errAppRequest)
	}
	return errAppRequest
}

func (vm *TestVM) AppRequest(nodeID ids.NodeID, requestID uint32, deadline time.Time, request []byte) error {
	if vm.AppRequestF != nil {
		return vm.AppRequestF(nodeID, requestID, deadline, request)
	}
	if !vm.CantAppRequest {
		return nil
	}
	if vm.T != nil {
		vm.T.Fatal(errAppRequest)
	}
	return errAppRequest
}

func (vm *TestVM) AppResponse(nodeID ids.NodeID, requestID uint32, response []byte) error {
	if vm.AppResponseF != nil {
		return vm.AppResponseF(nodeID, requestID, response)
	}
	if !vm.CantAppResponse {
		return nil
	}
	if vm.T != nil {
		vm.T.Fatal(errAppResponse)
	}
	return errAppResponse
}

func (vm *TestVM) AppGossip(nodeID ids.NodeID, msg []byte) error {
	if vm.AppGossipF != nil {
		return vm.AppGossipF(nodeID, msg)
	}
	if !vm.CantAppGossip {
		return nil
	}
	if vm.T != nil {
		vm.T.Fatal(errAppGossip)
	}
	return errAppGossip
}

func (vm *TestVM) Connected(id ids.NodeID, nodeVersion *version.Application) error {
	if vm.ConnectedF != nil {
		return vm.ConnectedF(id, nodeVersion)
	}
	if vm.CantConnected && vm.T != nil {
		vm.T.Fatal(errConnected)
	}
	return nil
}

func (vm *TestVM) Disconnected(id ids.NodeID) error {
	if vm.DisconnectedF != nil {
		return vm.DisconnectedF(id)
	}
	if vm.CantDisconnected && vm.T != nil {
		vm.T.Fatal(errDisconnected)
	}
	return nil
}

func (vm *TestVM) Version() (string, error) {
	if vm.VersionF != nil {
		return vm.VersionF()
	}
	if vm.CantVersion && vm.T != nil {
		vm.T.Fatal(errVersion)
	}
	return "", nil
}

```

avalanchego/snow/engine/common/timer.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package common

import (
	"time"
)

// Timer describes the standard interface for specifying a timeout
type Timer interface {
	// RegisterTimeout specifies how much time to delay the next timeout message
	// by. If the subnet has been bootstrapped, the timeout will fire
	// immediately.
	RegisterTimeout(time.Duration)
}

```

avalanchego/snow/engine/common/tracker/peers.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package tracker

import (
	"sync"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/validators"
	"github.com/ava-labs/avalanchego/version"
)

var _ Peers = &peers{}

type Peers interface {
	validators.SetCallbackListener
	validators.Connector

	// ConnectedWeight returns the currently connected stake weight
	ConnectedWeight() uint64
	// PreferredPeers returns the currently connected validators. If there are
	// no currently connected validators then it will return the currently
	// connected peers.
	PreferredPeers() ids.NodeIDSet
}

type peers struct {
	lock sync.RWMutex
	// validators maps nodeIDs to their current stake weight
	validators map[ids.NodeID]uint64
	// connectedWeight contains the sum of all connected validator weights
	connectedWeight uint64
	// connectedValidators is the set of currently connected peers with a
	// non-zero stake weight
	connectedValidators ids.NodeIDSet
	// connectedPeers is the set of all connected peers
	connectedPeers ids.NodeIDSet
}

func NewPeers() Peers {
	return &peers{
		validators: make(map[ids.NodeID]uint64),
	}
}

func (p *peers) OnValidatorAdded(nodeID ids.NodeID, weight uint64) {
	p.lock.Lock()
	defer p.lock.Unlock()

	p.validators[nodeID] = weight
	if p.connectedPeers.Contains(nodeID) {
		p.connectedWeight += weight
		p.connectedValidators.Add(nodeID)
	}
}

func (p *peers) OnValidatorRemoved(nodeID ids.NodeID, weight uint64) {
	p.lock.Lock()
	defer p.lock.Unlock()

	delete(p.validators, nodeID)
	if p.connectedPeers.Contains(nodeID) {
		p.connectedWeight -= weight
		p.connectedValidators.Remove(nodeID)
	}
}

func (p *peers) OnValidatorWeightChanged(nodeID ids.NodeID, oldWeight, newWeight uint64) {
	p.lock.Lock()
	defer p.lock.Unlock()

	p.validators[nodeID] = newWeight
	if p.connectedPeers.Contains(nodeID) {
		p.connectedWeight -= oldWeight
		p.connectedWeight += newWeight
	}
}

func (p *peers) Connected(nodeID ids.NodeID, _ *version.Application) error {
	p.lock.Lock()
	defer p.lock.Unlock()

	if weight, ok := p.validators[nodeID]; ok {
		p.connectedWeight += weight
		p.connectedValidators.Add(nodeID)
	}
	p.connectedPeers.Add(nodeID)
	return nil
}

func (p *peers) Disconnected(nodeID ids.NodeID) error {
	p.lock.Lock()
	defer p.lock.Unlock()

	if weight, ok := p.validators[nodeID]; ok {
		p.connectedWeight -= weight
		p.connectedValidators.Remove(nodeID)
	}
	p.connectedPeers.Remove(nodeID)
	return nil
}

func (p *peers) ConnectedWeight() uint64 {
	p.lock.RLock()
	defer p.lock.RUnlock()

	return p.connectedWeight
}

func (p *peers) PreferredPeers() ids.NodeIDSet {
	p.lock.RLock()
	defer p.lock.RUnlock()

	if p.connectedValidators.Len() == 0 {
		connectedPeers := ids.NewNodeIDSet(p.connectedPeers.Len())
		connectedPeers.Union(p.connectedPeers)
		return connectedPeers
	}

	connectedValidators := ids.NewNodeIDSet(p.connectedValidators.Len())
	connectedValidators.Union(p.connectedValidators)
	return connectedValidators
}

```

avalanchego/snow/engine/common/tracker/peers_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package tracker

import (
	"testing"

	"github.com/stretchr/testify/require"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/version"
)

func TestPeers(t *testing.T) {
	require := require.New(t)

	nodeID := ids.GenerateTestNodeID()

	p := NewPeers()

	require.Zero(p.ConnectedWeight())
	require.Empty(p.PreferredPeers())

	p.OnValidatorAdded(nodeID, 5)
	require.Zero(p.ConnectedWeight())
	require.Empty(p.PreferredPeers())

	err := p.Connected(nodeID, version.CurrentApp)
	require.NoError(err)
	require.EqualValues(5, p.ConnectedWeight())
	require.Contains(p.PreferredPeers(), nodeID)

	p.OnValidatorWeightChanged(nodeID, 5, 10)
	require.EqualValues(10, p.ConnectedWeight())
	require.Contains(p.PreferredPeers(), nodeID)

	p.OnValidatorRemoved(nodeID, 10)
	require.Zero(p.ConnectedWeight())
	require.Contains(p.PreferredPeers(), nodeID)

	p.OnValidatorAdded(nodeID, 5)
	require.EqualValues(5, p.ConnectedWeight())
	require.Contains(p.PreferredPeers(), nodeID)

	err = p.Disconnected(nodeID)
	require.NoError(err)
	require.Zero(p.ConnectedWeight())
	require.Empty(p.PreferredPeers())
}

```

avalanchego/snow/engine/common/tracker/startup.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package tracker

import (
	"sync"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/version"
)

var _ Startup = &startup{}

type Startup interface {
	Peers

	ShouldStart() bool
}

type startup struct {
	Peers

	lock          sync.RWMutex
	startupWeight uint64
	shouldStart   bool
}

func NewStartup(peers Peers, startupWeight uint64) Startup {
	return &startup{
		Peers:         peers,
		startupWeight: startupWeight,
		shouldStart:   peers.ConnectedWeight() >= startupWeight,
	}
}

func (s *startup) OnValidatorAdded(nodeID ids.NodeID, weight uint64) {
	s.lock.Lock()
	defer s.lock.Unlock()

	s.Peers.OnValidatorAdded(nodeID, weight)
	s.shouldStart = s.shouldStart || s.Peers.ConnectedWeight() >= s.startupWeight
}

func (s *startup) OnValidatorWeightChanged(nodeID ids.NodeID, oldWeight, newWeight uint64) {
	s.lock.Lock()
	defer s.lock.Unlock()

	s.Peers.OnValidatorWeightChanged(nodeID, oldWeight, newWeight)
	s.shouldStart = s.shouldStart || s.Peers.ConnectedWeight() >= s.startupWeight
}

func (s *startup) Connected(nodeID ids.NodeID, nodeVersion *version.Application) error {
	s.lock.Lock()
	defer s.lock.Unlock()

	if err := s.Peers.Connected(nodeID, nodeVersion); err != nil {
		return err
	}

	s.shouldStart = s.shouldStart || s.Peers.ConnectedWeight() >= s.startupWeight
	return nil
}

func (s *startup) ShouldStart() bool {
	s.lock.RLock()
	defer s.lock.RUnlock()

	return s.shouldStart
}

```

avalanchego/snow/engine/common/vm.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package common

import (
	"github.com/ava-labs/avalanchego/api/health"
	"github.com/ava-labs/avalanchego/database/manager"
	"github.com/ava-labs/avalanchego/snow"
	"github.com/ava-labs/avalanchego/snow/validators"
)

// VM describes the interface that all consensus VMs must implement
type VM interface {
	AppHandler

	// Returns nil if the VM is healthy.
	// Periodically called and reported via the node's Health API.
	health.Checker

	// Connector represents a handler that is called on connection connect/disconnect
	validators.Connector

	// Initialize this VM.
	// [ctx]: Metadata about this VM.
	//     [ctx.networkID]: The ID of the network this VM's chain is running on.
	//     [ctx.chainID]: The unique ID of the chain this VM is running on.
	//     [ctx.Log]: Used to log messages
	//     [ctx.NodeID]: The unique staker ID of this node.
	//     [ctx.Lock]: A Read/Write lock shared by this VM and the consensus
	//                 engine that manages this VM. The write lock is held
	//                 whenever code in the consensus engine calls the VM.
	// [dbManager]: The manager of the database this VM will persist data to.
	// [genesisBytes]: The byte-encoding of the genesis information of this
	//                 VM. The VM uses it to initialize its state. For
	//                 example, if this VM were an account-based payments
	//                 system, `genesisBytes` would probably contain a genesis
	//                 transaction that gives coins to some accounts, and this
	//                 transaction would be in the genesis block.
	// [toEngine]: The channel used to send messages to the consensus engine.
	// [fxs]: Feature extensions that attach to this VM.
	Initialize(
		ctx *snow.Context,
		dbManager manager.Manager,
		genesisBytes []byte,
		upgradeBytes []byte,
		configBytes []byte,
		toEngine chan<- Message,
		fxs []*Fx,
		appSender AppSender,
	) error

	// SetState communicates to VM its next state it starts
	SetState(state snow.State) error

	// Shutdown is called when the node is shutting down.
	Shutdown() error

	// Version returns the version of the VM this node is running.
	Version() (string, error)

	// Creates the HTTP handlers for custom VM network calls.
	//
	// This exposes handlers that the outside world can use to communicate with
	// a static reference to the VM. Each handler has the path:
	// [Address of node]/ext/VM/[VM ID]/[extension]
	//
	// Returns a mapping from [extension]s to HTTP handlers.
	//
	// Each extension can specify how locking is managed for convenience.
	//
	// For example, it might make sense to have an extension for creating
	// genesis bytes this VM can interpret.
	CreateStaticHandlers() (map[string]*HTTPHandler, error)

	// Creates the HTTP handlers for custom chain network calls.
	//
	// This exposes handlers that the outside world can use to communicate with
	// the chain. Each handler has the path:
	// [Address of node]/ext/bc/[chain ID]/[extension]
	//
	// Returns a mapping from [extension]s to HTTP handlers.
	//
	// Each extension can specify how locking is managed for convenience.
	//
	// For example, if this VM implements an account-based payments system,
	// it have an extension called `accounts`, where clients could get
	// information about their accounts.
	CreateHandlers() (map[string]*HTTPHandler, error)
}

```

avalanchego/snow/engine/snowman/ancestor_tree.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowman

import (
	"github.com/ava-labs/avalanchego/ids"
)

type AncestorTree interface {
	Add(blkID ids.ID, parentID ids.ID)
	Has(blkID ids.ID) bool
	GetRoot(blkID ids.ID) ids.ID
	Remove(blkID ids.ID)
	RemoveSubtree(blkID ids.ID)
	Len() int
}

type ancestorTree struct {
	childToParent    map[ids.ID]ids.ID
	parentToChildren map[ids.ID]ids.Set
}

func NewAncestorTree() AncestorTree {
	return &ancestorTree{
		childToParent:    make(map[ids.ID]ids.ID),
		parentToChildren: make(map[ids.ID]ids.Set),
	}
}

// Add maps given blkID to given parentID
func (p *ancestorTree) Add(blkID ids.ID, parentID ids.ID) {
	p.childToParent[blkID] = parentID

	children := p.parentToChildren[parentID]
	children.Add(blkID)
	p.parentToChildren[parentID] = children
}

// GetRoot returns the oldest parent of blkID, might return blkID if no parent is available.
func (p *ancestorTree) GetRoot(blkID ids.ID) ids.ID {
	for {
		parentID, ok := p.childToParent[blkID]
		// this is the furthest parent available, break loop and return blkID
		if !ok {
			return blkID
		}
		// continue to loop with parentID
		blkID = parentID
	}
}

// Has returns if blkID is in the tree or not
func (p *ancestorTree) Has(blkID ids.ID) bool {
	_, ok := p.childToParent[blkID]
	return ok
}

// Remove removes blkID from the tree
func (p *ancestorTree) Remove(blkID ids.ID) {
	parent, ok := p.childToParent[blkID]
	if !ok {
		return
	}
	delete(p.childToParent, blkID)

	// remove blkID from children
	children := p.parentToChildren[parent]
	children.Remove(blkID)
	// this parent has no more children, remove it from map
	if children.Len() == 0 {
		delete(p.parentToChildren, parent)
	}
}

// Returns tree length
func (p *ancestorTree) Len() int {
	return len(p.childToParent)
}

// RemoveSubtree removes whole subtree that blkID holds
func (p *ancestorTree) RemoveSubtree(blkID ids.ID) {
	childrenList := []ids.ID{blkID}
	for len(childrenList) > 0 {
		newChildrenSize := len(childrenList) - 1
		childID := childrenList[newChildrenSize]
		childrenList = childrenList[:newChildrenSize]
		p.Remove(childID)
		// get children of child
		for grandChildID := range p.parentToChildren[childID] {
			childrenList = append(childrenList, grandChildID)
		}
	}
}

```

avalanchego/snow/engine/snowman/ancestor_tree_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowman

import (
	"testing"

	"github.com/stretchr/testify/require"

	"github.com/ava-labs/avalanchego/ids"
)

func TestAdd(t *testing.T) {
	tests := map[string]struct {
		method func(require *require.Assertions, at AncestorTree)
	}{
		"should return false if not found": {
			method: func(require *require.Assertions, at AncestorTree) {
				id1 := ids.GenerateTestID()
				id := at.GetRoot(id1)
				require.Equal(id1, id)
			},
		},
		"should add to tree and return id2 root": {
			method: func(require *require.Assertions, at AncestorTree) {
				id1 := ids.GenerateTestID()
				id2 := ids.GenerateTestID()
				at.Add(id1, id2)
				require.True(at.Has(id1))
				result := at.GetRoot(id1)
				require.Equal(result, id2)
			},
		},
		"should return ancestor id3 through id2": {
			method: func(require *require.Assertions, at AncestorTree) {
				id1 := ids.GenerateTestID()
				id2 := ids.GenerateTestID()
				id3 := ids.GenerateTestID()
				at.Add(id1, id2)
				at.Add(id2, id3)
				require.True(at.Has(id2))
				result := at.GetRoot(id1)
				require.Equal(result, id3)
			},
		},
		"should also return root id3 for another child": {
			method: func(require *require.Assertions, at AncestorTree) {
				id1 := ids.GenerateTestID()
				id2 := ids.GenerateTestID()
				id3 := ids.GenerateTestID()
				id4 := ids.GenerateTestID()
				at.Add(id1, id2)
				at.Add(id2, id3)
				at.Add(id4, id2)
				result := at.GetRoot(id4)
				require.Equal(result, id3)
			},
		},
	}
	for name, test := range tests {
		t.Run(name, func(t *testing.T) {
			require := require.New(t)
			at := NewAncestorTree()
			test.method(require, at)
		})
	}
}

func TestRemove(t *testing.T) {
	tests := map[string]struct {
		method func(require *require.Assertions, at AncestorTree)
	}{
		"removing root should not affect child roots": {
			method: func(require *require.Assertions, at AncestorTree) {
				id1 := ids.GenerateTestID()
				id2 := ids.GenerateTestID()
				id3 := ids.GenerateTestID()
				at.Add(id1, id2)
				at.Add(id2, id3)
				at.Remove(id3)
				require.True(at.Has(id1))
				require.True(at.Has(id2))
				require.False(at.Has(id3))
				id := at.GetRoot(id2)
				require.Equal(id3, id)
				id = at.GetRoot(id1)
				require.Equal(id3, id)
			},
		},
		"removing parent should change root": {
			method: func(require *require.Assertions, at AncestorTree) {
				id1 := ids.GenerateTestID()
				id2 := ids.GenerateTestID()
				id3 := ids.GenerateTestID()
				id4 := ids.GenerateTestID()
				at.Add(id1, id2)
				at.Add(id2, id3)
				at.Add(id3, id4)
				id := at.GetRoot(id1)
				require.Equal(id4, id)
				at.Remove(id3)
				id = at.GetRoot(id1)
				require.Equal(id3, id)
			},
		},
	}
	for name, test := range tests {
		t.Run(name, func(t *testing.T) {
			require := require.New(t)
			at := NewAncestorTree()
			test.method(require, at)
		})
	}
}

func TestRemoveSubtree(t *testing.T) {
	tests := map[string]struct {
		method func(require *require.Assertions, at AncestorTree)
	}{
		"remove root's subtree": {
			method: func(require *require.Assertions, at AncestorTree) {
				id1 := ids.GenerateTestID()
				id2 := ids.GenerateTestID()
				id3 := ids.GenerateTestID()
				at.Add(id1, id2)
				at.Add(id2, id3)
				at.RemoveSubtree(id3)
				require.False(at.Has(id1))
				require.False(at.Has(id2))
				require.False(at.Has(id3))
				id := at.GetRoot(id2)
				require.Equal(id2, id)
				id = at.GetRoot(id1)
				require.Equal(id1, id)
			},
		},
		"remove subtree": {
			method: func(require *require.Assertions, at AncestorTree) {
				id1 := ids.GenerateTestID()
				id2 := ids.GenerateTestID()
				id3 := ids.GenerateTestID()
				id4 := ids.GenerateTestID()
				id5 := ids.GenerateTestID()
				at.Add(id1, id2)
				at.Add(id2, id3)
				at.Add(id3, id4)
				at.Add(id4, id5)
				at.RemoveSubtree(id3)
				require.False(at.Has(id1))
				require.False(at.Has(id2))
				require.False(at.Has(id3))
				id := at.GetRoot(id1)
				require.Equal(id, id1)
				id = at.GetRoot(id3)
				require.Equal(id, id3)
				require.True(at.Has(id4))
				id = at.GetRoot(id4)
				require.Equal(id5, id)
			},
		},
	}
	for name, test := range tests {
		t.Run(name, func(t *testing.T) {
			require := require.New(t)
			at := NewAncestorTree()
			test.method(require, at)
		})
	}
}

```

avalanchego/snow/engine/snowman/block/README.md:
```
# State Sync: Engine role and workflow

State sync promises to dramatically cut down the time it takes for a validator to join a chain by directly downloading and verifying the state of the chain, rather than rebuilding the state by processing all historical blocks.

Avalanche's approach to state sync leaves most of specifics to each VM, to allow maximal flexibility. However, the Avalanche engine plays a well defined role in selecting and validating `state summaries`, which are the state sync *seeds* used by a VM to kickstart its syncing process.

In this document, we outline the engine's role in state sync and the requirements for VMs implementing state sync.

## `StateSyncableVM` Interface

The [`StateSyncableVM`](./state_syncable_vm.go) interface enumerates all the features a VM must implement to support state sync.

The Avalanche engine begins bootstrapping a VM through state-sync if `StateSyncEnabled()` returns true. Otherwise, the engine falls back to processing all historical blocks.

The Avalanche engine performs two state-syncing phases:

- Frontier retrieval: retrieve the most recent state summaries from a random subset of validators
- Frontier validation: the retrieved state summaries are voted on by all connected validators

Security is guaranteed by accepting the state summary if and only if a sufficient fraction of stake has validated them. This prevents malicious actors from poisoning a VM with a corrupt or unavailable state summary.

### Frontier retrieval

The Avalanche engine waits to begin frontier retrieval until enough stake is connected.

The engine first calls `GetOngoingSyncStateSummary()` to retrieve a local state summary from the VM. If available, this state summary is added to the frontier.

A state summary may be locally available if the VM was previously shut down while state syncing. By revalidating this local summary, Avalanche engine helps the VM understand whether its local state summary is still widely supported by the network. If the local state summary is widely supported, the engine will allow the VM to resume state syncing from it. Otherwise the VM will be requested to drop it in favour of a fresher, more available state summary.

State summary frontier is collected as follows:

1. The Avalanche engine samples a random subset of validators and sends each of them a `GetStateSummaryFrontier` message to retrieve the latest state summaries.
2. Each target validator pulls its latest state summary from the VM via the `GetLastStateSummary` method, and responds with a `StateSummaryFrontier` message.
3. `StateSummaryFrontier` responses are parsed via the `ParseStateSummary` method and added to the state summary frontier to be validated.

The Avalanche engine does not pose major constraints on the state summary structure; as its parsing is left to the VM to implement. However, the Avalanche engine does require each state summary to be uniquely described by two identifiers, `Height` and `ID`. The reason for this double identification will be clearer as we describe the state summary validation phase.

`Height` is a `uint64` and represents the block height that the state summaries refers to. `Height` offers the most succinct way to address an accepted state summary.

`ID` is an `ids.ID` type and is the verifiable way to address a state summary, regardless of its status. In most implementations, a state summary `ID` is the hash of the state summary's bytes.

### Frontier validation

Once the frontiers have been retrieved, a network wide voting round is initiated to validate them. Avalanche sends a `GetAcceptedStateSummary` message to each connected validator, containing a list of the state summary frontier's `Height`s. `Height`s provide a unique yet succinct way to identify state summaries, and they help reduce the size of the  `GetAcceptedStateSummary` message.

When a validator receives a `GetAcceptedStateSummary` message, it will respond with `ID`s corresponding to the `Height`s sent in the message. This is done by calling `GetStateSummary(summaryHeight uint64)` on the VM for each `Height`. Unknown or unsupported state summary `Height`s are skipped. Responding with the summary `ID`s allows the client to verify votes easily and ensures the integrity of the state sync starting point.

`AcceptedStateSummary` messages returned to the state syncing node are validated by comparing responded state summaries `ID`s with the `ID`s calculated from state summary frontier previously retrieved. Valid responses are stored along with cumulative validator stake.

Once all validators respond or timeout, Avalanche will select all the state summaries with a sufficient stake verifying them.

If no state summary is backed by sufficient stake, the process of collecting a state frontier and validating it is restarted again, up to a configurable number of times.

Of all the valid state summaries, one is selected and passed to the VM by calling `Summary.Accept()`. The preferred state summary is selected as follows: if the locally available one is still valid and supported by the network it will be accepted to allow the VM to resume the previously interrupted state sync. Otherwise, the most recent state summary (with the highest `Height` value) is picked. Note that Avalanche engine will block upon `Summary.Accept()`'s response, hence the VM should perform actual state syncing asynchronously.

The Avalanche engine declares state syncing complete in the following cases:

1. `Summary.Accept()` returns `(false, nil)` signalling that the VM considered the summary valid but skips the whole syncing process. This may happen if the VM estimates that bootstrapping would be faster than state syncing with the provided summary.
2. The VM sends `StateSyncDone` via the `Notify` channel.

Note that any error returned from `Summary.Accept()` is considered fatal and causes the engine to shutdown.

After state sync is complete, Avalanche will continue bootstrapping the remaining blocks until the node has reached the block frontier.

```

avalanchego/snow/engine/snowman/block/batched_vm.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package block

import (
	"errors"
	"time"

	"github.com/ava-labs/avalanchego/database"
	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/consensus/snowman"
	"github.com/ava-labs/avalanchego/utils/wrappers"
)

var ErrRemoteVMNotImplemented = errors.New("vm does not implement RemoteVM interface")

// BatchedChainVM extends the minimal functionalities exposed by ChainVM for VMs
// communicating over network (gRPC in our case). This allows more efficient
// operations since calls over network can be duly batched
type BatchedChainVM interface {
	GetAncestors(
		blkID ids.ID, // first requested block
		maxBlocksNum int, // max number of blocks to be retrieved
		maxBlocksSize int, // max cumulated byte size of retrieved blocks
		maxBlocksRetrivalTime time.Duration, // max duration of retrival operation
	) ([][]byte, error)

	BatchedParseBlock(blks [][]byte) ([]snowman.Block, error)
}

func GetAncestors(
	vm Getter, // fetch blocks
	blkID ids.ID, // first requested block
	maxBlocksNum int, // max number of blocks to be retrieved
	maxBlocksSize int, // max cumulated byte size of retrieved blocks
	maxBlocksRetrivalTime time.Duration, // max duration of retrival operation
) ([][]byte, error) {
	// Try and batch GetBlock requests
	if vm, ok := vm.(BatchedChainVM); ok {
		blocks, err := vm.GetAncestors(
			blkID,
			maxBlocksNum,
			maxBlocksSize,
			maxBlocksRetrivalTime,
		)
		if err == nil {
			return blocks, nil
		}
		if err != ErrRemoteVMNotImplemented {
			return nil, err
		}
	}

	// RemoteVM did not work, try local logic
	startTime := time.Now()
	blk, err := vm.GetBlock(blkID)
	if err == database.ErrNotFound {
		// special case ErrNotFound as an empty response: this signals
		// the client to avoid contacting this node for further ancestors
		// as they may have been pruned or unavailable due to state-sync.
		return nil, nil
	} else if err != nil {
		return nil, err
	}

	// First elt is byte repr. of [blk], then its parent, then grandparent, etc.
	ancestorsBytes := make([][]byte, 1, maxBlocksNum)
	ancestorsBytes[0] = blk.Bytes()
	ancestorsBytesLen := len(blk.Bytes()) + wrappers.IntLen // length, in bytes, of all elements of ancestors

	for numFetched := 1; numFetched < maxBlocksNum && time.Since(startTime) < maxBlocksRetrivalTime; numFetched++ {
		if blk, err = vm.GetBlock(blk.Parent()); err != nil {
			break
		}
		blkBytes := blk.Bytes()
		// Ensure response size isn't too large. Include wrappers.IntLen because
		// the size of the message is included with each container, and the size
		// is repr. by an int.
		if newLen := ancestorsBytesLen + len(blkBytes) + wrappers.IntLen; newLen <= maxBlocksSize {
			ancestorsBytes = append(ancestorsBytes, blkBytes)
			ancestorsBytesLen = newLen
		} else { // reached maximum response size
			break
		}
	}

	return ancestorsBytes, nil
}

func BatchedParseBlock(vm Parser, blks [][]byte) ([]snowman.Block, error) {
	// Try and batch ParseBlock requests
	if vm, ok := vm.(BatchedChainVM); ok {
		blocks, err := vm.BatchedParseBlock(blks)
		if err == nil {
			return blocks, nil
		}
		if err != ErrRemoteVMNotImplemented {
			return nil, err
		}
	}

	// We couldn't batch the ParseBlock requests, try to parse them one at a
	// time.
	blocks := make([]snowman.Block, len(blks))
	for i, blockBytes := range blks {
		block, err := vm.ParseBlock(blockBytes)
		if err != nil {
			return nil, err
		}
		blocks[i] = block
	}
	return blocks, nil
}

```

avalanchego/snow/engine/snowman/block/batched_vm_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package block

import (
	"errors"
	"testing"
	"time"

	"github.com/stretchr/testify/require"

	"github.com/ava-labs/avalanchego/database"
	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/consensus/snowman"
)

func TestGetAncestorsDatabaseNotFound(t *testing.T) {
	vm := &TestVM{}
	someID := ids.GenerateTestID()
	vm.GetBlockF = func(id ids.ID) (snowman.Block, error) {
		require.Equal(t, someID, id)
		return nil, database.ErrNotFound
	}
	containers, err := GetAncestors(vm, someID, 10, 10, 1*time.Second)
	require.NoError(t, err)
	require.Len(t, containers, 0)
}

// TestGetAncestorsPropagatesErrors checks errors other than
// database.ErrNotFound propagate to caller.
func TestGetAncestorsPropagatesErrors(t *testing.T) {
	vm := &TestVM{}
	someID := ids.GenerateTestID()
	someError := errors.New("some error that is not ErrNotFound")
	vm.GetBlockF = func(id ids.ID) (snowman.Block, error) {
		require.Equal(t, someID, id)
		return nil, someError
	}
	containers, err := GetAncestors(vm, someID, 10, 10, 1*time.Second)
	require.Nil(t, containers)
	require.ErrorIs(t, err, someError)
}

```

avalanchego/snow/engine/snowman/block/height_indexed_vm.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.
package block

import (
	"errors"

	"github.com/ava-labs/avalanchego/ids"
)

var (
	ErrHeightIndexedVMNotImplemented = errors.New("vm does not implement HeightIndexedChainVM interface")
	ErrIndexIncomplete               = errors.New("query failed because height index is incomplete")
)

// HeightIndexedChainVM extends ChainVM to allow querying block IDs by height.
type HeightIndexedChainVM interface {
	// VerifyHeightIndex should return:
	// - nil if the height index is available.
	// - ErrHeightIndexedVMNotImplemented if the height index is not supported.
	// - ErrIndexIncomplete if the height index is not currently available.
	// - Any other non-standard error that may have occurred when verifying the
	//   index.
	VerifyHeightIndex() error

	// GetBlockIDAtHeight returns:
	// - The ID of the block that was accepted with [height].
	// - database.ErrNotFound if the [height] index is unknown.
	//
	// Note: A returned value of [database.ErrNotFound] typically means that the
	//       underlying VM was state synced and does not have access to the
	//       blockID at [height].
	GetBlockIDAtHeight(height uint64) (ids.ID, error)
}

```

avalanchego/snow/engine/snowman/block/mocks/chain_vm.go:
```
// Code generated by MockGen. DO NOT EDIT.
// Source: snow/engine/snowman/block/vm.go

// Package mocks is a generated GoMock package.
package mocks

import (
	reflect "reflect"
	time "time"

	manager "github.com/ava-labs/avalanchego/database/manager"
	ids "github.com/ava-labs/avalanchego/ids"
	snow "github.com/ava-labs/avalanchego/snow"
	snowman "github.com/ava-labs/avalanchego/snow/consensus/snowman"
	common "github.com/ava-labs/avalanchego/snow/engine/common"
	version "github.com/ava-labs/avalanchego/version"
	gomock "github.com/golang/mock/gomock"
)

// MockChainVM is a mock of ChainVM interface.
type MockChainVM struct {
	ctrl     *gomock.Controller
	recorder *MockChainVMMockRecorder
}

// MockChainVMMockRecorder is the mock recorder for MockChainVM.
type MockChainVMMockRecorder struct {
	mock *MockChainVM
}

// NewMockChainVM creates a new mock instance.
func NewMockChainVM(ctrl *gomock.Controller) *MockChainVM {
	mock := &MockChainVM{ctrl: ctrl}
	mock.recorder = &MockChainVMMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockChainVM) EXPECT() *MockChainVMMockRecorder {
	return m.recorder
}

// AppGossip mocks base method.
func (m *MockChainVM) AppGossip(nodeID ids.NodeID, msg []byte) error {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "AppGossip", nodeID, msg)
	ret0, _ := ret[0].(error)
	return ret0
}

// AppGossip indicates an expected call of AppGossip.
func (mr *MockChainVMMockRecorder) AppGossip(nodeID, msg interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "AppGossip", reflect.TypeOf((*MockChainVM)(nil).AppGossip), nodeID, msg)
}

// AppRequest mocks base method.
func (m *MockChainVM) AppRequest(nodeID ids.NodeID, requestID uint32, deadline time.Time, request []byte) error {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "AppRequest", nodeID, requestID, deadline, request)
	ret0, _ := ret[0].(error)
	return ret0
}

// AppRequest indicates an expected call of AppRequest.
func (mr *MockChainVMMockRecorder) AppRequest(nodeID, requestID, deadline, request interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "AppRequest", reflect.TypeOf((*MockChainVM)(nil).AppRequest), nodeID, requestID, deadline, request)
}

// AppRequestFailed mocks base method.
func (m *MockChainVM) AppRequestFailed(nodeID ids.NodeID, requestID uint32) error {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "AppRequestFailed", nodeID, requestID)
	ret0, _ := ret[0].(error)
	return ret0
}

// AppRequestFailed indicates an expected call of AppRequestFailed.
func (mr *MockChainVMMockRecorder) AppRequestFailed(nodeID, requestID interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "AppRequestFailed", reflect.TypeOf((*MockChainVM)(nil).AppRequestFailed), nodeID, requestID)
}

// AppResponse mocks base method.
func (m *MockChainVM) AppResponse(nodeID ids.NodeID, requestID uint32, response []byte) error {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "AppResponse", nodeID, requestID, response)
	ret0, _ := ret[0].(error)
	return ret0
}

// AppResponse indicates an expected call of AppResponse.
func (mr *MockChainVMMockRecorder) AppResponse(nodeID, requestID, response interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "AppResponse", reflect.TypeOf((*MockChainVM)(nil).AppResponse), nodeID, requestID, response)
}

// BuildBlock mocks base method.
func (m *MockChainVM) BuildBlock() (snowman.Block, error) {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "BuildBlock")
	ret0, _ := ret[0].(snowman.Block)
	ret1, _ := ret[1].(error)
	return ret0, ret1
}

// BuildBlock indicates an expected call of BuildBlock.
func (mr *MockChainVMMockRecorder) BuildBlock() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "BuildBlock", reflect.TypeOf((*MockChainVM)(nil).BuildBlock))
}

// Connected mocks base method.
func (m *MockChainVM) Connected(id ids.NodeID, nodeVersion *version.Application) error {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "Connected", id, nodeVersion)
	ret0, _ := ret[0].(error)
	return ret0
}

// Connected indicates an expected call of Connected.
func (mr *MockChainVMMockRecorder) Connected(id, nodeVersion interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Connected", reflect.TypeOf((*MockChainVM)(nil).Connected), id, nodeVersion)
}

// CreateHandlers mocks base method.
func (m *MockChainVM) CreateHandlers() (map[string]*common.HTTPHandler, error) {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "CreateHandlers")
	ret0, _ := ret[0].(map[string]*common.HTTPHandler)
	ret1, _ := ret[1].(error)
	return ret0, ret1
}

// CreateHandlers indicates an expected call of CreateHandlers.
func (mr *MockChainVMMockRecorder) CreateHandlers() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "CreateHandlers", reflect.TypeOf((*MockChainVM)(nil).CreateHandlers))
}

// CreateStaticHandlers mocks base method.
func (m *MockChainVM) CreateStaticHandlers() (map[string]*common.HTTPHandler, error) {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "CreateStaticHandlers")
	ret0, _ := ret[0].(map[string]*common.HTTPHandler)
	ret1, _ := ret[1].(error)
	return ret0, ret1
}

// CreateStaticHandlers indicates an expected call of CreateStaticHandlers.
func (mr *MockChainVMMockRecorder) CreateStaticHandlers() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "CreateStaticHandlers", reflect.TypeOf((*MockChainVM)(nil).CreateStaticHandlers))
}

// Disconnected mocks base method.
func (m *MockChainVM) Disconnected(id ids.NodeID) error {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "Disconnected", id)
	ret0, _ := ret[0].(error)
	return ret0
}

// Disconnected indicates an expected call of Disconnected.
func (mr *MockChainVMMockRecorder) Disconnected(id interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Disconnected", reflect.TypeOf((*MockChainVM)(nil).Disconnected), id)
}

// GetBlock mocks base method.
func (m *MockChainVM) GetBlock(arg0 ids.ID) (snowman.Block, error) {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "GetBlock", arg0)
	ret0, _ := ret[0].(snowman.Block)
	ret1, _ := ret[1].(error)
	return ret0, ret1
}

// GetBlock indicates an expected call of GetBlock.
func (mr *MockChainVMMockRecorder) GetBlock(arg0 interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "GetBlock", reflect.TypeOf((*MockChainVM)(nil).GetBlock), arg0)
}

// HealthCheck mocks base method.
func (m *MockChainVM) HealthCheck() (interface{}, error) {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "HealthCheck")
	ret0, _ := ret[0].(interface{})
	ret1, _ := ret[1].(error)
	return ret0, ret1
}

// HealthCheck indicates an expected call of HealthCheck.
func (mr *MockChainVMMockRecorder) HealthCheck() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "HealthCheck", reflect.TypeOf((*MockChainVM)(nil).HealthCheck))
}

// Initialize mocks base method.
func (m *MockChainVM) Initialize(ctx *snow.Context, dbManager manager.Manager, genesisBytes, upgradeBytes, configBytes []byte, toEngine chan<- common.Message, fxs []*common.Fx, appSender common.AppSender) error {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "Initialize", ctx, dbManager, genesisBytes, upgradeBytes, configBytes, toEngine, fxs, appSender)
	ret0, _ := ret[0].(error)
	return ret0
}

// Initialize indicates an expected call of Initialize.
func (mr *MockChainVMMockRecorder) Initialize(ctx, dbManager, genesisBytes, upgradeBytes, configBytes, toEngine, fxs, appSender interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Initialize", reflect.TypeOf((*MockChainVM)(nil).Initialize), ctx, dbManager, genesisBytes, upgradeBytes, configBytes, toEngine, fxs, appSender)
}

// LastAccepted mocks base method.
func (m *MockChainVM) LastAccepted() (ids.ID, error) {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "LastAccepted")
	ret0, _ := ret[0].(ids.ID)
	ret1, _ := ret[1].(error)
	return ret0, ret1
}

// LastAccepted indicates an expected call of LastAccepted.
func (mr *MockChainVMMockRecorder) LastAccepted() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "LastAccepted", reflect.TypeOf((*MockChainVM)(nil).LastAccepted))
}

// ParseBlock mocks base method.
func (m *MockChainVM) ParseBlock(arg0 []byte) (snowman.Block, error) {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "ParseBlock", arg0)
	ret0, _ := ret[0].(snowman.Block)
	ret1, _ := ret[1].(error)
	return ret0, ret1
}

// ParseBlock indicates an expected call of ParseBlock.
func (mr *MockChainVMMockRecorder) ParseBlock(arg0 interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "ParseBlock", reflect.TypeOf((*MockChainVM)(nil).ParseBlock), arg0)
}

// SetPreference mocks base method.
func (m *MockChainVM) SetPreference(arg0 ids.ID) error {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "SetPreference", arg0)
	ret0, _ := ret[0].(error)
	return ret0
}

// SetPreference indicates an expected call of SetPreference.
func (mr *MockChainVMMockRecorder) SetPreference(arg0 interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "SetPreference", reflect.TypeOf((*MockChainVM)(nil).SetPreference), arg0)
}

// SetState mocks base method.
func (m *MockChainVM) SetState(state snow.State) error {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "SetState", state)
	ret0, _ := ret[0].(error)
	return ret0
}

// SetState indicates an expected call of SetState.
func (mr *MockChainVMMockRecorder) SetState(state interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "SetState", reflect.TypeOf((*MockChainVM)(nil).SetState), state)
}

// Shutdown mocks base method.
func (m *MockChainVM) Shutdown() error {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "Shutdown")
	ret0, _ := ret[0].(error)
	return ret0
}

// Shutdown indicates an expected call of Shutdown.
func (mr *MockChainVMMockRecorder) Shutdown() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Shutdown", reflect.TypeOf((*MockChainVM)(nil).Shutdown))
}

// Version mocks base method.
func (m *MockChainVM) Version() (string, error) {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "Version")
	ret0, _ := ret[0].(string)
	ret1, _ := ret[1].(error)
	return ret0, ret1
}

// Version indicates an expected call of Version.
func (mr *MockChainVMMockRecorder) Version() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Version", reflect.TypeOf((*MockChainVM)(nil).Version))
}

// MockGetter is a mock of Getter interface.
type MockGetter struct {
	ctrl     *gomock.Controller
	recorder *MockGetterMockRecorder
}

// MockGetterMockRecorder is the mock recorder for MockGetter.
type MockGetterMockRecorder struct {
	mock *MockGetter
}

// NewMockGetter creates a new mock instance.
func NewMockGetter(ctrl *gomock.Controller) *MockGetter {
	mock := &MockGetter{ctrl: ctrl}
	mock.recorder = &MockGetterMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockGetter) EXPECT() *MockGetterMockRecorder {
	return m.recorder
}

// GetBlock mocks base method.
func (m *MockGetter) GetBlock(arg0 ids.ID) (snowman.Block, error) {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "GetBlock", arg0)
	ret0, _ := ret[0].(snowman.Block)
	ret1, _ := ret[1].(error)
	return ret0, ret1
}

// GetBlock indicates an expected call of GetBlock.
func (mr *MockGetterMockRecorder) GetBlock(arg0 interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "GetBlock", reflect.TypeOf((*MockGetter)(nil).GetBlock), arg0)
}

// MockParser is a mock of Parser interface.
type MockParser struct {
	ctrl     *gomock.Controller
	recorder *MockParserMockRecorder
}

// MockParserMockRecorder is the mock recorder for MockParser.
type MockParserMockRecorder struct {
	mock *MockParser
}

// NewMockParser creates a new mock instance.
func NewMockParser(ctrl *gomock.Controller) *MockParser {
	mock := &MockParser{ctrl: ctrl}
	mock.recorder = &MockParserMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockParser) EXPECT() *MockParserMockRecorder {
	return m.recorder
}

// ParseBlock mocks base method.
func (m *MockParser) ParseBlock(arg0 []byte) (snowman.Block, error) {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "ParseBlock", arg0)
	ret0, _ := ret[0].(snowman.Block)
	ret1, _ := ret[1].(error)
	return ret0, ret1
}

// ParseBlock indicates an expected call of ParseBlock.
func (mr *MockParserMockRecorder) ParseBlock(arg0 interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "ParseBlock", reflect.TypeOf((*MockParser)(nil).ParseBlock), arg0)
}

```

avalanchego/snow/engine/snowman/block/mocks/state_syncable_vm.go:
```
// Code generated by MockGen. DO NOT EDIT.
// Source: snow/engine/snowman/block/state_syncable_vm.go

// Package mocks is a generated GoMock package.
package mocks

import (
	reflect "reflect"

	block "github.com/ava-labs/avalanchego/snow/engine/snowman/block"
	gomock "github.com/golang/mock/gomock"
)

// MockStateSyncableVM is a mock of StateSyncableVM interface.
type MockStateSyncableVM struct {
	ctrl     *gomock.Controller
	recorder *MockStateSyncableVMMockRecorder
}

// MockStateSyncableVMMockRecorder is the mock recorder for MockStateSyncableVM.
type MockStateSyncableVMMockRecorder struct {
	mock *MockStateSyncableVM
}

// NewMockStateSyncableVM creates a new mock instance.
func NewMockStateSyncableVM(ctrl *gomock.Controller) *MockStateSyncableVM {
	mock := &MockStateSyncableVM{ctrl: ctrl}
	mock.recorder = &MockStateSyncableVMMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockStateSyncableVM) EXPECT() *MockStateSyncableVMMockRecorder {
	return m.recorder
}

// GetLastStateSummary mocks base method.
func (m *MockStateSyncableVM) GetLastStateSummary() (block.StateSummary, error) {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "GetLastStateSummary")
	ret0, _ := ret[0].(block.StateSummary)
	ret1, _ := ret[1].(error)
	return ret0, ret1
}

// GetLastStateSummary indicates an expected call of GetLastStateSummary.
func (mr *MockStateSyncableVMMockRecorder) GetLastStateSummary() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "GetLastStateSummary", reflect.TypeOf((*MockStateSyncableVM)(nil).GetLastStateSummary))
}

// GetOngoingSyncStateSummary mocks base method.
func (m *MockStateSyncableVM) GetOngoingSyncStateSummary() (block.StateSummary, error) {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "GetOngoingSyncStateSummary")
	ret0, _ := ret[0].(block.StateSummary)
	ret1, _ := ret[1].(error)
	return ret0, ret1
}

// GetOngoingSyncStateSummary indicates an expected call of GetOngoingSyncStateSummary.
func (mr *MockStateSyncableVMMockRecorder) GetOngoingSyncStateSummary() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "GetOngoingSyncStateSummary", reflect.TypeOf((*MockStateSyncableVM)(nil).GetOngoingSyncStateSummary))
}

// GetStateSummary mocks base method.
func (m *MockStateSyncableVM) GetStateSummary(summaryHeight uint64) (block.StateSummary, error) {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "GetStateSummary", summaryHeight)
	ret0, _ := ret[0].(block.StateSummary)
	ret1, _ := ret[1].(error)
	return ret0, ret1
}

// GetStateSummary indicates an expected call of GetStateSummary.
func (mr *MockStateSyncableVMMockRecorder) GetStateSummary(summaryHeight interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "GetStateSummary", reflect.TypeOf((*MockStateSyncableVM)(nil).GetStateSummary), summaryHeight)
}

// ParseStateSummary mocks base method.
func (m *MockStateSyncableVM) ParseStateSummary(summaryBytes []byte) (block.StateSummary, error) {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "ParseStateSummary", summaryBytes)
	ret0, _ := ret[0].(block.StateSummary)
	ret1, _ := ret[1].(error)
	return ret0, ret1
}

// ParseStateSummary indicates an expected call of ParseStateSummary.
func (mr *MockStateSyncableVMMockRecorder) ParseStateSummary(summaryBytes interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "ParseStateSummary", reflect.TypeOf((*MockStateSyncableVM)(nil).ParseStateSummary), summaryBytes)
}

// StateSyncEnabled mocks base method.
func (m *MockStateSyncableVM) StateSyncEnabled() (bool, error) {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "StateSyncEnabled")
	ret0, _ := ret[0].(bool)
	ret1, _ := ret[1].(error)
	return ret0, ret1
}

// StateSyncEnabled indicates an expected call of StateSyncEnabled.
func (mr *MockStateSyncableVMMockRecorder) StateSyncEnabled() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "StateSyncEnabled", reflect.TypeOf((*MockStateSyncableVM)(nil).StateSyncEnabled))
}

```

avalanchego/snow/engine/snowman/block/state_summary.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package block

import (
	"github.com/ava-labs/avalanchego/ids"
)

// StateSummary represents all the information needed to download, verify, and
// rebuild its state.
type StateSummary interface {
	// ID uniquely identifies this state summary, regardless of the chain state.
	ID() ids.ID

	// Height uniquely identifies this an accepted state summary.
	Height() uint64

	// Bytes returns a byte slice than can be used to reconstruct this summary.
	Bytes() []byte

	// Accept triggers the VM to start state syncing this summary.
	//
	// The returned boolean will be [true] if the VM has started state sync or
	// [false] if the VM has skipped state sync.
	Accept() (bool, error)
}

```

avalanchego/snow/engine/snowman/block/state_syncable_vm.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package block

import (
	"errors"
)

var ErrStateSyncableVMNotImplemented = errors.New("vm does not implement StateSyncableVM interface")

// StateSyncableVM contains the functionality to allow VMs to sync to a given
// state, rather then boostrapping from genesis.
type StateSyncableVM interface {
	// StateSyncEnabled indicates whether the state sync is enabled for this VM.
	// If StateSyncableVM is not implemented, as it may happen with a wrapper
	// VM, StateSyncEnabled should return false, nil
	StateSyncEnabled() (bool, error)

	// GetOngoingSyncStateSummary returns an in-progress state summary if it
	// exists.
	//
	// The engine can then ask the network if the ongoing summary is still
	// supported, thus helping the VM decide whether to continue an in-progress
	// sync or start over.
	//
	// Returns database.ErrNotFound if there is no in-progress sync.
	GetOngoingSyncStateSummary() (StateSummary, error)

	// GetLastStateSummary returns the latest state summary.
	//
	// Returns database.ErrNotFound if no summary is available.
	GetLastStateSummary() (StateSummary, error)

	// ParseStateSummary parses a state summary out of [summaryBytes].
	ParseStateSummary(summaryBytes []byte) (StateSummary, error)

	// GetStateSummary retrieves the state summary that was generated at height
	// [summaryHeight].
	//
	// Returns database.ErrNotFound if no summary is available at
	// [summaryHeight].
	GetStateSummary(summaryHeight uint64) (StateSummary, error)
}

```

avalanchego/snow/engine/snowman/block/test_batched_vm.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package block

import (
	"errors"
	"testing"
	"time"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/consensus/snowman"
)

var (
	errGetAncestor       = errors.New("unexpectedly called GetAncestor")
	errBatchedParseBlock = errors.New("unexpectedly called BatchedParseBlock")

	_ BatchedChainVM = &TestBatchedVM{}
)

// TestBatchedVM is a BatchedVM that is useful for testing.
type TestBatchedVM struct {
	T *testing.T

	CantGetAncestors    bool
	CantBatchParseBlock bool

	GetAncestorsF func(
		blkID ids.ID,
		maxBlocksNum int,
		maxBlocksSize int,
		maxBlocksRetrivalTime time.Duration,
	) ([][]byte, error)

	BatchedParseBlockF func(blks [][]byte) ([]snowman.Block, error)
}

func (vm *TestBatchedVM) Default(cant bool) {
	vm.CantGetAncestors = cant
	vm.CantBatchParseBlock = cant
}

func (vm *TestBatchedVM) GetAncestors(
	blkID ids.ID,
	maxBlocksNum int,
	maxBlocksSize int,
	maxBlocksRetrivalTime time.Duration,
) ([][]byte, error) {
	if vm.GetAncestorsF != nil {
		return vm.GetAncestorsF(blkID, maxBlocksNum, maxBlocksSize, maxBlocksRetrivalTime)
	}
	if vm.CantGetAncestors && vm.T != nil {
		vm.T.Fatal(errGetAncestor)
	}
	return nil, errGetAncestor
}

func (vm *TestBatchedVM) BatchedParseBlock(blks [][]byte) ([]snowman.Block, error) {
	if vm.BatchedParseBlockF != nil {
		return vm.BatchedParseBlockF(blks)
	}
	if vm.CantBatchParseBlock && vm.T != nil {
		vm.T.Fatal(errBatchedParseBlock)
	}
	return nil, errBatchedParseBlock
}

```

avalanchego/snow/engine/snowman/block/test_height_indexed_vm.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package block

import (
	"errors"
	"testing"

	"github.com/ava-labs/avalanchego/ids"
)

var (
	errVerifyHeightIndex  = errors.New("unexpectedly called VerifyHeightIndex")
	errGetBlockIDAtHeight = errors.New("unexpectedly called GetBlockIDAtHeight")

	_ HeightIndexedChainVM = &TestHeightIndexedVM{}
)

// TestBatchedVM is a BatchedVM that is useful for testing.
type TestHeightIndexedVM struct {
	T *testing.T

	CantVerifyHeightIndex  bool
	CantGetBlockIDAtHeight bool

	VerifyHeightIndexF  func() error
	GetBlockIDAtHeightF func(height uint64) (ids.ID, error)
}

func (vm *TestHeightIndexedVM) VerifyHeightIndex() error {
	if vm.VerifyHeightIndexF != nil {
		return vm.VerifyHeightIndexF()
	}
	if vm.CantVerifyHeightIndex && vm.T != nil {
		vm.T.Fatal(errVerifyHeightIndex)
	}
	return errVerifyHeightIndex
}

func (vm *TestHeightIndexedVM) GetBlockIDAtHeight(height uint64) (ids.ID, error) {
	if vm.GetBlockIDAtHeightF != nil {
		return vm.GetBlockIDAtHeightF(height)
	}
	if vm.CantGetBlockIDAtHeight && vm.T != nil {
		vm.T.Fatal(errGetAncestor)
	}
	return ids.Empty, errGetBlockIDAtHeight
}

```

avalanchego/snow/engine/snowman/block/test_state_summary.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package block

import (
	"errors"
	"testing"

	"github.com/ava-labs/avalanchego/ids"
)

var (
	_ StateSummary = &TestStateSummary{}

	errAccept = errors.New("unexpectedly called Accept")
)

type TestStateSummary struct {
	IDV     ids.ID
	HeightV uint64
	BytesV  []byte

	T          *testing.T
	CantAccept bool
	AcceptF    func() (bool, error)
}

func (s *TestStateSummary) ID() ids.ID     { return s.IDV }
func (s *TestStateSummary) Height() uint64 { return s.HeightV }
func (s *TestStateSummary) Bytes() []byte  { return s.BytesV }

func (s *TestStateSummary) Accept() (bool, error) {
	if s.AcceptF != nil {
		return s.AcceptF()
	}
	if s.CantAccept && s.T != nil {
		s.T.Fatal(errAccept)
	}
	return false, errAccept
}

```

avalanchego/snow/engine/snowman/block/test_state_syncable_vm.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package block

import (
	"errors"
	"testing"
)

var (
	_ StateSyncableVM = &TestStateSyncableVM{}

	errStateSyncEnabled           = errors.New("unexpectedly called StateSyncEnabled")
	errStateSyncGetOngoingSummary = errors.New("unexpectedly called StateSyncGetOngoingSummary")
	errGetLastStateSummary        = errors.New("unexpectedly called GetLastStateSummary")
	errParseStateSummary          = errors.New("unexpectedly called ParseStateSummary")
	errGetStateSummary            = errors.New("unexpectedly called GetStateSummary")
)

type TestStateSyncableVM struct {
	T *testing.T

	CantStateSyncEnabled,
	CantStateSyncGetOngoingSummary,
	CantGetLastStateSummary,
	CantParseStateSummary,
	CantGetStateSummary bool

	StateSyncEnabledF           func() (bool, error)
	GetOngoingSyncStateSummaryF func() (StateSummary, error)
	GetLastStateSummaryF        func() (StateSummary, error)
	ParseStateSummaryF          func(summaryBytes []byte) (StateSummary, error)
	GetStateSummaryF            func(uint64) (StateSummary, error)
}

func (vm *TestStateSyncableVM) StateSyncEnabled() (bool, error) {
	if vm.StateSyncEnabledF != nil {
		return vm.StateSyncEnabledF()
	}
	if vm.CantStateSyncEnabled && vm.T != nil {
		vm.T.Fatal(errStateSyncEnabled)
	}
	return false, errStateSyncEnabled
}

func (vm *TestStateSyncableVM) GetOngoingSyncStateSummary() (StateSummary, error) {
	if vm.GetOngoingSyncStateSummaryF != nil {
		return vm.GetOngoingSyncStateSummaryF()
	}
	if vm.CantStateSyncGetOngoingSummary && vm.T != nil {
		vm.T.Fatal(errStateSyncGetOngoingSummary)
	}
	return nil, errStateSyncGetOngoingSummary
}

func (vm *TestStateSyncableVM) GetLastStateSummary() (StateSummary, error) {
	if vm.GetLastStateSummaryF != nil {
		return vm.GetLastStateSummaryF()
	}
	if vm.CantGetLastStateSummary && vm.T != nil {
		vm.T.Fatal(errGetLastStateSummary)
	}
	return nil, errGetLastStateSummary
}

func (vm *TestStateSyncableVM) ParseStateSummary(summaryBytes []byte) (StateSummary, error) {
	if vm.ParseStateSummaryF != nil {
		return vm.ParseStateSummaryF(summaryBytes)
	}
	if vm.CantParseStateSummary && vm.T != nil {
		vm.T.Fatal(errParseStateSummary)
	}
	return nil, errParseStateSummary
}

func (vm *TestStateSyncableVM) GetStateSummary(key uint64) (StateSummary, error) {
	if vm.GetStateSummaryF != nil {
		return vm.GetStateSummaryF(key)
	}
	if vm.CantGetStateSummary && vm.T != nil {
		vm.T.Fatal(errGetStateSummary)
	}
	return nil, errGetStateSummary
}

```

avalanchego/snow/engine/snowman/block/test_vm.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package block

import (
	"errors"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/consensus/snowman"
	"github.com/ava-labs/avalanchego/snow/engine/common"
)

var (
	errBuildBlock   = errors.New("unexpectedly called BuildBlock")
	errParseBlock   = errors.New("unexpectedly called ParseBlock")
	errGetBlock     = errors.New("unexpectedly called GetBlock")
	errLastAccepted = errors.New("unexpectedly called LastAccepted")

	_ ChainVM = &TestVM{}
)

// TestVM is a ChainVM that is useful for testing.
type TestVM struct {
	common.TestVM

	CantBuildBlock,
	CantParseBlock,
	CantGetBlock,
	CantSetPreference,
	CantLastAccepted bool

	BuildBlockF    func() (snowman.Block, error)
	ParseBlockF    func([]byte) (snowman.Block, error)
	GetBlockF      func(ids.ID) (snowman.Block, error)
	SetPreferenceF func(ids.ID) error
	LastAcceptedF  func() (ids.ID, error)
}

func (vm *TestVM) Default(cant bool) {
	vm.TestVM.Default(cant)

	vm.CantBuildBlock = cant
	vm.CantParseBlock = cant
	vm.CantGetBlock = cant
	vm.CantSetPreference = cant
	vm.CantLastAccepted = cant
}

func (vm *TestVM) BuildBlock() (snowman.Block, error) {
	if vm.BuildBlockF != nil {
		return vm.BuildBlockF()
	}
	if vm.CantBuildBlock && vm.T != nil {
		vm.T.Fatal(errBuildBlock)
	}
	return nil, errBuildBlock
}

func (vm *TestVM) ParseBlock(b []byte) (snowman.Block, error) {
	if vm.ParseBlockF != nil {
		return vm.ParseBlockF(b)
	}
	if vm.CantParseBlock && vm.T != nil {
		vm.T.Fatal(errParseBlock)
	}
	return nil, errParseBlock
}

func (vm *TestVM) GetBlock(id ids.ID) (snowman.Block, error) {
	if vm.GetBlockF != nil {
		return vm.GetBlockF(id)
	}
	if vm.CantGetBlock && vm.T != nil {
		vm.T.Fatal(errGetBlock)
	}
	return nil, errGetBlock
}

func (vm *TestVM) SetPreference(id ids.ID) error {
	if vm.SetPreferenceF != nil {
		return vm.SetPreferenceF(id)
	}
	if vm.CantSetPreference && vm.T != nil {
		vm.T.Fatalf("Unexpectedly called SetPreference")
	}
	return nil
}

func (vm *TestVM) LastAccepted() (ids.ID, error) {
	if vm.LastAcceptedF != nil {
		return vm.LastAcceptedF()
	}
	if vm.CantLastAccepted && vm.T != nil {
		vm.T.Fatal(errLastAccepted)
	}
	return ids.ID{}, errLastAccepted
}

```

avalanchego/snow/engine/snowman/block/vm.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package block

import (
	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/consensus/snowman"
	"github.com/ava-labs/avalanchego/snow/engine/common"
)

// ChainVM defines the required functionality of a Snowman VM.
//
// A Snowman VM is responsible for defining the representation of state,
// the representation of operations on that state, the application of operations
// on that state, and the creation of the operations. Consensus will decide on
// if the operation is executed and the order operations are executed in.
//
// For example, suppose we have a VM that tracks an increasing number that
// is agreed upon by the network.
// The state is a single number.
// The operation is setting the number to a new, larger value.
// Applying the operation will save to the database the new value.
// The VM can attempt to issue a new number, of larger value, at any time.
// Consensus will ensure the network agrees on the number at every block height.
type ChainVM interface {
	common.VM

	Getter
	Parser

	// Attempt to create a new block from data contained in the VM.
	//
	// If the VM doesn't want to issue a new block, an error should be
	// returned.
	BuildBlock() (snowman.Block, error)

	// Notify the VM of the currently preferred block.
	//
	// This should always be a block that has no children known to consensus.
	SetPreference(ids.ID) error

	// LastAccepted returns the ID of the last accepted block.
	//
	// If no blocks have been accepted by consensus yet, it is assumed there is
	// a definitionally accepted block, the Genesis block, that will be
	// returned.
	LastAccepted() (ids.ID, error)
}

// Getter defines the functionality for fetching a block by its ID.
type Getter interface {
	// Attempt to load a block.
	//
	// If the block does not exist, database.ErrNotFound should be returned.
	//
	// It is expected that blocks that have been successfully verified should be
	// returned correctly. It is also expected that blocks that have been
	// accepted by the consensus engine should be able to be fetched. It is not
	// required for blocks that have been rejected by the consensus engine to be
	// able to be fetched.
	GetBlock(ids.ID) (snowman.Block, error)
}

// Parser defines the functionality for fetching a block by its bytes.
type Parser interface {
	// Attempt to create a block from a stream of bytes.
	//
	// The block should be represented by the full byte array, without extra
	// bytes.
	//
	// It is expected for all historical blocks to be parseable.
	ParseBlock([]byte) (snowman.Block, error)
}

```

avalanchego/snow/engine/snowman/bootstrap/block_job.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package bootstrap

import (
	"errors"
	"fmt"

	"github.com/prometheus/client_golang/prometheus"

	"go.uber.org/zap"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/choices"
	"github.com/ava-labs/avalanchego/snow/consensus/snowman"
	"github.com/ava-labs/avalanchego/snow/engine/common/queue"
	"github.com/ava-labs/avalanchego/snow/engine/snowman/block"
	"github.com/ava-labs/avalanchego/utils/logging"
)

var errMissingDependenciesOnAccept = errors.New("attempting to accept a block with missing dependencies")

type parser struct {
	log                     logging.Logger
	numAccepted, numDropped prometheus.Counter
	vm                      block.ChainVM
}

func (p *parser) Parse(blkBytes []byte) (queue.Job, error) {
	blk, err := p.vm.ParseBlock(blkBytes)
	if err != nil {
		return nil, err
	}
	return &blockJob{
		parser:      p,
		log:         p.log,
		numAccepted: p.numAccepted,
		numDropped:  p.numDropped,
		blk:         blk,
		vm:          p.vm,
	}, nil
}

type blockJob struct {
	parser                  *parser
	log                     logging.Logger
	numAccepted, numDropped prometheus.Counter
	blk                     snowman.Block
	vm                      block.Getter
}

func (b *blockJob) ID() ids.ID { return b.blk.ID() }
func (b *blockJob) MissingDependencies() (ids.Set, error) {
	missing := ids.Set{}
	parentID := b.blk.Parent()
	if parent, err := b.vm.GetBlock(parentID); err != nil || parent.Status() != choices.Accepted {
		missing.Add(parentID)
	}
	return missing, nil
}

func (b *blockJob) HasMissingDependencies() (bool, error) {
	parentID := b.blk.Parent()
	if parent, err := b.vm.GetBlock(parentID); err != nil || parent.Status() != choices.Accepted {
		return true, nil
	}
	return false, nil
}

func (b *blockJob) Execute() error {
	hasMissingDeps, err := b.HasMissingDependencies()
	if err != nil {
		return err
	}
	if hasMissingDeps {
		b.numDropped.Inc()
		return errMissingDependenciesOnAccept
	}
	status := b.blk.Status()
	switch status {
	case choices.Unknown, choices.Rejected:
		b.numDropped.Inc()
		return fmt.Errorf("attempting to execute block with status %s", status)
	case choices.Processing:
		blkID := b.blk.ID()
		if err := b.blk.Verify(); err != nil {
			b.log.Error("block failed verification during bootstrapping",
				zap.Stringer("blkID", blkID),
				zap.Error(err),
			)
			return fmt.Errorf("failed to verify block in bootstrapping: %w", err)
		}

		b.numAccepted.Inc()
		b.log.Trace("accepting block in bootstrapping",
			zap.Stringer("blkID", blkID),
			zap.Uint64("blkHeight", b.blk.Height()),
		)
		if err := b.blk.Accept(); err != nil {
			b.log.Debug("failed to accept block during bootstrapping",
				zap.Stringer("blkID", blkID),
				zap.Error(err),
			)
			return fmt.Errorf("failed to accept block in bootstrapping: %w", err)
		}
	}
	return nil
}
func (b *blockJob) Bytes() []byte { return b.blk.Bytes() }

```

avalanchego/snow/engine/snowman/bootstrap/bootstrapper.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package bootstrap

import (
	"errors"
	"fmt"
	"math"
	"time"

	"go.uber.org/zap"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow"
	"github.com/ava-labs/avalanchego/snow/choices"
	"github.com/ava-labs/avalanchego/snow/consensus/snowman"
	"github.com/ava-labs/avalanchego/snow/engine/common"
	"github.com/ava-labs/avalanchego/snow/engine/snowman/block"
	"github.com/ava-labs/avalanchego/utils/timer"
	"github.com/ava-labs/avalanchego/version"
)

// Parameters for delaying bootstrapping to avoid potential CPU burns
const bootstrappingDelay = 10 * time.Second

var (
	_ common.BootstrapableEngine = &bootstrapper{}

	errUnexpectedTimeout = errors.New("unexpected timeout fired")
)

type bootstrapper struct {
	Config

	// list of NoOpsHandler for messages dropped by bootstrapper
	common.StateSummaryFrontierHandler
	common.AcceptedStateSummaryHandler
	common.PutHandler
	common.QueryHandler
	common.ChitsHandler
	common.AppHandler

	common.Bootstrapper
	common.Fetcher
	*metrics

	started bool

	// Greatest height of the blocks passed in ForceAccepted
	tipHeight uint64
	// Height of the last accepted block when bootstrapping starts
	startingHeight uint64
	// Number of blocks that were fetched on ForceAccepted
	initiallyFetched uint64
	// Time that ForceAccepted was last called
	startTime time.Time

	// number of state transitions executed
	executedStateTransitions int

	parser *parser

	awaitingTimeout bool

	// fetchFrom is the set of nodes that we can fetch the next container from.
	// When a container is fetched, the nodeID is removed from [fetchFrom] to
	// attempt to limit a single request to a peer at any given time. When the
	// response is received, either and Ancestors or an AncestorsFailed, the
	// nodeID will be added back to [fetchFrom] unless the Ancestors message is
	// empty. This is to attempt to prevent requesting containers from that peer
	// again.
	fetchFrom ids.NodeIDSet
}

func New(config Config, onFinished func(lastReqID uint32) error) (common.BootstrapableEngine, error) {
	metrics, err := newMetrics("bs", config.Ctx.Registerer)
	if err != nil {
		return nil, err
	}

	b := &bootstrapper{
		Config:                      config,
		metrics:                     metrics,
		StateSummaryFrontierHandler: common.NewNoOpStateSummaryFrontierHandler(config.Ctx.Log),
		AcceptedStateSummaryHandler: common.NewNoOpAcceptedStateSummaryHandler(config.Ctx.Log),
		PutHandler:                  common.NewNoOpPutHandler(config.Ctx.Log),
		QueryHandler:                common.NewNoOpQueryHandler(config.Ctx.Log),
		ChitsHandler:                common.NewNoOpChitsHandler(config.Ctx.Log),
		AppHandler:                  common.NewNoOpAppHandler(config.Ctx.Log),

		Fetcher: common.Fetcher{
			OnFinished: onFinished,
		},
		executedStateTransitions: math.MaxInt32,
	}

	b.parser = &parser{
		log:         config.Ctx.Log,
		numAccepted: b.numAccepted,
		numDropped:  b.numDropped,
		vm:          b.VM,
	}
	if err := b.Blocked.SetParser(b.parser); err != nil {
		return nil, err
	}

	config.Bootstrapable = b
	b.Bootstrapper = common.NewCommonBootstrapper(config.Config)

	return b, nil
}

func (b *bootstrapper) Start(startReqID uint32) error {
	b.Ctx.Log.Info("starting bootstrapper")

	b.Ctx.SetState(snow.Bootstrapping)
	if err := b.VM.SetState(snow.Bootstrapping); err != nil {
		return fmt.Errorf("failed to notify VM that bootstrapping has started: %w",
			err)
	}

	// Set the starting height
	lastAcceptedID, err := b.VM.LastAccepted()
	if err != nil {
		return fmt.Errorf("couldn't get last accepted ID: %w", err)
	}
	lastAccepted, err := b.VM.GetBlock(lastAcceptedID)
	if err != nil {
		return fmt.Errorf("couldn't get last accepted block: %w", err)
	}
	b.startingHeight = lastAccepted.Height()
	b.Config.SharedCfg.RequestID = startReqID

	if !b.StartupTracker.ShouldStart() {
		return nil
	}

	b.started = true
	return b.Startup()
}

// Ancestors handles the receipt of multiple containers. Should be received in
// response to a GetAncestors message to [nodeID] with request ID [requestID]
func (b *bootstrapper) Ancestors(nodeID ids.NodeID, requestID uint32, blks [][]byte) error {
	// Make sure this is in response to a request we made
	wantedBlkID, ok := b.OutstandingRequests.Remove(nodeID, requestID)
	if !ok { // this message isn't in response to a request we made
		b.Ctx.Log.Debug("received unexpected Ancestors",
			zap.Stringer("nodeID", nodeID),
			zap.Uint32("requestID", requestID),
		)
		return nil
	}

	lenBlks := len(blks)
	if lenBlks == 0 {
		b.Ctx.Log.Debug("received Ancestors with no block",
			zap.Stringer("nodeID", nodeID),
			zap.Uint32("requestID", requestID),
		)

		b.markUnavailable(nodeID)

		// Send another request for this
		return b.fetch(wantedBlkID)
	}

	// This node has responded - so add it back into the set
	b.fetchFrom.Add(nodeID)

	if lenBlks > b.Config.AncestorsMaxContainersReceived {
		blks = blks[:b.Config.AncestorsMaxContainersReceived]
		b.Ctx.Log.Debug("ignoring containers in Ancestors",
			zap.Int("numContainers", lenBlks-b.Config.AncestorsMaxContainersReceived),
			zap.Stringer("nodeID", nodeID),
			zap.Uint32("requestID", requestID),
		)
	}

	blocks, err := block.BatchedParseBlock(b.VM, blks)
	if err != nil { // the provided blocks couldn't be parsed
		b.Ctx.Log.Debug("failed to parse blocks in Ancestors",
			zap.Stringer("nodeID", nodeID),
			zap.Uint32("requestID", requestID),
			zap.Error(err),
		)
		return b.fetch(wantedBlkID)
	}

	if len(blocks) == 0 {
		b.Ctx.Log.Debug("parsing blocks returned an empty set of blocks",
			zap.Stringer("nodeID", nodeID),
			zap.Uint32("requestID", requestID),
		)
		return b.fetch(wantedBlkID)
	}

	requestedBlock := blocks[0]
	if actualID := requestedBlock.ID(); actualID != wantedBlkID {
		b.Ctx.Log.Debug("first block is not the requested block",
			zap.Stringer("expectedBlkID", wantedBlkID),
			zap.Stringer("blkID", actualID),
		)
		return b.fetch(wantedBlkID)
	}

	blockSet := make(map[ids.ID]snowman.Block, len(blocks))
	for _, block := range blocks[1:] {
		blockSet[block.ID()] = block
	}
	return b.process(requestedBlock, blockSet)
}

func (b *bootstrapper) GetAncestorsFailed(nodeID ids.NodeID, requestID uint32) error {
	blkID, ok := b.OutstandingRequests.Remove(nodeID, requestID)
	if !ok {
		b.Ctx.Log.Debug("unexpectedly called GetAncestorsFailed",
			zap.Stringer("nodeID", nodeID),
			zap.Uint32("requestID", requestID),
		)
		return nil
	}

	// This node timed out their request, so we can add them back to [fetchFrom]
	b.fetchFrom.Add(nodeID)

	// Send another request for this
	return b.fetch(blkID)
}

func (b *bootstrapper) Connected(nodeID ids.NodeID, nodeVersion *version.Application) error {
	if err := b.VM.Connected(nodeID, nodeVersion); err != nil {
		return err
	}

	if err := b.StartupTracker.Connected(nodeID, nodeVersion); err != nil {
		return err
	}
	// Ensure fetchFrom reflects proper validator list
	if b.Beacons.Contains(nodeID) {
		b.fetchFrom.Add(nodeID)
	}

	if b.started || !b.StartupTracker.ShouldStart() {
		return nil
	}

	b.started = true
	return b.Startup()
}

func (b *bootstrapper) Disconnected(nodeID ids.NodeID) error {
	if err := b.VM.Disconnected(nodeID); err != nil {
		return err
	}

	if err := b.StartupTracker.Disconnected(nodeID); err != nil {
		return err
	}

	b.markUnavailable(nodeID)
	return nil
}

func (b *bootstrapper) Timeout() error {
	if !b.awaitingTimeout {
		return errUnexpectedTimeout
	}
	b.awaitingTimeout = false

	if !b.Config.Subnet.IsBootstrapped() {
		return b.Restart(true)
	}
	b.fetchETA.Set(0)
	return b.OnFinished(b.Config.SharedCfg.RequestID)
}

func (b *bootstrapper) Gossip() error { return nil }

func (b *bootstrapper) Shutdown() error {
	b.Ctx.Log.Info("shutting down bootstrapper")
	return b.VM.Shutdown()
}

func (b *bootstrapper) Notify(common.Message) error { return nil }

func (b *bootstrapper) HealthCheck() (interface{}, error) {
	vmIntf, vmErr := b.VM.HealthCheck()
	intf := map[string]interface{}{
		"consensus": struct{}{},
		"vm":        vmIntf,
	}
	return intf, vmErr
}

func (b *bootstrapper) GetVM() common.VM { return b.VM }

func (b *bootstrapper) ForceAccepted(acceptedContainerIDs []ids.ID) error {
	pendingContainerIDs := b.Blocked.MissingIDs()

	// Initialize the fetch from set to the currently preferred peers
	b.fetchFrom = b.StartupTracker.PreferredPeers()

	// Append the list of accepted container IDs to pendingContainerIDs to ensure
	// we iterate over every container that must be traversed.
	pendingContainerIDs = append(pendingContainerIDs, acceptedContainerIDs...)
	toProcess := make([]snowman.Block, 0, len(pendingContainerIDs))
	b.Ctx.Log.Debug("starting bootstrapping",
		zap.Int("numPendingBlocks", len(pendingContainerIDs)),
		zap.Int("numAcceptedBlocks", len(acceptedContainerIDs)),
	)
	for _, blkID := range pendingContainerIDs {
		b.Blocked.AddMissingID(blkID)

		// TODO: if `GetBlock` returns an error other than
		// `database.ErrNotFound`, then the error should be propagated.
		blk, err := b.VM.GetBlock(blkID)
		if err != nil {
			if err := b.fetch(blkID); err != nil {
				return err
			}
			continue
		}
		toProcess = append(toProcess, blk)
	}

	b.initiallyFetched = b.Blocked.PendingJobs()
	b.startTime = time.Now()

	// Process received blocks
	for _, blk := range toProcess {
		if err := b.process(blk, nil); err != nil {
			return err
		}
	}

	return b.checkFinish()
}

// Get block [blkID] and its ancestors from a validator
func (b *bootstrapper) fetch(blkID ids.ID) error {
	// Make sure we haven't already requested this block
	if b.OutstandingRequests.Contains(blkID) {
		return nil
	}

	// Make sure we don't already have this block
	if _, err := b.VM.GetBlock(blkID); err == nil {
		return b.checkFinish()
	}

	validatorID, ok := b.fetchFrom.Peek()
	if !ok {
		return fmt.Errorf("dropping request for %s as there are no validators", blkID)
	}

	// We only allow one outbound request at a time from a node
	b.markUnavailable(validatorID)

	b.Config.SharedCfg.RequestID++

	b.OutstandingRequests.Add(validatorID, b.Config.SharedCfg.RequestID, blkID)
	b.Config.Sender.SendGetAncestors(validatorID, b.Config.SharedCfg.RequestID, blkID) // request block and ancestors
	return nil
}

// markUnavailable removes [nodeID] from the set of peers used to fetch
// ancestors. If the set becomes empty, it is reset to the currently preferred
// peers so bootstrapping can continue.
func (b *bootstrapper) markUnavailable(nodeID ids.NodeID) {
	b.fetchFrom.Remove(nodeID)

	// if [fetchFrom] has become empty, reset it to the currently preferred
	// peers
	if b.fetchFrom.Len() == 0 {
		b.fetchFrom = b.StartupTracker.PreferredPeers()
	}
}

func (b *bootstrapper) Clear() error {
	if err := b.Config.Blocked.Clear(); err != nil {
		return err
	}
	return b.Config.Blocked.Commit()
}

// process a series of consecutive blocks starting at [blk].
//
//   - blk is a block that is assumed to have been marked as acceptable by the
//     bootstrapping engine.
//   - processingBlocks is a set of blocks that can be used to lookup blocks.
//     This enables the engine to process multiple blocks without relying on the
//     VM to have stored blocks during `ParseBlock`.
//
// If [blk]'s height is <= the last accepted height, then it will be removed
// from the missingIDs set.
func (b *bootstrapper) process(blk snowman.Block, processingBlocks map[ids.ID]snowman.Block) error {
	for {
		blkID := blk.ID()
		if b.Halted() {
			// We must add in [blkID] to the set of missing IDs so that we are
			// guaranteed to continue processing from this state when the
			// bootstrapper is restarted.
			b.Blocked.AddMissingID(blkID)
			return b.Blocked.Commit()
		}

		b.Blocked.RemoveMissingID(blkID)

		status := blk.Status()
		// The status should never be rejected here - but we check to fail as
		// quickly as possible
		if status == choices.Rejected {
			return fmt.Errorf("bootstrapping wants to accept %s, however it was previously rejected", blkID)
		}

		blkHeight := blk.Height()
		if status == choices.Accepted || blkHeight <= b.startingHeight {
			// We can stop traversing, as we have reached the accepted frontier
			if err := b.Blocked.Commit(); err != nil {
				return err
			}
			return b.checkFinish()
		}

		// If this block is going to be accepted, make sure to update the
		// tipHeight for logging
		if blkHeight > b.tipHeight {
			b.tipHeight = blkHeight
		}

		pushed, err := b.Blocked.Push(&blockJob{
			parser:      b.parser,
			log:         b.Ctx.Log,
			numAccepted: b.numAccepted,
			numDropped:  b.numDropped,
			blk:         blk,
			vm:          b.VM,
		})
		if err != nil {
			return err
		}

		if !pushed {
			// We can stop traversing, as we have reached a block that we
			// previously pushed onto the jobs queue
			if err := b.Blocked.Commit(); err != nil {
				return err
			}
			return b.checkFinish()
		}

		// We added a new block to the queue, so track that it was fetched
		b.numFetched.Inc()

		// Periodically log progress
		blocksFetchedSoFar := b.Blocked.Jobs.PendingJobs()
		if blocksFetchedSoFar%common.StatusUpdateFrequency == 0 {
			totalBlocksToFetch := b.tipHeight - b.startingHeight
			eta := timer.EstimateETA(
				b.startTime,
				blocksFetchedSoFar-b.initiallyFetched, // Number of blocks we have fetched during this run
				totalBlocksToFetch-b.initiallyFetched, // Number of blocks we expect to fetch during this run
			)
			b.fetchETA.Set(float64(eta))

			if !b.Config.SharedCfg.Restarted {
				b.Ctx.Log.Info("fetching blocks",
					zap.Uint64("numFetchedBlocks", blocksFetchedSoFar),
					zap.Uint64("numTotalBlocks", totalBlocksToFetch),
					zap.Duration("eta", eta),
				)
			} else {
				b.Ctx.Log.Debug("fetching blocks",
					zap.Uint64("numFetchedBlocks", blocksFetchedSoFar),
					zap.Uint64("numTotalBlocks", totalBlocksToFetch),
					zap.Duration("eta", eta),
				)
			}
		}

		// Attempt to traverse to the next block
		parentID := blk.Parent()

		// First check if the parent is in the processing blocks set
		parent, ok := processingBlocks[parentID]
		if ok {
			blk = parent
			continue
		}

		// If the parent is not available in processing blocks, attempt to get
		// the block from the vm
		parent, err = b.VM.GetBlock(parentID)
		if err == nil {
			blk = parent
			continue
		}
		// TODO: report errors that aren't `database.ErrNotFound`

		// If the block wasn't able to be acquired immediately, attempt to fetch
		// it
		b.Blocked.AddMissingID(parentID)
		if err := b.fetch(parentID); err != nil {
			return err
		}

		if err := b.Blocked.Commit(); err != nil {
			return err
		}
		return b.checkFinish()
	}
}

// checkFinish repeatedly executes pending transactions and requests new frontier vertices until there aren't any new ones
// after which it finishes the bootstrap process
func (b *bootstrapper) checkFinish() error {
	if numPending := b.Blocked.NumMissingIDs(); numPending != 0 {
		return nil
	}

	if b.IsBootstrapped() || b.awaitingTimeout {
		return nil
	}

	if !b.Config.SharedCfg.Restarted {
		b.Ctx.Log.Info("executing blocks",
			zap.Uint64("numPendingJobs", b.Blocked.PendingJobs()),
		)
	} else {
		b.Ctx.Log.Debug("executing blocks",
			zap.Uint64("numPendingJobs", b.Blocked.PendingJobs()),
		)
	}

	executedBlocks, err := b.Blocked.ExecuteAll(
		b.Config.Ctx,
		b,
		b.Config.SharedCfg.Restarted,
		b.Ctx.ConsensusAcceptor,
		b.Ctx.DecisionAcceptor,
	)
	if err != nil || b.Halted() {
		return err
	}

	previouslyExecuted := b.executedStateTransitions
	b.executedStateTransitions = executedBlocks

	// Note that executedBlocks < c*previouslyExecuted ( 0 <= c < 1 ) is enforced
	// so that the bootstrapping process will terminate even as new blocks are
	// being issued.
	if b.Config.RetryBootstrap && executedBlocks > 0 && executedBlocks < previouslyExecuted/2 {
		return b.Restart(true)
	}

	// If there is an additional callback, notify them that this chain has been
	// synced.
	if b.Bootstrapped != nil {
		b.Bootstrapped()
	}

	// Notify the subnet that this chain is synced
	b.Config.Subnet.Bootstrapped(b.Ctx.ChainID)

	// If the subnet hasn't finished bootstrapping, this chain should remain
	// syncing.
	if !b.Config.Subnet.IsBootstrapped() {
		if !b.Config.SharedCfg.Restarted {
			b.Ctx.Log.Info("waiting for the remaining chains in this subnet to finish syncing")
		} else {
			b.Ctx.Log.Debug("waiting for the remaining chains in this subnet to finish syncing")
		}
		// Restart bootstrapping after [bootstrappingDelay] to keep up to date
		// on the latest tip.
		b.Config.Timer.RegisterTimeout(bootstrappingDelay)
		b.awaitingTimeout = true
		return nil
	}
	b.fetchETA.Set(0)
	return b.OnFinished(b.Config.SharedCfg.RequestID)
}

```

avalanchego/snow/engine/snowman/bootstrap/bootstrapper_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package bootstrap

import (
	"bytes"
	"errors"
	"testing"

	"github.com/prometheus/client_golang/prometheus"

	"github.com/stretchr/testify/require"

	"github.com/ava-labs/avalanchego/database"
	"github.com/ava-labs/avalanchego/database/memdb"
	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow"
	"github.com/ava-labs/avalanchego/snow/choices"
	"github.com/ava-labs/avalanchego/snow/consensus/snowman"
	"github.com/ava-labs/avalanchego/snow/engine/common"
	"github.com/ava-labs/avalanchego/snow/engine/common/queue"
	"github.com/ava-labs/avalanchego/snow/engine/common/tracker"
	"github.com/ava-labs/avalanchego/snow/engine/snowman/block"
	"github.com/ava-labs/avalanchego/snow/engine/snowman/getter"
	"github.com/ava-labs/avalanchego/snow/validators"
	"github.com/ava-labs/avalanchego/utils"
	"github.com/ava-labs/avalanchego/version"
)

var errUnknownBlock = errors.New("unknown block")

func newConfig(t *testing.T) (Config, ids.NodeID, *common.SenderTest, *block.TestVM) {
	ctx := snow.DefaultConsensusContextTest()

	peers := validators.NewSet()

	sender := &common.SenderTest{}
	vm := &block.TestVM{}

	sender.T = t
	vm.T = t

	sender.Default(true)
	vm.Default(true)

	isBootstrapped := false
	subnet := &common.SubnetTest{
		T:               t,
		IsBootstrappedF: func() bool { return isBootstrapped },
		BootstrappedF:   func(ids.ID) { isBootstrapped = true },
	}

	sender.CantSendGetAcceptedFrontier = false

	peer := ids.GenerateTestNodeID()
	if err := peers.AddWeight(peer, 1); err != nil {
		t.Fatal(err)
	}

	peerTracker := tracker.NewPeers()
	startupTracker := tracker.NewStartup(peerTracker, peers.Weight()/2+1)
	peers.RegisterCallbackListener(startupTracker)

	if err := startupTracker.Connected(peer, version.CurrentApp); err != nil {
		t.Fatal(err)
	}

	commonConfig := common.Config{
		Ctx:                            ctx,
		Validators:                     peers,
		Beacons:                        peers,
		SampleK:                        peers.Len(),
		Alpha:                          peers.Weight()/2 + 1,
		StartupTracker:                 startupTracker,
		Sender:                         sender,
		Subnet:                         subnet,
		Timer:                          &common.TimerTest{},
		AncestorsMaxContainersSent:     2000,
		AncestorsMaxContainersReceived: 2000,
		SharedCfg:                      &common.SharedConfig{},
	}

	snowGetHandler, err := getter.New(vm, commonConfig)
	if err != nil {
		t.Fatal(err)
	}

	blocker, _ := queue.NewWithMissing(memdb.New(), "", prometheus.NewRegistry())
	return Config{
		Config:        commonConfig,
		AllGetsServer: snowGetHandler,
		Blocked:       blocker,
		VM:            vm,
	}, peer, sender, vm
}

func TestBootstrapperStartsOnlyIfEnoughStakeIsConnected(t *testing.T) {
	require := require.New(t)

	sender := &common.SenderTest{T: t}
	vm := &block.TestVM{
		TestVM: common.TestVM{T: t},
	}

	sender.Default(true)
	vm.Default(true)

	// create boostrapper configuration
	peers := validators.NewSet()
	sampleK := 2
	alpha := uint64(10)
	startupAlpha := alpha

	peerTracker := tracker.NewPeers()
	startupTracker := tracker.NewStartup(peerTracker, startupAlpha)
	peers.RegisterCallbackListener(startupTracker)

	commonCfg := common.Config{
		Ctx:                            snow.DefaultConsensusContextTest(),
		Validators:                     peers,
		Beacons:                        peers,
		SampleK:                        sampleK,
		Alpha:                          alpha,
		StartupTracker:                 startupTracker,
		Sender:                         sender,
		Subnet:                         &common.SubnetTest{},
		Timer:                          &common.TimerTest{},
		AncestorsMaxContainersSent:     2000,
		AncestorsMaxContainersReceived: 2000,
		SharedCfg:                      &common.SharedConfig{},
	}

	blocker, _ := queue.NewWithMissing(memdb.New(), "", prometheus.NewRegistry())
	snowGetHandler, err := getter.New(vm, commonCfg)
	require.NoError(err)
	cfg := Config{
		Config:        commonCfg,
		AllGetsServer: snowGetHandler,
		Blocked:       blocker,
		VM:            vm,
	}

	blkID0 := ids.Empty.Prefix(0)
	blkBytes0 := []byte{0}
	blk0 := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     blkID0,
			StatusV: choices.Accepted,
		},
		HeightV: 0,
		BytesV:  blkBytes0,
	}
	vm.CantLastAccepted = false
	vm.LastAcceptedF = func() (ids.ID, error) { return blk0.ID(), nil }
	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		require.Equal(blk0.ID(), blkID)
		return blk0, nil
	}

	// create bootstrapper
	dummyCallback := func(lastReqID uint32) error { cfg.Ctx.SetState(snow.NormalOp); return nil }
	bs, err := New(cfg, dummyCallback)
	require.NoError(err)

	vm.CantSetState = false
	vm.CantConnected = true
	vm.ConnectedF = func(ids.NodeID, *version.Application) error { return nil }

	frontierRequested := false
	sender.CantSendGetAcceptedFrontier = false
	sender.SendGetAcceptedFrontierF = func(ss ids.NodeIDSet, u uint32) {
		frontierRequested = true
	}

	// attempt starting bootstrapper with no stake connected. Bootstrapper should stall.
	require.NoError(bs.Start(0))
	require.False(frontierRequested)

	// attempt starting bootstrapper with not enough stake connected. Bootstrapper should stall.
	vdr0 := ids.GenerateTestNodeID()
	require.NoError(peers.AddWeight(vdr0, startupAlpha/2))
	require.NoError(bs.Connected(vdr0, version.CurrentApp))

	require.NoError(bs.Start(0))
	require.False(frontierRequested)

	// finally attempt starting bootstrapper with enough stake connected. Frontiers should be requested.
	vdr := ids.GenerateTestNodeID()
	require.NoError(peers.AddWeight(vdr, startupAlpha))
	require.NoError(bs.Connected(vdr, version.CurrentApp))
	require.True(frontierRequested)
}

// Single node in the accepted frontier; no need to fetch parent
func TestBootstrapperSingleFrontier(t *testing.T) {
	config, _, _, vm := newConfig(t)

	blkID0 := ids.Empty.Prefix(0)
	blkID1 := ids.Empty.Prefix(1)

	blkBytes0 := []byte{0}
	blkBytes1 := []byte{1}

	blk0 := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     blkID0,
			StatusV: choices.Accepted,
		},
		HeightV: 0,
		BytesV:  blkBytes0,
	}
	blk1 := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     blkID1,
			StatusV: choices.Processing,
		},
		ParentV: blk0.IDV,
		HeightV: 1,
		BytesV:  blkBytes1,
	}

	vm.CantLastAccepted = false
	vm.LastAcceptedF = func() (ids.ID, error) { return blk0.ID(), nil }
	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		require.Equal(t, blk0.ID(), blkID)
		return blk0, nil
	}

	bs, err := New(
		config,
		func(lastReqID uint32) error { config.Ctx.SetState(snow.NormalOp); return nil },
	)
	if err != nil {
		t.Fatal(err)
	}

	vm.CantSetState = false
	if err := bs.Start(0); err != nil {
		t.Fatal(err)
	}

	acceptedIDs := []ids.ID{blkID1}

	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		switch blkID {
		case blkID1:
			return blk1, nil
		case blkID0:
			return blk0, nil
		default:
			t.Fatal(database.ErrNotFound)
			panic(database.ErrNotFound)
		}
	}
	vm.ParseBlockF = func(blkBytes []byte) (snowman.Block, error) {
		switch {
		case bytes.Equal(blkBytes, blkBytes1):
			return blk1, nil
		case bytes.Equal(blkBytes, blkBytes0):
			return blk0, nil
		}
		t.Fatal(errUnknownBlock)
		return nil, errUnknownBlock
	}

	err = bs.ForceAccepted(acceptedIDs)
	switch {
	case err != nil: // should finish
		t.Fatal(err)
	case config.Ctx.GetState() != snow.NormalOp:
		t.Fatalf("Bootstrapping should have finished")
	case blk1.Status() != choices.Accepted:
		t.Fatalf("Block should be accepted")
	}
}

// Requests the unknown block and gets back a Ancestors with unexpected request ID.
// Requests again and gets response from unexpected peer.
// Requests again and gets an unexpected block.
// Requests again and gets the expected block.
func TestBootstrapperUnknownByzantineResponse(t *testing.T) {
	config, peerID, sender, vm := newConfig(t)

	blkID0 := ids.Empty.Prefix(0)
	blkID1 := ids.Empty.Prefix(1)
	blkID2 := ids.Empty.Prefix(2)

	blkBytes0 := []byte{0}
	blkBytes1 := []byte{1}
	blkBytes2 := []byte{2}

	blk0 := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     blkID0,
			StatusV: choices.Accepted,
		},
		HeightV: 0,
		BytesV:  blkBytes0,
	}
	blk1 := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     blkID1,
			StatusV: choices.Unknown,
		},
		ParentV: blk0.IDV,
		HeightV: 1,
		BytesV:  blkBytes1,
	}
	blk2 := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     blkID2,
			StatusV: choices.Processing,
		},
		ParentV: blk1.IDV,
		HeightV: 2,
		BytesV:  blkBytes2,
	}

	vm.CantSetState = false
	vm.CantLastAccepted = false
	vm.LastAcceptedF = func() (ids.ID, error) { return blk0.ID(), nil }
	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		require.Equal(t, blk0.ID(), blkID)
		return blk0, nil
	}

	bs, err := New(
		config,
		func(lastReqID uint32) error { config.Ctx.SetState(snow.NormalOp); return nil },
	)
	if err != nil {
		t.Fatal(err)
	}

	if err := bs.Start(0); err != nil {
		t.Fatal(err)
	}

	acceptedIDs := []ids.ID{blkID2}

	parsedBlk1 := false
	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		switch blkID {
		case blkID0:
			return blk0, nil
		case blkID1:
			if parsedBlk1 {
				return blk1, nil
			}
			return nil, database.ErrNotFound
		case blkID2:
			return blk2, nil
		default:
			t.Fatal(database.ErrNotFound)
			panic(database.ErrNotFound)
		}
	}
	vm.ParseBlockF = func(blkBytes []byte) (snowman.Block, error) {
		switch {
		case bytes.Equal(blkBytes, blkBytes0):
			return blk0, nil
		case bytes.Equal(blkBytes, blkBytes1):
			blk1.StatusV = choices.Processing
			parsedBlk1 = true
			return blk1, nil
		case bytes.Equal(blkBytes, blkBytes2):
			return blk2, nil
		}
		t.Fatal(errUnknownBlock)
		return nil, errUnknownBlock
	}

	requestID := new(uint32)
	sender.SendGetAncestorsF = func(vdr ids.NodeID, reqID uint32, vtxID ids.ID) {
		if vdr != peerID {
			t.Fatalf("Should have requested block from %s, requested from %s", peerID, vdr)
		}
		switch {
		case vtxID == blkID1:
		default:
			t.Fatalf("should have requested blk1")
		}
		*requestID = reqID
	}

	vm.CantSetState = false
	if err := bs.ForceAccepted(acceptedIDs); err != nil { // should request blk1
		t.Fatal(err)
	}

	oldReqID := *requestID
	if err := bs.Ancestors(peerID, *requestID+1, [][]byte{blkBytes1}); err != nil { // respond with wrong request ID
		t.Fatal(err)
	} else if oldReqID != *requestID {
		t.Fatal("should not have sent new request")
	}

	if err := bs.Ancestors(ids.NodeID{1, 2, 3}, *requestID, [][]byte{blkBytes1}); err != nil { // respond from wrong peer
		t.Fatal(err)
	} else if oldReqID != *requestID {
		t.Fatal("should not have sent new request")
	}

	if err := bs.Ancestors(peerID, *requestID, [][]byte{blkBytes0}); err != nil { // respond with wrong block
		t.Fatal(err)
	} else if oldReqID == *requestID {
		t.Fatal("should have sent new request")
	}

	err = bs.Ancestors(peerID, *requestID, [][]byte{blkBytes1})
	switch {
	case err != nil: // respond with right block
		t.Fatal(err)
	case config.Ctx.GetState() != snow.NormalOp:
		t.Fatalf("Bootstrapping should have finished")
	case blk0.Status() != choices.Accepted:
		t.Fatalf("Block should be accepted")
	case blk1.Status() != choices.Accepted:
		t.Fatalf("Block should be accepted")
	case blk2.Status() != choices.Accepted:
		t.Fatalf("Block should be accepted")
	}
}

// There are multiple needed blocks and Ancestors returns one at a time
func TestBootstrapperPartialFetch(t *testing.T) {
	config, peerID, sender, vm := newConfig(t)

	blkID0 := ids.Empty.Prefix(0)
	blkID1 := ids.Empty.Prefix(1)
	blkID2 := ids.Empty.Prefix(2)
	blkID3 := ids.Empty.Prefix(3)

	blkBytes0 := []byte{0}
	blkBytes1 := []byte{1}
	blkBytes2 := []byte{2}
	blkBytes3 := []byte{3}

	blk0 := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     blkID0,
			StatusV: choices.Accepted,
		},
		HeightV: 0,
		BytesV:  blkBytes0,
	}
	blk1 := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     blkID1,
			StatusV: choices.Unknown,
		},
		ParentV: blk0.IDV,
		HeightV: 1,
		BytesV:  blkBytes1,
	}
	blk2 := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     blkID2,
			StatusV: choices.Unknown,
		},
		ParentV: blk1.IDV,
		HeightV: 2,
		BytesV:  blkBytes2,
	}
	blk3 := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     blkID3,
			StatusV: choices.Processing,
		},
		ParentV: blk2.IDV,
		HeightV: 3,
		BytesV:  blkBytes3,
	}

	vm.CantLastAccepted = false
	vm.LastAcceptedF = func() (ids.ID, error) { return blk0.ID(), nil }
	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		require.Equal(t, blk0.ID(), blkID)
		return blk0, nil
	}

	bs, err := New(
		config,
		func(lastReqID uint32) error { config.Ctx.SetState(snow.NormalOp); return nil },
	)
	if err != nil {
		t.Fatal(err)
	}

	vm.CantSetState = false
	if err := bs.Start(0); err != nil {
		t.Fatal(err)
	}

	acceptedIDs := []ids.ID{blkID3}

	parsedBlk1 := false
	parsedBlk2 := false
	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		switch blkID {
		case blkID0:
			return blk0, nil
		case blkID1:
			if parsedBlk1 {
				return blk1, nil
			}
			return nil, database.ErrNotFound
		case blkID2:
			if parsedBlk2 {
				return blk2, nil
			}
			return nil, database.ErrNotFound
		case blkID3:
			return blk3, nil
		default:
			t.Fatal(database.ErrNotFound)
			panic(database.ErrNotFound)
		}
	}
	vm.ParseBlockF = func(blkBytes []byte) (snowman.Block, error) {
		switch {
		case bytes.Equal(blkBytes, blkBytes0):
			return blk0, nil
		case bytes.Equal(blkBytes, blkBytes1):
			blk1.StatusV = choices.Processing
			parsedBlk1 = true
			return blk1, nil
		case bytes.Equal(blkBytes, blkBytes2):
			blk2.StatusV = choices.Processing
			parsedBlk2 = true
			return blk2, nil
		case bytes.Equal(blkBytes, blkBytes3):
			return blk3, nil
		}
		t.Fatal(errUnknownBlock)
		return nil, errUnknownBlock
	}

	requestID := new(uint32)
	requested := ids.Empty
	sender.SendGetAncestorsF = func(vdr ids.NodeID, reqID uint32, vtxID ids.ID) {
		if vdr != peerID {
			t.Fatalf("Should have requested block from %s, requested from %s", peerID, vdr)
		}
		switch vtxID {
		case blkID1, blkID2:
		default:
			t.Fatalf("should have requested blk1 or blk2")
		}
		*requestID = reqID
		requested = vtxID
	}

	if err := bs.ForceAccepted(acceptedIDs); err != nil { // should request blk2
		t.Fatal(err)
	}

	if err := bs.Ancestors(peerID, *requestID, [][]byte{blkBytes2}); err != nil { // respond with blk2
		t.Fatal(err)
	} else if requested != blkID1 {
		t.Fatal("should have requested blk1")
	}

	if err := bs.Ancestors(peerID, *requestID, [][]byte{blkBytes1}); err != nil { // respond with blk1
		t.Fatal(err)
	} else if requested != blkID1 {
		t.Fatal("should not have requested another block")
	}

	switch {
	case config.Ctx.GetState() != snow.NormalOp:
		t.Fatalf("Bootstrapping should have finished")
	case blk0.Status() != choices.Accepted:
		t.Fatalf("Block should be accepted")
	case blk1.Status() != choices.Accepted:
		t.Fatalf("Block should be accepted")
	case blk2.Status() != choices.Accepted:
		t.Fatalf("Block should be accepted")
	}
}

// There are multiple needed blocks and some validators do not have all the blocks
// This test was modeled after TestBootstrapperPartialFetch.
func TestBootstrapperEmptyResponse(t *testing.T) {
	config, peerID, sender, vm := newConfig(t)

	blkID0 := ids.Empty.Prefix(0)
	blkID1 := ids.Empty.Prefix(1)
	blkID2 := ids.Empty.Prefix(2)
	blkID3 := ids.Empty.Prefix(3)

	blkBytes0 := []byte{0}
	blkBytes1 := []byte{1}
	blkBytes2 := []byte{2}
	blkBytes3 := []byte{3}

	blk0 := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     blkID0,
			StatusV: choices.Accepted,
		},
		HeightV: 0,
		BytesV:  blkBytes0,
	}
	blk1 := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     blkID1,
			StatusV: choices.Unknown,
		},
		ParentV: blk0.IDV,
		HeightV: 1,
		BytesV:  blkBytes1,
	}
	blk2 := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     blkID2,
			StatusV: choices.Unknown,
		},
		ParentV: blk1.IDV,
		HeightV: 2,
		BytesV:  blkBytes2,
	}
	blk3 := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     blkID3,
			StatusV: choices.Processing,
		},
		ParentV: blk2.IDV,
		HeightV: 3,
		BytesV:  blkBytes3,
	}

	vm.CantLastAccepted = false
	vm.LastAcceptedF = func() (ids.ID, error) { return blk0.ID(), nil }
	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		require.Equal(t, blk0.ID(), blkID)
		return blk0, nil
	}

	bs, err := New(
		config,
		func(lastReqID uint32) error { config.Ctx.SetState(snow.NormalOp); return nil },
	)
	if err != nil {
		t.Fatal(err)
	}

	vm.CantSetState = false
	if err := bs.Start(0); err != nil {
		t.Fatal(err)
	}

	acceptedIDs := []ids.ID{blkID3}

	parsedBlk1 := false
	parsedBlk2 := false
	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		switch blkID {
		case blkID0:
			return blk0, nil
		case blkID1:
			if parsedBlk1 {
				return blk1, nil
			}
			return nil, database.ErrNotFound
		case blkID2:
			if parsedBlk2 {
				return blk2, nil
			}
			return nil, database.ErrNotFound
		case blkID3:
			return blk3, nil
		default:
			t.Fatal(database.ErrNotFound)
			panic(database.ErrNotFound)
		}
	}
	vm.ParseBlockF = func(blkBytes []byte) (snowman.Block, error) {
		switch {
		case bytes.Equal(blkBytes, blkBytes0):
			return blk0, nil
		case bytes.Equal(blkBytes, blkBytes1):
			blk1.StatusV = choices.Processing
			parsedBlk1 = true
			return blk1, nil
		case bytes.Equal(blkBytes, blkBytes2):
			blk2.StatusV = choices.Processing
			parsedBlk2 = true
			return blk2, nil
		case bytes.Equal(blkBytes, blkBytes3):
			return blk3, nil
		}
		t.Fatal(errUnknownBlock)
		return nil, errUnknownBlock
	}

	requestedVdr := ids.EmptyNodeID
	requestID := uint32(0)
	requestedBlock := ids.Empty
	sender.SendGetAncestorsF = func(vdr ids.NodeID, reqID uint32, blkID ids.ID) {
		requestedVdr = vdr
		requestID = reqID
		requestedBlock = blkID
	}

	// should request blk2
	err = bs.ForceAccepted(acceptedIDs)
	switch {
	case err != nil:
		t.Fatal(err)
	case requestedVdr != peerID:
		t.Fatal("should have requested from peerID")
	case requestedBlock != blkID2:
		t.Fatal("should have requested blk2")
	}

	// add another two validators to the fetch set to test behavior on empty response
	newPeerID := ids.GenerateTestNodeID()
	bs.(*bootstrapper).fetchFrom.Add(newPeerID)

	newPeerID = ids.GenerateTestNodeID()
	bs.(*bootstrapper).fetchFrom.Add(newPeerID)

	if err := bs.Ancestors(peerID, requestID, [][]byte{blkBytes2}); err != nil { // respond with blk2
		t.Fatal(err)
	} else if requestedBlock != blkID1 {
		t.Fatal("should have requested blk1")
	}

	peerToBlacklist := requestedVdr

	// respond with empty
	err = bs.Ancestors(peerToBlacklist, requestID, nil)
	switch {
	case err != nil:
		t.Fatal(err)
	case requestedVdr == peerToBlacklist:
		t.Fatal("shouldn't have requested from peerToBlacklist")
	case requestedBlock != blkID1:
		t.Fatal("should have requested blk1")
	}

	if err := bs.Ancestors(requestedVdr, requestID, [][]byte{blkBytes1}); err != nil { // respond with blk1
		t.Fatal(err)
	}

	switch {
	case config.Ctx.GetState() != snow.NormalOp:
		t.Fatalf("Bootstrapping should have finished")
	case blk0.Status() != choices.Accepted:
		t.Fatalf("Block should be accepted")
	case blk1.Status() != choices.Accepted:
		t.Fatalf("Block should be accepted")
	case blk2.Status() != choices.Accepted:
		t.Fatalf("Block should be accepted")
	}

	// check peerToBlacklist was removed from the fetch set
	require.False(t, bs.(*bootstrapper).fetchFrom.Contains(peerToBlacklist))
}

// There are multiple needed blocks and Ancestors returns all at once
func TestBootstrapperAncestors(t *testing.T) {
	config, peerID, sender, vm := newConfig(t)

	blkID0 := ids.Empty.Prefix(0)
	blkID1 := ids.Empty.Prefix(1)
	blkID2 := ids.Empty.Prefix(2)
	blkID3 := ids.Empty.Prefix(3)

	blkBytes0 := []byte{0}
	blkBytes1 := []byte{1}
	blkBytes2 := []byte{2}
	blkBytes3 := []byte{3}

	blk0 := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     blkID0,
			StatusV: choices.Accepted,
		},
		HeightV: 0,
		BytesV:  blkBytes0,
	}
	blk1 := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     blkID1,
			StatusV: choices.Unknown,
		},
		ParentV: blk0.IDV,
		HeightV: 1,
		BytesV:  blkBytes1,
	}
	blk2 := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     blkID2,
			StatusV: choices.Unknown,
		},
		ParentV: blk1.IDV,
		HeightV: 2,
		BytesV:  blkBytes2,
	}
	blk3 := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     blkID3,
			StatusV: choices.Processing,
		},
		ParentV: blk2.IDV,
		HeightV: 3,
		BytesV:  blkBytes3,
	}

	vm.CantSetState = false
	vm.CantLastAccepted = false
	vm.LastAcceptedF = func() (ids.ID, error) { return blk0.ID(), nil }
	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		require.Equal(t, blk0.ID(), blkID)
		return blk0, nil
	}

	bs, err := New(
		config,
		func(lastReqID uint32) error { config.Ctx.SetState(snow.NormalOp); return nil },
	)
	if err != nil {
		t.Fatal(err)
	}

	if err := bs.Start(0); err != nil {
		t.Fatal(err)
	}

	acceptedIDs := []ids.ID{blkID3}

	parsedBlk1 := false
	parsedBlk2 := false
	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		switch blkID {
		case blkID0:
			return blk0, nil
		case blkID1:
			if parsedBlk1 {
				return blk1, nil
			}
			return nil, database.ErrNotFound
		case blkID2:
			if parsedBlk2 {
				return blk2, nil
			}
			return nil, database.ErrNotFound
		case blkID3:
			return blk3, nil
		default:
			t.Fatal(database.ErrNotFound)
			panic(database.ErrNotFound)
		}
	}
	vm.ParseBlockF = func(blkBytes []byte) (snowman.Block, error) {
		switch {
		case bytes.Equal(blkBytes, blkBytes0):
			return blk0, nil
		case bytes.Equal(blkBytes, blkBytes1):
			blk1.StatusV = choices.Processing
			parsedBlk1 = true
			return blk1, nil
		case bytes.Equal(blkBytes, blkBytes2):
			blk2.StatusV = choices.Processing
			parsedBlk2 = true
			return blk2, nil
		case bytes.Equal(blkBytes, blkBytes3):
			return blk3, nil
		}
		t.Fatal(errUnknownBlock)
		return nil, errUnknownBlock
	}

	requestID := new(uint32)
	requested := ids.Empty
	sender.SendGetAncestorsF = func(vdr ids.NodeID, reqID uint32, vtxID ids.ID) {
		if vdr != peerID {
			t.Fatalf("Should have requested block from %s, requested from %s", peerID, vdr)
		}
		switch vtxID {
		case blkID1, blkID2:
		default:
			t.Fatalf("should have requested blk1 or blk2")
		}
		*requestID = reqID
		requested = vtxID
	}

	if err := bs.ForceAccepted(acceptedIDs); err != nil { // should request blk2
		t.Fatal(err)
	}

	if err := bs.Ancestors(peerID, *requestID, [][]byte{blkBytes2, blkBytes1}); err != nil { // respond with blk2 and blk1
		t.Fatal(err)
	} else if requested != blkID2 {
		t.Fatal("should not have requested another block")
	}

	switch {
	case config.Ctx.GetState() != snow.NormalOp:
		t.Fatalf("Bootstrapping should have finished")
	case blk0.Status() != choices.Accepted:
		t.Fatalf("Block should be accepted")
	case blk1.Status() != choices.Accepted:
		t.Fatalf("Block should be accepted")
	case blk2.Status() != choices.Accepted:
		t.Fatalf("Block should be accepted")
	}
}

func TestBootstrapperFinalized(t *testing.T) {
	config, peerID, sender, vm := newConfig(t)

	blkID0 := ids.Empty.Prefix(0)
	blkID1 := ids.Empty.Prefix(1)
	blkID2 := ids.Empty.Prefix(2)

	blkBytes0 := []byte{0}
	blkBytes1 := []byte{1}
	blkBytes2 := []byte{2}

	blk0 := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     blkID0,
			StatusV: choices.Accepted,
		},
		HeightV: 0,
		BytesV:  blkBytes0,
	}
	blk1 := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     blkID1,
			StatusV: choices.Unknown,
		},
		ParentV: blk0.IDV,
		HeightV: 1,
		BytesV:  blkBytes1,
	}
	blk2 := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     blkID2,
			StatusV: choices.Unknown,
		},
		ParentV: blk1.IDV,
		HeightV: 2,
		BytesV:  blkBytes2,
	}

	vm.CantLastAccepted = false
	vm.LastAcceptedF = func() (ids.ID, error) { return blk0.ID(), nil }
	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		require.Equal(t, blk0.ID(), blkID)
		return blk0, nil
	}
	bs, err := New(
		config,
		func(lastReqID uint32) error { config.Ctx.SetState(snow.NormalOp); return nil },
	)
	if err != nil {
		t.Fatal(err)
	}

	vm.CantSetState = false
	if err := bs.Start(0); err != nil {
		t.Fatal(err)
	}

	parsedBlk1 := false
	parsedBlk2 := false
	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		switch blkID {
		case blkID0:
			return blk0, nil
		case blkID1:
			if parsedBlk1 {
				return blk1, nil
			}
			return nil, database.ErrNotFound
		case blkID2:
			if parsedBlk2 {
				return blk2, nil
			}
			return nil, database.ErrNotFound
		default:
			t.Fatal(database.ErrNotFound)
			panic(database.ErrNotFound)
		}
	}
	vm.ParseBlockF = func(blkBytes []byte) (snowman.Block, error) {
		switch {
		case bytes.Equal(blkBytes, blkBytes0):
			return blk0, nil
		case bytes.Equal(blkBytes, blkBytes1):
			blk1.StatusV = choices.Processing
			parsedBlk1 = true
			return blk1, nil
		case bytes.Equal(blkBytes, blkBytes2):
			blk2.StatusV = choices.Processing
			parsedBlk2 = true
			return blk2, nil
		}
		t.Fatal(errUnknownBlock)
		return nil, errUnknownBlock
	}

	requestIDs := map[ids.ID]uint32{}
	sender.SendGetAncestorsF = func(vdr ids.NodeID, reqID uint32, vtxID ids.ID) {
		if vdr != peerID {
			t.Fatalf("Should have requested block from %s, requested from %s", peerID, vdr)
		}
		requestIDs[vtxID] = reqID
	}

	if err := bs.ForceAccepted([]ids.ID{blkID1, blkID2}); err != nil { // should request blk2 and blk1
		t.Fatal(err)
	}

	reqIDBlk2, ok := requestIDs[blkID2]
	if !ok {
		t.Fatalf("should have requested blk2")
	}

	if err := bs.Ancestors(peerID, reqIDBlk2, [][]byte{blkBytes2, blkBytes1}); err != nil {
		t.Fatal(err)
	}

	switch {
	case config.Ctx.GetState() != snow.NormalOp:
		t.Fatalf("Bootstrapping should have finished")
	case blk0.Status() != choices.Accepted:
		t.Fatalf("Block should be accepted")
	case blk1.Status() != choices.Accepted:
		t.Fatalf("Block should be accepted")
	case blk2.Status() != choices.Accepted:
		t.Fatalf("Block should be accepted")
	}
}

func TestRestartBootstrapping(t *testing.T) {
	config, peerID, sender, vm := newConfig(t)

	blkID0 := ids.Empty.Prefix(0)
	blkID1 := ids.Empty.Prefix(1)
	blkID2 := ids.Empty.Prefix(2)
	blkID3 := ids.Empty.Prefix(3)
	blkID4 := ids.Empty.Prefix(4)

	blkBytes0 := []byte{0}
	blkBytes1 := []byte{1}
	blkBytes2 := []byte{2}
	blkBytes3 := []byte{3}
	blkBytes4 := []byte{4}

	blk0 := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     blkID0,
			StatusV: choices.Accepted,
		},
		HeightV: 0,
		BytesV:  blkBytes0,
	}
	blk1 := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     blkID1,
			StatusV: choices.Unknown,
		},
		ParentV: blk0.IDV,
		HeightV: 1,
		BytesV:  blkBytes1,
	}
	blk2 := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     blkID2,
			StatusV: choices.Unknown,
		},
		ParentV: blk1.IDV,
		HeightV: 2,
		BytesV:  blkBytes2,
	}
	blk3 := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     blkID3,
			StatusV: choices.Unknown,
		},
		ParentV: blk2.IDV,
		HeightV: 3,
		BytesV:  blkBytes3,
	}
	blk4 := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     blkID4,
			StatusV: choices.Unknown,
		},
		ParentV: blk3.IDV,
		HeightV: 4,
		BytesV:  blkBytes4,
	}

	vm.CantLastAccepted = false
	vm.LastAcceptedF = func() (ids.ID, error) { return blk0.ID(), nil }
	parsedBlk1 := false
	parsedBlk2 := false
	parsedBlk3 := false
	parsedBlk4 := false
	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		switch blkID {
		case blkID0:
			return blk0, nil
		case blkID1:
			if parsedBlk1 {
				return blk1, nil
			}
			return nil, database.ErrNotFound
		case blkID2:
			if parsedBlk2 {
				return blk2, nil
			}
			return nil, database.ErrNotFound
		case blkID3:
			if parsedBlk3 {
				return blk3, nil
			}
			return nil, database.ErrNotFound
		case blkID4:
			if parsedBlk4 {
				return blk4, nil
			}
			return nil, database.ErrNotFound
		default:
			t.Fatal(database.ErrNotFound)
			panic(database.ErrNotFound)
		}
	}
	vm.ParseBlockF = func(blkBytes []byte) (snowman.Block, error) {
		switch {
		case bytes.Equal(blkBytes, blkBytes0):
			return blk0, nil
		case bytes.Equal(blkBytes, blkBytes1):
			blk1.StatusV = choices.Processing
			parsedBlk1 = true
			return blk1, nil
		case bytes.Equal(blkBytes, blkBytes2):
			blk2.StatusV = choices.Processing
			parsedBlk2 = true
			return blk2, nil
		case bytes.Equal(blkBytes, blkBytes3):
			blk3.StatusV = choices.Processing
			parsedBlk3 = true
			return blk3, nil
		case bytes.Equal(blkBytes, blkBytes4):
			blk4.StatusV = choices.Processing
			parsedBlk4 = true
			return blk4, nil
		}
		t.Fatal(errUnknownBlock)
		return nil, errUnknownBlock
	}

	bsIntf, err := New(
		config,
		func(lastReqID uint32) error { config.Ctx.SetState(snow.NormalOp); return nil },
	)
	if err != nil {
		t.Fatal(err)
	}
	bs, ok := bsIntf.(*bootstrapper)
	if !ok {
		t.Fatal("unexpected bootstrapper type")
	}

	vm.CantSetState = false
	if err := bs.Start(0); err != nil {
		t.Fatal(err)
	}

	requestIDs := map[ids.ID]uint32{}
	sender.SendGetAncestorsF = func(vdr ids.NodeID, reqID uint32, vtxID ids.ID) {
		if vdr != peerID {
			t.Fatalf("Should have requested block from %s, requested from %s", peerID, vdr)
		}
		requestIDs[vtxID] = reqID
	}

	// Force Accept blk3
	if err := bs.ForceAccepted([]ids.ID{blkID3}); err != nil { // should request blk3
		t.Fatal(err)
	}

	reqID, ok := requestIDs[blkID3]
	if !ok {
		t.Fatalf("should have requested blk3")
	}

	if err := bs.Ancestors(peerID, reqID, [][]byte{blkBytes3, blkBytes2}); err != nil {
		t.Fatal(err)
	}

	if _, ok := requestIDs[blkID1]; !ok {
		t.Fatal("should have requested blk1")
	}

	// Remove request, so we can restart bootstrapping via ForceAccepted
	if removed := bs.OutstandingRequests.RemoveAny(blkID1); !removed {
		t.Fatal("Expected to find an outstanding request for blk1")
	}
	requestIDs = map[ids.ID]uint32{}

	if err := bs.ForceAccepted([]ids.ID{blkID4}); err != nil {
		t.Fatal(err)
	}

	blk1RequestID, ok := requestIDs[blkID1]
	if !ok {
		t.Fatal("should have re-requested blk1 on restart")
	}
	blk4RequestID, ok := requestIDs[blkID4]
	if !ok {
		t.Fatal("should have requested blk4 as new accepted frontier")
	}

	if err := bs.Ancestors(peerID, blk1RequestID, [][]byte{blkBytes1}); err != nil {
		t.Fatal(err)
	}

	if config.Ctx.GetState() == snow.NormalOp {
		t.Fatal("Bootstrapping should not have finished with outstanding request for blk4")
	}

	if err := bs.Ancestors(peerID, blk4RequestID, [][]byte{blkBytes4}); err != nil {
		t.Fatal(err)
	}

	switch {
	case config.Ctx.GetState() != snow.NormalOp:
		t.Fatalf("Bootstrapping should have finished")
	case blk0.Status() != choices.Accepted:
		t.Fatalf("Block should be accepted")
	case blk1.Status() != choices.Accepted:
		t.Fatalf("Block should be accepted")
	case blk2.Status() != choices.Accepted:
		t.Fatalf("Block should be accepted")
	case blk3.Status() != choices.Accepted:
		t.Fatalf("Block should be accepted")
	case blk4.Status() != choices.Accepted:
		t.Fatalf("Block should be accepted")
	}
}

func TestBootstrapOldBlockAfterStateSync(t *testing.T) {
	config, peerID, sender, vm := newConfig(t)

	blk0 := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		HeightV: 0,
		BytesV:  utils.RandomBytes(32),
	}
	blk1 := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Accepted,
		},
		ParentV: blk0.IDV,
		HeightV: 1,
		BytesV:  utils.RandomBytes(32),
	}

	vm.LastAcceptedF = func() (ids.ID, error) { return blk1.ID(), nil }
	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		switch blkID {
		case blk0.ID():
			return nil, database.ErrNotFound
		case blk1.ID():
			return blk1, nil
		default:
			t.Fatal(database.ErrNotFound)
			panic(database.ErrNotFound)
		}
	}
	vm.ParseBlockF = func(blkBytes []byte) (snowman.Block, error) {
		switch {
		case bytes.Equal(blkBytes, blk0.Bytes()):
			return blk0, nil
		case bytes.Equal(blkBytes, blk1.Bytes()):
			return blk1, nil
		}
		t.Fatal(errUnknownBlock)
		return nil, errUnknownBlock
	}

	bsIntf, err := New(
		config,
		func(lastReqID uint32) error { config.Ctx.SetState(snow.NormalOp); return nil },
	)
	if err != nil {
		t.Fatal(err)
	}
	bs, ok := bsIntf.(*bootstrapper)
	if !ok {
		t.Fatal("unexpected bootstrapper type")
	}

	vm.CantSetState = false
	if err := bs.Start(0); err != nil {
		t.Fatal(err)
	}

	requestIDs := map[ids.ID]uint32{}
	sender.SendGetAncestorsF = func(vdr ids.NodeID, reqID uint32, vtxID ids.ID) {
		if vdr != peerID {
			t.Fatalf("Should have requested block from %s, requested from %s", peerID, vdr)
		}
		requestIDs[vtxID] = reqID
	}

	// Force Accept, the already transitively accepted, blk0
	if err := bs.ForceAccepted([]ids.ID{blk0.ID()}); err != nil { // should request blk0
		t.Fatal(err)
	}

	reqID, ok := requestIDs[blk0.ID()]
	if !ok {
		t.Fatalf("should have requested blk0")
	}

	if err := bs.Ancestors(peerID, reqID, [][]byte{blk0.Bytes()}); err != nil {
		t.Fatal(err)
	}

	switch {
	case config.Ctx.GetState() != snow.NormalOp:
		t.Fatalf("Bootstrapping should have finished")
	case blk0.Status() != choices.Processing:
		t.Fatalf("Block should be processing")
	case blk1.Status() != choices.Accepted:
		t.Fatalf("Block should be accepted")
	}
}

func TestBootstrapContinueAfterHalt(t *testing.T) {
	config, _, _, vm := newConfig(t)

	blk0 := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Accepted,
		},
		HeightV: 0,
		BytesV:  utils.RandomBytes(32),
	}
	blk1 := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentV: blk0.IDV,
		HeightV: 1,
		BytesV:  utils.RandomBytes(32),
	}
	blk2 := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentV: blk1.IDV,
		HeightV: 2,
		BytesV:  utils.RandomBytes(32),
	}

	vm.LastAcceptedF = func() (ids.ID, error) { return blk0.ID(), nil }

	bsIntf, err := New(
		config,
		func(lastReqID uint32) error { config.Ctx.SetState(snow.NormalOp); return nil },
	)
	if err != nil {
		t.Fatal(err)
	}
	bs, ok := bsIntf.(*bootstrapper)
	if !ok {
		t.Fatal("unexpected bootstrapper type")
	}

	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		switch blkID {
		case blk0.ID():
			return blk0, nil
		case blk1.ID():
			bs.Halt()
			return blk1, nil
		case blk2.ID():
			return blk2, nil
		default:
			t.Fatal(database.ErrNotFound)
			panic(database.ErrNotFound)
		}
	}

	vm.CantSetState = false
	if err := bs.Start(0); err != nil {
		t.Fatal(err)
	}

	if err := bs.ForceAccepted([]ids.ID{blk2.ID()}); err != nil {
		t.Fatal(err)
	}

	if bs.Blocked.NumMissingIDs() != 1 {
		t.Fatal("Should have left blk1 as missing")
	}
}

```

avalanchego/snow/engine/snowman/bootstrap/config.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package bootstrap

import (
	"github.com/ava-labs/avalanchego/snow/engine/common"
	"github.com/ava-labs/avalanchego/snow/engine/common/queue"
	"github.com/ava-labs/avalanchego/snow/engine/snowman/block"
)

type Config struct {
	common.Config
	common.AllGetsServer

	// Blocked tracks operations that are blocked on blocks
	//
	// It should be guaranteed that `MissingIDs` should contain all IDs
	// referenced by the `MissingDependencies` that have not already been added
	// to the queue.
	Blocked *queue.JobsWithMissing

	VM block.ChainVM

	Bootstrapped func()
}

```

avalanchego/snow/engine/snowman/bootstrap/metrics.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package bootstrap

import (
	"github.com/prometheus/client_golang/prometheus"

	"github.com/ava-labs/avalanchego/utils/wrappers"
)

type metrics struct {
	numFetched, numDropped, numAccepted prometheus.Counter
	fetchETA                            prometheus.Gauge
}

func newMetrics(namespace string, registerer prometheus.Registerer) (*metrics, error) {
	m := &metrics{
		numFetched: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: namespace,
			Name:      "fetched",
			Help:      "Number of blocks fetched during bootstrapping",
		}),
		numDropped: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: namespace,
			Name:      "dropped",
			Help:      "Number of blocks dropped during bootstrapping",
		}),
		numAccepted: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: namespace,
			Name:      "accepted",
			Help:      "Number of blocks accepted during bootstrapping",
		}),
		fetchETA: prometheus.NewGauge(prometheus.GaugeOpts{
			Namespace: namespace,
			Name:      "eta_fetching_complete",
			Help:      "ETA in nanoseconds until fetching phase of bootstrapping finishes",
		}),
	}

	errs := wrappers.Errs{}
	errs.Add(
		registerer.Register(m.numFetched),
		registerer.Register(m.numDropped),
		registerer.Register(m.numAccepted),
		registerer.Register(m.fetchETA),
	)
	return m, errs.Err
}

```

avalanchego/snow/engine/snowman/config.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowman

import (
	"github.com/ava-labs/avalanchego/snow"
	"github.com/ava-labs/avalanchego/snow/consensus/snowball"
	"github.com/ava-labs/avalanchego/snow/consensus/snowman"
	"github.com/ava-labs/avalanchego/snow/engine/common"
	"github.com/ava-labs/avalanchego/snow/engine/snowman/block"
	"github.com/ava-labs/avalanchego/snow/validators"
)

// Config wraps all the parameters needed for a snowman engine
type Config struct {
	common.AllGetsServer

	Ctx        *snow.ConsensusContext
	VM         block.ChainVM
	Sender     common.Sender
	Validators validators.Set
	Params     snowball.Parameters
	Consensus  snowman.Consensus
}

```

avalanchego/snow/engine/snowman/config_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowman

import (
	"github.com/ava-labs/avalanchego/snow/consensus/snowball"
	"github.com/ava-labs/avalanchego/snow/consensus/snowman"
	"github.com/ava-labs/avalanchego/snow/engine/common"
	"github.com/ava-labs/avalanchego/snow/engine/snowman/block"
)

func DefaultConfigs() Config {
	commonCfg := common.DefaultConfigTest()
	return Config{
		Ctx:        commonCfg.Ctx,
		Sender:     commonCfg.Sender,
		Validators: commonCfg.Validators,
		VM:         &block.TestVM{},
		Params: snowball.Parameters{
			K:                       1,
			Alpha:                   1,
			BetaVirtuous:            1,
			BetaRogue:               2,
			ConcurrentRepolls:       1,
			OptimalProcessing:       100,
			MaxOutstandingItems:     1,
			MaxItemProcessingTime:   1,
			MixedQueryNumPushNonVdr: 1,
		},
		Consensus: &snowman.Topological{},
	}
}

```

avalanchego/snow/engine/snowman/engine.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowman

import (
	"github.com/ava-labs/avalanchego/snow/engine/common"
	"github.com/ava-labs/avalanchego/snow/engine/snowman/block"
)

// Engine describes the events that can occur to a Snowman instance.
//
// The engine is used to fetch, order, and decide on the fate of blocks. This
// engine runs the leaderless version of the Snowman consensus protocol.
// Therefore, the liveness of this protocol tolerant to O(sqrt(n)) Byzantine
// Nodes where n is the number of nodes in the network. Therefore, this protocol
// should only be run in a Crash Fault Tolerant environment, or in an
// environment where lose of liveness and manual intervention is tolerable.
type Engine interface {
	common.Engine
	block.Getter
}

```

avalanchego/snow/engine/snowman/getter/getter.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package getter

import (
	"go.uber.org/zap"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/choices"
	"github.com/ava-labs/avalanchego/snow/engine/common"
	"github.com/ava-labs/avalanchego/snow/engine/snowman/block"
	"github.com/ava-labs/avalanchego/utils/constants"
	"github.com/ava-labs/avalanchego/utils/logging"
	"github.com/ava-labs/avalanchego/utils/metric"
)

// Get requests are always served, regardless node state (bootstrapping or normal operations).
var _ common.AllGetsServer = &getter{}

func New(
	vm block.ChainVM,
	commonCfg common.Config,
) (common.AllGetsServer, error) {
	ssVM, _ := vm.(block.StateSyncableVM)
	gh := &getter{
		vm:     vm,
		ssVM:   ssVM,
		sender: commonCfg.Sender,
		cfg:    commonCfg,
		log:    commonCfg.Ctx.Log,
	}

	var err error
	gh.getAncestorsBlks, err = metric.NewAverager(
		"bs",
		"get_ancestors_blks",
		"blocks fetched in a call to GetAncestors",
		commonCfg.Ctx.Registerer,
	)
	return gh, err
}

type getter struct {
	vm     block.ChainVM
	ssVM   block.StateSyncableVM // can be nil
	sender common.Sender
	cfg    common.Config

	log              logging.Logger
	getAncestorsBlks metric.Averager
}

func (gh *getter) GetStateSummaryFrontier(nodeID ids.NodeID, requestID uint32) error {
	// Note: we do not check if gh.ssVM.StateSyncEnabled since we want all
	// nodes, including those disabling state sync to serve state summaries if
	// these are available
	if gh.ssVM == nil {
		gh.log.Debug("dropping GetStateSummaryFrontier message",
			zap.String("reason", "state sync not supported"),
			zap.Stringer("nodeID", nodeID),
			zap.Uint32("requestID", requestID),
		)
		return nil
	}

	summary, err := gh.ssVM.GetLastStateSummary()
	if err != nil {
		gh.log.Debug("dropping GetStateSummaryFrontier message",
			zap.String("reason", "couldn't get state summary frontier"),
			zap.Stringer("nodeID", nodeID),
			zap.Uint32("requestID", requestID),
			zap.Error(err),
		)
		return nil
	}

	gh.sender.SendStateSummaryFrontier(nodeID, requestID, summary.Bytes())
	return nil
}

func (gh *getter) GetAcceptedStateSummary(nodeID ids.NodeID, requestID uint32, heights []uint64) error {
	// If there are no requested heights, then we can return the result
	// immediately, regardless of if the underlying VM implements state sync.
	if len(heights) == 0 {
		gh.sender.SendAcceptedStateSummary(nodeID, requestID, nil)
		return nil
	}

	// Note: we do not check if gh.ssVM.StateSyncEnabled since we want all
	// nodes, including those disabling state sync to serve state summaries if
	// these are available
	if gh.ssVM == nil {
		gh.log.Debug("dropping GetAcceptedStateSummary message",
			zap.String("reason", "state sync not supported"),
			zap.Stringer("nodeID", nodeID),
			zap.Uint32("requestID", requestID),
		)
		return nil
	}

	summaryIDs := make([]ids.ID, 0, len(heights))
	for _, height := range heights {
		summary, err := gh.ssVM.GetStateSummary(height)
		if err == block.ErrStateSyncableVMNotImplemented {
			gh.log.Debug("dropping GetAcceptedStateSummary message",
				zap.String("reason", "state sync not supported"),
				zap.Stringer("nodeID", nodeID),
				zap.Uint32("requestID", requestID),
			)
			return nil
		}
		if err != nil {
			gh.log.Debug("couldn't get state summary",
				zap.Uint64("height", height),
				zap.Error(err),
			)
			continue
		}
		summaryIDs = append(summaryIDs, summary.ID())
	}

	gh.sender.SendAcceptedStateSummary(nodeID, requestID, summaryIDs)
	return nil
}

func (gh *getter) GetAcceptedFrontier(nodeID ids.NodeID, requestID uint32) error {
	lastAccepted, err := gh.vm.LastAccepted()
	if err != nil {
		return err
	}
	gh.sender.SendAcceptedFrontier(nodeID, requestID, []ids.ID{lastAccepted})
	return nil
}

func (gh *getter) GetAccepted(nodeID ids.NodeID, requestID uint32, containerIDs []ids.ID) error {
	acceptedIDs := make([]ids.ID, 0, len(containerIDs))
	for _, blkID := range containerIDs {
		if blk, err := gh.vm.GetBlock(blkID); err == nil && blk.Status() == choices.Accepted {
			acceptedIDs = append(acceptedIDs, blkID)
		}
	}
	gh.sender.SendAccepted(nodeID, requestID, acceptedIDs)
	return nil
}

func (gh *getter) GetAncestors(nodeID ids.NodeID, requestID uint32, blkID ids.ID) error {
	ancestorsBytes, err := block.GetAncestors(
		gh.vm,
		blkID,
		gh.cfg.AncestorsMaxContainersSent,
		constants.MaxContainersLen,
		gh.cfg.MaxTimeGetAncestors,
	)
	if err != nil {
		gh.log.Verbo("dropping GetAncestors message",
			zap.String("reason", "couldn't get ancestors"),
			zap.Stringer("nodeID", nodeID),
			zap.Uint32("requestID", requestID),
			zap.Stringer("blkID", blkID),
			zap.Error(err),
		)
		return nil
	}

	gh.getAncestorsBlks.Observe(float64(len(ancestorsBytes)))
	gh.sender.SendAncestors(nodeID, requestID, ancestorsBytes)
	return nil
}

func (gh *getter) Get(nodeID ids.NodeID, requestID uint32, blkID ids.ID) error {
	blk, err := gh.vm.GetBlock(blkID)
	if err != nil {
		// If we failed to get the block, that means either an unexpected error
		// has occurred, [vdr] is not following the protocol, or the
		// block has been pruned.
		gh.log.Debug("failed Get request",
			zap.Stringer("nodeID", nodeID),
			zap.Uint32("requestID", requestID),
			zap.Stringer("blkID", blkID),
			zap.Error(err),
		)
		return nil
	}

	// Respond to the validator with the fetched block and the same requestID.
	gh.sender.SendPut(nodeID, requestID, blk.Bytes())
	return nil
}

```

avalanchego/snow/engine/snowman/getter/getter_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package getter

import (
	"errors"
	"testing"

	"github.com/golang/mock/gomock"

	"github.com/stretchr/testify/require"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow"
	"github.com/ava-labs/avalanchego/snow/choices"
	"github.com/ava-labs/avalanchego/snow/consensus/snowman"
	"github.com/ava-labs/avalanchego/snow/engine/common"
	"github.com/ava-labs/avalanchego/snow/engine/snowman/block"
	"github.com/ava-labs/avalanchego/snow/engine/snowman/block/mocks"
	"github.com/ava-labs/avalanchego/snow/validators"
)

var errUnknownBlock = errors.New("unknown block")

type StateSyncEnabledMock struct {
	*block.TestVM
	*mocks.MockStateSyncableVM
}

func testSetup(
	t *testing.T,
	ctrl *gomock.Controller,
) (StateSyncEnabledMock, *common.SenderTest, common.Config) {
	ctx := snow.DefaultConsensusContextTest()

	peers := validators.NewSet()
	sender := &common.SenderTest{}
	vm := StateSyncEnabledMock{
		TestVM:              &block.TestVM{},
		MockStateSyncableVM: mocks.NewMockStateSyncableVM(ctrl),
	}

	sender.T = t

	sender.Default(true)

	isBootstrapped := false
	subnet := &common.SubnetTest{
		T:               t,
		IsBootstrappedF: func() bool { return isBootstrapped },
		BootstrappedF:   func(ids.ID) { isBootstrapped = true },
	}

	sender.CantSendGetAcceptedFrontier = false

	peer := ids.GenerateTestNodeID()
	if err := peers.AddWeight(peer, 1); err != nil {
		t.Fatal(err)
	}

	commonConfig := common.Config{
		Ctx:                            ctx,
		Validators:                     peers,
		Beacons:                        peers,
		SampleK:                        peers.Len(),
		Alpha:                          peers.Weight()/2 + 1,
		Sender:                         sender,
		Subnet:                         subnet,
		Timer:                          &common.TimerTest{},
		AncestorsMaxContainersSent:     2000,
		AncestorsMaxContainersReceived: 2000,
		SharedCfg:                      &common.SharedConfig{},
	}

	return vm, sender, commonConfig
}

func TestAcceptedFrontier(t *testing.T) {
	ctrl := gomock.NewController(t)
	defer ctrl.Finish()

	vm, sender, config := testSetup(t, ctrl)

	blkID := ids.GenerateTestID()

	dummyBlk := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     blkID,
			StatusV: choices.Accepted,
		},
		HeightV: 0,
		BytesV:  []byte{1, 2, 3},
	}
	vm.CantLastAccepted = false
	vm.LastAcceptedF = func() (ids.ID, error) { return blkID, nil }
	vm.GetBlockF = func(bID ids.ID) (snowman.Block, error) {
		require.Equal(t, blkID, bID)
		return dummyBlk, nil
	}

	bsIntf, err := New(vm, config)
	if err != nil {
		t.Fatal(err)
	}
	bs, ok := bsIntf.(*getter)
	if !ok {
		t.Fatal("Unexpected get handler")
	}

	var accepted []ids.ID
	sender.SendAcceptedFrontierF = func(_ ids.NodeID, _ uint32, frontier []ids.ID) {
		accepted = frontier
	}

	if err := bs.GetAcceptedFrontier(ids.EmptyNodeID, 0); err != nil {
		t.Fatal(err)
	}

	if len(accepted) != 1 {
		t.Fatalf("Only one block should be accepted")
	}
	if accepted[0] != blkID {
		t.Fatalf("Blk should be accepted")
	}
}

func TestFilterAccepted(t *testing.T) {
	ctrl := gomock.NewController(t)
	defer ctrl.Finish()

	vm, sender, config := testSetup(t, ctrl)

	blkID0 := ids.GenerateTestID()
	blkID1 := ids.GenerateTestID()
	blkID2 := ids.GenerateTestID()

	blk0 := &snowman.TestBlock{TestDecidable: choices.TestDecidable{
		IDV:     blkID0,
		StatusV: choices.Accepted,
	}}
	blk1 := &snowman.TestBlock{TestDecidable: choices.TestDecidable{
		IDV:     blkID1,
		StatusV: choices.Accepted,
	}}

	vm.CantLastAccepted = false
	vm.LastAcceptedF = func() (ids.ID, error) { return blk1.ID(), nil }
	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		require.Equal(t, blk1.ID(), blkID)
		return blk1, nil
	}

	bsIntf, err := New(vm, config)
	if err != nil {
		t.Fatal(err)
	}
	bs, ok := bsIntf.(*getter)
	if !ok {
		t.Fatal("Unexpected get handler")
	}

	blkIDs := []ids.ID{blkID0, blkID1, blkID2}
	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		switch blkID {
		case blkID0:
			return blk0, nil
		case blkID1:
			return blk1, nil
		case blkID2:
			return nil, errUnknownBlock
		}
		t.Fatal(errUnknownBlock)
		return nil, errUnknownBlock
	}

	var accepted []ids.ID
	sender.SendAcceptedF = func(_ ids.NodeID, _ uint32, frontier []ids.ID) {
		accepted = frontier
	}

	if err := bs.GetAccepted(ids.EmptyNodeID, 0, blkIDs); err != nil {
		t.Fatal(err)
	}

	acceptedSet := ids.Set{}
	acceptedSet.Add(accepted...)

	if acceptedSet.Len() != 2 {
		t.Fatalf("Two blocks should be accepted")
	}
	if !acceptedSet.Contains(blkID0) {
		t.Fatalf("Blk should be accepted")
	}
	if !acceptedSet.Contains(blkID1) {
		t.Fatalf("Blk should be accepted")
	}
	if acceptedSet.Contains(blkID2) {
		t.Fatalf("Blk shouldn't be accepted")
	}
}

```

avalanchego/snow/engine/snowman/issuer.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowman

import (
	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/consensus/snowman"
)

// issuer issues [blk] into to consensus after its dependencies are met.
type issuer struct {
	t         *Transitive
	blk       snowman.Block
	abandoned bool
	deps      ids.Set
}

func (i *issuer) Dependencies() ids.Set { return i.deps }

// Mark that a dependency has been met
func (i *issuer) Fulfill(id ids.ID) {
	i.deps.Remove(id)
	i.Update()
}

// Abandon the attempt to issue [i.block]
func (i *issuer) Abandon(ids.ID) {
	if !i.abandoned {
		blkID := i.blk.ID()
		i.t.removeFromPending(i.blk)
		i.t.addToNonVerifieds(i.blk)
		i.t.blocked.Abandon(blkID)

		// Tracks performance statistics
		i.t.metrics.numRequests.Set(float64(i.t.blkReqs.Len()))
		i.t.metrics.numBlocked.Set(float64(len(i.t.pending)))
		i.t.metrics.numBlockers.Set(float64(i.t.blocked.Len()))
	}
	i.abandoned = true
}

func (i *issuer) Update() {
	if i.abandoned || i.deps.Len() != 0 || i.t.errs.Errored() {
		return
	}
	// Issue the block into consensus
	i.t.errs.Add(i.t.deliver(i.blk))
}

```

avalanchego/snow/engine/snowman/memory_block.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowman

import (
	"github.com/ava-labs/avalanchego/snow/consensus/snowman"
)

// memoryBlock wraps a snowman Block to manage non-verified blocks
type memoryBlock struct {
	snowman.Block

	tree    AncestorTree
	metrics *metrics
}

// Accept accepts the underlying block & removes sibling subtrees
func (mb *memoryBlock) Accept() error {
	mb.tree.RemoveSubtree(mb.Parent())
	mb.metrics.numNonVerifieds.Set(float64(mb.tree.Len()))
	return mb.Block.Accept()
}

// Reject rejects the underlying block & removes child subtrees
func (mb *memoryBlock) Reject() error {
	mb.tree.RemoveSubtree(mb.ID())
	mb.metrics.numNonVerifieds.Set(float64(mb.tree.Len()))
	return mb.Block.Reject()
}

```

avalanchego/snow/engine/snowman/metrics.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowman

import (
	"github.com/prometheus/client_golang/prometheus"

	"github.com/ava-labs/avalanchego/utils/metric"
	"github.com/ava-labs/avalanchego/utils/wrappers"
)

type metrics struct {
	bootstrapFinished, numRequests, numBlocked, numBlockers, numNonVerifieds prometheus.Gauge
	numBuilt, numBuildsFailed, numUselessPutBytes, numUselessPushQueryBytes  prometheus.Counter
	getAncestorsBlks                                                         metric.Averager
}

// Initialize the metrics
func (m *metrics) Initialize(namespace string, reg prometheus.Registerer) error {
	errs := wrappers.Errs{}
	m.bootstrapFinished = prometheus.NewGauge(prometheus.GaugeOpts{
		Namespace: namespace,
		Name:      "bootstrap_finished",
		Help:      "Whether or not bootstrap process has completed. 1 is success, 0 is fail or ongoing.",
	})
	m.numRequests = prometheus.NewGauge(prometheus.GaugeOpts{
		Namespace: namespace,
		Name:      "requests",
		Help:      "Number of outstanding block requests",
	})
	m.numBlocked = prometheus.NewGauge(prometheus.GaugeOpts{
		Namespace: namespace,
		Name:      "blocked",
		Help:      "Number of blocks that are pending issuance",
	})
	m.numBlockers = prometheus.NewGauge(prometheus.GaugeOpts{
		Namespace: namespace,
		Name:      "blockers",
		Help:      "Number of blocks that are blocking other blocks from being issued because they haven't been issued",
	})
	m.numBuilt = prometheus.NewCounter(prometheus.CounterOpts{
		Namespace: namespace,
		Name:      "blks_built",
		Help:      "Number of blocks that have been built locally",
	})
	m.numBuildsFailed = prometheus.NewCounter(prometheus.CounterOpts{
		Namespace: namespace,
		Name:      "blk_builds_failed",
		Help:      "Number of BuildBlock calls that have failed",
	})
	m.numUselessPutBytes = prometheus.NewCounter(prometheus.CounterOpts{
		Namespace: namespace,
		Name:      "num_useless_put_bytes",
		Help:      "Amount of useless bytes received in Put messages",
	})
	m.numUselessPushQueryBytes = prometheus.NewCounter(prometheus.CounterOpts{
		Namespace: namespace,
		Name:      "num_useless_push_query_bytes",
		Help:      "Amount of useless bytes received in PushQuery messages",
	})
	m.getAncestorsBlks = metric.NewAveragerWithErrs(
		namespace,
		"get_ancestors_blks",
		"blocks fetched in a call to GetAncestors",
		reg,
		&errs,
	)
	m.numNonVerifieds = prometheus.NewGauge(prometheus.GaugeOpts{
		Namespace: namespace,
		Name:      "non_verified_blks",
		Help:      "Number of non-verified blocks in the memory",
	})

	errs.Add(
		reg.Register(m.bootstrapFinished),
		reg.Register(m.numRequests),
		reg.Register(m.numBlocked),
		reg.Register(m.numBlockers),
		reg.Register(m.numNonVerifieds),
		reg.Register(m.numBuilt),
		reg.Register(m.numBuildsFailed),
		reg.Register(m.numUselessPutBytes),
		reg.Register(m.numUselessPushQueryBytes),
	)
	return errs.Err
}

```

avalanchego/snow/engine/snowman/mocks/engine.go:
```
// Code generated by mockery v2.12.1. DO NOT EDIT.

package mocks

import (
	consensussnowman "github.com/ava-labs/avalanchego/snow/consensus/snowman"
	common "github.com/ava-labs/avalanchego/snow/engine/common"

	ids "github.com/ava-labs/avalanchego/ids"

	mock "github.com/stretchr/testify/mock"

	snow "github.com/ava-labs/avalanchego/snow"

	testing "testing"

	time "time"

	version "github.com/ava-labs/avalanchego/version"
)

// Engine is an autogenerated mock type for the Engine type
type Engine struct {
	mock.Mock
}

// Accepted provides a mock function with given fields: validatorID, requestID, containerIDs
func (_m *Engine) Accepted(validatorID ids.NodeID, requestID uint32, containerIDs []ids.ID) error {
	ret := _m.Called(validatorID, requestID, containerIDs)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32, []ids.ID) error); ok {
		r0 = rf(validatorID, requestID, containerIDs)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// AcceptedFrontier provides a mock function with given fields: validatorID, requestID, containerIDs
func (_m *Engine) AcceptedFrontier(validatorID ids.NodeID, requestID uint32, containerIDs []ids.ID) error {
	ret := _m.Called(validatorID, requestID, containerIDs)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32, []ids.ID) error); ok {
		r0 = rf(validatorID, requestID, containerIDs)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// AcceptedStateSummary provides a mock function with given fields: validatorID, requestID, summaryIDs
func (_m *Engine) AcceptedStateSummary(validatorID ids.NodeID, requestID uint32, summaryIDs []ids.ID) error {
	ret := _m.Called(validatorID, requestID, summaryIDs)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32, []ids.ID) error); ok {
		r0 = rf(validatorID, requestID, summaryIDs)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// Ancestors provides a mock function with given fields: validatorID, requestID, containers
func (_m *Engine) Ancestors(validatorID ids.NodeID, requestID uint32, containers [][]byte) error {
	ret := _m.Called(validatorID, requestID, containers)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32, [][]byte) error); ok {
		r0 = rf(validatorID, requestID, containers)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// AppGossip provides a mock function with given fields: nodeID, msg
func (_m *Engine) AppGossip(nodeID ids.NodeID, msg []byte) error {
	ret := _m.Called(nodeID, msg)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, []byte) error); ok {
		r0 = rf(nodeID, msg)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// AppRequest provides a mock function with given fields: nodeID, requestID, deadline, request
func (_m *Engine) AppRequest(nodeID ids.NodeID, requestID uint32, deadline time.Time, request []byte) error {
	ret := _m.Called(nodeID, requestID, deadline, request)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32, time.Time, []byte) error); ok {
		r0 = rf(nodeID, requestID, deadline, request)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// AppRequestFailed provides a mock function with given fields: nodeID, requestID
func (_m *Engine) AppRequestFailed(nodeID ids.NodeID, requestID uint32) error {
	ret := _m.Called(nodeID, requestID)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32) error); ok {
		r0 = rf(nodeID, requestID)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// AppResponse provides a mock function with given fields: nodeID, requestID, response
func (_m *Engine) AppResponse(nodeID ids.NodeID, requestID uint32, response []byte) error {
	ret := _m.Called(nodeID, requestID, response)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32, []byte) error); ok {
		r0 = rf(nodeID, requestID, response)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// Chits provides a mock function with given fields: validatorID, requestID, containerIDs
func (_m *Engine) Chits(validatorID ids.NodeID, requestID uint32, containerIDs []ids.ID) error {
	ret := _m.Called(validatorID, requestID, containerIDs)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32, []ids.ID) error); ok {
		r0 = rf(validatorID, requestID, containerIDs)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// Connected provides a mock function with given fields: id, nodeVersion
func (_m *Engine) Connected(id ids.NodeID, nodeVersion *version.Application) error {
	ret := _m.Called(id, nodeVersion)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, *version.Application) error); ok {
		r0 = rf(id, nodeVersion)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// Context provides a mock function with given fields:
func (_m *Engine) Context() *snow.ConsensusContext {
	ret := _m.Called()

	var r0 *snow.ConsensusContext
	if rf, ok := ret.Get(0).(func() *snow.ConsensusContext); ok {
		r0 = rf()
	} else {
		if ret.Get(0) != nil {
			r0 = ret.Get(0).(*snow.ConsensusContext)
		}
	}

	return r0
}

// Disconnected provides a mock function with given fields: id
func (_m *Engine) Disconnected(id ids.NodeID) error {
	ret := _m.Called(id)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID) error); ok {
		r0 = rf(id)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// Get provides a mock function with given fields: validatorID, requestID, containerID
func (_m *Engine) Get(validatorID ids.NodeID, requestID uint32, containerID ids.ID) error {
	ret := _m.Called(validatorID, requestID, containerID)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32, ids.ID) error); ok {
		r0 = rf(validatorID, requestID, containerID)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// GetAccepted provides a mock function with given fields: validatorID, requestID, containerIDs
func (_m *Engine) GetAccepted(validatorID ids.NodeID, requestID uint32, containerIDs []ids.ID) error {
	ret := _m.Called(validatorID, requestID, containerIDs)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32, []ids.ID) error); ok {
		r0 = rf(validatorID, requestID, containerIDs)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// GetAcceptedFailed provides a mock function with given fields: validatorID, requestID
func (_m *Engine) GetAcceptedFailed(validatorID ids.NodeID, requestID uint32) error {
	ret := _m.Called(validatorID, requestID)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32) error); ok {
		r0 = rf(validatorID, requestID)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// GetAcceptedFrontier provides a mock function with given fields: validatorID, requestID
func (_m *Engine) GetAcceptedFrontier(validatorID ids.NodeID, requestID uint32) error {
	ret := _m.Called(validatorID, requestID)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32) error); ok {
		r0 = rf(validatorID, requestID)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// GetAcceptedFrontierFailed provides a mock function with given fields: validatorID, requestID
func (_m *Engine) GetAcceptedFrontierFailed(validatorID ids.NodeID, requestID uint32) error {
	ret := _m.Called(validatorID, requestID)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32) error); ok {
		r0 = rf(validatorID, requestID)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// GetAcceptedStateSummary provides a mock function with given fields: validatorID, requestID, keys
func (_m *Engine) GetAcceptedStateSummary(validatorID ids.NodeID, requestID uint32, keys []uint64) error {
	ret := _m.Called(validatorID, requestID, keys)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32, []uint64) error); ok {
		r0 = rf(validatorID, requestID, keys)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// GetAcceptedStateSummaryFailed provides a mock function with given fields: validatorID, requestID
func (_m *Engine) GetAcceptedStateSummaryFailed(validatorID ids.NodeID, requestID uint32) error {
	ret := _m.Called(validatorID, requestID)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32) error); ok {
		r0 = rf(validatorID, requestID)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// GetAncestors provides a mock function with given fields: validatorID, requestID, containerID
func (_m *Engine) GetAncestors(validatorID ids.NodeID, requestID uint32, containerID ids.ID) error {
	ret := _m.Called(validatorID, requestID, containerID)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32, ids.ID) error); ok {
		r0 = rf(validatorID, requestID, containerID)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// GetAncestorsFailed provides a mock function with given fields: validatorID, requestID
func (_m *Engine) GetAncestorsFailed(validatorID ids.NodeID, requestID uint32) error {
	ret := _m.Called(validatorID, requestID)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32) error); ok {
		r0 = rf(validatorID, requestID)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// GetBlock provides a mock function with given fields: _a0
func (_m *Engine) GetBlock(_a0 ids.ID) (consensussnowman.Block, error) {
	ret := _m.Called(_a0)

	var r0 consensussnowman.Block
	if rf, ok := ret.Get(0).(func(ids.ID) consensussnowman.Block); ok {
		r0 = rf(_a0)
	} else {
		if ret.Get(0) != nil {
			r0 = ret.Get(0).(consensussnowman.Block)
		}
	}

	var r1 error
	if rf, ok := ret.Get(1).(func(ids.ID) error); ok {
		r1 = rf(_a0)
	} else {
		r1 = ret.Error(1)
	}

	return r0, r1
}

// GetFailed provides a mock function with given fields: validatorID, requestID
func (_m *Engine) GetFailed(validatorID ids.NodeID, requestID uint32) error {
	ret := _m.Called(validatorID, requestID)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32) error); ok {
		r0 = rf(validatorID, requestID)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// GetStateSummaryFrontier provides a mock function with given fields: validatorID, requestID
func (_m *Engine) GetStateSummaryFrontier(validatorID ids.NodeID, requestID uint32) error {
	ret := _m.Called(validatorID, requestID)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32) error); ok {
		r0 = rf(validatorID, requestID)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// GetStateSummaryFrontierFailed provides a mock function with given fields: validatorID, requestID
func (_m *Engine) GetStateSummaryFrontierFailed(validatorID ids.NodeID, requestID uint32) error {
	ret := _m.Called(validatorID, requestID)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32) error); ok {
		r0 = rf(validatorID, requestID)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// GetVM provides a mock function with given fields:
func (_m *Engine) GetVM() common.VM {
	ret := _m.Called()

	var r0 common.VM
	if rf, ok := ret.Get(0).(func() common.VM); ok {
		r0 = rf()
	} else {
		if ret.Get(0) != nil {
			r0 = ret.Get(0).(common.VM)
		}
	}

	return r0
}

// Gossip provides a mock function with given fields:
func (_m *Engine) Gossip() error {
	ret := _m.Called()

	var r0 error
	if rf, ok := ret.Get(0).(func() error); ok {
		r0 = rf()
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// Halt provides a mock function with given fields:
func (_m *Engine) Halt() {
	_m.Called()
}

// HealthCheck provides a mock function with given fields:
func (_m *Engine) HealthCheck() (interface{}, error) {
	ret := _m.Called()

	var r0 interface{}
	if rf, ok := ret.Get(0).(func() interface{}); ok {
		r0 = rf()
	} else {
		if ret.Get(0) != nil {
			r0 = ret.Get(0).(interface{})
		}
	}

	var r1 error
	if rf, ok := ret.Get(1).(func() error); ok {
		r1 = rf()
	} else {
		r1 = ret.Error(1)
	}

	return r0, r1
}

// Notify provides a mock function with given fields: _a0
func (_m *Engine) Notify(_a0 common.Message) error {
	ret := _m.Called(_a0)

	var r0 error
	if rf, ok := ret.Get(0).(func(common.Message) error); ok {
		r0 = rf(_a0)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// PullQuery provides a mock function with given fields: validatorID, requestID, containerID
func (_m *Engine) PullQuery(validatorID ids.NodeID, requestID uint32, containerID ids.ID) error {
	ret := _m.Called(validatorID, requestID, containerID)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32, ids.ID) error); ok {
		r0 = rf(validatorID, requestID, containerID)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// PushQuery provides a mock function with given fields: validatorID, requestID, container
func (_m *Engine) PushQuery(validatorID ids.NodeID, requestID uint32, container []byte) error {
	ret := _m.Called(validatorID, requestID, container)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32, []byte) error); ok {
		r0 = rf(validatorID, requestID, container)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// Put provides a mock function with given fields: validatorID, requestID, container
func (_m *Engine) Put(validatorID ids.NodeID, requestID uint32, container []byte) error {
	ret := _m.Called(validatorID, requestID, container)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32, []byte) error); ok {
		r0 = rf(validatorID, requestID, container)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// QueryFailed provides a mock function with given fields: validatorID, requestID
func (_m *Engine) QueryFailed(validatorID ids.NodeID, requestID uint32) error {
	ret := _m.Called(validatorID, requestID)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32) error); ok {
		r0 = rf(validatorID, requestID)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// Shutdown provides a mock function with given fields:
func (_m *Engine) Shutdown() error {
	ret := _m.Called()

	var r0 error
	if rf, ok := ret.Get(0).(func() error); ok {
		r0 = rf()
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// Start provides a mock function with given fields: startReqID
func (_m *Engine) Start(startReqID uint32) error {
	ret := _m.Called(startReqID)

	var r0 error
	if rf, ok := ret.Get(0).(func(uint32) error); ok {
		r0 = rf(startReqID)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// StateSummaryFrontier provides a mock function with given fields: validatorID, requestID, summary
func (_m *Engine) StateSummaryFrontier(validatorID ids.NodeID, requestID uint32, summary []byte) error {
	ret := _m.Called(validatorID, requestID, summary)

	var r0 error
	if rf, ok := ret.Get(0).(func(ids.NodeID, uint32, []byte) error); ok {
		r0 = rf(validatorID, requestID, summary)
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// Timeout provides a mock function with given fields:
func (_m *Engine) Timeout() error {
	ret := _m.Called()

	var r0 error
	if rf, ok := ret.Get(0).(func() error); ok {
		r0 = rf()
	} else {
		r0 = ret.Error(0)
	}

	return r0
}

// NewEngine creates a new instance of Engine. It also registers the testing.TB interface on the mock and a cleanup function to assert the mocks expectations.
func NewEngine(t testing.TB) *Engine {
	mock := &Engine{}
	mock.Mock.Test(t)

	t.Cleanup(func() { mock.AssertExpectations(t) })

	return mock
}

```

avalanchego/snow/engine/snowman/syncer/config.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package syncer

import (
	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/engine/common"
	"github.com/ava-labs/avalanchego/snow/engine/snowman/block"
	"github.com/ava-labs/avalanchego/snow/validators"
)

type Config struct {
	common.Config
	common.AllGetsServer

	// SampleK determines the number of nodes to attempt to fetch the latest
	// state sync summary from. In order for a round of voting to succeed, there
	// must be at least one correct node sampled.
	SampleK int

	// Alpha specifies the amount of weight that validators must put behind a
	// state summary to consider it valid to sync to.
	Alpha uint64

	// StateSyncBeacons are the nodes that will be used to sample and vote over
	// state summaries.
	StateSyncBeacons validators.Set

	VM block.ChainVM
}

func NewConfig(
	commonCfg common.Config,
	stateSyncerIDs []ids.NodeID,
	snowGetHandler common.AllGetsServer,
	vm block.ChainVM,
) (Config, error) {
	// Initialize the default values that will be used if stateSyncerIDs is
	// empty.
	var (
		stateSyncBeacons = commonCfg.Beacons
		syncAlpha        = commonCfg.Alpha
		syncSampleK      = commonCfg.SampleK
	)

	// If the user has manually provided state syncer IDs, then override the
	// state sync beacons to them.
	if len(stateSyncerIDs) != 0 {
		stateSyncBeacons = validators.NewSet()
		for _, peerID := range stateSyncerIDs {
			if err := stateSyncBeacons.AddWeight(peerID, 1); err != nil {
				return Config{}, err
			}
		}
		stateSyncingWeight := stateSyncBeacons.Weight()
		if uint64(syncSampleK) > stateSyncingWeight {
			syncSampleK = int(stateSyncingWeight)
		}
		syncAlpha = stateSyncingWeight/2 + 1 // must be > 50%
	}
	return Config{
		Config:           commonCfg,
		AllGetsServer:    snowGetHandler,
		SampleK:          syncSampleK,
		Alpha:            syncAlpha,
		StateSyncBeacons: stateSyncBeacons,
		VM:               vm,
	}, nil
}

```

avalanchego/snow/engine/snowman/syncer/state_syncer.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package syncer

import (
	"fmt"
	"time"

	stdmath "math"

	"go.uber.org/zap"

	"github.com/ava-labs/avalanchego/database"
	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow"
	"github.com/ava-labs/avalanchego/snow/engine/common"
	"github.com/ava-labs/avalanchego/snow/engine/snowman/block"
	"github.com/ava-labs/avalanchego/snow/validators"
	"github.com/ava-labs/avalanchego/utils/math"
	"github.com/ava-labs/avalanchego/version"
)

var _ common.StateSyncer = &stateSyncer{}

// summary content as received from network, along with accumulated weight.
type weightedSummary struct {
	summary block.StateSummary
	weight  uint64
}

type stateSyncer struct {
	Config

	// list of NoOpsHandler for messages dropped by state syncer
	common.AcceptedFrontierHandler
	common.AcceptedHandler
	common.AncestorsHandler
	common.PutHandler
	common.QueryHandler
	common.ChitsHandler
	common.AppHandler

	started bool

	// Tracks the last requestID that was used in a request
	requestID uint32

	stateSyncVM        block.StateSyncableVM
	onDoneStateSyncing func(lastReqID uint32) error

	// we track the (possibly nil) local summary to help engine
	// choosing among multiple validated summaries
	locallyAvailableSummary block.StateSummary

	// Holds the beacons that were sampled for the accepted frontier
	// Won't be consumed as seeders are reached out. Used to rescale
	// alpha for frontiers
	frontierSeeders validators.Set
	// IDs of validators we should request state summary frontier from.
	// Will be consumed seeders are reached out for frontier.
	targetSeeders ids.NodeIDSet
	// IDs of validators we requested a state summary frontier from
	// but haven't received a reply yet. ID is cleared if/when reply arrives.
	pendingSeeders ids.NodeIDSet
	// IDs of validators that failed to respond with their state summary frontier
	failedSeeders ids.NodeIDSet

	// IDs of validators we should request filtering the accepted state summaries from
	targetVoters ids.NodeIDSet
	// IDs of validators we requested filtering the accepted state summaries from
	// but haven't received a reply yet. ID is cleared if/when reply arrives.
	pendingVoters ids.NodeIDSet
	// IDs of validators that failed to respond with their filtered accepted state summaries
	failedVoters ids.NodeIDSet

	// summaryID --> (summary, weight)
	weightedSummaries map[ids.ID]*weightedSummary

	// summaries received may be different even if referring to the same height
	// we keep a list of deduplcated height ready for voting
	summariesHeights       map[uint64]struct{}
	uniqueSummariesHeights []uint64

	// number of times the state sync has been attempted
	attempts int
}

func New(
	cfg Config,
	onDoneStateSyncing func(lastReqID uint32) error,
) common.StateSyncer {
	ssVM, _ := cfg.VM.(block.StateSyncableVM)
	return &stateSyncer{
		Config:                  cfg,
		AcceptedFrontierHandler: common.NewNoOpAcceptedFrontierHandler(cfg.Ctx.Log),
		AcceptedHandler:         common.NewNoOpAcceptedHandler(cfg.Ctx.Log),
		AncestorsHandler:        common.NewNoOpAncestorsHandler(cfg.Ctx.Log),
		PutHandler:              common.NewNoOpPutHandler(cfg.Ctx.Log),
		QueryHandler:            common.NewNoOpQueryHandler(cfg.Ctx.Log),
		ChitsHandler:            common.NewNoOpChitsHandler(cfg.Ctx.Log),
		AppHandler:              common.NewNoOpAppHandler(cfg.Ctx.Log),
		stateSyncVM:             ssVM,
		onDoneStateSyncing:      onDoneStateSyncing,
	}
}

func (ss *stateSyncer) StateSummaryFrontier(nodeID ids.NodeID, requestID uint32, summaryBytes []byte) error {
	// ignores any late responses
	if requestID != ss.requestID {
		ss.Ctx.Log.Debug("received out-of-sync StateSummaryFrontier message",
			zap.Stringer("nodeID", nodeID),
			zap.Uint32("expectedRequestID", ss.requestID),
			zap.Uint32("requestID", requestID),
		)
		return nil
	}

	if !ss.pendingSeeders.Contains(nodeID) {
		ss.Ctx.Log.Debug("received unexpected StateSummaryFrontier message",
			zap.Stringer("nodeID", nodeID),
		)
		return nil
	}

	// Mark that we received a response from [nodeID]
	ss.pendingSeeders.Remove(nodeID)

	// retrieve summary ID and register frontier;
	// make sure next beacons are reached out
	// even in case invalid summaries are received
	if summary, err := ss.stateSyncVM.ParseStateSummary(summaryBytes); err == nil {
		ss.weightedSummaries[summary.ID()] = &weightedSummary{
			summary: summary,
		}

		height := summary.Height()
		if _, exists := ss.summariesHeights[height]; !exists {
			ss.summariesHeights[height] = struct{}{}
			ss.uniqueSummariesHeights = append(ss.uniqueSummariesHeights, height)
		}
	} else {
		ss.Ctx.Log.Debug("failed to parse summary",
			zap.Error(err),
		)
		ss.Ctx.Log.Verbo("failed to parse summary",
			zap.Binary("summary", summaryBytes),
			zap.Error(err),
		)
	}

	return ss.receivedStateSummaryFrontier()
}

func (ss *stateSyncer) GetStateSummaryFrontierFailed(nodeID ids.NodeID, requestID uint32) error {
	// ignores any late responses
	if requestID != ss.requestID {
		ss.Ctx.Log.Debug("received out-of-sync GetStateSummaryFrontierFailed message",
			zap.Stringer("nodeID", nodeID),
			zap.Uint32("expectedRequestID", ss.requestID),
			zap.Uint32("requestID", requestID),
		)
		return nil
	}

	// Mark that we didn't get a response from [nodeID]
	ss.failedSeeders.Add(nodeID)
	ss.pendingSeeders.Remove(nodeID)

	return ss.receivedStateSummaryFrontier()
}

func (ss *stateSyncer) receivedStateSummaryFrontier() error {
	ss.sendGetStateSummaryFrontiers()

	// still waiting on requests
	if ss.pendingSeeders.Len() != 0 {
		return nil
	}

	// All nodes reached out for the summary frontier have responded or timed out.
	// If enough of them have indeed responded we'll go ahead and ask
	// each state syncer (not just a sample) to filter the list of state summaries
	// that we were told are on the accepted frontier.
	// If we got too many timeouts, we restart state syncing hoping that network
	// problems will go away and we can collect a qualified frontier.
	// We assume the frontier is qualified after an alpha proportion of frontier seeders have responded
	frontierAlpha := float64(ss.frontierSeeders.Weight()*ss.Alpha) / float64(ss.StateSyncBeacons.Weight())
	failedBeaconWeight, err := ss.StateSyncBeacons.SubsetWeight(ss.failedSeeders)
	if err != nil {
		return err
	}

	frontierStake := ss.frontierSeeders.Weight() - failedBeaconWeight
	if float64(frontierStake) < frontierAlpha {
		ss.Ctx.Log.Debug("didn't receive enough frontiers",
			zap.Int("numFailedValidators", ss.failedSeeders.Len()),
			zap.Int("numStateSyncAttempts", ss.attempts),
		)

		if ss.Config.RetryBootstrap {
			ss.Ctx.Log.Debug("restarting state sync")
			return ss.restart()
		}
	}

	ss.requestID++
	ss.sendGetAcceptedStateSummaries()
	return nil
}

func (ss *stateSyncer) AcceptedStateSummary(nodeID ids.NodeID, requestID uint32, summaryIDs []ids.ID) error {
	// ignores any late responses
	if requestID != ss.requestID {
		ss.Ctx.Log.Debug("received out-of-sync AcceptedStateSummary message",
			zap.Stringer("nodeID", nodeID),
			zap.Uint32("expectedRequestID", ss.requestID),
			zap.Uint32("requestID", requestID),
		)
		return nil
	}

	if !ss.pendingVoters.Contains(nodeID) {
		ss.Ctx.Log.Debug("received unexpected AcceptedStateSummary message",
			zap.Stringer("nodeID", nodeID),
		)
		return nil
	}

	// Mark that we received a response from [nodeID]
	ss.pendingVoters.Remove(nodeID)

	weight, _ := ss.StateSyncBeacons.GetWeight(nodeID)
	for _, summaryID := range summaryIDs {
		ws, ok := ss.weightedSummaries[summaryID]
		if !ok {
			ss.Ctx.Log.Debug("skipping summary",
				zap.String("reason", "received a vote from validator for unknown summary"),
				zap.Stringer("nodeID", nodeID),
				zap.Stringer("summaryID", summaryID),
			)
			continue
		}

		newWeight, err := math.Add64(weight, ws.weight)
		if err != nil {
			ss.Ctx.Log.Error("failed to calculate the Accepted votes",
				zap.Uint64("weight", weight),
				zap.Uint64("previousWeight", ws.weight),
				zap.Error(err),
			)
			newWeight = stdmath.MaxUint64
		}
		ws.weight = newWeight
	}

	ss.sendGetAcceptedStateSummaries()

	// wait on pending responses
	if ss.pendingVoters.Len() != 0 {
		return nil
	}

	// We've received the filtered accepted frontier from every state sync validator
	// Drop all summaries without a sufficient weight behind them
	for summaryID, ws := range ss.weightedSummaries {
		if ws.weight < ss.Alpha {
			ss.Ctx.Log.Debug("removing summary",
				zap.String("reason", "insufficient weight"),
				zap.Uint64("currentWeight", ws.weight),
				zap.Uint64("requiredWeight", ss.Alpha),
			)
			delete(ss.weightedSummaries, summaryID)
		}
	}

	// if we don't have enough weight for the state summary to be accepted then retry or fail the state sync
	size := len(ss.weightedSummaries)
	if size == 0 {
		// retry the state sync if the weight is not enough to state sync
		failedBeaconWeight, err := ss.StateSyncBeacons.SubsetWeight(ss.failedVoters)
		if err != nil {
			return err
		}

		// if we had too many timeouts when asking for validator votes, we should restart
		// state sync hoping for the network problems to go away; otherwise, we received
		// enough (>= ss.Alpha) responses, but no state summary was supported by a majority
		// of validators (i.e. votes are split between minorities supporting different state
		// summaries), so there is no point in retrying state sync; we should move ahead to bootstrapping
		votingStakes := ss.StateSyncBeacons.Weight() - failedBeaconWeight
		if ss.Config.RetryBootstrap && votingStakes < ss.Alpha {
			ss.Ctx.Log.Debug("restarting state sync",
				zap.String("reason", "not enough votes received"),
				zap.Int("numBeacons", ss.StateSyncBeacons.Len()),
				zap.Int("numFailedSyncers", ss.failedVoters.Len()),
				zap.Int("numAttempts", ss.attempts),
			)
			return ss.restart()
		}

		ss.Ctx.Log.Info("skipping state sync",
			zap.String("reason", "no acceptable summaries found"),
		)

		// if we do not restart state sync, move on to bootstrapping.
		return ss.onDoneStateSyncing(ss.requestID)
	}

	preferredStateSummary := ss.selectSyncableStateSummary()
	ss.Ctx.Log.Info("selected summary start state sync",
		zap.Stringer("summaryID", preferredStateSummary.ID()),
		zap.Int("numTotalSummaries", size),
	)

	startedSyncing, err := preferredStateSummary.Accept()
	if err != nil {
		return err
	}
	if startedSyncing {
		// summary was accepted and VM is state syncing.
		// Engine will wait for notification of state sync done.
		return nil
	}

	// VM did not accept the summary, move on to bootstrapping.
	return ss.onDoneStateSyncing(ss.requestID)
}

// selectSyncableStateSummary chooses a state summary from all
// the network validated summaries.
func (ss *stateSyncer) selectSyncableStateSummary() block.StateSummary {
	var (
		maxSummaryHeight      uint64
		preferredStateSummary block.StateSummary
	)

	// by default pick highest summary, unless locallyAvailableSummary is still valid.
	// In such case we pick locallyAvailableSummary to allow VM resuming state syncing.
	for id, ws := range ss.weightedSummaries {
		if ss.locallyAvailableSummary != nil && id == ss.locallyAvailableSummary.ID() {
			return ss.locallyAvailableSummary
		}

		height := ws.summary.Height()
		if maxSummaryHeight <= height {
			maxSummaryHeight = height
			preferredStateSummary = ws.summary
		}
	}
	return preferredStateSummary
}

func (ss *stateSyncer) GetAcceptedStateSummaryFailed(nodeID ids.NodeID, requestID uint32) error {
	// ignores any late responses
	if requestID != ss.requestID {
		ss.Ctx.Log.Debug("received out-of-sync GetAcceptedStateSummaryFailed message",
			zap.Stringer("nodeID", nodeID),
			zap.Uint32("expectedRequestID", ss.requestID),
			zap.Uint32("requestID", requestID),
		)
		return nil
	}

	// If we can't get a response from [nodeID], act as though they said that
	// they think none of the containers we sent them in GetAccepted are
	// accepted
	ss.failedVoters.Add(nodeID)

	return ss.AcceptedStateSummary(nodeID, requestID, nil)
}

func (ss *stateSyncer) Start(startReqID uint32) error {
	ss.Ctx.Log.Info("starting state sync")

	ss.Ctx.SetState(snow.StateSyncing)
	if err := ss.VM.SetState(snow.StateSyncing); err != nil {
		return fmt.Errorf("failed to notify VM that state syncing has started: %w", err)
	}

	ss.requestID = startReqID

	if !ss.StartupTracker.ShouldStart() {
		return nil
	}

	ss.started = true
	return ss.startup()
}

// startup do start the whole state sync process by
// sampling frontier seeders, listing state syncers to request votes to
// and reaching out frontier seeders if any. Othewise it move immediately
// to bootstrapping. Unlike Start, startup does not check
// whether sufficient stake amount is connected.
func (ss *stateSyncer) startup() error {
	ss.Config.Ctx.Log.Info("starting state sync")

	// clear up messages trackers
	ss.weightedSummaries = make(map[ids.ID]*weightedSummary)
	ss.summariesHeights = make(map[uint64]struct{})
	ss.uniqueSummariesHeights = nil

	ss.targetSeeders.Clear()
	ss.pendingSeeders.Clear()
	ss.failedSeeders.Clear()
	ss.targetVoters.Clear()
	ss.pendingVoters.Clear()
	ss.failedVoters.Clear()

	// sample K beacons to retrieve frontier from
	beacons, err := ss.StateSyncBeacons.Sample(ss.Config.SampleK)
	if err != nil {
		return err
	}

	ss.frontierSeeders = validators.NewSet()
	if err = ss.frontierSeeders.Set(beacons); err != nil {
		return err
	}

	for _, vdr := range beacons {
		vdrID := vdr.ID()
		ss.targetSeeders.Add(vdrID)
	}

	// list all beacons, to reach them for voting on frontier
	for _, vdr := range ss.StateSyncBeacons.List() {
		vdrID := vdr.ID()
		ss.targetVoters.Add(vdrID)
	}

	// check if there is an ongoing state sync; if so add its state summary
	// to the frontier to request votes on
	// Note: database.ErrNotFound means there is no ongoing summary
	localSummary, err := ss.stateSyncVM.GetOngoingSyncStateSummary()
	switch err {
	case database.ErrNotFound:
		// no action needed
	case nil:
		ss.locallyAvailableSummary = localSummary
		ss.weightedSummaries[localSummary.ID()] = &weightedSummary{
			summary: localSummary,
		}

		height := localSummary.Height()
		ss.summariesHeights[height] = struct{}{}
		ss.uniqueSummariesHeights = append(ss.uniqueSummariesHeights, height)
	default:
		return err
	}

	// initiate messages exchange
	ss.attempts++
	if ss.targetSeeders.Len() == 0 {
		ss.Ctx.Log.Info("State syncing skipped due to no provided syncers")
		return ss.onDoneStateSyncing(ss.requestID)
	}

	ss.requestID++
	ss.sendGetStateSummaryFrontiers()
	return nil
}

func (ss *stateSyncer) restart() error {
	if ss.attempts > 0 && ss.attempts%ss.RetryBootstrapWarnFrequency == 0 {
		ss.Ctx.Log.Debug("check internet connection",
			zap.Int("numSyncAttempts", ss.attempts),
		)
	}

	return ss.startup()
}

// Ask up to [common.MaxOutstandingBroadcastRequests] state sync validators at a time
// to send their accepted state summary. It is called again until there are
// no more seeders to be reached in the pending set
func (ss *stateSyncer) sendGetStateSummaryFrontiers() {
	vdrs := ids.NewNodeIDSet(1)
	for ss.targetSeeders.Len() > 0 && ss.pendingSeeders.Len() < common.MaxOutstandingBroadcastRequests {
		vdr, _ := ss.targetSeeders.Pop()
		vdrs.Add(vdr)
		ss.pendingSeeders.Add(vdr)
	}

	if vdrs.Len() > 0 {
		ss.Sender.SendGetStateSummaryFrontier(vdrs, ss.requestID)
	}
}

// Ask up to [common.MaxOutstandingStateSyncRequests] syncers validators to send
// their filtered accepted frontier. It is called again until there are
// no more voters to be reached in the pending set.
func (ss *stateSyncer) sendGetAcceptedStateSummaries() {
	vdrs := ids.NewNodeIDSet(1)
	for ss.targetVoters.Len() > 0 && ss.pendingVoters.Len() < common.MaxOutstandingBroadcastRequests {
		vdr, _ := ss.targetVoters.Pop()
		vdrs.Add(vdr)
		ss.pendingVoters.Add(vdr)
	}

	if len(vdrs) > 0 {
		ss.Sender.SendGetAcceptedStateSummary(vdrs, ss.requestID, ss.uniqueSummariesHeights)
		ss.Ctx.Log.Debug("sent GetAcceptedStateSummary messages",
			zap.Int("numSent", vdrs.Len()),
			zap.Int("numPending", ss.targetVoters.Len()),
		)
	}
}

func (ss *stateSyncer) AppRequest(nodeID ids.NodeID, requestID uint32, deadline time.Time, request []byte) error {
	return ss.VM.AppRequest(nodeID, requestID, deadline, request)
}

func (ss *stateSyncer) AppResponse(nodeID ids.NodeID, requestID uint32, response []byte) error {
	return ss.VM.AppResponse(nodeID, requestID, response)
}

func (ss *stateSyncer) AppRequestFailed(nodeID ids.NodeID, requestID uint32) error {
	return ss.VM.AppRequestFailed(nodeID, requestID)
}

func (ss *stateSyncer) Notify(msg common.Message) error {
	if msg != common.StateSyncDone {
		ss.Ctx.Log.Warn("received an unexpected message from the VM",
			zap.Stringer("msg", msg),
		)
		return nil
	}
	return ss.onDoneStateSyncing(ss.requestID)
}

func (ss *stateSyncer) Connected(nodeID ids.NodeID, nodeVersion *version.Application) error {
	if err := ss.VM.Connected(nodeID, nodeVersion); err != nil {
		return err
	}

	if err := ss.StartupTracker.Connected(nodeID, nodeVersion); err != nil {
		return err
	}

	if ss.started || !ss.StartupTracker.ShouldStart() {
		return nil
	}

	ss.started = true
	return ss.startup()
}

func (ss *stateSyncer) Disconnected(nodeID ids.NodeID) error {
	if err := ss.VM.Disconnected(nodeID); err != nil {
		return err
	}

	return ss.StartupTracker.Disconnected(nodeID)
}

func (ss *stateSyncer) Gossip() error { return nil }

func (ss *stateSyncer) Shutdown() error {
	ss.Config.Ctx.Log.Info("shutting down state syncer")
	return ss.VM.Shutdown()
}

func (ss *stateSyncer) Halt() {}

func (ss *stateSyncer) Timeout() error { return nil }

func (ss *stateSyncer) HealthCheck() (interface{}, error) {
	vmIntf, vmErr := ss.VM.HealthCheck()
	intf := map[string]interface{}{
		"consensus": struct{}{},
		"vm":        vmIntf,
	}
	return intf, vmErr
}

func (ss *stateSyncer) GetVM() common.VM { return ss.VM }

func (ss *stateSyncer) IsEnabled() (bool, error) {
	if ss.stateSyncVM == nil {
		// state sync is not implemented
		return false, nil
	}

	return ss.stateSyncVM.StateSyncEnabled()
}

```

avalanchego/snow/engine/snowman/syncer/state_syncer_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package syncer

import (
	"bytes"
	"errors"
	"math"
	"testing"

	"github.com/stretchr/testify/require"

	"github.com/ava-labs/avalanchego/database"
	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow"
	"github.com/ava-labs/avalanchego/snow/engine/common"
	"github.com/ava-labs/avalanchego/snow/engine/common/tracker"
	"github.com/ava-labs/avalanchego/snow/engine/snowman/block"
	"github.com/ava-labs/avalanchego/snow/engine/snowman/getter"
	"github.com/ava-labs/avalanchego/version"

	safeMath "github.com/ava-labs/avalanchego/utils/math"
)

func TestStateSyncerIsEnabledIfVMSupportsStateSyncing(t *testing.T) {
	require := require.New(t)

	// Build state syncer
	sender := &common.SenderTest{T: t}
	commonCfg := &common.Config{
		Ctx:    snow.DefaultConsensusContextTest(),
		Sender: sender,
	}

	// Non state syncableVM case
	nonStateSyncableVM := &block.TestVM{
		TestVM: common.TestVM{T: t},
	}
	dummyGetter, err := getter.New(nonStateSyncableVM, *commonCfg)
	require.NoError(err)

	cfg, err := NewConfig(*commonCfg, nil, dummyGetter, nonStateSyncableVM)
	require.NoError(err)
	syncer := New(cfg, func(lastReqID uint32) error { return nil })

	enabled, err := syncer.IsEnabled()
	require.NoError(err)
	require.False(enabled)

	// State syncableVM case
	commonCfg.Ctx = snow.DefaultConsensusContextTest() // reset metrics

	fullVM := &fullVM{
		TestVM: &block.TestVM{
			TestVM: common.TestVM{T: t},
		},
		TestStateSyncableVM: &block.TestStateSyncableVM{
			T: t,
		},
	}
	dummyGetter, err = getter.New(fullVM, *commonCfg)
	require.NoError(err)

	cfg, err = NewConfig(*commonCfg, nil, dummyGetter, fullVM)
	require.NoError(err)
	syncer = New(cfg, func(lastReqID uint32) error { return nil })

	// test: VM does not support state syncing
	fullVM.StateSyncEnabledF = func() (bool, error) { return false, nil }
	enabled, err = syncer.IsEnabled()
	require.NoError(err)
	require.False(enabled)

	// test: VM does support state syncing
	fullVM.StateSyncEnabledF = func() (bool, error) { return true, nil }
	enabled, err = syncer.IsEnabled()
	require.NoError(err)
	require.True(enabled)
}

func TestStateSyncingStartsOnlyIfEnoughStakeIsConnected(t *testing.T) {
	require := require.New(t)

	vdrs := buildTestPeers(t)
	alpha := vdrs.Weight()
	startupAlpha := alpha

	peers := tracker.NewPeers()
	startup := tracker.NewStartup(peers, startupAlpha)
	vdrs.RegisterCallbackListener(startup)

	commonCfg := common.Config{
		Ctx:            snow.DefaultConsensusContextTest(),
		Validators:     vdrs,
		Beacons:        vdrs,
		SampleK:        vdrs.Len(),
		Alpha:          alpha,
		StartupTracker: startup,
	}
	syncer, _, sender := buildTestsObjects(t, &commonCfg)

	sender.CantSendGetStateSummaryFrontier = true
	sender.SendGetStateSummaryFrontierF = func(ss ids.NodeIDSet, u uint32) {}
	startReqID := uint32(0)

	// attempt starting bootstrapper with no stake connected. Bootstrapper should stall.
	require.False(commonCfg.StartupTracker.ShouldStart())
	require.NoError(syncer.Start(startReqID))
	require.False(syncer.started)

	// attempt starting bootstrapper with not enough stake connected. Bootstrapper should stall.
	vdr0 := ids.GenerateTestNodeID()
	require.NoError(vdrs.AddWeight(vdr0, startupAlpha/2))
	require.NoError(syncer.Connected(vdr0, version.CurrentApp))

	require.False(commonCfg.StartupTracker.ShouldStart())
	require.NoError(syncer.Start(startReqID))
	require.False(syncer.started)

	// finally attempt starting bootstrapper with enough stake connected. Frontiers should be requested.
	vdr := ids.GenerateTestNodeID()
	require.NoError(vdrs.AddWeight(vdr, startupAlpha))
	require.NoError(syncer.Connected(vdr, version.CurrentApp))

	require.True(commonCfg.StartupTracker.ShouldStart())
	require.NoError(syncer.Start(startReqID))
	require.True(syncer.started)
}

func TestStateSyncLocalSummaryIsIncludedAmongFrontiersIfAvailable(t *testing.T) {
	require := require.New(t)

	vdrs := buildTestPeers(t)
	startupAlpha := (3*vdrs.Weight() + 3) / 4

	peers := tracker.NewPeers()
	startup := tracker.NewStartup(peers, startupAlpha)
	vdrs.RegisterCallbackListener(startup)

	commonCfg := common.Config{
		Ctx:            snow.DefaultConsensusContextTest(),
		Beacons:        vdrs,
		SampleK:        vdrs.Len(),
		Alpha:          (vdrs.Weight() + 1) / 2,
		StartupTracker: startup,
	}
	syncer, fullVM, _ := buildTestsObjects(t, &commonCfg)

	// mock VM to simulate a valid summary is returned
	localSummary := &block.TestStateSummary{
		HeightV: 2000,
		IDV:     summaryID,
		BytesV:  summaryBytes,
	}
	fullVM.CantStateSyncGetOngoingSummary = true
	fullVM.GetOngoingSyncStateSummaryF = func() (block.StateSummary, error) {
		return localSummary, nil
	}

	// Connect enough stake to start syncer
	for _, vdr := range vdrs.List() {
		require.NoError(syncer.Connected(vdr.ID(), version.CurrentApp))
	}

	require.True(syncer.locallyAvailableSummary == localSummary)
	ws, ok := syncer.weightedSummaries[summaryID]
	require.True(ok)
	require.True(bytes.Equal(ws.summary.Bytes(), summaryBytes))
	require.Zero(ws.weight)
}

func TestStateSyncNotFoundOngoingSummaryIsNotIncludedAmongFrontiers(t *testing.T) {
	require := require.New(t)

	vdrs := buildTestPeers(t)
	startupAlpha := (3*vdrs.Weight() + 3) / 4

	peers := tracker.NewPeers()
	startup := tracker.NewStartup(peers, startupAlpha)
	vdrs.RegisterCallbackListener(startup)

	commonCfg := common.Config{
		Ctx:            snow.DefaultConsensusContextTest(),
		Beacons:        vdrs,
		SampleK:        vdrs.Len(),
		Alpha:          (vdrs.Weight() + 1) / 2,
		StartupTracker: startup,
	}
	syncer, fullVM, _ := buildTestsObjects(t, &commonCfg)

	// mock VM to simulate a no summary returned
	fullVM.CantStateSyncGetOngoingSummary = true
	fullVM.GetOngoingSyncStateSummaryF = func() (block.StateSummary, error) {
		return nil, database.ErrNotFound
	}

	// Connect enough stake to start syncer
	for _, vdr := range vdrs.List() {
		require.NoError(syncer.Connected(vdr.ID(), version.CurrentApp))
	}

	require.Nil(syncer.locallyAvailableSummary)
	require.Empty(syncer.weightedSummaries)
}

func TestBeaconsAreReachedForFrontiersUponStartup(t *testing.T) {
	require := require.New(t)

	vdrs := buildTestPeers(t)
	startupAlpha := (3*vdrs.Weight() + 3) / 4

	peers := tracker.NewPeers()
	startup := tracker.NewStartup(peers, startupAlpha)
	vdrs.RegisterCallbackListener(startup)

	commonCfg := common.Config{
		Ctx:            snow.DefaultConsensusContextTest(),
		Beacons:        vdrs,
		SampleK:        vdrs.Len(),
		Alpha:          (vdrs.Weight() + 1) / 2,
		StartupTracker: startup,
	}
	syncer, _, sender := buildTestsObjects(t, &commonCfg)

	// set sender to track nodes reached out
	contactedFrontiersProviders := ids.NewNodeIDSet(3)
	sender.CantSendGetStateSummaryFrontier = true
	sender.SendGetStateSummaryFrontierF = func(ss ids.NodeIDSet, u uint32) {
		contactedFrontiersProviders.Union(ss)
	}

	// Connect enough stake to start syncer
	for _, vdr := range vdrs.List() {
		require.NoError(syncer.Connected(vdr.ID(), version.CurrentApp))
	}

	// check that vdrs are reached out for frontiers
	require.True(len(contactedFrontiersProviders) == safeMath.Min(vdrs.Len(), common.MaxOutstandingBroadcastRequests))
	for beaconID := range contactedFrontiersProviders {
		// check that beacon is duly marked as reached out
		require.True(syncer.pendingSeeders.Contains(beaconID))
	}

	// check that, obviously, no summary is yet registered
	require.True(len(syncer.weightedSummaries) == 0)
}

func TestUnRequestedStateSummaryFrontiersAreDropped(t *testing.T) {
	require := require.New(t)

	vdrs := buildTestPeers(t)
	startupAlpha := (3*vdrs.Weight() + 3) / 4

	peers := tracker.NewPeers()
	startup := tracker.NewStartup(peers, startupAlpha)
	vdrs.RegisterCallbackListener(startup)

	commonCfg := common.Config{
		Ctx:            snow.DefaultConsensusContextTest(),
		Beacons:        vdrs,
		SampleK:        vdrs.Len(),
		Alpha:          (vdrs.Weight() + 1) / 2,
		StartupTracker: startup,
	}
	syncer, fullVM, sender := buildTestsObjects(t, &commonCfg)

	// set sender to track nodes reached out
	contactedFrontiersProviders := make(map[ids.NodeID]uint32) // nodeID -> reqID map
	sender.CantSendGetStateSummaryFrontier = true
	sender.SendGetStateSummaryFrontierF = func(ss ids.NodeIDSet, reqID uint32) {
		for nodeID := range ss {
			contactedFrontiersProviders[nodeID] = reqID
		}
	}

	// Connect enough stake to start syncer
	for _, vdr := range vdrs.List() {
		require.NoError(syncer.Connected(vdr.ID(), version.CurrentApp))
	}

	initiallyReachedOutBeaconsSize := len(contactedFrontiersProviders)
	require.True(initiallyReachedOutBeaconsSize > 0)
	require.True(initiallyReachedOutBeaconsSize <= common.MaxOutstandingBroadcastRequests)

	// mock VM to simulate a valid summary is returned
	fullVM.CantParseStateSummary = true
	fullVM.ParseStateSummaryF = func(summaryBytes []byte) (block.StateSummary, error) {
		return &block.TestStateSummary{
			HeightV: key,
			IDV:     summaryID,
			BytesV:  summaryBytes,
		}, nil
	}

	// pick one of the vdrs that have been reached out
	responsiveBeaconID := pickRandomFrom(contactedFrontiersProviders)
	responsiveBeaconReqID := contactedFrontiersProviders[responsiveBeaconID]

	// check a response with wrong request ID is dropped
	require.NoError(syncer.StateSummaryFrontier(
		responsiveBeaconID,
		math.MaxInt32,
		summaryBytes,
	))
	require.True(syncer.pendingSeeders.Contains(responsiveBeaconID)) // responsiveBeacon still pending
	require.True(len(syncer.weightedSummaries) == 0)

	// check a response from unsolicited node is dropped
	unsolicitedNodeID := ids.GenerateTestNodeID()
	require.NoError(syncer.StateSummaryFrontier(
		unsolicitedNodeID,
		responsiveBeaconReqID,
		summaryBytes,
	))
	require.True(len(syncer.weightedSummaries) == 0)

	// check a valid response is duly recorded
	require.NoError(syncer.StateSummaryFrontier(
		responsiveBeaconID,
		responsiveBeaconReqID,
		summaryBytes,
	))

	// responsiveBeacon not pending anymore
	require.False(syncer.pendingSeeders.Contains(responsiveBeaconID))

	// valid summary is recorded
	ws, ok := syncer.weightedSummaries[summaryID]
	require.True(ok)
	require.True(bytes.Equal(ws.summary.Bytes(), summaryBytes))

	// other listed vdrs are reached for data
	require.True(
		len(contactedFrontiersProviders) > initiallyReachedOutBeaconsSize ||
			len(contactedFrontiersProviders) == vdrs.Len())
}

func TestMalformedStateSummaryFrontiersAreDropped(t *testing.T) {
	require := require.New(t)

	vdrs := buildTestPeers(t)
	startupAlpha := (3*vdrs.Weight() + 3) / 4

	peers := tracker.NewPeers()
	startup := tracker.NewStartup(peers, startupAlpha)
	vdrs.RegisterCallbackListener(startup)

	commonCfg := common.Config{
		Ctx:            snow.DefaultConsensusContextTest(),
		Beacons:        vdrs,
		SampleK:        vdrs.Len(),
		Alpha:          (vdrs.Weight() + 1) / 2,
		StartupTracker: startup,
	}
	syncer, fullVM, sender := buildTestsObjects(t, &commonCfg)

	// set sender to track nodes reached out
	contactedFrontiersProviders := make(map[ids.NodeID]uint32) // nodeID -> reqID map
	sender.CantSendGetStateSummaryFrontier = true
	sender.SendGetStateSummaryFrontierF = func(ss ids.NodeIDSet, reqID uint32) {
		for nodeID := range ss {
			contactedFrontiersProviders[nodeID] = reqID
		}
	}

	// Connect enough stake to start syncer
	for _, vdr := range vdrs.List() {
		require.NoError(syncer.Connected(vdr.ID(), version.CurrentApp))
	}

	initiallyReachedOutBeaconsSize := len(contactedFrontiersProviders)
	require.True(initiallyReachedOutBeaconsSize > 0)
	require.True(initiallyReachedOutBeaconsSize <= common.MaxOutstandingBroadcastRequests)

	// mock VM to simulate an invalid summary is returned
	summary := []byte{'s', 'u', 'm', 'm', 'a', 'r', 'y'}
	isSummaryDecoded := false
	fullVM.CantParseStateSummary = true
	fullVM.ParseStateSummaryF = func(summaryBytes []byte) (block.StateSummary, error) {
		isSummaryDecoded = true
		return nil, errors.New("invalid state summary")
	}

	// pick one of the vdrs that have been reached out
	responsiveBeaconID := pickRandomFrom(contactedFrontiersProviders)
	responsiveBeaconReqID := contactedFrontiersProviders[responsiveBeaconID]

	// response is valid, but invalid summary is not recorded
	require.NoError(syncer.StateSummaryFrontier(
		responsiveBeaconID,
		responsiveBeaconReqID,
		summary,
	))

	// responsiveBeacon not pending anymore
	require.False(syncer.pendingSeeders.Contains(responsiveBeaconID))

	// invalid summary is not recorded
	require.True(isSummaryDecoded)
	require.True(len(syncer.weightedSummaries) == 0)

	// even in case of invalid summaries, other listed vdrs
	// are reached for data
	require.True(
		len(contactedFrontiersProviders) > initiallyReachedOutBeaconsSize ||
			len(contactedFrontiersProviders) == vdrs.Len())
}

func TestLateResponsesFromUnresponsiveFrontiersAreNotRecorded(t *testing.T) {
	require := require.New(t)

	vdrs := buildTestPeers(t)
	startupAlpha := (3*vdrs.Weight() + 3) / 4

	peers := tracker.NewPeers()
	startup := tracker.NewStartup(peers, startupAlpha)
	vdrs.RegisterCallbackListener(startup)

	commonCfg := common.Config{
		Ctx:            snow.DefaultConsensusContextTest(),
		Beacons:        vdrs,
		SampleK:        vdrs.Len(),
		Alpha:          (vdrs.Weight() + 1) / 2,
		StartupTracker: startup,
	}
	syncer, fullVM, sender := buildTestsObjects(t, &commonCfg)

	// set sender to track nodes reached out
	contactedFrontiersProviders := make(map[ids.NodeID]uint32) // nodeID -> reqID map
	sender.CantSendGetStateSummaryFrontier = true
	sender.SendGetStateSummaryFrontierF = func(ss ids.NodeIDSet, reqID uint32) {
		for nodeID := range ss {
			contactedFrontiersProviders[nodeID] = reqID
		}
	}

	// Connect enough stake to start syncer
	for _, vdr := range vdrs.List() {
		require.NoError(syncer.Connected(vdr.ID(), version.CurrentApp))
	}

	initiallyReachedOutBeaconsSize := len(contactedFrontiersProviders)
	require.True(initiallyReachedOutBeaconsSize > 0)
	require.True(initiallyReachedOutBeaconsSize <= common.MaxOutstandingBroadcastRequests)

	// pick one of the vdrs that have been reached out
	unresponsiveBeaconID := pickRandomFrom(contactedFrontiersProviders)
	unresponsiveBeaconReqID := contactedFrontiersProviders[unresponsiveBeaconID]

	fullVM.CantParseStateSummary = true
	fullVM.ParseStateSummaryF = func(summaryBytes []byte) (block.StateSummary, error) {
		require.True(len(summaryBytes) == 0)
		return nil, errors.New("empty summary")
	}

	// assume timeout is reached and vdrs is marked as unresponsive
	require.NoError(syncer.GetStateSummaryFrontierFailed(
		unresponsiveBeaconID,
		unresponsiveBeaconReqID,
	))

	// unresponsiveBeacon not pending anymore
	require.False(syncer.pendingSeeders.Contains(unresponsiveBeaconID))
	require.True(syncer.failedSeeders.Contains(unresponsiveBeaconID))

	// even in case of timeouts, other listed vdrs
	// are reached for data
	require.True(
		len(contactedFrontiersProviders) > initiallyReachedOutBeaconsSize ||
			len(contactedFrontiersProviders) == vdrs.Len())

	// mock VM to simulate a valid but late summary is returned
	fullVM.CantParseStateSummary = true
	fullVM.ParseStateSummaryF = func(summaryBytes []byte) (block.StateSummary, error) {
		return &block.TestStateSummary{
			HeightV: key,
			IDV:     summaryID,
			BytesV:  summaryBytes,
		}, nil
	}

	// check a valid but late response is not recorded
	require.NoError(syncer.StateSummaryFrontier(
		unresponsiveBeaconID,
		unresponsiveBeaconReqID,
		summaryBytes,
	))

	// late summary is not recorded
	require.True(len(syncer.weightedSummaries) == 0)
}

func TestStateSyncIsRestartedIfTooManyFrontierSeedersTimeout(t *testing.T) {
	require := require.New(t)

	vdrs := buildTestPeers(t)
	startupAlpha := (3*vdrs.Weight() + 3) / 4

	peers := tracker.NewPeers()
	startup := tracker.NewStartup(peers, startupAlpha)
	vdrs.RegisterCallbackListener(startup)

	commonCfg := common.Config{
		Ctx:                         snow.DefaultConsensusContextTest(),
		Beacons:                     vdrs,
		SampleK:                     vdrs.Len(),
		Alpha:                       (vdrs.Weight() + 1) / 2,
		StartupTracker:              startup,
		RetryBootstrap:              true,
		RetryBootstrapWarnFrequency: 1,
	}
	syncer, fullVM, sender := buildTestsObjects(t, &commonCfg)

	// set sender to track nodes reached out
	contactedFrontiersProviders := make(map[ids.NodeID]uint32) // nodeID -> reqID map
	sender.CantSendGetStateSummaryFrontier = true
	sender.SendGetStateSummaryFrontierF = func(ss ids.NodeIDSet, reqID uint32) {
		for nodeID := range ss {
			contactedFrontiersProviders[nodeID] = reqID
		}
	}

	// mock VM to simulate a valid summary is returned
	fullVM.CantParseStateSummary = true
	fullVM.ParseStateSummaryF = func(b []byte) (block.StateSummary, error) {
		switch {
		case bytes.Equal(b, summaryBytes):
			return &block.TestStateSummary{
				HeightV: key,
				IDV:     summaryID,
				BytesV:  summaryBytes,
			}, nil
		case bytes.Equal(b, nil):
			return nil, errors.New("Empty Summary")
		default:
			return nil, errors.New("unexpected Summary")
		}
	}

	contactedVoters := make(map[ids.NodeID]uint32) // nodeID -> reqID map
	sender.CantSendGetAcceptedStateSummary = true
	sender.SendGetAcceptedStateSummaryF = func(ss ids.NodeIDSet, reqID uint32, sl []uint64) {
		for nodeID := range ss {
			contactedVoters[nodeID] = reqID
		}
	}

	// Connect enough stake to start syncer
	for _, vdr := range vdrs.List() {
		require.NoError(syncer.Connected(vdr.ID(), version.CurrentApp))
	}
	require.True(syncer.pendingSeeders.Len() != 0)

	// let just one node respond and all others timeout
	maxResponses := 1
	reachedSeedersCount := syncer.Config.SampleK
	for reachedSeedersCount >= 0 {
		beaconID, found := syncer.pendingSeeders.Peek()
		require.True(found)
		reqID := contactedFrontiersProviders[beaconID]

		if maxResponses > 0 {
			require.NoError(syncer.StateSummaryFrontier(
				beaconID,
				reqID,
				summaryBytes,
			))
		} else {
			require.NoError(syncer.GetStateSummaryFrontierFailed(
				beaconID,
				reqID,
			))
		}
		maxResponses--
		reachedSeedersCount--
	}

	// check that some frontier seeders are reached again for the frontier
	require.True(syncer.pendingSeeders.Len() > 0)

	// check that no vote requests are issued
	require.True(len(contactedVoters) == 0)
}

func TestVoteRequestsAreSentAsAllFrontierBeaconsResponded(t *testing.T) {
	require := require.New(t)

	vdrs := buildTestPeers(t)
	startupAlpha := (3*vdrs.Weight() + 3) / 4

	peers := tracker.NewPeers()
	startup := tracker.NewStartup(peers, startupAlpha)
	vdrs.RegisterCallbackListener(startup)

	commonCfg := common.Config{
		Ctx:            snow.DefaultConsensusContextTest(),
		Beacons:        vdrs,
		SampleK:        vdrs.Len(),
		Alpha:          (vdrs.Weight() + 1) / 2,
		StartupTracker: startup,
	}
	syncer, fullVM, sender := buildTestsObjects(t, &commonCfg)

	// set sender to track nodes reached out
	contactedFrontiersProviders := make(map[ids.NodeID]uint32) // nodeID -> reqID map
	sender.CantSendGetStateSummaryFrontier = true
	sender.SendGetStateSummaryFrontierF = func(ss ids.NodeIDSet, reqID uint32) {
		for nodeID := range ss {
			contactedFrontiersProviders[nodeID] = reqID
		}
	}

	// mock VM to simulate a valid summary is returned
	fullVM.CantParseStateSummary = true
	fullVM.ParseStateSummaryF = func(b []byte) (block.StateSummary, error) {
		require.True(bytes.Equal(b, summaryBytes))
		return &block.TestStateSummary{
			HeightV: key,
			IDV:     summaryID,
			BytesV:  summaryBytes,
		}, nil
	}

	contactedVoters := make(map[ids.NodeID]uint32) // nodeID -> reqID map
	sender.CantSendGetAcceptedStateSummary = true
	sender.SendGetAcceptedStateSummaryF = func(ss ids.NodeIDSet, reqID uint32, sl []uint64) {
		for nodeID := range ss {
			contactedVoters[nodeID] = reqID
		}
	}

	// Connect enough stake to start syncer
	for _, vdr := range vdrs.List() {
		require.NoError(syncer.Connected(vdr.ID(), version.CurrentApp))
	}
	require.True(syncer.pendingSeeders.Len() != 0)

	// let all contacted vdrs respond
	for syncer.pendingSeeders.Len() != 0 {
		beaconID, found := syncer.pendingSeeders.Peek()
		require.True(found)
		reqID := contactedFrontiersProviders[beaconID]

		require.NoError(syncer.StateSummaryFrontier(
			beaconID,
			reqID,
			summaryBytes,
		))
	}
	require.False(syncer.pendingSeeders.Len() != 0)

	// check that vote requests are issued
	initiallyContactedVotersSize := len(contactedVoters)
	require.True(initiallyContactedVotersSize > 0)
	require.True(initiallyContactedVotersSize <= common.MaxOutstandingBroadcastRequests)
}

func TestUnRequestedVotesAreDropped(t *testing.T) {
	require := require.New(t)

	vdrs := buildTestPeers(t)
	startupAlpha := (3*vdrs.Weight() + 3) / 4

	peers := tracker.NewPeers()
	startup := tracker.NewStartup(peers, startupAlpha)
	vdrs.RegisterCallbackListener(startup)

	commonCfg := common.Config{
		Ctx:            snow.DefaultConsensusContextTest(),
		Beacons:        vdrs,
		SampleK:        vdrs.Len(),
		Alpha:          (vdrs.Weight() + 1) / 2,
		StartupTracker: startup,
	}
	syncer, fullVM, sender := buildTestsObjects(t, &commonCfg)

	// set sender to track nodes reached out
	contactedFrontiersProviders := make(map[ids.NodeID]uint32) // nodeID -> reqID map
	sender.CantSendGetStateSummaryFrontier = true
	sender.SendGetStateSummaryFrontierF = func(ss ids.NodeIDSet, reqID uint32) {
		for nodeID := range ss {
			contactedFrontiersProviders[nodeID] = reqID
		}
	}

	// mock VM to simulate a valid summary is returned
	fullVM.CantParseStateSummary = true
	fullVM.ParseStateSummaryF = func(summaryBytes []byte) (block.StateSummary, error) {
		return &block.TestStateSummary{
			HeightV: key,
			IDV:     summaryID,
			BytesV:  summaryBytes,
		}, nil
	}

	contactedVoters := make(map[ids.NodeID]uint32) // nodeID -> reqID map
	sender.CantSendGetAcceptedStateSummary = true
	sender.SendGetAcceptedStateSummaryF = func(ss ids.NodeIDSet, reqID uint32, sl []uint64) {
		for nodeID := range ss {
			contactedVoters[nodeID] = reqID
		}
	}

	// Connect enough stake to start syncer
	for _, vdr := range vdrs.List() {
		require.NoError(syncer.Connected(vdr.ID(), version.CurrentApp))
	}
	require.True(syncer.pendingSeeders.Len() != 0)

	// let all contacted vdrs respond
	for syncer.pendingSeeders.Len() != 0 {
		beaconID, found := syncer.pendingSeeders.Peek()
		require.True(found)
		reqID := contactedFrontiersProviders[beaconID]

		require.NoError(syncer.StateSummaryFrontier(
			beaconID,
			reqID,
			summaryBytes,
		))
	}
	require.False(syncer.pendingSeeders.Len() != 0)

	// check that vote requests are issued
	initiallyContactedVotersSize := len(contactedVoters)
	require.True(initiallyContactedVotersSize > 0)
	require.True(initiallyContactedVotersSize <= common.MaxOutstandingBroadcastRequests)

	_, found := syncer.weightedSummaries[summaryID]
	require.True(found)

	// pick one of the voters that have been reached out
	responsiveVoterID := pickRandomFrom(contactedVoters)
	responsiveVoterReqID := contactedVoters[responsiveVoterID]

	// check a response with wrong request ID is dropped
	require.NoError(syncer.AcceptedStateSummary(
		responsiveVoterID,
		math.MaxInt32,
		[]ids.ID{summaryID},
	))

	// responsiveVoter still pending
	require.True(syncer.pendingVoters.Contains(responsiveVoterID))
	require.True(syncer.weightedSummaries[summaryID].weight == 0)

	// check a response from unsolicited node is dropped
	unsolicitedVoterID := ids.GenerateTestNodeID()
	require.NoError(syncer.AcceptedStateSummary(
		unsolicitedVoterID,
		responsiveVoterReqID,
		[]ids.ID{summaryID},
	))
	require.True(syncer.weightedSummaries[summaryID].weight == 0)

	// check a valid response is duly recorded
	require.NoError(syncer.AcceptedStateSummary(
		responsiveVoterID,
		responsiveVoterReqID,
		[]ids.ID{summaryID},
	))

	// responsiveBeacon not pending anymore
	require.False(syncer.pendingSeeders.Contains(responsiveVoterID))
	voterWeight, found := vdrs.GetWeight(responsiveVoterID)
	require.True(found)
	require.True(syncer.weightedSummaries[summaryID].weight == voterWeight)

	// other listed voters are reached out
	require.True(
		len(contactedVoters) > initiallyContactedVotersSize ||
			len(contactedVoters) == vdrs.Len())
}

func TestVotesForUnknownSummariesAreDropped(t *testing.T) {
	require := require.New(t)

	vdrs := buildTestPeers(t)
	startupAlpha := (3*vdrs.Weight() + 3) / 4

	peers := tracker.NewPeers()
	startup := tracker.NewStartup(peers, startupAlpha)
	vdrs.RegisterCallbackListener(startup)

	commonCfg := common.Config{
		Ctx:            snow.DefaultConsensusContextTest(),
		Beacons:        vdrs,
		SampleK:        vdrs.Len(),
		Alpha:          (vdrs.Weight() + 1) / 2,
		StartupTracker: startup,
	}
	syncer, fullVM, sender := buildTestsObjects(t, &commonCfg)

	// set sender to track nodes reached out
	contactedFrontiersProviders := make(map[ids.NodeID]uint32) // nodeID -> reqID map
	sender.CantSendGetStateSummaryFrontier = true
	sender.SendGetStateSummaryFrontierF = func(ss ids.NodeIDSet, reqID uint32) {
		for nodeID := range ss {
			contactedFrontiersProviders[nodeID] = reqID
		}
	}

	// mock VM to simulate a valid summary is returned
	fullVM.CantParseStateSummary = true
	fullVM.ParseStateSummaryF = func(summaryBytes []byte) (block.StateSummary, error) {
		return &block.TestStateSummary{
			HeightV: key,
			IDV:     summaryID,
			BytesV:  summaryBytes,
		}, nil
	}

	contactedVoters := make(map[ids.NodeID]uint32) // nodeID -> reqID map
	sender.CantSendGetAcceptedStateSummary = true
	sender.SendGetAcceptedStateSummaryF = func(ss ids.NodeIDSet, reqID uint32, sl []uint64) {
		for nodeID := range ss {
			contactedVoters[nodeID] = reqID
		}
	}

	// Connect enough stake to start syncer
	for _, vdr := range vdrs.List() {
		require.NoError(syncer.Connected(vdr.ID(), version.CurrentApp))
	}
	require.True(syncer.pendingSeeders.Len() != 0)

	// let all contacted vdrs respond
	for syncer.pendingSeeders.Len() != 0 {
		beaconID, found := syncer.pendingSeeders.Peek()
		require.True(found)
		reqID := contactedFrontiersProviders[beaconID]

		require.NoError(syncer.StateSummaryFrontier(
			beaconID,
			reqID,
			summaryBytes,
		))
	}
	require.False(syncer.pendingSeeders.Len() != 0)

	// check that vote requests are issued
	initiallyContactedVotersSize := len(contactedVoters)
	require.True(initiallyContactedVotersSize > 0)
	require.True(initiallyContactedVotersSize <= common.MaxOutstandingBroadcastRequests)

	_, found := syncer.weightedSummaries[summaryID]
	require.True(found)

	// pick one of the voters that have been reached out
	responsiveVoterID := pickRandomFrom(contactedVoters)
	responsiveVoterReqID := contactedVoters[responsiveVoterID]

	// check a response for unRequested summary is dropped
	require.NoError(syncer.AcceptedStateSummary(
		responsiveVoterID,
		responsiveVoterReqID,
		[]ids.ID{unknownSummaryID},
	))
	_, found = syncer.weightedSummaries[unknownSummaryID]
	require.False(found)

	// check that responsiveVoter cannot cast another vote
	require.False(syncer.pendingSeeders.Contains(responsiveVoterID))
	require.NoError(syncer.AcceptedStateSummary(
		responsiveVoterID,
		responsiveVoterReqID,
		[]ids.ID{summaryID},
	))
	require.True(syncer.weightedSummaries[summaryID].weight == 0)

	// other listed voters are reached out, even in the face of vote
	// on unknown summary
	require.True(
		len(contactedVoters) > initiallyContactedVotersSize ||
			len(contactedVoters) == vdrs.Len())
}

func TestStateSummaryIsPassedToVMAsMajorityOfVotesIsCastedForIt(t *testing.T) {
	require := require.New(t)

	vdrs := buildTestPeers(t)
	startupAlpha := (3*vdrs.Weight() + 3) / 4

	peers := tracker.NewPeers()
	startup := tracker.NewStartup(peers, startupAlpha)
	vdrs.RegisterCallbackListener(startup)

	commonCfg := common.Config{
		Ctx:            snow.DefaultConsensusContextTest(),
		Beacons:        vdrs,
		SampleK:        vdrs.Len(),
		Alpha:          (vdrs.Weight() + 1) / 2,
		StartupTracker: startup,
	}
	syncer, fullVM, sender := buildTestsObjects(t, &commonCfg)

	// set sender to track nodes reached out
	contactedFrontiersProviders := make(map[ids.NodeID]uint32) // nodeID -> reqID map
	sender.CantSendGetStateSummaryFrontier = true
	sender.SendGetStateSummaryFrontierF = func(ss ids.NodeIDSet, reqID uint32) {
		for nodeID := range ss {
			contactedFrontiersProviders[nodeID] = reqID
		}
	}

	// mock VM to simulate a valid summary is returned
	summary := &block.TestStateSummary{
		HeightV: key,
		IDV:     summaryID,
		BytesV:  summaryBytes,
		T:       t,
	}
	minoritySummary := &block.TestStateSummary{
		HeightV: minorityKey,
		IDV:     minoritySummaryID,
		BytesV:  minoritySummaryBytes,
		T:       t,
	}

	fullVM.CantParseStateSummary = true
	fullVM.ParseStateSummaryF = func(b []byte) (block.StateSummary, error) {
		switch {
		case bytes.Equal(b, summaryBytes):
			return summary, nil
		case bytes.Equal(b, minoritySummaryBytes):
			return minoritySummary, nil
		default:
			return nil, errors.New("unknown state summary")
		}
	}

	contactedVoters := make(map[ids.NodeID]uint32) // nodeID -> reqID map
	sender.CantSendGetAcceptedStateSummary = true
	sender.SendGetAcceptedStateSummaryF = func(ss ids.NodeIDSet, reqID uint32, sl []uint64) {
		for nodeID := range ss {
			contactedVoters[nodeID] = reqID
		}
	}

	// Connect enough stake to start syncer
	for _, vdr := range vdrs.List() {
		require.NoError(syncer.Connected(vdr.ID(), version.CurrentApp))
	}
	require.True(syncer.pendingSeeders.Len() != 0)

	// let all contacted vdrs respond with majority or minority summaries
	for {
		reachedSeeders := syncer.pendingSeeders.Len()
		if reachedSeeders == 0 {
			break
		}
		beaconID, found := syncer.pendingSeeders.Peek()
		require.True(found)
		reqID := contactedFrontiersProviders[beaconID]

		if reachedSeeders%2 == 0 {
			require.NoError(syncer.StateSummaryFrontier(
				beaconID,
				reqID,
				summaryBytes,
			))
		} else {
			require.NoError(syncer.StateSummaryFrontier(
				beaconID,
				reqID,
				minoritySummaryBytes,
			))
		}
	}
	require.False(syncer.pendingSeeders.Len() != 0)

	majoritySummaryCalled := false
	minoritySummaryCalled := false
	summary.AcceptF = func() (bool, error) {
		majoritySummaryCalled = true
		return true, nil
	}
	minoritySummary.AcceptF = func() (bool, error) {
		minoritySummaryCalled = true
		return true, nil
	}

	// let a majority of voters return summaryID, and a minority return minoritySummaryID. The rest timeout.
	cumulatedWeight := uint64(0)
	for syncer.pendingVoters.Len() != 0 {
		voterID, found := syncer.pendingVoters.Peek()
		require.True(found)
		reqID := contactedVoters[voterID]

		switch {
		case cumulatedWeight < commonCfg.Alpha/2:
			require.NoError(syncer.AcceptedStateSummary(
				voterID,
				reqID,
				[]ids.ID{summaryID, minoritySummaryID},
			))
			bw, _ := vdrs.GetWeight(voterID)
			cumulatedWeight += bw

		case cumulatedWeight < commonCfg.Alpha:
			require.NoError(syncer.AcceptedStateSummary(
				voterID,
				reqID,
				[]ids.ID{summaryID},
			))
			bw, _ := vdrs.GetWeight(voterID)
			cumulatedWeight += bw

		default:
			require.NoError(syncer.GetAcceptedStateSummaryFailed(
				voterID,
				reqID,
			))
		}
	}

	// check that finally summary is passed to VM
	require.True(majoritySummaryCalled)
	require.False(minoritySummaryCalled)
}

func TestVotingIsRestartedIfMajorityIsNotReachedDueToTimeouts(t *testing.T) {
	require := require.New(t)

	vdrs := buildTestPeers(t)
	startupAlpha := (3*vdrs.Weight() + 3) / 4

	peers := tracker.NewPeers()
	startup := tracker.NewStartup(peers, startupAlpha)
	vdrs.RegisterCallbackListener(startup)

	commonCfg := common.Config{
		Ctx:                         snow.DefaultConsensusContextTest(),
		Beacons:                     vdrs,
		SampleK:                     vdrs.Len(),
		Alpha:                       (vdrs.Weight() + 1) / 2,
		StartupTracker:              startup,
		RetryBootstrap:              true, // this sets RetryStateSyncing too
		RetryBootstrapWarnFrequency: 1,    // this sets RetrySyncingWarnFrequency too
	}
	syncer, fullVM, sender := buildTestsObjects(t, &commonCfg)

	// set sender to track nodes reached out
	contactedFrontiersProviders := make(map[ids.NodeID]uint32) // nodeID -> reqID map
	sender.CantSendGetStateSummaryFrontier = true
	sender.SendGetStateSummaryFrontierF = func(ss ids.NodeIDSet, reqID uint32) {
		for nodeID := range ss {
			contactedFrontiersProviders[nodeID] = reqID
		}
	}

	// mock VM to simulate a valid summary is returned
	minoritySummary := &block.TestStateSummary{
		HeightV: minorityKey,
		IDV:     minoritySummaryID,
		BytesV:  minoritySummaryBytes,
		T:       t,
	}
	fullVM.CantParseStateSummary = true
	fullVM.ParseStateSummaryF = func(summaryBytes []byte) (block.StateSummary, error) {
		return minoritySummary, nil
	}

	contactedVoters := make(map[ids.NodeID]uint32) // nodeID -> reqID map
	sender.CantSendGetAcceptedStateSummary = true
	sender.SendGetAcceptedStateSummaryF = func(ss ids.NodeIDSet, reqID uint32, sl []uint64) {
		for nodeID := range ss {
			contactedVoters[nodeID] = reqID
		}
	}

	// Connect enough stake to start syncer
	for _, vdr := range vdrs.List() {
		require.NoError(syncer.Connected(vdr.ID(), version.CurrentApp))
	}
	require.True(syncer.pendingSeeders.Len() != 0)

	// let all contacted vdrs respond
	for syncer.pendingSeeders.Len() != 0 {
		beaconID, found := syncer.pendingSeeders.Peek()
		require.True(found)
		reqID := contactedFrontiersProviders[beaconID]

		require.NoError(syncer.StateSummaryFrontier(
			beaconID,
			reqID,
			summaryBytes,
		))
	}
	require.False(syncer.pendingSeeders.Len() != 0)

	minoritySummaryCalled := false
	minoritySummary.AcceptF = func() (bool, error) {
		minoritySummaryCalled = true
		return true, nil
	}

	// Let a majority of voters timeout.
	timedOutWeight := uint64(0)
	for syncer.pendingVoters.Len() != 0 {
		voterID, found := syncer.pendingVoters.Peek()
		require.True(found)
		reqID := contactedVoters[voterID]

		// vdr carries the largest weight by far. Make sure it fails
		if timedOutWeight <= commonCfg.Alpha {
			require.NoError(syncer.GetAcceptedStateSummaryFailed(
				voterID,
				reqID,
			))
			bw, _ := vdrs.GetWeight(voterID)
			timedOutWeight += bw
		} else {
			require.NoError(syncer.AcceptedStateSummary(
				voterID,
				reqID,
				[]ids.ID{summaryID},
			))
		}
	}

	// No state summary is passed to VM
	require.False(minoritySummaryCalled)

	// instead the whole process is restared
	require.False(syncer.pendingVoters.Len() != 0) // no voters reached
	require.True(syncer.pendingSeeders.Len() != 0) // frontiers providers reached again
}

func TestStateSyncIsStoppedIfEnoughVotesAreCastedWithNoClearMajority(t *testing.T) {
	require := require.New(t)

	vdrs := buildTestPeers(t)
	startupAlpha := (3*vdrs.Weight() + 3) / 4

	peers := tracker.NewPeers()
	startup := tracker.NewStartup(peers, startupAlpha)
	vdrs.RegisterCallbackListener(startup)

	commonCfg := common.Config{
		Ctx:            snow.DefaultConsensusContextTest(),
		Beacons:        vdrs,
		SampleK:        vdrs.Len(),
		Alpha:          (vdrs.Weight() + 1) / 2,
		StartupTracker: startup,
	}
	syncer, fullVM, sender := buildTestsObjects(t, &commonCfg)

	// set sender to track nodes reached out
	contactedFrontiersProviders := make(map[ids.NodeID]uint32) // nodeID -> reqID map
	sender.CantSendGetStateSummaryFrontier = true
	sender.SendGetStateSummaryFrontierF = func(ss ids.NodeIDSet, reqID uint32) {
		for nodeID := range ss {
			contactedFrontiersProviders[nodeID] = reqID
		}
	}

	// mock VM to simulate a valid minoritySummary1 is returned
	minoritySummary1 := &block.TestStateSummary{
		HeightV: key,
		IDV:     summaryID,
		BytesV:  summaryBytes,
		T:       t,
	}
	minoritySummary2 := &block.TestStateSummary{
		HeightV: minorityKey,
		IDV:     minoritySummaryID,
		BytesV:  minoritySummaryBytes,
		T:       t,
	}

	fullVM.CantParseStateSummary = true
	fullVM.ParseStateSummaryF = func(b []byte) (block.StateSummary, error) {
		switch {
		case bytes.Equal(b, summaryBytes):
			return minoritySummary1, nil
		case bytes.Equal(b, minoritySummaryBytes):
			return minoritySummary2, nil
		default:
			return nil, errors.New("unknown state summary")
		}
	}

	contactedVoters := make(map[ids.NodeID]uint32) // nodeID -> reqID map
	sender.CantSendGetAcceptedStateSummary = true
	sender.SendGetAcceptedStateSummaryF = func(ss ids.NodeIDSet, reqID uint32, sl []uint64) {
		for nodeID := range ss {
			contactedVoters[nodeID] = reqID
		}
	}

	// Connect enough stake to start syncer
	for _, vdr := range vdrs.List() {
		require.NoError(syncer.Connected(vdr.ID(), version.CurrentApp))
	}
	require.True(syncer.pendingSeeders.Len() != 0)

	// let all contacted vdrs respond with majority or minority summaries
	for {
		reachedSeeders := syncer.pendingSeeders.Len()
		if reachedSeeders == 0 {
			break
		}
		beaconID, found := syncer.pendingSeeders.Peek()
		require.True(found)
		reqID := contactedFrontiersProviders[beaconID]

		if reachedSeeders%2 == 0 {
			require.NoError(syncer.StateSummaryFrontier(
				beaconID,
				reqID,
				summaryBytes,
			))
		} else {
			require.NoError(syncer.StateSummaryFrontier(
				beaconID,
				reqID,
				minoritySummaryBytes,
			))
		}
	}
	require.False(syncer.pendingSeeders.Len() != 0)

	majoritySummaryCalled := false
	minoritySummaryCalled := false
	minoritySummary1.AcceptF = func() (bool, error) {
		majoritySummaryCalled = true
		return true, nil
	}
	minoritySummary2.AcceptF = func() (bool, error) {
		minoritySummaryCalled = true
		return true, nil
	}

	stateSyncFullyDone := false
	syncer.onDoneStateSyncing = func(lastReqID uint32) error {
		stateSyncFullyDone = true
		return nil
	}

	// let all votes respond in time without any summary reaching a majority.
	// We achieve it by making most nodes voting for an invalid summaryID.
	votingWeightStake := uint64(0)
	for syncer.pendingVoters.Len() != 0 {
		voterID, found := syncer.pendingVoters.Peek()
		require.True(found)
		reqID := contactedVoters[voterID]

		switch {
		case votingWeightStake < commonCfg.Alpha/2:
			require.NoError(syncer.AcceptedStateSummary(
				voterID,
				reqID,
				[]ids.ID{minoritySummary1.ID(), minoritySummary2.ID()},
			))
			bw, _ := vdrs.GetWeight(voterID)
			votingWeightStake += bw

		default:
			require.NoError(syncer.AcceptedStateSummary(
				voterID,
				reqID,
				[]ids.ID{{'u', 'n', 'k', 'n', 'o', 'w', 'n', 'I', 'D'}},
			))
			bw, _ := vdrs.GetWeight(voterID)
			votingWeightStake += bw
		}
	}

	// check that finally summary is passed to VM
	require.False(majoritySummaryCalled)
	require.False(minoritySummaryCalled)
	require.True(stateSyncFullyDone) // no restart, just move to boostrapping
}

func TestStateSyncIsDoneOnceVMNotifies(t *testing.T) {
	require := require.New(t)

	vdrs := buildTestPeers(t)
	startupAlpha := (3*vdrs.Weight() + 3) / 4

	peers := tracker.NewPeers()
	startup := tracker.NewStartup(peers, startupAlpha)
	vdrs.RegisterCallbackListener(startup)

	commonCfg := common.Config{
		Ctx:                         snow.DefaultConsensusContextTest(),
		Beacons:                     vdrs,
		SampleK:                     vdrs.Len(),
		Alpha:                       (vdrs.Weight() + 1) / 2,
		StartupTracker:              startup,
		RetryBootstrap:              true, // this sets RetryStateSyncing too
		RetryBootstrapWarnFrequency: 1,    // this sets RetrySyncingWarnFrequency too
	}
	syncer, fullVM, _ := buildTestsObjects(t, &commonCfg)
	_ = fullVM

	stateSyncFullyDone := false
	syncer.onDoneStateSyncing = func(lastReqID uint32) error {
		stateSyncFullyDone = true
		return nil
	}

	// Any Put response before StateSyncDone is received from VM is dropped
	require.NoError(syncer.Notify(common.StateSyncDone))
	require.True(stateSyncFullyDone)
}

```

avalanchego/snow/engine/snowman/syncer/utils_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package syncer

import (
	"testing"

	"github.com/stretchr/testify/require"

	"github.com/ava-labs/avalanchego/database"
	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/engine/common"
	"github.com/ava-labs/avalanchego/snow/engine/snowman/block"
	"github.com/ava-labs/avalanchego/snow/engine/snowman/getter"
	"github.com/ava-labs/avalanchego/snow/validators"
	"github.com/ava-labs/avalanchego/utils/hashing"
)

const (
	key         uint64 = 2022
	minorityKey uint64 = 2000
)

var (
	_ block.ChainVM         = fullVM{}
	_ block.StateSyncableVM = fullVM{}

	unknownSummaryID = ids.ID{'g', 'a', 'r', 'b', 'a', 'g', 'e'}

	summaryBytes = []byte{'s', 'u', 'm', 'm', 'a', 'r', 'y'}
	summaryID    ids.ID

	minoritySummaryBytes = []byte{'m', 'i', 'n', 'o', 'r', 'i', 't', 'y'}
	minoritySummaryID    ids.ID
)

func init() {
	var err error
	summaryID, err = ids.ToID(hashing.ComputeHash256(summaryBytes))
	if err != nil {
		panic(err)
	}

	minoritySummaryID, err = ids.ToID(hashing.ComputeHash256(minoritySummaryBytes))
	if err != nil {
		panic(err)
	}
}

type fullVM struct {
	*block.TestVM
	*block.TestStateSyncableVM
}

func buildTestPeers(t *testing.T) validators.Set {
	// we consider more than common.MaxOutstandingBroadcastRequests peers
	// so to test the effect of cap on number of requests sent out
	vdrs := validators.NewSet()
	for idx := 0; idx < 2*common.MaxOutstandingBroadcastRequests; idx++ {
		beaconID := ids.GenerateTestNodeID()
		require.NoError(t, vdrs.AddWeight(beaconID, uint64(1)))
	}
	return vdrs
}

func buildTestsObjects(t *testing.T, commonCfg *common.Config) (
	*stateSyncer,
	*fullVM,
	*common.SenderTest,
) {
	sender := &common.SenderTest{T: t}
	commonCfg.Sender = sender

	fullVM := &fullVM{
		TestVM: &block.TestVM{
			TestVM: common.TestVM{T: t},
		},
		TestStateSyncableVM: &block.TestStateSyncableVM{
			T: t,
		},
	}
	dummyGetter, err := getter.New(fullVM, *commonCfg)
	require.NoError(t, err)

	cfg, err := NewConfig(*commonCfg, nil, dummyGetter, fullVM)
	require.NoError(t, err)
	commonSyncer := New(cfg, func(lastReqID uint32) error { return nil })
	syncer, ok := commonSyncer.(*stateSyncer)
	require.True(t, ok)
	require.True(t, syncer.stateSyncVM != nil)

	fullVM.GetOngoingSyncStateSummaryF = func() (block.StateSummary, error) {
		return nil, database.ErrNotFound
	}

	return syncer, fullVM, sender
}

func pickRandomFrom(nodes map[ids.NodeID]uint32) ids.NodeID {
	for node := range nodes {
		return node
	}
	return ids.EmptyNodeID
}

```

avalanchego/snow/engine/snowman/test_snowman_engine.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowman

import (
	"errors"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/consensus/snowman"
	"github.com/ava-labs/avalanchego/snow/engine/common"
)

var (
	_ Engine = &EngineTest{}

	errGetBlock = errors.New("unexpectedly called GetBlock")
)

// EngineTest is a test engine
type EngineTest struct {
	common.EngineTest

	CantGetBlock bool
	GetBlockF    func(ids.ID) (snowman.Block, error)
}

func (e *EngineTest) Default(cant bool) {
	e.EngineTest.Default(cant)
	e.CantGetBlock = false
}

func (e *EngineTest) GetBlock(blkID ids.ID) (snowman.Block, error) {
	if e.GetBlockF != nil {
		return e.GetBlockF(blkID)
	}
	if e.CantGetBlock && e.T != nil {
		e.T.Fatalf("Unexpectedly called GetBlock")
	}
	return nil, errGetBlock
}

```

avalanchego/snow/engine/snowman/transitive.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowman

import (
	"fmt"
	"time"

	"go.uber.org/zap"

	"github.com/ava-labs/avalanchego/cache"
	"github.com/ava-labs/avalanchego/cache/metercacher"
	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow"
	"github.com/ava-labs/avalanchego/snow/choices"
	"github.com/ava-labs/avalanchego/snow/consensus/snowman"
	"github.com/ava-labs/avalanchego/snow/consensus/snowman/poll"
	"github.com/ava-labs/avalanchego/snow/engine/common"
	"github.com/ava-labs/avalanchego/snow/events"
	"github.com/ava-labs/avalanchego/utils/wrappers"
	"github.com/ava-labs/avalanchego/version"
)

const nonVerifiedCacheSize = 128

var _ Engine = &Transitive{}

func New(config Config) (Engine, error) {
	return newTransitive(config)
}

// Transitive implements the Engine interface by attempting to fetch all
// Transitive dependencies.
type Transitive struct {
	Config
	metrics

	// list of NoOpsHandler for messages dropped by engine
	common.StateSummaryFrontierHandler
	common.AcceptedStateSummaryHandler
	common.AcceptedFrontierHandler
	common.AcceptedHandler
	common.AncestorsHandler

	RequestID uint32

	// track outstanding preference requests
	polls poll.Set

	// blocks that have we have sent get requests for but haven't yet received
	blkReqs common.Requests

	// blocks that are queued to be issued to consensus once missing dependencies are fetched
	// Block ID --> Block
	pending map[ids.ID]snowman.Block

	// Block ID --> Parent ID
	nonVerifieds AncestorTree

	// Block ID --> Block.
	// A block is put into this cache if it was not able to be issued. A block
	// fails to be issued if verification on the block or one of its ancestors
	// occurs.
	nonVerifiedCache cache.Cacher

	// operations that are blocked on a block being issued. This could be
	// issuing another block, responding to a query, or applying votes to consensus
	blocked events.Blocker

	// number of times build block needs to be called once the number of
	// processing blocks has gone below the optimal number.
	pendingBuildBlocks int

	// errs tracks if an error has occurred in a callback
	errs wrappers.Errs
}

func newTransitive(config Config) (*Transitive, error) {
	config.Ctx.Log.Info("initializing consensus engine")

	nonVerifiedCache, err := metercacher.New(
		"non_verified_cache",
		config.Ctx.Registerer,
		&cache.LRU{Size: nonVerifiedCacheSize},
	)
	if err != nil {
		return nil, err
	}
	factory := poll.NewEarlyTermNoTraversalFactory(config.Params.Alpha)
	t := &Transitive{
		Config:                      config,
		StateSummaryFrontierHandler: common.NewNoOpStateSummaryFrontierHandler(config.Ctx.Log),
		AcceptedStateSummaryHandler: common.NewNoOpAcceptedStateSummaryHandler(config.Ctx.Log),
		AcceptedFrontierHandler:     common.NewNoOpAcceptedFrontierHandler(config.Ctx.Log),
		AcceptedHandler:             common.NewNoOpAcceptedHandler(config.Ctx.Log),
		AncestorsHandler:            common.NewNoOpAncestorsHandler(config.Ctx.Log),
		pending:                     make(map[ids.ID]snowman.Block),
		nonVerifieds:                NewAncestorTree(),
		nonVerifiedCache:            nonVerifiedCache,
		polls: poll.NewSet(factory,
			config.Ctx.Log,
			"",
			config.Ctx.Registerer,
		),
	}

	return t, t.metrics.Initialize("", config.Ctx.Registerer)
}

func (t *Transitive) Put(nodeID ids.NodeID, requestID uint32, blkBytes []byte) error {
	blk, err := t.VM.ParseBlock(blkBytes)
	if err != nil {
		t.Ctx.Log.Debug("failed to parse block",
			zap.Stringer("nodeID", nodeID),
			zap.Uint32("requestID", requestID),
			zap.Error(err),
		)
		t.Ctx.Log.Verbo("failed to parse block",
			zap.Stringer("nodeID", nodeID),
			zap.Uint32("requestID", requestID),
			zap.Binary("block", blkBytes),
			zap.Error(err),
		)
		// because GetFailed doesn't utilize the assumption that we actually
		// sent a Get message, we can safely call GetFailed here to potentially
		// abandon the request.
		return t.GetFailed(nodeID, requestID)
	}

	actualBlkID := blk.ID()
	expectedBlkID, ok := t.blkReqs.Get(nodeID, requestID)
	// If the provided block is not the requested block, we need to explicitly
	// mark the request as failed to avoid having a dangling dependency.
	if ok && actualBlkID != expectedBlkID {
		t.Ctx.Log.Debug("incorrect block returned in Put",
			zap.Stringer("nodeID", nodeID),
			zap.Uint32("requestID", requestID),
			zap.Stringer("blkID", actualBlkID),
			zap.Stringer("expectedBlkID", expectedBlkID),
		)
		// We assume that [blk] is useless because it doesn't match what we
		// expected.
		return t.GetFailed(nodeID, requestID)
	}

	if t.wasIssued(blk) {
		t.metrics.numUselessPutBytes.Add(float64(len(blkBytes)))
	}

	// issue the block into consensus. If the block has already been issued,
	// this will be a noop. If this block has missing dependencies, vdr will
	// receive requests to fill the ancestry. dependencies that have already
	// been fetched, but with missing dependencies themselves won't be requested
	// from the vdr.
	if _, err := t.issueFrom(nodeID, blk); err != nil {
		return err
	}
	return t.buildBlocks()
}

func (t *Transitive) GetFailed(nodeID ids.NodeID, requestID uint32) error {
	// We don't assume that this function is called after a failed Get message.
	// Check to see if we have an outstanding request and also get what the request was for if it exists.
	blkID, ok := t.blkReqs.Remove(nodeID, requestID)
	if !ok {
		t.Ctx.Log.Debug("unexpected GetFailed",
			zap.Stringer("nodeID", nodeID),
			zap.Uint32("requestID", requestID),
		)
		return nil
	}

	// Because the get request was dropped, we no longer expect blkID to be issued.
	t.blocked.Abandon(blkID)
	t.metrics.numRequests.Set(float64(t.blkReqs.Len()))
	t.metrics.numBlockers.Set(float64(t.blocked.Len()))
	return t.buildBlocks()
}

func (t *Transitive) PullQuery(nodeID ids.NodeID, requestID uint32, blkID ids.ID) error {
	t.Sender.SendChits(nodeID, requestID, []ids.ID{t.Consensus.Preference()})

	// Try to issue [blkID] to consensus.
	// If we're missing an ancestor, request it from [vdr]
	if _, err := t.issueFromByID(nodeID, blkID); err != nil {
		return err
	}

	return t.buildBlocks()
}

func (t *Transitive) PushQuery(nodeID ids.NodeID, requestID uint32, blkBytes []byte) error {
	t.Sender.SendChits(nodeID, requestID, []ids.ID{t.Consensus.Preference()})

	blk, err := t.VM.ParseBlock(blkBytes)
	// If parsing fails, we just drop the request, as we didn't ask for it
	if err != nil {
		t.Ctx.Log.Debug("failed to parse block",
			zap.Stringer("nodeID", nodeID),
			zap.Uint32("requestID", requestID),
			zap.Error(err),
		)
		t.Ctx.Log.Verbo("failed to parse block",
			zap.Stringer("nodeID", nodeID),
			zap.Uint32("requestID", requestID),
			zap.Binary("block", blkBytes),
			zap.Error(err),
		)
		return nil
	}

	if t.wasIssued(blk) {
		t.metrics.numUselessPushQueryBytes.Add(float64(len(blkBytes)))
	}

	// issue the block into consensus. If the block has already been issued,
	// this will be a noop. If this block has missing dependencies, nodeID will
	// receive requests to fill the ancestry. dependencies that have already
	// been fetched, but with missing dependencies themselves won't be requested
	// from the vdr.
	if _, err := t.issueFrom(nodeID, blk); err != nil {
		return err
	}

	return t.buildBlocks()
}

func (t *Transitive) Chits(nodeID ids.NodeID, requestID uint32, votes []ids.ID) error {
	// Since this is a linear chain, there should only be one ID in the vote set
	if len(votes) != 1 {
		t.Ctx.Log.Debug("failing Chits",
			zap.String("reason", "expected only 1 vote"),
			zap.Int("numVotes", len(votes)),
			zap.Stringer("nodeID", nodeID),
			zap.Uint32("requestID", requestID),
		)
		// because QueryFailed doesn't utilize the assumption that we actually
		// sent a Query message, we can safely call QueryFailed here to
		// potentially abandon the request.
		return t.QueryFailed(nodeID, requestID)
	}
	blkID := votes[0]

	t.Ctx.Log.Verbo("called Chits for the block",
		zap.Stringer("blkID", blkID),
		zap.Stringer("nodeID", nodeID),
		zap.Uint32("requestID", requestID))

	// Will record chits once [blkID] has been issued into consensus
	v := &voter{
		t:         t,
		vdr:       nodeID,
		requestID: requestID,
		response:  blkID,
	}

	added, err := t.issueFromByID(nodeID, blkID)
	if err != nil {
		return err
	}
	// Wait until [blkID] has been issued to consensus before applying this chit.
	if !added {
		v.deps.Add(blkID)
	}

	t.blocked.Register(v)
	t.metrics.numBlockers.Set(float64(t.blocked.Len()))
	return t.buildBlocks()
}

func (t *Transitive) QueryFailed(vdr ids.NodeID, requestID uint32) error {
	t.blocked.Register(&voter{
		t:         t,
		vdr:       vdr,
		requestID: requestID,
	})
	t.metrics.numBlockers.Set(float64(t.blocked.Len()))
	return t.buildBlocks()
}

func (t *Transitive) AppRequest(nodeID ids.NodeID, requestID uint32, deadline time.Time, request []byte) error {
	// Notify the VM of this request
	return t.VM.AppRequest(nodeID, requestID, deadline, request)
}

func (t *Transitive) AppRequestFailed(nodeID ids.NodeID, requestID uint32) error {
	// Notify the VM that a request it made failed
	return t.VM.AppRequestFailed(nodeID, requestID)
}

func (t *Transitive) AppResponse(nodeID ids.NodeID, requestID uint32, response []byte) error {
	// Notify the VM of a response to its request
	return t.VM.AppResponse(nodeID, requestID, response)
}

func (t *Transitive) AppGossip(nodeID ids.NodeID, msg []byte) error {
	// Notify the VM of this message which has been gossiped to it
	return t.VM.AppGossip(nodeID, msg)
}

func (t *Transitive) Connected(nodeID ids.NodeID, nodeVersion *version.Application) error {
	return t.VM.Connected(nodeID, nodeVersion)
}

func (t *Transitive) Disconnected(nodeID ids.NodeID) error {
	return t.VM.Disconnected(nodeID)
}

func (t *Transitive) Timeout() error { return nil }

func (t *Transitive) Gossip() error {
	blkID, err := t.VM.LastAccepted()
	if err != nil {
		return err
	}
	blk, err := t.GetBlock(blkID)
	if err != nil {
		t.Ctx.Log.Warn("dropping gossip request",
			zap.String("reason", "block couldn't be loaded"),
			zap.Stringer("blkID", blkID),
			zap.Error(err),
		)
		return nil
	}
	t.Ctx.Log.Verbo("gossiping accepted block to the network",
		zap.Stringer("blkID", blkID),
	)
	t.Sender.SendGossip(blk.Bytes())
	return nil
}

func (t *Transitive) Halt() {}

func (t *Transitive) Shutdown() error {
	t.Ctx.Log.Info("shutting down consensus engine")
	return t.VM.Shutdown()
}

func (t *Transitive) Notify(msg common.Message) error {
	if msg != common.PendingTxs {
		t.Ctx.Log.Warn("received an unexpected message from the VM",
			zap.Stringer("messageString", msg),
		)
		return nil
	}

	// the pending txs message means we should attempt to build a block.
	t.pendingBuildBlocks++
	return t.buildBlocks()
}

func (t *Transitive) Context() *snow.ConsensusContext {
	return t.Ctx
}

func (t *Transitive) Start(startReqID uint32) error {
	t.RequestID = startReqID
	lastAcceptedID, err := t.VM.LastAccepted()
	if err != nil {
		return err
	}
	lastAccepted, err := t.GetBlock(lastAcceptedID)
	if err != nil {
		t.Ctx.Log.Error("failed to get last accepted block",
			zap.Error(err),
		)
		return err
	}

	// initialize consensus to the last accepted blockID
	if err := t.Consensus.Initialize(t.Ctx, t.Params, lastAcceptedID, lastAccepted.Height()); err != nil {
		return err
	}

	// to maintain the invariant that oracle blocks are issued in the correct
	// preferences, we need to handle the case that we are bootstrapping into an oracle block
	if oracleBlk, ok := lastAccepted.(snowman.OracleBlock); ok {
		options, err := oracleBlk.Options()
		switch {
		case err == snowman.ErrNotOracle:
			// if there aren't blocks we need to deliver on startup, we need to set
			// the preference to the last accepted block
			if err := t.VM.SetPreference(lastAcceptedID); err != nil {
				return err
			}
		case err != nil:
			return err
		default:
			for _, blk := range options {
				// note that deliver will set the VM's preference
				if err := t.deliver(blk); err != nil {
					return err
				}
			}
		}
	} else if err := t.VM.SetPreference(lastAcceptedID); err != nil {
		return err
	}

	t.Ctx.Log.Info("consensus starting",
		zap.Stringer("lastAcceptedBlock", lastAcceptedID),
	)
	t.metrics.bootstrapFinished.Set(1)

	t.Ctx.SetState(snow.NormalOp)
	if err := t.VM.SetState(snow.NormalOp); err != nil {
		return fmt.Errorf("failed to notify VM that consensus is starting: %w",
			err)
	}
	return nil
}

func (t *Transitive) HealthCheck() (interface{}, error) {
	consensusIntf, consensusErr := t.Consensus.HealthCheck()
	vmIntf, vmErr := t.VM.HealthCheck()
	intf := map[string]interface{}{
		"consensus": consensusIntf,
		"vm":        vmIntf,
	}
	if consensusErr == nil {
		return intf, vmErr
	}
	if vmErr == nil {
		return intf, consensusErr
	}
	return intf, fmt.Errorf("vm: %s ; consensus: %s", vmErr, consensusErr)
}

func (t *Transitive) GetVM() common.VM {
	return t.VM
}

func (t *Transitive) GetBlock(blkID ids.ID) (snowman.Block, error) {
	if blk, ok := t.pending[blkID]; ok {
		return blk, nil
	}
	if blk, ok := t.nonVerifiedCache.Get(blkID); ok {
		return blk.(snowman.Block), nil
	}
	return t.VM.GetBlock(blkID)
}

// Build blocks if they have been requested and the number of processing blocks
// is less than optimal.
func (t *Transitive) buildBlocks() error {
	if err := t.errs.Err; err != nil {
		return err
	}
	for t.pendingBuildBlocks > 0 && t.Consensus.NumProcessing() < t.Params.OptimalProcessing {
		t.pendingBuildBlocks--

		blk, err := t.VM.BuildBlock()
		if err != nil {
			t.Ctx.Log.Debug("failed building block",
				zap.Error(err),
			)
			t.numBuildsFailed.Inc()
			return nil
		}
		t.numBuilt.Inc()

		// a newly created block is expected to be processing. If this check
		// fails, there is potentially an error in the VM this engine is running
		if status := blk.Status(); status != choices.Processing {
			t.Ctx.Log.Warn("attempting to issue block with unexpected status",
				zap.Stringer("expectedStatus", choices.Processing),
				zap.Stringer("status", status),
			)
		}

		// The newly created block should be built on top of the preferred block.
		// Otherwise, the new block doesn't have the best chance of being confirmed.
		parentID := blk.Parent()
		if pref := t.Consensus.Preference(); parentID != pref {
			t.Ctx.Log.Warn("built block with unexpected parent",
				zap.Stringer("expectedParentID", pref),
				zap.Stringer("parentID", parentID),
			)
		}

		added, err := t.issueWithAncestors(blk)
		if err != nil {
			return err
		}

		// issuing the block shouldn't have any missing dependencies
		if added {
			t.Ctx.Log.Verbo("successfully issued new block from the VM")
		} else {
			t.Ctx.Log.Warn("built block with unissued ancestors")
		}
	}
	return nil
}

// Issue another poll to the network, asking what it prefers given the block we prefer.
// Helps move consensus along.
func (t *Transitive) repoll() {
	// if we are issuing a repoll, we should gossip our current preferences to
	// propagate the most likely branch as quickly as possible
	prefID := t.Consensus.Preference()

	for i := t.polls.Len(); i < t.Params.ConcurrentRepolls; i++ {
		t.pullQuery(prefID)
	}
}

// issueFromByID attempts to issue the branch ending with a block [blkID] into consensus.
// If we do not have [blkID], request it.
// Returns true if the block is processing in consensus or is decided.
func (t *Transitive) issueFromByID(nodeID ids.NodeID, blkID ids.ID) (bool, error) {
	blk, err := t.GetBlock(blkID)
	if err != nil {
		t.sendRequest(nodeID, blkID)
		return false, nil
	}
	return t.issueFrom(nodeID, blk)
}

// issueFrom attempts to issue the branch ending with block [blkID] to consensus.
// Returns true if the block is processing in consensus or is decided.
// If a dependency is missing, request it from [vdr].
func (t *Transitive) issueFrom(nodeID ids.NodeID, blk snowman.Block) (bool, error) {
	blkID := blk.ID()
	// issue [blk] and its ancestors to consensus.
	for !t.wasIssued(blk) {
		if err := t.issue(blk); err != nil {
			return false, err
		}

		blkID = blk.Parent()
		var err error
		blk, err = t.GetBlock(blkID)

		// If we don't have this ancestor, request it from [vdr]
		if err != nil || !blk.Status().Fetched() {
			t.sendRequest(nodeID, blkID)
			return false, nil
		}
	}

	// Remove any outstanding requests for this block
	t.blkReqs.RemoveAny(blkID)

	issued := t.Consensus.Decided(blk) || t.Consensus.Processing(blkID)
	if issued {
		// A dependency should never be waiting on a decided or processing
		// block. However, if the block was marked as rejected by the VM, the
		// dependencies may still be waiting. Therefore, they should abandoned.
		t.blocked.Abandon(blkID)
	}

	// Tracks performance statistics
	t.metrics.numRequests.Set(float64(t.blkReqs.Len()))
	t.metrics.numBlockers.Set(float64(t.blocked.Len()))
	return issued, t.errs.Err
}

// issueWithAncestors attempts to issue the branch ending with [blk] to consensus.
// Returns true if the block is processing in consensus or is decided.
// If a dependency is missing and the dependency hasn't been requested, the issuance will be abandoned.
func (t *Transitive) issueWithAncestors(blk snowman.Block) (bool, error) {
	blkID := blk.ID()
	// issue [blk] and its ancestors into consensus
	status := blk.Status()
	for status.Fetched() && !t.wasIssued(blk) {
		if err := t.issue(blk); err != nil {
			return false, err
		}
		blkID = blk.Parent()
		var err error
		if blk, err = t.GetBlock(blkID); err != nil {
			status = choices.Unknown
			break
		}
		status = blk.Status()
	}

	// The block was issued into consensus. This is the happy path.
	if status != choices.Unknown && (t.Consensus.Decided(blk) || t.Consensus.Processing(blkID)) {
		return true, nil
	}

	// There's an outstanding request for this block.
	// We can just wait for that request to succeed or fail.
	if t.blkReqs.Contains(blkID) {
		return false, nil
	}

	// We don't have this block and have no reason to expect that we will get it.
	// Abandon the block to avoid a memory leak.
	t.blocked.Abandon(blkID)
	t.metrics.numBlockers.Set(float64(t.blocked.Len()))
	return false, t.errs.Err
}

// If the block has been decided, then it is marked as having been issued.
// If the block is processing, then it was issued.
// If the block is queued to be added to consensus, then it was issued.
func (t *Transitive) wasIssued(blk snowman.Block) bool {
	blkID := blk.ID()
	return t.Consensus.Decided(blk) || t.Consensus.Processing(blkID) || t.pendingContains(blkID)
}

// Issue [blk] to consensus once its ancestors have been issued.
func (t *Transitive) issue(blk snowman.Block) error {
	blkID := blk.ID()

	// mark that the block is queued to be added to consensus once its ancestors have been
	t.pending[blkID] = blk

	// Remove any outstanding requests for this block
	t.blkReqs.RemoveAny(blkID)

	// Will add [blk] to consensus once its ancestors have been
	i := &issuer{
		t:   t,
		blk: blk,
	}

	// block on the parent if needed
	parentID := blk.Parent()
	if parent, err := t.GetBlock(parentID); err != nil || !(t.Consensus.Decided(parent) || t.Consensus.Processing(parentID)) {
		t.Ctx.Log.Verbo("block waiting for parent to be issued",
			zap.Stringer("blkID", blkID),
			zap.Stringer("parentID", parentID),
		)
		i.deps.Add(parentID)
	}

	t.blocked.Register(i)

	// Tracks performance statistics
	t.metrics.numRequests.Set(float64(t.blkReqs.Len()))
	t.metrics.numBlocked.Set(float64(len(t.pending)))
	t.metrics.numBlockers.Set(float64(t.blocked.Len()))
	return t.errs.Err
}

// Request that [vdr] send us block [blkID]
func (t *Transitive) sendRequest(nodeID ids.NodeID, blkID ids.ID) {
	// There is already an outstanding request for this block
	if t.blkReqs.Contains(blkID) {
		return
	}

	t.RequestID++
	t.blkReqs.Add(nodeID, t.RequestID, blkID)
	t.Ctx.Log.Verbo("sending Get request",
		zap.Stringer("nodeID", nodeID),
		zap.Uint32("requestID", t.RequestID),
		zap.Stringer("blkID", blkID),
	)
	t.Sender.SendGet(nodeID, t.RequestID, blkID)

	// Tracks performance statistics
	t.metrics.numRequests.Set(float64(t.blkReqs.Len()))
}

// send a pull query for this block ID
func (t *Transitive) pullQuery(blkID ids.ID) {
	t.Ctx.Log.Verbo("sampling from validators",
		zap.Stringer("validators", t.Validators),
	)
	// The validators we will query
	vdrs, err := t.Validators.Sample(t.Params.K)
	if err != nil {
		t.Ctx.Log.Error("dropped query for block",
			zap.String("reason", "insufficient number of validators"),
			zap.Stringer("blkID", blkID),
		)
		return
	}

	vdrBag := ids.NodeIDBag{}
	for _, vdr := range vdrs {
		vdrBag.Add(vdr.ID())
	}

	t.RequestID++
	if t.polls.Add(t.RequestID, vdrBag) {
		vdrList := vdrBag.List()
		vdrSet := ids.NewNodeIDSet(len(vdrList))
		vdrSet.Add(vdrList...)
		t.Sender.SendPullQuery(vdrSet, t.RequestID, blkID)
	}
}

// Send a query for this block. Some validators will be sent
// a Push Query and some will be sent a Pull Query.
func (t *Transitive) sendMixedQuery(blk snowman.Block) {
	t.Ctx.Log.Verbo("sampling from validators",
		zap.Stringer("validators", t.Validators),
	)
	vdrs, err := t.Validators.Sample(t.Params.K)
	if err != nil {
		t.Ctx.Log.Error("dropped query for block",
			zap.String("reason", "insufficient number of validators"),
			zap.Stringer("blkID", blk.ID()),
		)
		return
	}

	vdrBag := ids.NodeIDBag{}
	for _, vdr := range vdrs {
		vdrBag.Add(vdr.ID())
	}

	t.RequestID++
	if t.polls.Add(t.RequestID, vdrBag) {
		// Send a push query to some of the validators, and a pull query to the rest.
		numPushTo := t.Params.MixedQueryNumPushVdr
		if !t.Validators.Contains(t.Ctx.NodeID) {
			numPushTo = t.Params.MixedQueryNumPushNonVdr
		}
		common.SendMixedQuery(
			t.Sender,
			vdrBag.List(), // Note that this doesn't contain duplicates; length may be < k
			numPushTo,
			t.RequestID,
			blk.ID(),
			blk.Bytes(),
		)
	}
}

// issue [blk] to consensus
func (t *Transitive) deliver(blk snowman.Block) error {
	blkID := blk.ID()
	if t.Consensus.Decided(blk) || t.Consensus.Processing(blkID) {
		return nil
	}

	// we are no longer waiting on adding the block to consensus, so it is no
	// longer pending
	t.removeFromPending(blk)
	parentID := blk.Parent()
	parent, err := t.GetBlock(parentID)
	// Because the dependency must have been fulfilled by the time this function
	// is called - we don't expect [err] to be non-nil. But it is handled for
	// completness and future proofing.
	if err != nil || !(parent.Status() == choices.Accepted || t.Consensus.Processing(parentID)) {
		// if the parent isn't processing or the last accepted block, then this
		// block is effectively rejected
		t.blocked.Abandon(blkID)
		t.metrics.numBlocked.Set(float64(len(t.pending))) // Tracks performance statistics
		t.metrics.numBlockers.Set(float64(t.blocked.Len()))
		return t.errs.Err
	}

	// By ensuring that the parent is either processing or accepted, it is
	// guaranteed that the parent was successfully verified. This means that
	// calling Verify on this block is allowed.
	blkAdded, err := t.addUnverifiedBlockToConsensus(blk)
	if err != nil {
		return err
	}
	if !blkAdded {
		t.blocked.Abandon(blkID)
		t.metrics.numBlocked.Set(float64(len(t.pending))) // Tracks performance statistics
		t.metrics.numBlockers.Set(float64(t.blocked.Len()))
		return t.errs.Err
	}

	// Add all the oracle blocks if they exist. We call verify on all the blocks
	// and add them to consensus before marking anything as fulfilled to avoid
	// any potential reentrant bugs.
	added := []snowman.Block{}
	dropped := []snowman.Block{}
	if blk, ok := blk.(snowman.OracleBlock); ok {
		options, err := blk.Options()
		if err != snowman.ErrNotOracle {
			if err != nil {
				return err
			}

			for _, blk := range options {
				blkAdded, err := t.addUnverifiedBlockToConsensus(blk)
				if err != nil {
					return err
				}
				if blkAdded {
					added = append(added, blk)
				} else {
					dropped = append(dropped, blk)
				}
			}
		}
	}

	if err := t.VM.SetPreference(t.Consensus.Preference()); err != nil {
		return err
	}

	// If the block is now preferred, query the network for its preferences
	// with this new block.
	if t.Consensus.IsPreferred(blk) {
		t.sendMixedQuery(blk)
	}

	t.blocked.Fulfill(blkID)
	for _, blk := range added {
		if t.Consensus.IsPreferred(blk) {
			t.sendMixedQuery(blk)
		}

		blkID := blk.ID()
		t.removeFromPending(blk)
		t.blocked.Fulfill(blkID)
		t.blkReqs.RemoveAny(blkID)
	}
	for _, blk := range dropped {
		blkID := blk.ID()
		t.removeFromPending(blk)
		t.blocked.Abandon(blkID)
		t.blkReqs.RemoveAny(blkID)
	}

	// If we should issue multiple queries at the same time, we need to repoll
	t.repoll()

	// Tracks performance statistics
	t.metrics.numRequests.Set(float64(t.blkReqs.Len()))
	t.metrics.numBlocked.Set(float64(len(t.pending)))
	t.metrics.numBlockers.Set(float64(t.blocked.Len()))
	return t.errs.Err
}

// Returns true if the block whose ID is [blkID] is waiting to be issued to consensus
func (t *Transitive) pendingContains(blkID ids.ID) bool {
	_, ok := t.pending[blkID]
	return ok
}

func (t *Transitive) removeFromPending(blk snowman.Block) {
	delete(t.pending, blk.ID())
}

func (t *Transitive) addToNonVerifieds(blk snowman.Block) {
	// don't add this blk if it's decided or processing.
	blkID := blk.ID()
	if t.Consensus.Decided(blk) || t.Consensus.Processing(blkID) {
		return
	}
	parentID := blk.Parent()
	// we might still need this block so we can bubble votes to the parent
	// only add blocks with parent already in the tree or processing.
	// decided parents should not be in this map.
	if t.nonVerifieds.Has(parentID) || t.Consensus.Processing(parentID) {
		t.nonVerifieds.Add(blkID, parentID)
		t.nonVerifiedCache.Put(blkID, blk)
		t.metrics.numNonVerifieds.Set(float64(t.nonVerifieds.Len()))
	}
}

// addUnverifiedBlockToConsensus returns whether the block was added and an
// error if one occurred while adding it to consensus.
func (t *Transitive) addUnverifiedBlockToConsensus(blk snowman.Block) (bool, error) {
	// make sure this block is valid
	if err := blk.Verify(); err != nil {
		t.Ctx.Log.Debug("block verification failed",
			zap.Error(err),
		)

		// if verify fails, then all descendants are also invalid
		t.addToNonVerifieds(blk)
		return false, nil
	}

	blkID := blk.ID()
	t.nonVerifieds.Remove(blkID)
	t.nonVerifiedCache.Evict(blkID)
	t.metrics.numNonVerifieds.Set(float64(t.nonVerifieds.Len()))
	t.Ctx.Log.Verbo("adding block to consensus",
		zap.Stringer("blkID", blkID),
	)
	return true, t.Consensus.Add(&memoryBlock{
		Block:   blk,
		metrics: &t.metrics,
		tree:    t.nonVerifieds,
	})
}

```

avalanchego/snow/engine/snowman/transitive_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowman

import (
	"bytes"
	"errors"
	"fmt"
	"testing"

	"github.com/stretchr/testify/require"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/choices"
	"github.com/ava-labs/avalanchego/snow/consensus/snowball"
	"github.com/ava-labs/avalanchego/snow/consensus/snowman"
	"github.com/ava-labs/avalanchego/snow/engine/common"
	"github.com/ava-labs/avalanchego/snow/engine/snowman/block"
	"github.com/ava-labs/avalanchego/snow/engine/snowman/getter"
	"github.com/ava-labs/avalanchego/snow/validators"
	"github.com/ava-labs/avalanchego/utils/constants"
	"github.com/ava-labs/avalanchego/utils/wrappers"
)

var (
	errUnknownBlock = errors.New("unknown block")
	errUnknownBytes = errors.New("unknown bytes")
	Genesis         = ids.GenerateTestID()
)

func setup(t *testing.T, commonCfg common.Config, engCfg Config) (ids.NodeID, validators.Set, *common.SenderTest, *block.TestVM, *Transitive, snowman.Block) {
	vals := validators.NewSet()
	engCfg.Validators = vals

	vdr := ids.GenerateTestNodeID()
	if err := vals.AddWeight(vdr, 1); err != nil {
		t.Fatal(err)
	}

	sender := &common.SenderTest{T: t}
	engCfg.Sender = sender
	commonCfg.Sender = sender
	sender.Default(true)

	vm := &block.TestVM{}
	vm.T = t
	engCfg.VM = vm

	snowGetHandler, err := getter.New(vm, commonCfg)
	if err != nil {
		t.Fatal(err)
	}
	engCfg.AllGetsServer = snowGetHandler

	vm.Default(true)
	vm.CantSetState = false
	vm.CantSetPreference = false

	gBlk := &snowman.TestBlock{TestDecidable: choices.TestDecidable{
		IDV:     Genesis,
		StatusV: choices.Accepted,
	}}

	vm.LastAcceptedF = func() (ids.ID, error) { return gBlk.ID(), nil }

	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		switch blkID {
		case gBlk.ID():
			return gBlk, nil
		default:
			return nil, errUnknownBlock
		}
	}

	te, err := newTransitive(engCfg)
	if err != nil {
		t.Fatal(err)
	}

	if err := te.Start(0); err != nil {
		t.Fatal(err)
	}

	vm.GetBlockF = nil
	vm.LastAcceptedF = nil
	return vdr, vals, sender, vm, te, gBlk
}

func setupDefaultConfig(t *testing.T) (ids.NodeID, validators.Set, *common.SenderTest, *block.TestVM, *Transitive, snowman.Block) {
	commonCfg := common.DefaultConfigTest()
	engCfg := DefaultConfigs()
	return setup(t, commonCfg, engCfg)
}

func TestEngineShutdown(t *testing.T) {
	_, _, _, vm, transitive, _ := setupDefaultConfig(t)
	vmShutdownCalled := false
	vm.ShutdownF = func() error { vmShutdownCalled = true; return nil }
	vm.CantShutdown = false
	if err := transitive.Shutdown(); err != nil {
		t.Fatal(err)
	}
	if !vmShutdownCalled {
		t.Fatal("Shutting down the Transitive did not shutdown the VM")
	}
}

func TestEngineAdd(t *testing.T) {
	vdr, _, sender, vm, te, gBlk := setupDefaultConfig(t)

	if te.Ctx.ChainID != ids.Empty {
		t.Fatalf("Wrong chain ID")
	}

	parent := &snowman.TestBlock{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Unknown,
	}}
	blk := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentV: parent.IDV,
		HeightV: 1,
		BytesV:  []byte{1},
	}

	asked := new(bool)
	reqID := new(uint32)
	sender.SendGetF = func(inVdr ids.NodeID, requestID uint32, blkID ids.ID) {
		*reqID = requestID
		if *asked {
			t.Fatalf("Asked multiple times")
		}
		*asked = true
		if vdr != inVdr {
			t.Fatalf("Asking wrong validator for block")
		}
		if blkID != blk.Parent() {
			t.Fatalf("Asking for wrong block")
		}
	}

	vm.ParseBlockF = func(b []byte) (snowman.Block, error) {
		if !bytes.Equal(b, blk.Bytes()) {
			t.Fatalf("Wrong bytes")
		}
		return blk, nil
	}

	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		switch blkID {
		case gBlk.ID():
			return gBlk, nil
		case parent.ID():
			return parent, nil
		default:
			return nil, errUnknownBlock
		}
	}

	if err := te.Put(vdr, 0, blk.Bytes()); err != nil {
		t.Fatal(err)
	}

	vm.ParseBlockF = nil

	if !*asked {
		t.Fatalf("Didn't ask for a missing block")
	}

	if len(te.blocked) != 1 {
		t.Fatalf("Should have been blocking on request")
	}

	vm.ParseBlockF = func(b []byte) (snowman.Block, error) { return nil, errUnknownBytes }

	if err := te.Put(vdr, *reqID, nil); err != nil {
		t.Fatal(err)
	}

	vm.ParseBlockF = nil

	if len(te.blocked) != 0 {
		t.Fatalf("Should have finished blocking issue")
	}
}

func TestEngineQuery(t *testing.T) {
	vdr, _, sender, vm, te, gBlk := setupDefaultConfig(t)

	blk := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentV: gBlk.ID(),
		HeightV: 1,
		BytesV:  []byte{1},
	}

	chitted := new(bool)
	sender.SendChitsF = func(inVdr ids.NodeID, requestID uint32, prefSet []ids.ID) {
		if *chitted {
			t.Fatalf("Sent multiple chits")
		}
		*chitted = true
		if requestID != 15 {
			t.Fatalf("Wrong request ID")
		}
		if len(prefSet) != 1 {
			t.Fatal("Should only be one vote")
		}
		if gBlk.ID() != prefSet[0] {
			t.Fatalf("Wrong chits block")
		}
	}

	blocked := new(bool)
	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		*blocked = true
		switch blkID {
		case gBlk.ID():
			return gBlk, nil
		default:
			return nil, errUnknownBlock
		}
	}

	asked := new(bool)
	getRequestID := new(uint32)
	sender.SendGetF = func(inVdr ids.NodeID, requestID uint32, blkID ids.ID) {
		if *asked {
			t.Fatalf("Asked multiple times")
		}
		*asked = true
		*getRequestID = requestID
		if vdr != inVdr {
			t.Fatalf("Asking wrong validator for block")
		}
		if blk.ID() != blkID && gBlk.ID() != blkID {
			t.Fatalf("Asking for wrong block")
		}
	}

	if err := te.PullQuery(vdr, 15, blk.ID()); err != nil {
		t.Fatal(err)
	}
	if !*chitted {
		t.Fatalf("Didn't respond with chits")
	}
	if !*blocked {
		t.Fatalf("Didn't request block")
	}
	if !*asked {
		t.Fatalf("Didn't request block from validator")
	}

	queried := new(bool)
	queryRequestID := new(uint32)
	sender.SendPushQueryF = func(inVdrs ids.NodeIDSet, requestID uint32, blkBytes []byte) {
		if *queried {
			t.Fatalf("Asked multiple times")
		}
		*queried = true
		*queryRequestID = requestID
		vdrSet := ids.NodeIDSet{}
		vdrSet.Add(vdr)
		if !inVdrs.Equals(vdrSet) {
			t.Fatalf("Asking wrong validator for preference")
		}
		if !bytes.Equal(blk.Bytes(), blkBytes) {
			t.Fatalf("Asking for wrong block")
		}
	}

	vm.ParseBlockF = func(b []byte) (snowman.Block, error) {
		if !bytes.Equal(b, blk.Bytes()) {
			t.Fatalf("Wrong bytes")
		}
		return blk, nil
	}
	if err := te.Put(vdr, *getRequestID, blk.Bytes()); err != nil {
		t.Fatal(err)
	}
	vm.ParseBlockF = nil

	if !*queried {
		t.Fatalf("Didn't ask for preferences")
	}

	blk1 := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentV: blk.IDV,
		HeightV: 2,
		BytesV:  []byte{5, 4, 3, 2, 1, 9},
	}

	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		switch blkID {
		case blk.ID():
			return nil, errUnknownBlock
		case blk1.ID():
			return nil, errUnknownBlock
		}
		t.Fatalf("Wrong block requested")
		panic("Should have failed")
	}

	*asked = false
	sender.SendGetF = func(inVdr ids.NodeID, requestID uint32, blkID ids.ID) {
		if *asked {
			t.Fatalf("Asked multiple times")
		}
		*asked = true
		*getRequestID = requestID
		if vdr != inVdr {
			t.Fatalf("Asking wrong validator for block")
		}
		if blk1.ID() != blkID {
			t.Fatalf("Asking for wrong block")
		}
	}
	if err := te.Chits(vdr, *queryRequestID, []ids.ID{blk1.ID()}); err != nil {
		t.Fatal(err)
	}

	*queried = false
	sender.SendPushQueryF = func(inVdrs ids.NodeIDSet, requestID uint32, blkBytes []byte) {
		if *queried {
			t.Fatalf("Asked multiple times")
		}
		*queried = true
		*queryRequestID = requestID
		vdrSet := ids.NodeIDSet{}
		vdrSet.Add(vdr)
		if !inVdrs.Equals(vdrSet) {
			t.Fatalf("Asking wrong validator for preference")
		}
		if !bytes.Equal(blk1.Bytes(), blkBytes) {
			t.Fatalf("Asking for wrong block")
		}
	}

	vm.ParseBlockF = func(b []byte) (snowman.Block, error) {
		if !bytes.Equal(b, blk1.Bytes()) {
			t.Fatalf("Wrong bytes")
		}

		vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
			switch blkID {
			case blk.ID():
				return blk, nil
			case blk1.ID():
				return blk1, nil
			}
			t.Fatalf("Wrong block requested")
			panic("Should have failed")
		}

		return blk1, nil
	}
	if err := te.Put(vdr, *getRequestID, blk1.Bytes()); err != nil {
		t.Fatal(err)
	}
	vm.ParseBlockF = nil

	if blk1.Status() != choices.Accepted {
		t.Fatalf("Should have executed block")
	}
	if len(te.blocked) != 0 {
		t.Fatalf("Should have finished blocking")
	}

	_ = te.polls.String() // Shouldn't panic

	if err := te.QueryFailed(vdr, *queryRequestID); err != nil {
		t.Fatal(err)
	}
	if len(te.blocked) != 0 {
		t.Fatalf("Should have finished blocking")
	}
}

func TestEngineMultipleQuery(t *testing.T) {
	engCfg := DefaultConfigs()
	engCfg.Params = snowball.Parameters{
		K:                       3,
		Alpha:                   2,
		BetaVirtuous:            1,
		BetaRogue:               2,
		ConcurrentRepolls:       1,
		OptimalProcessing:       1,
		MaxOutstandingItems:     1,
		MaxItemProcessingTime:   1,
		MixedQueryNumPushNonVdr: 3,
	}

	vals := validators.NewSet()
	engCfg.Validators = vals

	vdr0 := ids.GenerateTestNodeID()
	vdr1 := ids.GenerateTestNodeID()
	vdr2 := ids.GenerateTestNodeID()

	errs := wrappers.Errs{}
	errs.Add(
		vals.AddWeight(vdr0, 1),
		vals.AddWeight(vdr1, 1),
		vals.AddWeight(vdr2, 1),
	)
	if errs.Errored() {
		t.Fatal(errs.Err)
	}

	sender := &common.SenderTest{T: t}
	engCfg.Sender = sender
	sender.Default(true)

	vm := &block.TestVM{}
	vm.T = t
	engCfg.VM = vm

	vm.Default(true)
	vm.CantSetState = false
	vm.CantSetPreference = false

	gBlk := &snowman.TestBlock{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}

	vm.LastAcceptedF = func() (ids.ID, error) { return gBlk.ID(), nil }
	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		if blkID != gBlk.ID() {
			t.Fatalf("Wrong block requested")
		}
		return gBlk, nil
	}

	te, err := newTransitive(engCfg)
	if err != nil {
		t.Fatal(err)
	}

	if err := te.Start(0); err != nil {
		t.Fatal(err)
	}

	vm.GetBlockF = nil
	vm.LastAcceptedF = nil

	blk0 := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentV: gBlk.IDV,
		HeightV: 1,
		BytesV:  []byte{1},
	}

	queried := new(bool)
	queryRequestID := new(uint32)
	sender.SendPushQueryF = func(inVdrs ids.NodeIDSet, requestID uint32, blkBytes []byte) {
		if *queried {
			t.Fatalf("Asked multiple times")
		}
		*queried = true
		*queryRequestID = requestID
		vdrSet := ids.NodeIDSet{}
		vdrSet.Add(vdr0, vdr1, vdr2)
		if !inVdrs.Equals(vdrSet) {
			t.Fatalf("Asking wrong validator for preference")
		}
		if !bytes.Equal(blk0.Bytes(), blkBytes) {
			t.Fatalf("Asking for wrong block")
		}
	}

	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		switch blkID {
		case gBlk.ID():
			return gBlk, nil
		default:
			return nil, errUnknownBlock
		}
	}

	if err := te.issue(blk0); err != nil {
		t.Fatal(err)
	}

	blk1 := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentV: blk0.IDV,
		HeightV: 2,
		BytesV:  []byte{2},
	}

	vm.GetBlockF = func(id ids.ID) (snowman.Block, error) {
		switch id {
		case gBlk.ID():
			return gBlk, nil
		case blk0.ID():
			return blk0, nil
		case blk1.ID():
			return nil, errUnknownBlock
		}
		t.Fatalf("Unknown block")
		panic("Should have errored")
	}

	asked := new(bool)
	getRequestID := new(uint32)
	sender.SendGetF = func(inVdr ids.NodeID, requestID uint32, blkID ids.ID) {
		if *asked {
			t.Fatalf("Asked multiple times")
		}
		*asked = true
		*getRequestID = requestID
		if vdr0 != inVdr {
			t.Fatalf("Asking wrong validator for block")
		}
		if blk1.ID() != blkID {
			t.Fatalf("Asking for wrong block")
		}
	}
	blkSet := []ids.ID{blk1.ID()}
	if err := te.Chits(vdr0, *queryRequestID, blkSet); err != nil {
		t.Fatal(err)
	}
	if err := te.Chits(vdr1, *queryRequestID, blkSet); err != nil {
		t.Fatal(err)
	}

	vm.ParseBlockF = func(b []byte) (snowman.Block, error) {
		vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
			switch {
			case blkID == blk0.ID():
				return blk0, nil
			case blkID == blk1.ID():
				return blk1, nil
			}
			t.Fatalf("Wrong block requested")
			panic("Should have failed")
		}

		return blk1, nil
	}

	*queried = false
	secondQueryRequestID := new(uint32)
	sender.SendPushQueryF = func(inVdrs ids.NodeIDSet, requestID uint32, blkBytes []byte) {
		if *queried {
			t.Fatalf("Asked multiple times")
		}
		*queried = true
		*secondQueryRequestID = requestID
		vdrSet := ids.NodeIDSet{}
		vdrSet.Add(vdr0, vdr1, vdr2)
		if !inVdrs.Equals(vdrSet) {
			t.Fatalf("Asking wrong validator for preference")
		}
		if !bytes.Equal(blk1.Bytes(), blkBytes) {
			t.Fatalf("Asking for wrong block")
		}
	}
	if err := te.Put(vdr0, *getRequestID, blk1.Bytes()); err != nil {
		t.Fatal(err)
	}

	// Should be dropped because the query was already filled
	blkSet = []ids.ID{blk0.ID()}
	if err := te.Chits(vdr2, *queryRequestID, blkSet); err != nil {
		t.Fatal(err)
	}

	if blk1.Status() != choices.Accepted {
		t.Fatalf("Should have executed block")
	}
	if len(te.blocked) != 0 {
		t.Fatalf("Should have finished blocking")
	}
}

func TestEngineBlockedIssue(t *testing.T) {
	_, _, sender, vm, te, gBlk := setupDefaultConfig(t)

	sender.Default(false)

	blk0 := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Unknown,
		},
		ParentV: gBlk.ID(),
		HeightV: 1,
		BytesV:  []byte{1},
	}
	blk1 := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentV: blk0.IDV,
		HeightV: 2,
		BytesV:  []byte{2},
	}

	sender.SendGetF = func(ids.NodeID, uint32, ids.ID) {}
	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		switch blkID {
		case gBlk.ID():
			return gBlk, nil
		case blk0.ID():
			return blk0, nil
		default:
			return nil, errUnknownBlock
		}
	}

	if err := te.issue(blk1); err != nil {
		t.Fatal(err)
	}

	blk0.StatusV = choices.Processing
	if err := te.issue(blk0); err != nil {
		t.Fatal(err)
	}

	if blk1.ID() != te.Consensus.Preference() {
		t.Fatalf("Should have issued blk1")
	}
}

func TestEngineAbandonResponse(t *testing.T) {
	vdr, _, sender, vm, te, gBlk := setupDefaultConfig(t)

	sender.Default(false)

	blk := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Unknown,
		},
		ParentV: gBlk.ID(),
		HeightV: 1,
		BytesV:  []byte{1},
	}

	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		switch {
		case blkID == gBlk.ID():
			return gBlk, nil
		case blkID == blk.ID():
			return nil, errUnknownBlock
		}
		t.Fatalf("Wrong block requested")
		return nil, errUnknownBlock
	}

	if err := te.issue(blk); err != nil {
		t.Fatal(err)
	}
	if err := te.QueryFailed(vdr, 1); err != nil {
		t.Fatal(err)
	}

	if len(te.blocked) != 0 {
		t.Fatalf("Should have removed blocking event")
	}
}

func TestEngineFetchBlock(t *testing.T) {
	vdr, _, sender, vm, te, gBlk := setupDefaultConfig(t)

	sender.Default(false)

	vm.GetBlockF = func(id ids.ID) (snowman.Block, error) {
		if id == gBlk.ID() {
			return gBlk, nil
		}
		t.Fatalf("Unknown block")
		panic("Should have failed")
	}

	added := new(bool)
	sender.SendPutF = func(inVdr ids.NodeID, requestID uint32, blk []byte) {
		if vdr != inVdr {
			t.Fatalf("Wrong validator")
		}
		if requestID != 123 {
			t.Fatalf("Wrong request id")
		}
		if !bytes.Equal(gBlk.Bytes(), blk) {
			t.Fatalf("Asking for wrong block")
		}
		*added = true
	}

	if err := te.Get(vdr, 123, gBlk.ID()); err != nil {
		t.Fatal(err)
	}

	if !*added {
		t.Fatalf("Should have sent block to peer")
	}
}

func TestEnginePushQuery(t *testing.T) {
	vdr, _, sender, vm, te, gBlk := setupDefaultConfig(t)

	sender.Default(true)

	blk := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentV: gBlk.ID(),
		HeightV: 1,
		BytesV:  []byte{1},
	}

	vm.ParseBlockF = func(b []byte) (snowman.Block, error) {
		if bytes.Equal(b, blk.Bytes()) {
			return blk, nil
		}
		return nil, errUnknownBytes
	}

	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		switch blkID {
		case gBlk.ID():
			return gBlk, nil
		case blk.ID():
			return blk, nil
		default:
			return nil, errUnknownBlock
		}
	}

	chitted := new(bool)
	sender.SendChitsF = func(inVdr ids.NodeID, requestID uint32, votes []ids.ID) {
		if *chitted {
			t.Fatalf("Sent chit multiple times")
		}
		*chitted = true
		if inVdr != vdr {
			t.Fatalf("Asking wrong validator for preference")
		}
		if requestID != 20 {
			t.Fatalf("Wrong request id")
		}
		if len(votes) != 1 {
			t.Fatal("votes should only have one element")
		}
		if gBlk.ID() != votes[0] {
			t.Fatalf("Asking for wrong block")
		}
	}

	queried := new(bool)
	sender.SendPushQueryF = func(inVdrs ids.NodeIDSet, _ uint32, blkBytes []byte) {
		if *queried {
			t.Fatalf("Asked multiple times")
		}
		*queried = true
		vdrSet := ids.NodeIDSet{}
		vdrSet.Add(vdr)
		if !inVdrs.Equals(vdrSet) {
			t.Fatalf("Asking wrong validator for preference")
		}
		if !bytes.Equal(blk.Bytes(), blkBytes) {
			t.Fatalf("Asking for wrong block")
		}
	}

	if err := te.PushQuery(vdr, 20, blk.Bytes()); err != nil {
		t.Fatal(err)
	}

	if !*chitted {
		t.Fatalf("Should have sent a chit to the peer")
	}
	if !*queried {
		t.Fatalf("Should have sent a query to the peer")
	}
}

func TestEngineBuildBlock(t *testing.T) {
	vdr, _, sender, vm, te, gBlk := setupDefaultConfig(t)

	sender.Default(true)

	blk := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentV: gBlk.ID(),
		HeightV: 1,
		BytesV:  []byte{1},
	}

	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		switch blkID {
		case gBlk.ID():
			return gBlk, nil
		default:
			return nil, errUnknownBlock
		}
	}

	queried := new(bool)
	sender.SendPushQueryF = func(inVdrs ids.NodeIDSet, _ uint32, blkBytes []byte) {
		if *queried {
			t.Fatalf("Asked multiple times")
		}
		*queried = true
		vdrSet := ids.NodeIDSet{}
		vdrSet.Add(vdr)
		if !inVdrs.Equals(vdrSet) {
			t.Fatalf("Asking wrong validator for preference")
		}
	}

	vm.BuildBlockF = func() (snowman.Block, error) { return blk, nil }
	if err := te.Notify(common.PendingTxs); err != nil {
		t.Fatal(err)
	}

	if !*queried {
		t.Fatalf("Should have sent a query to the peer")
	}
}

func TestEngineRepoll(t *testing.T) {
	vdr, _, sender, _, te, _ := setupDefaultConfig(t)

	sender.Default(true)

	queried := new(bool)
	sender.SendPullQueryF = func(inVdrs ids.NodeIDSet, _ uint32, blkID ids.ID) {
		if *queried {
			t.Fatalf("Asked multiple times")
		}
		*queried = true
		vdrSet := ids.NodeIDSet{}
		vdrSet.Add(vdr)
		if !inVdrs.Equals(vdrSet) {
			t.Fatalf("Asking wrong validator for preference")
		}
	}

	te.repoll()

	if !*queried {
		t.Fatalf("Should have sent a query to the peer")
	}
}

func TestVoteCanceling(t *testing.T) {
	engCfg := DefaultConfigs()
	engCfg.Params = snowball.Parameters{
		K:                       3,
		Alpha:                   2,
		BetaVirtuous:            1,
		BetaRogue:               2,
		ConcurrentRepolls:       1,
		OptimalProcessing:       1,
		MaxOutstandingItems:     1,
		MaxItemProcessingTime:   1,
		MixedQueryNumPushNonVdr: 3,
	}

	vals := validators.NewSet()
	engCfg.Validators = vals

	vdr0 := ids.GenerateTestNodeID()
	vdr1 := ids.GenerateTestNodeID()
	vdr2 := ids.GenerateTestNodeID()

	errs := wrappers.Errs{}
	errs.Add(
		vals.AddWeight(vdr0, 1),
		vals.AddWeight(vdr1, 1),
		vals.AddWeight(vdr2, 1),
	)
	if errs.Errored() {
		t.Fatal(errs.Err)
	}

	sender := &common.SenderTest{T: t}
	engCfg.Sender = sender
	sender.Default(true)

	vm := &block.TestVM{}
	vm.T = t
	engCfg.VM = vm

	vm.Default(true)
	vm.CantSetState = false
	vm.CantSetPreference = false

	gBlk := &snowman.TestBlock{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}

	vm.LastAcceptedF = func() (ids.ID, error) { return gBlk.ID(), nil }
	vm.GetBlockF = func(id ids.ID) (snowman.Block, error) {
		switch id {
		case gBlk.ID():
			return gBlk, nil
		default:
			t.Fatalf("Loaded unknown block")
			panic("Should have failed")
		}
	}

	te, err := newTransitive(engCfg)
	if err != nil {
		t.Fatal(err)
	}

	if err := te.Start(0); err != nil {
		t.Fatal(err)
	}

	vm.LastAcceptedF = nil

	blk := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentV: gBlk.IDV,
		HeightV: 1,
		BytesV:  []byte{1},
	}

	queried := new(bool)
	queryRequestID := new(uint32)
	sender.SendPushQueryF = func(inVdrs ids.NodeIDSet, requestID uint32, blkBytes []byte) {
		if *queried {
			t.Fatalf("Asked multiple times")
		}
		*queried = true
		*queryRequestID = requestID
		vdrSet := ids.NodeIDSet{}
		vdrSet.Add(vdr0, vdr1, vdr2)
		if !inVdrs.Equals(vdrSet) {
			t.Fatalf("Asking wrong validator for preference")
		}
		if !bytes.Equal(blk.Bytes(), blkBytes) {
			t.Fatalf("Asking for wrong block")
		}
	}

	if err := te.issue(blk); err != nil {
		t.Fatal(err)
	}

	if te.polls.Len() != 1 {
		t.Fatalf("Shouldn't have finished blocking issue")
	}

	if err := te.QueryFailed(vdr0, *queryRequestID); err != nil {
		t.Fatal(err)
	}

	if te.polls.Len() != 1 {
		t.Fatalf("Shouldn't have finished blocking issue")
	}

	repolled := new(bool)
	sender.SendPullQueryF = func(inVdrs ids.NodeIDSet, requestID uint32, blkID ids.ID) {
		*repolled = true
	}
	if err := te.QueryFailed(vdr1, *queryRequestID); err != nil {
		t.Fatal(err)
	}

	if !*repolled {
		t.Fatalf("Should have finished blocking issue and repolled the network")
	}
}

func TestEngineNoQuery(t *testing.T) {
	engCfg := DefaultConfigs()

	sender := &common.SenderTest{T: t}
	engCfg.Sender = sender
	sender.Default(true)

	gBlk := &snowman.TestBlock{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}

	vm := &block.TestVM{}
	vm.T = t
	vm.LastAcceptedF = func() (ids.ID, error) { return gBlk.ID(), nil }

	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		if blkID == gBlk.ID() {
			return gBlk, nil
		}
		return nil, errUnknownBlock
	}

	engCfg.VM = vm

	te, err := newTransitive(engCfg)
	if err != nil {
		t.Fatal(err)
	}

	if err := te.Start(0); err != nil {
		t.Fatal(err)
	}

	blk := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentV: gBlk.IDV,
		HeightV: 1,
		BytesV:  []byte{1},
	}

	if err := te.issue(blk); err != nil {
		t.Fatal(err)
	}
}

func TestEngineNoRepollQuery(t *testing.T) {
	engCfg := DefaultConfigs()

	sender := &common.SenderTest{T: t}
	engCfg.Sender = sender
	sender.Default(true)

	gBlk := &snowman.TestBlock{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}

	vm := &block.TestVM{}
	vm.T = t
	vm.LastAcceptedF = func() (ids.ID, error) { return gBlk.ID(), nil }

	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		if blkID == gBlk.ID() {
			return gBlk, nil
		}
		return nil, errUnknownBlock
	}

	engCfg.VM = vm

	te, err := newTransitive(engCfg)
	if err != nil {
		t.Fatal(err)
	}

	if err := te.Start(0); err != nil {
		t.Fatal(err)
	}

	te.repoll()
}

func TestEngineAbandonQuery(t *testing.T) {
	vdr, _, sender, vm, te, _ := setupDefaultConfig(t)

	sender.Default(true)

	blkID := ids.GenerateTestID()

	vm.GetBlockF = func(id ids.ID) (snowman.Block, error) {
		switch id {
		case blkID:
			return nil, errUnknownBlock
		default:
			t.Fatalf("Loaded unknown block")
			panic("Should have failed")
		}
	}

	reqID := new(uint32)
	sender.SendGetF = func(_ ids.NodeID, requestID uint32, _ ids.ID) {
		*reqID = requestID
	}

	sender.CantSendChits = false

	if err := te.PullQuery(vdr, 0, blkID); err != nil {
		t.Fatal(err)
	}

	if te.blkReqs.Len() != 1 {
		t.Fatalf("Should have issued request")
	}

	if err := te.GetFailed(vdr, *reqID); err != nil {
		t.Fatal(err)
	}

	if te.blkReqs.Len() != 0 {
		t.Fatalf("Should have removed request")
	}
}

func TestEngineAbandonChit(t *testing.T) {
	vdr, _, sender, vm, te, gBlk := setupDefaultConfig(t)

	sender.Default(true)

	blk := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentV: gBlk.ID(),
		HeightV: 1,
		BytesV:  []byte{1},
	}

	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		switch blkID {
		case gBlk.ID():
			return gBlk, nil
		case blk.ID():
			return nil, errUnknownBlock
		}
		t.Fatalf("Wrong block requested")
		return nil, errUnknownBlock
	}

	sender.CantSendPushQuery = false

	if err := te.issue(blk); err != nil {
		t.Fatal(err)
	}

	fakeBlkID := ids.GenerateTestID()
	vm.GetBlockF = func(id ids.ID) (snowman.Block, error) {
		switch id {
		case fakeBlkID:
			return nil, errUnknownBlock
		default:
			t.Fatalf("Loaded unknown block")
			panic("Should have failed")
		}
	}

	reqID := new(uint32)
	sender.SendGetF = func(_ ids.NodeID, requestID uint32, _ ids.ID) {
		*reqID = requestID
	}

	if err := te.Chits(vdr, 0, []ids.ID{fakeBlkID}); err != nil {
		t.Fatal(err)
	}

	if len(te.blocked) != 1 {
		t.Fatalf("Should have blocked on request")
	}

	if err := te.GetFailed(vdr, *reqID); err != nil {
		t.Fatal(err)
	}

	if len(te.blocked) != 0 {
		t.Fatalf("Should have removed request")
	}
}

func TestEngineBlockingChitRequest(t *testing.T) {
	vdr, _, sender, vm, te, gBlk := setupDefaultConfig(t)

	sender.Default(true)

	missingBlk := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Unknown,
		},
		ParentV: gBlk.ID(),
		HeightV: 1,
		BytesV:  []byte{1},
	}
	parentBlk := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentV: missingBlk.IDV,
		HeightV: 2,
		BytesV:  []byte{2},
	}
	blockingBlk := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentV: parentBlk.IDV,
		HeightV: 3,
		BytesV:  []byte{3},
	}

	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		switch blkID {
		case gBlk.ID():
			return gBlk, nil
		case blockingBlk.ID():
			return blockingBlk, nil
		default:
			return nil, errUnknownBlock
		}
	}

	sender.SendGetF = func(ids.NodeID, uint32, ids.ID) {}

	vm.ParseBlockF = func(b []byte) (snowman.Block, error) {
		switch {
		case bytes.Equal(b, blockingBlk.Bytes()):
			return blockingBlk, nil
		default:
			t.Fatalf("Loaded unknown block")
			panic("Should have failed")
		}
	}

	if err := te.issue(parentBlk); err != nil {
		t.Fatal(err)
	}

	sender.CantSendChits = false

	if err := te.PushQuery(vdr, 0, blockingBlk.Bytes()); err != nil {
		t.Fatal(err)
	}

	if len(te.blocked) != 2 {
		t.Fatalf("Both inserts should be blocking")
	}

	sender.CantSendPushQuery = false

	missingBlk.StatusV = choices.Processing
	if err := te.issue(missingBlk); err != nil {
		t.Fatal(err)
	}

	if len(te.blocked) != 0 {
		t.Fatalf("Both inserts should not longer be blocking")
	}
}

func TestEngineBlockingChitResponse(t *testing.T) {
	vdr, _, sender, vm, te, gBlk := setupDefaultConfig(t)

	sender.Default(true)

	issuedBlk := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentV: gBlk.ID(),
		HeightV: 1,
		BytesV:  []byte{1},
	}
	missingBlk := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Unknown,
		},
		ParentV: gBlk.ID(),
		HeightV: 1,
		BytesV:  []byte{2},
	}
	blockingBlk := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentV: missingBlk.IDV,
		HeightV: 2,
		BytesV:  []byte{3},
	}

	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		switch blkID {
		case gBlk.ID():
			return gBlk, nil
		case blockingBlk.ID():
			return blockingBlk, nil
		default:
			return nil, errUnknownBlock
		}
	}

	if err := te.issue(blockingBlk); err != nil {
		t.Fatal(err)
	}

	queryRequestID := new(uint32)
	sender.SendPushQueryF = func(inVdrs ids.NodeIDSet, requestID uint32, blkBytes []byte) {
		*queryRequestID = requestID
		vdrSet := ids.NodeIDSet{}
		vdrSet.Add(vdr)
		if !inVdrs.Equals(vdrSet) {
			t.Fatalf("Asking wrong validator for preference")
		}
		if !bytes.Equal(issuedBlk.Bytes(), blkBytes) {
			t.Fatalf("Asking for wrong block")
		}
	}

	if err := te.issue(issuedBlk); err != nil {
		t.Fatal(err)
	}

	sender.SendPushQueryF = nil
	sender.CantSendPushQuery = false

	if err := te.Chits(vdr, *queryRequestID, []ids.ID{blockingBlk.ID()}); err != nil {
		t.Fatal(err)
	}

	require.Len(t, te.blocked, 2)
	sender.CantSendPullQuery = false

	missingBlk.StatusV = choices.Processing
	if err := te.issue(missingBlk); err != nil {
		t.Fatal(err)
	}
}

func TestEngineRetryFetch(t *testing.T) {
	vdr, _, sender, vm, te, gBlk := setupDefaultConfig(t)

	sender.Default(true)

	missingBlk := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Unknown,
		},
		ParentV: gBlk.ID(),
		HeightV: 1,
		BytesV:  []byte{1},
	}

	vm.CantGetBlock = false

	reqID := new(uint32)
	sender.SendGetF = func(_ ids.NodeID, requestID uint32, _ ids.ID) {
		*reqID = requestID
	}
	sender.CantSendChits = false

	if err := te.PullQuery(vdr, 0, missingBlk.ID()); err != nil {
		t.Fatal(err)
	}

	vm.CantGetBlock = true
	sender.SendGetF = nil

	if err := te.GetFailed(vdr, *reqID); err != nil {
		t.Fatal(err)
	}

	vm.CantGetBlock = false

	called := new(bool)
	sender.SendGetF = func(ids.NodeID, uint32, ids.ID) {
		*called = true
	}

	if err := te.PullQuery(vdr, 0, missingBlk.ID()); err != nil {
		t.Fatal(err)
	}

	vm.CantGetBlock = true
	sender.SendGetF = nil

	if !*called {
		t.Fatalf("Should have requested the block again")
	}
}

func TestEngineUndeclaredDependencyDeadlock(t *testing.T) {
	vdr, _, sender, vm, te, gBlk := setupDefaultConfig(t)

	sender.Default(true)

	validBlk := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentV: gBlk.ID(),
		HeightV: 1,
		BytesV:  []byte{1},
	}
	invalidBlk := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentV: validBlk.IDV,
		HeightV: 2,
		VerifyV: errors.New(""),
		BytesV:  []byte{2},
	}

	invalidBlkID := invalidBlk.ID()

	reqID := new(uint32)
	sender.SendPushQueryF = func(_ ids.NodeIDSet, requestID uint32, _ []byte) {
		*reqID = requestID
	}
	sender.SendPullQueryF = func(_ ids.NodeIDSet, requestID uint32, _ ids.ID) {}

	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		switch blkID {
		case gBlk.ID():
			return gBlk, nil
		case validBlk.ID():
			return validBlk, nil
		case invalidBlk.ID():
			return invalidBlk, nil
		default:
			return nil, errUnknownBlock
		}
	}
	if err := te.issue(validBlk); err != nil {
		t.Fatal(err)
	}
	sender.SendPushQueryF = nil
	if err := te.issue(invalidBlk); err != nil {
		t.Fatal(err)
	}

	if err := te.Chits(vdr, *reqID, []ids.ID{invalidBlkID}); err != nil {
		t.Fatal(err)
	}

	if status := validBlk.Status(); status != choices.Accepted {
		t.Log(status)
		t.Fatalf("Should have bubbled invalid votes to the valid parent")
	}
}

func TestEngineGossip(t *testing.T) {
	_, _, sender, vm, te, gBlk := setupDefaultConfig(t)

	vm.LastAcceptedF = func() (ids.ID, error) { return gBlk.ID(), nil }
	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		if blkID == gBlk.ID() {
			return gBlk, nil
		}
		t.Fatal(errUnknownBlock)
		return nil, errUnknownBlock
	}

	called := new(bool)
	sender.SendGossipF = func(blkBytes []byte) {
		*called = true
		if !bytes.Equal(blkBytes, gBlk.Bytes()) {
			t.Fatal(errUnknownBytes)
		}
	}

	if err := te.Gossip(); err != nil {
		t.Fatal(err)
	}

	if !*called {
		t.Fatalf("Should have gossiped the block")
	}
}

func TestEngineInvalidBlockIgnoredFromUnexpectedPeer(t *testing.T) {
	vdr, vdrs, sender, vm, te, gBlk := setupDefaultConfig(t)

	secondVdr := ids.GenerateTestNodeID()
	if err := vdrs.AddWeight(secondVdr, 1); err != nil {
		t.Fatal(err)
	}

	sender.Default(true)

	missingBlk := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Unknown,
		},
		ParentV: gBlk.ID(),
		HeightV: 1,
		BytesV:  []byte{1},
	}
	pendingBlk := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentV: missingBlk.IDV,
		HeightV: 2,
		BytesV:  []byte{2},
	}

	parsed := new(bool)
	vm.ParseBlockF = func(b []byte) (snowman.Block, error) {
		if bytes.Equal(b, pendingBlk.Bytes()) {
			*parsed = true
			return pendingBlk, nil
		}
		return nil, errUnknownBlock
	}

	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		switch blkID {
		case gBlk.ID():
			return gBlk, nil
		case pendingBlk.ID():
			if !*parsed {
				return nil, errUnknownBlock
			}
			return pendingBlk, nil
		default:
			return nil, errUnknownBlock
		}
	}

	reqID := new(uint32)
	sender.SendGetF = func(reqVdr ids.NodeID, requestID uint32, blkID ids.ID) {
		*reqID = requestID
		if reqVdr != vdr {
			t.Fatalf("Wrong validator requested")
		}
		if blkID != missingBlk.ID() {
			t.Fatalf("Wrong block requested")
		}
	}
	sender.CantSendChits = false

	if err := te.PushQuery(vdr, 0, pendingBlk.Bytes()); err != nil {
		t.Fatal(err)
	}

	if err := te.Put(secondVdr, *reqID, []byte{3}); err != nil {
		t.Fatal(err)
	}

	*parsed = false
	vm.ParseBlockF = func(b []byte) (snowman.Block, error) {
		if bytes.Equal(b, missingBlk.Bytes()) {
			*parsed = true
			return missingBlk, nil
		}
		return nil, errUnknownBlock
	}
	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		switch blkID {
		case gBlk.ID():
			return gBlk, nil
		case missingBlk.ID():
			if !*parsed {
				return nil, errUnknownBlock
			}
			return missingBlk, nil
		default:
			return nil, errUnknownBlock
		}
	}
	sender.CantSendPushQuery = false

	missingBlk.StatusV = choices.Processing

	if err := te.Put(vdr, *reqID, missingBlk.Bytes()); err != nil {
		t.Fatal(err)
	}

	pref := te.Consensus.Preference()
	if pref != pendingBlk.ID() {
		t.Fatalf("Shouldn't have abandoned the pending block")
	}
}

func TestEnginePushQueryRequestIDConflict(t *testing.T) {
	vdr, _, sender, vm, te, gBlk := setupDefaultConfig(t)

	sender.Default(true)

	missingBlk := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Unknown,
		},
		ParentV: gBlk.ID(),
		HeightV: 1,
		BytesV:  []byte{1},
	}
	pendingBlk := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentV: missingBlk.IDV,
		HeightV: 2,
		BytesV:  []byte{2},
	}

	parsed := new(bool)
	vm.ParseBlockF = func(b []byte) (snowman.Block, error) {
		if bytes.Equal(b, pendingBlk.Bytes()) {
			*parsed = true
			return pendingBlk, nil
		}
		return nil, errUnknownBlock
	}

	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		switch blkID {
		case gBlk.ID():
			return gBlk, nil
		case pendingBlk.ID():
			if !*parsed {
				return nil, errUnknownBlock
			}
			return pendingBlk, nil
		default:
			return nil, errUnknownBlock
		}
	}

	reqID := new(uint32)
	sender.SendGetF = func(reqVdr ids.NodeID, requestID uint32, blkID ids.ID) {
		*reqID = requestID
		if reqVdr != vdr {
			t.Fatalf("Wrong validator requested")
		}
		if blkID != missingBlk.ID() {
			t.Fatalf("Wrong block requested")
		}
	}
	sender.CantSendChits = false

	if err := te.PushQuery(vdr, 0, pendingBlk.Bytes()); err != nil {
		t.Fatal(err)
	}

	sender.SendGetF = nil
	sender.CantSendGet = false

	if err := te.PushQuery(vdr, *reqID, []byte{3}); err != nil {
		t.Fatal(err)
	}

	*parsed = false
	vm.ParseBlockF = func(b []byte) (snowman.Block, error) {
		if bytes.Equal(b, missingBlk.Bytes()) {
			*parsed = true
			return missingBlk, nil
		}
		return nil, errUnknownBlock
	}
	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		switch blkID {
		case gBlk.ID():
			return gBlk, nil
		case missingBlk.ID():
			if !*parsed {
				return nil, errUnknownBlock
			}
			return missingBlk, nil
		default:
			return nil, errUnknownBlock
		}
	}
	sender.CantSendPushQuery = false

	if err := te.Put(vdr, *reqID, missingBlk.Bytes()); err != nil {
		t.Fatal(err)
	}

	pref := te.Consensus.Preference()
	if pref != pendingBlk.ID() {
		t.Fatalf("Shouldn't have abandoned the pending block")
	}
}

func TestEngineAggressivePolling(t *testing.T) {
	engCfg := DefaultConfigs()
	engCfg.Params.ConcurrentRepolls = 2

	vals := validators.NewSet()
	engCfg.Validators = vals

	vdr := ids.GenerateTestNodeID()
	if err := vals.AddWeight(vdr, 1); err != nil {
		t.Fatal(err)
	}

	sender := &common.SenderTest{T: t}
	engCfg.Sender = sender
	sender.Default(true)

	vm := &block.TestVM{}
	vm.T = t
	engCfg.VM = vm

	vm.Default(true)
	vm.CantSetState = false
	vm.CantSetPreference = false

	gBlk := &snowman.TestBlock{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}

	vm.LastAcceptedF = func() (ids.ID, error) { return gBlk.ID(), nil }
	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		if blkID != gBlk.ID() {
			t.Fatalf("Wrong block requested")
		}
		return gBlk, nil
	}

	te, err := newTransitive(engCfg)
	if err != nil {
		t.Fatal(err)
	}

	if err := te.Start(0); err != nil {
		t.Fatal(err)
	}

	vm.GetBlockF = nil
	vm.LastAcceptedF = nil

	pendingBlk := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentV: gBlk.IDV,
		HeightV: 1,
		BytesV:  []byte{1},
	}

	parsed := new(bool)
	vm.ParseBlockF = func(b []byte) (snowman.Block, error) {
		if bytes.Equal(b, pendingBlk.Bytes()) {
			*parsed = true
			return pendingBlk, nil
		}
		return nil, errUnknownBlock
	}

	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		switch blkID {
		case gBlk.ID():
			return gBlk, nil
		case pendingBlk.ID():
			if !*parsed {
				return nil, errUnknownBlock
			}
			return pendingBlk, nil
		default:
			return nil, errUnknownBlock
		}
	}

	numPushed := new(int)
	sender.SendPushQueryF = func(ids.NodeIDSet, uint32, []byte) { *numPushed++ }

	numPulled := new(int)
	sender.SendPullQueryF = func(ids.NodeIDSet, uint32, ids.ID) { *numPulled++ }

	if err := te.Put(vdr, 0, pendingBlk.Bytes()); err != nil {
		t.Fatal(err)
	}

	if *numPushed != 1 {
		t.Fatalf("Should have initially sent a push query")
	}

	if *numPulled != 1 {
		t.Fatalf("Should have sent an additional pull query")
	}
}

func TestEngineDoubleChit(t *testing.T) {
	engCfg := DefaultConfigs()
	engCfg.Params = snowball.Parameters{
		K:                       2,
		Alpha:                   2,
		BetaVirtuous:            1,
		BetaRogue:               2,
		ConcurrentRepolls:       1,
		OptimalProcessing:       1,
		MaxOutstandingItems:     1,
		MaxItemProcessingTime:   1,
		MixedQueryNumPushNonVdr: 2,
	}

	vals := validators.NewSet()
	engCfg.Validators = vals

	vdr0 := ids.GenerateTestNodeID()
	vdr1 := ids.GenerateTestNodeID()

	if err := vals.AddWeight(vdr0, 1); err != nil {
		t.Fatal(err)
	}
	if err := vals.AddWeight(vdr1, 1); err != nil {
		t.Fatal(err)
	}

	sender := &common.SenderTest{T: t}
	engCfg.Sender = sender

	sender.Default(true)

	vm := &block.TestVM{}
	vm.T = t
	engCfg.VM = vm

	vm.Default(true)
	vm.CantSetState = false
	vm.CantSetPreference = false

	gBlk := &snowman.TestBlock{TestDecidable: choices.TestDecidable{
		IDV:     ids.GenerateTestID(),
		StatusV: choices.Accepted,
	}}

	vm.LastAcceptedF = func() (ids.ID, error) { return gBlk.ID(), nil }
	vm.GetBlockF = func(id ids.ID) (snowman.Block, error) {
		if id == gBlk.ID() {
			return gBlk, nil
		}
		t.Fatalf("Unknown block")
		panic("Should have errored")
	}

	te, err := newTransitive(engCfg)
	if err != nil {
		t.Fatal(err)
	}

	if err := te.Start(0); err != nil {
		t.Fatal(err)
	}

	vm.LastAcceptedF = nil

	blk := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentV: gBlk.IDV,
		HeightV: 1,
		BytesV:  []byte{1},
	}

	queried := new(bool)
	queryRequestID := new(uint32)
	sender.SendPushQueryF = func(inVdrs ids.NodeIDSet, requestID uint32, blkBytes []byte) {
		if *queried {
			t.Fatalf("Asked multiple times")
		}
		*queried = true
		*queryRequestID = requestID
		vdrSet := ids.NodeIDSet{}
		vdrSet.Add(vdr0, vdr1)
		if !inVdrs.Equals(vdrSet) {
			t.Fatalf("Asking wrong validator for preference")
		}
		if !bytes.Equal(blk.Bytes(), blkBytes) {
			t.Fatalf("Asking for wrong block")
		}
	}

	if err := te.issue(blk); err != nil {
		t.Fatal(err)
	}

	vm.GetBlockF = func(id ids.ID) (snowman.Block, error) {
		switch id {
		case gBlk.ID():
			return gBlk, nil
		case blk.ID():
			return blk, nil
		}
		t.Fatalf("Unknown block")
		panic("Should have errored")
	}

	blkSet := []ids.ID{blk.ID()}

	if status := blk.Status(); status != choices.Processing {
		t.Fatalf("Wrong status: %s ; expected: %s", status, choices.Processing)
	}

	if err := te.Chits(vdr0, *queryRequestID, blkSet); err != nil {
		t.Fatal(err)
	}

	if status := blk.Status(); status != choices.Processing {
		t.Fatalf("Wrong status: %s ; expected: %s", status, choices.Processing)
	}

	if err := te.Chits(vdr0, *queryRequestID, blkSet); err != nil {
		t.Fatal(err)
	}

	if status := blk.Status(); status != choices.Processing {
		t.Fatalf("Wrong status: %s ; expected: %s", status, choices.Processing)
	}

	if err := te.Chits(vdr1, *queryRequestID, blkSet); err != nil {
		t.Fatal(err)
	}

	if status := blk.Status(); status != choices.Accepted {
		t.Fatalf("Wrong status: %s ; expected: %s", status, choices.Accepted)
	}
}

func TestEngineBuildBlockLimit(t *testing.T) {
	engCfg := DefaultConfigs()
	engCfg.Params.K = 1
	engCfg.Params.Alpha = 1
	engCfg.Params.OptimalProcessing = 1

	vals := validators.NewSet()
	engCfg.Validators = vals

	vdr := ids.GenerateTestNodeID()
	if err := vals.AddWeight(vdr, 1); err != nil {
		t.Fatal(err)
	}

	sender := &common.SenderTest{T: t}
	engCfg.Sender = sender
	sender.Default(true)

	vm := &block.TestVM{}
	vm.T = t
	engCfg.VM = vm

	vm.Default(true)
	vm.CantSetState = false
	vm.CantSetPreference = false

	gBlk := &snowman.TestBlock{TestDecidable: choices.TestDecidable{
		IDV:     Genesis,
		StatusV: choices.Accepted,
	}}

	vm.LastAcceptedF = func() (ids.ID, error) { return gBlk.ID(), nil }
	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		if blkID != gBlk.ID() {
			t.Fatalf("Wrong block requested")
		}
		return gBlk, nil
	}

	te, err := newTransitive(engCfg)
	if err != nil {
		t.Fatal(err)
	}

	if err := te.Start(0); err != nil {
		t.Fatal(err)
	}

	vm.GetBlockF = nil
	vm.LastAcceptedF = nil

	blk0 := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentV: gBlk.IDV,
		HeightV: 1,
		BytesV:  []byte{1},
	}
	blk1 := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentV: blk0.IDV,
		HeightV: 2,
		BytesV:  []byte{2},
	}
	blks := []snowman.Block{blk0, blk1}

	var (
		queried bool
		reqID   uint32
	)
	sender.SendPushQueryF = func(inVdrs ids.NodeIDSet, rID uint32, _ []byte) {
		reqID = rID
		if queried {
			t.Fatalf("Asked multiple times")
		}
		queried = true
		vdrSet := ids.NodeIDSet{}
		vdrSet.Add(vdr)
		if !inVdrs.Equals(vdrSet) {
			t.Fatalf("Asking wrong validator for preference")
		}
	}

	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		switch blkID {
		case gBlk.ID():
			return gBlk, nil
		default:
			return nil, errUnknownBlock
		}
	}

	blkToReturn := 0
	vm.BuildBlockF = func() (snowman.Block, error) {
		if blkToReturn >= len(blks) {
			t.Fatalf("Built too many blocks")
		}
		blk := blks[blkToReturn]
		blkToReturn++
		return blk, nil
	}
	if err := te.Notify(common.PendingTxs); err != nil {
		t.Fatal(err)
	}

	if !queried {
		t.Fatalf("Should have sent a query to the peer")
	}

	queried = false
	if err := te.Notify(common.PendingTxs); err != nil {
		t.Fatal(err)
	}

	if queried {
		t.Fatalf("Shouldn't have sent a query to the peer")
	}

	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		switch blkID {
		case gBlk.ID():
			return gBlk, nil
		case blk0.ID():
			return blk0, nil
		default:
			return nil, errUnknownBlock
		}
	}

	if err := te.Chits(vdr, reqID, []ids.ID{blk0.ID()}); err != nil {
		t.Fatal(err)
	}

	if !queried {
		t.Fatalf("Should have sent a query to the peer")
	}
}

func TestEngineReceiveNewRejectedBlock(t *testing.T) {
	vdr, _, sender, vm, te, gBlk := setupDefaultConfig(t)

	acceptedBlk := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentV: gBlk.ID(),
		HeightV: 1,
		BytesV:  []byte{1},
	}
	rejectedBlk := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Unknown,
		},
		ParentV: gBlk.ID(),
		HeightV: 1,
		BytesV:  []byte{2},
	}
	pendingBlk := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentV: rejectedBlk.IDV,
		HeightV: 2,
		BytesV:  []byte{3},
	}

	vm.ParseBlockF = func(b []byte) (snowman.Block, error) {
		switch {
		case bytes.Equal(b, acceptedBlk.Bytes()):
			return acceptedBlk, nil
		case bytes.Equal(b, rejectedBlk.Bytes()):
			return rejectedBlk, nil
		case bytes.Equal(b, pendingBlk.Bytes()):
			return pendingBlk, nil
		default:
			t.Fatalf("Unknown block bytes")
			return nil, nil
		}
	}

	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		switch blkID {
		case gBlk.ID():
			return gBlk, nil
		case acceptedBlk.ID():
			return acceptedBlk, nil
		default:
			return nil, errUnknownBlock
		}
	}

	var (
		asked bool
		reqID uint32
	)
	sender.SendPushQueryF = func(_ ids.NodeIDSet, rID uint32, blkBytes []byte) {
		asked = true
		reqID = rID
	}

	if err := te.Put(vdr, 0, acceptedBlk.Bytes()); err != nil {
		t.Fatal(err)
	}

	if !asked {
		t.Fatalf("Didn't query for the new block")
	}

	if err := te.Chits(vdr, reqID, []ids.ID{acceptedBlk.ID()}); err != nil {
		t.Fatal(err)
	}

	sender.SendPushQueryF = nil
	asked = false

	sender.SendGetF = func(_ ids.NodeID, rID uint32, _ ids.ID) {
		asked = true
		reqID = rID
	}

	if err := te.Put(vdr, 0, pendingBlk.Bytes()); err != nil {
		t.Fatal(err)
	}

	if !asked {
		t.Fatalf("Didn't request the missing block")
	}

	rejectedBlk.StatusV = choices.Rejected

	if err := te.Put(vdr, reqID, rejectedBlk.Bytes()); err != nil {
		t.Fatal(err)
	}

	if te.blkReqs.Len() != 0 {
		t.Fatalf("Should have finished all requests")
	}
}

func TestEngineRejectionAmplification(t *testing.T) {
	vdr, _, sender, vm, te, gBlk := setupDefaultConfig(t)

	acceptedBlk := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentV: gBlk.ID(),
		HeightV: 1,
		BytesV:  []byte{1},
	}
	rejectedBlk := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Unknown,
		},
		ParentV: gBlk.ID(),
		HeightV: 1,
		BytesV:  []byte{2},
	}
	pendingBlk := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentV: rejectedBlk.IDV,
		HeightV: 2,
		BytesV:  []byte{3},
	}

	vm.ParseBlockF = func(b []byte) (snowman.Block, error) {
		switch {
		case bytes.Equal(b, acceptedBlk.Bytes()):
			return acceptedBlk, nil
		case bytes.Equal(b, rejectedBlk.Bytes()):
			return rejectedBlk, nil
		case bytes.Equal(b, pendingBlk.Bytes()):
			return pendingBlk, nil
		default:
			t.Fatalf("Unknown block bytes")
			return nil, nil
		}
	}

	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		switch blkID {
		case gBlk.ID():
			return gBlk, nil
		case acceptedBlk.ID():
			return acceptedBlk, nil
		default:
			return nil, errUnknownBlock
		}
	}

	var (
		queried bool
		reqID   uint32
	)
	sender.SendPushQueryF = func(_ ids.NodeIDSet, rID uint32, _ []byte) {
		queried = true
		reqID = rID
	}

	if err := te.Put(vdr, 0, acceptedBlk.Bytes()); err != nil {
		t.Fatal(err)
	}

	if !queried {
		t.Fatalf("Didn't query for the new block")
	}

	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		switch blkID {
		case gBlk.ID():
			return gBlk, nil
		case acceptedBlk.ID():
			return acceptedBlk, nil
		default:
			return nil, errUnknownBlock
		}
	}

	if err := te.Chits(vdr, reqID, []ids.ID{acceptedBlk.ID()}); err != nil {
		t.Fatal(err)
	}

	if !te.Consensus.Finalized() {
		t.Fatalf("Should have finalized the consensus instance")
	}

	queried = false
	var asked bool
	sender.SendPushQueryF = func(ids.NodeIDSet, uint32, []byte) {
		queried = true
	}
	sender.SendGetF = func(_ ids.NodeID, rID uint32, blkID ids.ID) {
		asked = true
		reqID = rID

		if blkID != rejectedBlk.ID() {
			t.Fatalf("requested %s but should have requested %s", blkID, rejectedBlk.ID())
		}
	}

	if err := te.Put(vdr, 0, pendingBlk.Bytes()); err != nil {
		t.Fatal(err)
	}

	if queried {
		t.Fatalf("Queried for the pending block")
	}
	if !asked {
		t.Fatalf("Should have asked for the missing block")
	}

	rejectedBlk.StatusV = choices.Processing
	if err := te.Put(vdr, reqID, rejectedBlk.Bytes()); err != nil {
		t.Fatal(err)
	}

	if queried {
		t.Fatalf("Queried for the rejected block")
	}
}

// Test that the node will not issue a block into consensus that it knows will
// be rejected because the parent is rejected.
func TestEngineTransitiveRejectionAmplificationDueToRejectedParent(t *testing.T) {
	vdr, _, sender, vm, te, gBlk := setupDefaultConfig(t)

	acceptedBlk := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentV: gBlk.ID(),
		HeightV: 1,
		BytesV:  []byte{1},
	}
	rejectedBlk := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Rejected,
		},
		ParentV: gBlk.ID(),
		HeightV: 1,
		BytesV:  []byte{2},
	}
	pendingBlk := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			RejectV: errors.New("shouldn't have issued to consensus"),
			StatusV: choices.Processing,
		},
		ParentV: rejectedBlk.IDV,
		HeightV: 2,
		BytesV:  []byte{3},
	}

	vm.ParseBlockF = func(b []byte) (snowman.Block, error) {
		switch {
		case bytes.Equal(b, acceptedBlk.Bytes()):
			return acceptedBlk, nil
		case bytes.Equal(b, rejectedBlk.Bytes()):
			return rejectedBlk, nil
		case bytes.Equal(b, pendingBlk.Bytes()):
			return pendingBlk, nil
		default:
			t.Fatalf("Unknown block bytes")
			return nil, nil
		}
	}

	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		switch blkID {
		case gBlk.ID():
			return gBlk, nil
		case acceptedBlk.ID():
			return acceptedBlk, nil
		case rejectedBlk.ID():
			return rejectedBlk, nil
		default:
			return nil, errUnknownBlock
		}
	}

	var (
		queried bool
		reqID   uint32
	)
	sender.SendPushQueryF = func(_ ids.NodeIDSet, rID uint32, _ []byte) {
		queried = true
		reqID = rID
	}

	if err := te.Put(vdr, 0, acceptedBlk.Bytes()); err != nil {
		t.Fatal(err)
	}

	if !queried {
		t.Fatalf("Didn't query for the new block")
	}

	if err := te.Chits(vdr, reqID, []ids.ID{acceptedBlk.ID()}); err != nil {
		t.Fatal(err)
	}

	if !te.Consensus.Finalized() {
		t.Fatalf("Should have finalized the consensus instance")
	}

	if err := te.Put(vdr, 0, pendingBlk.Bytes()); err != nil {
		t.Fatal(err)
	}

	if !te.Consensus.Finalized() {
		t.Fatalf("Should have finalized the consensus instance")
	}

	if len(te.pending) != 0 {
		t.Fatalf("Shouldn't have any pending blocks")
	}
}

// Test that the node will not issue a block into consensus that it knows will
// be rejected because the parent is failing verification.
func TestEngineTransitiveRejectionAmplificationDueToInvalidParent(t *testing.T) {
	vdr, _, sender, vm, te, gBlk := setupDefaultConfig(t)

	acceptedBlk := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentV: gBlk.ID(),
		HeightV: 1,
		BytesV:  []byte{1},
	}
	rejectedBlk := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentV: gBlk.ID(),
		HeightV: 1,
		VerifyV: errors.New("invalid"),
		BytesV:  []byte{2},
	}
	pendingBlk := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			RejectV: errors.New("shouldn't have issued to consensus"),
			StatusV: choices.Processing,
		},
		ParentV: rejectedBlk.IDV,
		HeightV: 2,
		BytesV:  []byte{3},
	}

	vm.ParseBlockF = func(b []byte) (snowman.Block, error) {
		switch {
		case bytes.Equal(b, acceptedBlk.Bytes()):
			return acceptedBlk, nil
		case bytes.Equal(b, rejectedBlk.Bytes()):
			return rejectedBlk, nil
		case bytes.Equal(b, pendingBlk.Bytes()):
			return pendingBlk, nil
		default:
			t.Fatalf("Unknown block bytes")
			return nil, nil
		}
	}

	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		switch blkID {
		case gBlk.ID():
			return gBlk, nil
		default:
			return nil, errUnknownBlock
		}
	}

	var (
		queried bool
		reqID   uint32
	)
	sender.SendPushQueryF = func(_ ids.NodeIDSet, rID uint32, blkBytes []byte) {
		queried = true
		reqID = rID
	}

	if err := te.Put(vdr, 0, acceptedBlk.Bytes()); err != nil {
		t.Fatal(err)
	}

	if !queried {
		t.Fatalf("Didn't query for the new block")
	}

	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		switch blkID {
		case gBlk.ID():
			return gBlk, nil
		case rejectedBlk.ID():
			return rejectedBlk, nil
		case acceptedBlk.ID():
			return acceptedBlk, nil
		default:
			return nil, errUnknownBlock
		}
	}

	if err := te.Chits(vdr, reqID, []ids.ID{acceptedBlk.ID()}); err != nil {
		t.Fatal(err)
	}

	if !te.Consensus.Finalized() {
		t.Fatalf("Should have finalized the consensus instance")
	}

	if err := te.Put(vdr, 0, pendingBlk.Bytes()); err != nil {
		t.Fatal(err)
	}

	if !te.Consensus.Finalized() {
		t.Fatalf("Should have finalized the consensus instance")
	}

	if len(te.pending) != 0 {
		t.Fatalf("Shouldn't have any pending blocks")
	}
}

// Test that the node will not gossip a block that isn't preferred.
func TestEngineNonPreferredAmplification(t *testing.T) {
	vdr, _, sender, vm, te, gBlk := setupDefaultConfig(t)

	preferredBlk := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentV: gBlk.ID(),
		HeightV: 1,
		BytesV:  []byte{1},
	}
	nonPreferredBlk := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentV: gBlk.ID(),
		HeightV: 1,
		BytesV:  []byte{2},
	}

	vm.ParseBlockF = func(b []byte) (snowman.Block, error) {
		switch {
		case bytes.Equal(b, preferredBlk.Bytes()):
			return preferredBlk, nil
		case bytes.Equal(b, nonPreferredBlk.Bytes()):
			return nonPreferredBlk, nil
		default:
			t.Fatalf("Unknown block bytes")
			return nil, nil
		}
	}

	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		switch blkID {
		case gBlk.ID():
			return gBlk, nil
		default:
			return nil, errUnknownBlock
		}
	}

	sender.SendPushQueryF = func(_ ids.NodeIDSet, _ uint32, blkBytes []byte) {
		if bytes.Equal(nonPreferredBlk.Bytes(), blkBytes) {
			t.Fatalf("gossiped non-preferred block")
		}
	}
	sender.SendPullQueryF = func(_ ids.NodeIDSet, _ uint32, blkID ids.ID) {
		if blkID == nonPreferredBlk.ID() {
			t.Fatalf("gossiped non-preferred block")
		}
	}

	if err := te.Put(vdr, 0, preferredBlk.Bytes()); err != nil {
		t.Fatal(err)
	}

	if err := te.Put(vdr, 0, nonPreferredBlk.Bytes()); err != nil {
		t.Fatal(err)
	}
}

// Test that in the following scenario, if block B fails verification, votes
// will still be bubbled through to the valid block A. This is a regression test
// to ensure that the consensus engine correctly handles the case that votes can
// be bubbled correctly through a block that cannot pass verification until one
// of its ancestors has been marked as accepted.
//  G
//  |
//  A
//  |
//  B
func TestEngineBubbleVotesThroughInvalidBlock(t *testing.T) {
	vdr, _, sender, vm, te, gBlk := setupDefaultConfig(t)

	// [blk1] is a child of [gBlk] and currently passes verification
	blk1 := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentV: gBlk.ID(),
		HeightV: 1,
		BytesV:  []byte{1},
	}
	// [blk2] is a child of [blk1] and cannot pass verification until [blk1]
	// has been marked as accepted.
	blk2 := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentV: blk1.ID(),
		HeightV: 2,
		BytesV:  []byte{2},
		VerifyV: errors.New("blk2 does not pass verification until after blk1 is accepted"),
	}

	// The VM should be able to parse [blk1] and [blk2]
	vm.ParseBlockF = func(b []byte) (snowman.Block, error) {
		switch {
		case bytes.Equal(b, blk1.Bytes()):
			return blk1, nil
		case bytes.Equal(b, blk2.Bytes()):
			return blk2, nil
		default:
			t.Fatalf("Unknown block bytes")
			return nil, nil
		}
	}

	// The VM should only be able to retrieve [gBlk] from storage
	// TODO GetBlockF should be updated after blocks are verified/accepted
	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		switch blkID {
		case gBlk.ID():
			return gBlk, nil
		default:
			return nil, errUnknownBlock
		}
	}

	asked := new(bool)
	reqID := new(uint32)
	sender.SendGetF = func(inVdr ids.NodeID, requestID uint32, blkID ids.ID) {
		*reqID = requestID
		if *asked {
			t.Fatalf("Asked multiple times")
		}
		if blkID != blk1.ID() {
			t.Fatalf("Expected engine to request blk1")
		}
		if inVdr != vdr {
			t.Fatalf("Expected engine to request blk2 from vdr")
		}
		*asked = true
	}
	// Receive Gossip message for [blk2] first and expect the sender to issue a Get request for
	// its ancestor: [blk1].
	if err := te.Put(vdr, constants.GossipMsgRequestID, blk2.Bytes()); err != nil {
		t.Fatal(err)
	}

	if !*asked {
		t.Fatalf("Didn't ask for missing blk1")
	}

	// Prepare to PushQuery [blk1] after our Get request is fulfilled. We should not PushQuery
	// [blk2] since it currently fails verification.
	queried := new(bool)
	queryRequestID := new(uint32)
	sender.SendPushQueryF = func(inVdrs ids.NodeIDSet, requestID uint32, blkBytes []byte) {
		if *queried {
			t.Fatalf("Asked multiple times")
		}
		*queried = true
		*queryRequestID = requestID
		vdrSet := ids.NodeIDSet{}
		vdrSet.Add(vdr)
		if !inVdrs.Equals(vdrSet) {
			t.Fatalf("Asking wrong validator for preference")
		}
		if !bytes.Equal(blk1.Bytes(), blkBytes) {
			t.Fatalf("Asking for wrong block")
		}
	}

	// Answer the request, this should allow [blk1] to be issued and cause [blk2] to
	// fail verification.
	if err := te.Put(vdr, *reqID, blk1.Bytes()); err != nil {
		t.Fatal(err)
	}

	// now blk1 is verified, vm can return it
	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		switch blkID {
		case gBlk.ID():
			return gBlk, nil
		case blk1.ID():
			return blk1, nil
		default:
			return nil, errUnknownBlock
		}
	}

	if !*queried {
		t.Fatalf("Didn't ask for preferences regarding blk1")
	}

	sendReqID := new(uint32)
	reqVdr := new(ids.NodeID)
	// Update GetF to produce a more detailed error message in the case that receiving a Chits
	// message causes us to send another Get request.
	sender.SendGetF = func(inVdr ids.NodeID, requestID uint32, blkID ids.ID) {
		switch blkID {
		case blk1.ID():
			t.Fatal("Unexpectedly sent a Get request for blk1")
		case blk2.ID():
			*sendReqID = requestID
			*reqVdr = inVdr
			return
		default:
			t.Fatal("Unexpectedly sent a Get request for unknown block")
		}
	}

	sender.SendPullQueryF = func(_ ids.NodeIDSet, _ uint32, blkID ids.ID) {
		switch blkID {
		case blk1.ID():
			t.Fatal("Unexpectedly sent a PullQuery request for blk1")
		case blk2.ID():
			t.Fatal("Unexpectedly sent a PullQuery request for blk2")
		default:
			t.Fatal("Unexpectedly sent a PullQuery request for unknown block")
		}
	}

	// Now we are expecting a Chits message, and we receive it for blk2 instead of blk1
	// The votes should be bubbled through blk2 despite the fact that it is failing verification.
	if err := te.Chits(vdr, *queryRequestID, []ids.ID{blk2.ID()}); err != nil {
		t.Fatal(err)
	}

	if err := te.Put(*reqVdr, *sendReqID, blk2.Bytes()); err != nil {
		t.Fatal(err)
	}

	// The vote should be bubbled through [blk2], such that [blk1] gets marked as Accepted.
	if blk1.Status() != choices.Accepted {
		t.Fatalf("Expected blk1 to be Accepted, but found status: %s", blk1.Status())
	}

	// Now that [blk1] has been marked as Accepted, [blk2] can pass verification.
	blk2.VerifyV = nil
	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		switch blkID {
		case gBlk.ID():
			return gBlk, nil
		case blk1.ID():
			return blk1, nil
		case blk2.ID():
			return blk2, nil
		default:
			return nil, errUnknownBlock
		}
	}
	*queried = false
	// Prepare to PushQuery [blk2] after receiving a Gossip message with [blk2].
	sender.SendPushQueryF = func(inVdrs ids.NodeIDSet, requestID uint32, blkBytes []byte) {
		if *queried {
			t.Fatalf("Asked multiple times")
		}
		*queried = true
		*queryRequestID = requestID
		vdrSet := ids.NodeIDSet{}
		vdrSet.Add(vdr)
		if !inVdrs.Equals(vdrSet) {
			t.Fatalf("Asking wrong validator for preference")
		}
		if !bytes.Equal(blk2.Bytes(), blkBytes) {
			t.Fatalf("Asking for wrong block")
		}
	}
	// Expect that the Engine will send a PushQuery after receiving this Gossip message for [blk2].
	if err := te.Put(vdr, constants.GossipMsgRequestID, blk2.Bytes()); err != nil {
		t.Fatal(err)
	}

	if !*queried {
		t.Fatalf("Didn't ask for preferences regarding blk2")
	}

	// After a single vote for [blk2], it should be marked as accepted.
	if err := te.Chits(vdr, *queryRequestID, []ids.ID{blk2.ID()}); err != nil {
		t.Fatal(err)
	}

	if blk2.Status() != choices.Accepted {
		t.Fatalf("Expected blk2 to be Accepted, but found status: %s", blk2.Status())
	}
}

// Test that in the following scenario, if block B fails verification, votes
// will still be bubbled through from block C to the valid block A. This is a
// regression test to ensure that the consensus engine correctly handles the
// case that votes can be bubbled correctly through a chain that cannot pass
// verification until one of its ancestors has been marked as accepted.
//  G
//  |
//  A
//  |
//  B
//  |
//  C
func TestEngineBubbleVotesThroughInvalidChain(t *testing.T) {
	vdr, _, sender, vm, te, gBlk := setupDefaultConfig(t)

	// [blk1] is a child of [gBlk] and currently passes verification
	blk1 := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentV: gBlk.ID(),
		HeightV: 1,
		BytesV:  []byte{1},
	}
	// [blk2] is a child of [blk1] and cannot pass verification until [blk1]
	// has been marked as accepted.
	blk2 := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentV: blk1.ID(),
		HeightV: 2,
		BytesV:  []byte{2},
		VerifyV: errors.New("blk2 does not pass verification until after blk1 is accepted"),
	}
	// [blk3] is a child of [blk2] and will not attempt to be issued until
	// [blk2] has successfully been verified.
	blk3 := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentV: blk2.ID(),
		HeightV: 3,
		BytesV:  []byte{3},
	}

	// The VM should be able to parse [blk1], [blk2], and [blk3]
	vm.ParseBlockF = func(b []byte) (snowman.Block, error) {
		switch {
		case bytes.Equal(b, blk1.Bytes()):
			return blk1, nil
		case bytes.Equal(b, blk2.Bytes()):
			return blk2, nil
		case bytes.Equal(b, blk3.Bytes()):
			return blk3, nil
		default:
			t.Fatalf("Unknown block bytes")
			return nil, nil
		}
	}

	// The VM should be able to retrieve [gBlk] and [blk1] from storage
	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		switch blkID {
		case gBlk.ID():
			return gBlk, nil
		case blk1.ID():
			return blk1, nil
		default:
			return nil, errUnknownBlock
		}
	}

	asked := new(bool)
	reqID := new(uint32)
	sender.SendGetF = func(inVdr ids.NodeID, requestID uint32, blkID ids.ID) {
		*reqID = requestID
		if *asked {
			t.Fatalf("Asked multiple times")
		}
		if blkID != blk2.ID() {
			t.Fatalf("Expected engine to request blk2")
		}
		if inVdr != vdr {
			t.Fatalf("Expected engine to request blk2 from vdr")
		}
		*asked = true
	}
	// Receive Gossip message for [blk3] first and expect the sender to issue a
	// Get request for its ancestor: [blk2].
	if err := te.Put(vdr, constants.GossipMsgRequestID, blk3.Bytes()); err != nil {
		t.Fatal(err)
	}

	if !*asked {
		t.Fatalf("Didn't ask for missing blk2")
	}

	// Prepare to PushQuery [blk1] after our request for [blk2] is fulfilled.
	// We should not PushQuery [blk2] since it currently fails verification.
	// We should not PushQuery [blk3] because [blk2] wasn't issued.
	queried := new(bool)
	queryRequestID := new(uint32)
	sender.SendPushQueryF = func(inVdrs ids.NodeIDSet, requestID uint32, blkBytes []byte) {
		if *queried {
			t.Fatalf("Asked multiple times")
		}
		*queried = true
		*queryRequestID = requestID
		vdrSet := ids.NodeIDSet{}
		vdrSet.Add(vdr)
		if !inVdrs.Equals(vdrSet) {
			t.Fatalf("Asking wrong validator for preference")
		}
		if !bytes.Equal(blk1.Bytes(), blkBytes) {
			t.Fatalf("Asking for wrong block")
		}
	}

	// Answer the request, this should result in [blk1] being issued as well.
	if err := te.Put(vdr, *reqID, blk2.Bytes()); err != nil {
		t.Fatal(err)
	}

	if !*queried {
		t.Fatalf("Didn't ask for preferences regarding blk1")
	}

	sendReqID := new(uint32)
	reqVdr := new(ids.NodeID)
	// Update GetF to produce a more detailed error message in the case that receiving a Chits
	// message causes us to send another Get request.
	sender.SendGetF = func(inVdr ids.NodeID, requestID uint32, blkID ids.ID) {
		switch blkID {
		case blk1.ID():
			t.Fatal("Unexpectedly sent a Get request for blk1")
		case blk2.ID():
			t.Logf("sending get for blk2 with %d", requestID)
			*sendReqID = requestID
			*reqVdr = inVdr
			return
		case blk3.ID():
			t.Logf("sending get for blk3 with %d", requestID)
			*sendReqID = requestID
			*reqVdr = inVdr
			return
		default:
			t.Fatal("Unexpectedly sent a Get request for unknown block")
		}
	}

	sender.SendPullQueryF = func(_ ids.NodeIDSet, _ uint32, blkID ids.ID) {
		switch blkID {
		case blk1.ID():
			t.Fatal("Unexpectedly sent a PullQuery request for blk1")
		case blk2.ID():
			t.Fatal("Unexpectedly sent a PullQuery request for blk2")
		case blk3.ID():
			t.Fatal("Unexpectedly sent a PullQuery request for blk3")
		default:
			t.Fatal("Unexpectedly sent a PullQuery request for unknown block")
		}
	}

	// Now we are expecting a Chits message, and we receive it for [blk3]
	// instead of blk1. This will cause the node to again request [blk3].
	if err := te.Chits(vdr, *queryRequestID, []ids.ID{blk3.ID()}); err != nil {
		t.Fatal(err)
	}

	// Drop the re-request for blk3 to cause the poll to termindate. The votes
	// should be bubbled through blk3 despite the fact that it hasn't been
	// issued.
	if err := te.GetFailed(*reqVdr, *sendReqID); err != nil {
		t.Fatal(err)
	}

	// The vote should be bubbled through [blk3] and [blk2] such that [blk1]
	// gets marked as Accepted.
	if blk1.Status() != choices.Accepted {
		t.Fatalf("Expected blk1 to be Accepted, but found status: %s", blk1.Status())
	}
}

func TestMixedQueryNumPushSet(t *testing.T) {
	for i := 0; i < 3; i++ {
		t.Run(
			fmt.Sprint(i),
			func(t *testing.T) {
				engCfg := DefaultConfigs()
				engCfg.Params.MixedQueryNumPushVdr = i
				te, err := newTransitive(engCfg)
				if err != nil {
					t.Fatal(err)
				}
				if te.Params.MixedQueryNumPushVdr != i {
					t.Fatalf("expected to push query %v validators but got %v", i, te.Config.Params.MixedQueryNumPushVdr)
				}
			},
		)
	}
}

func TestSendMixedQuery(t *testing.T) {
	type test struct {
		isVdr bool
	}
	tests := []test{
		{isVdr: true},
		{isVdr: false},
	}
	for _, tt := range tests {
		t.Run(
			fmt.Sprintf("is validator: %v", tt.isVdr),
			func(t *testing.T) {
				engConfig := DefaultConfigs()
				commonCfg := common.DefaultConfigTest()
				// Override the parameters k and MixedQueryNumPushNonVdr,
				// and update the validator set to have k validators.
				engConfig.Params.Alpha = 12
				engConfig.Params.MixedQueryNumPushNonVdr = 12
				engConfig.Params.MixedQueryNumPushVdr = 14
				engConfig.Params.K = 20
				_, vdrSet, sender, vm, te, gBlk := setup(t, commonCfg, engConfig)

				vdrsList := []validators.Validator{}
				vdrs := ids.NodeIDSet{}
				for i := 0; i < te.Config.Params.K; i++ {
					vdr := ids.GenerateTestNodeID()
					vdrs.Add(vdr)
					vdrsList = append(vdrsList, validators.NewValidator(vdr, 1))
				}
				if tt.isVdr {
					vdrs.Add(te.Ctx.NodeID)
					vdrsList = append(vdrsList, validators.NewValidator(te.Ctx.NodeID, 1))
				}
				if err := vdrSet.Set(vdrsList); err != nil {
					t.Fatal(err)
				}

				// [blk1] is a child of [gBlk] and passes verification
				blk1 := &snowman.TestBlock{
					TestDecidable: choices.TestDecidable{
						IDV:     ids.GenerateTestID(),
						StatusV: choices.Processing,
					},
					ParentV: gBlk.ID(),
					HeightV: 1,
					BytesV:  []byte{1},
				}

				// The VM should be able to parse [blk1]
				vm.ParseBlockF = func(b []byte) (snowman.Block, error) {
					switch {
					case bytes.Equal(b, blk1.Bytes()):
						return blk1, nil
					default:
						t.Fatalf("Unknown block bytes")
						return nil, nil
					}
				}

				// The VM should only be able to retrieve [gBlk] from storage
				vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
					switch blkID {
					case gBlk.ID():
						return gBlk, nil
					default:
						return nil, errUnknownBlock
					}
				}

				pullQuerySent := new(bool)
				pullQueryReqID := new(uint32)
				pullQueriedVdrs := ids.NodeIDSet{}
				sender.SendPullQueryF = func(inVdrs ids.NodeIDSet, requestID uint32, blkID ids.ID) {
					switch {
					case *pullQuerySent:
						t.Fatalf("Asked multiple times")
					case blkID != blk1.ID():
						t.Fatalf("Expected engine to request blk1")
					}
					pullQueriedVdrs.Union(inVdrs)
					*pullQuerySent = true
					*pullQueryReqID = requestID
				}

				pushQuerySent := new(bool)
				pushQueryReqID := new(uint32)
				pushQueriedVdrs := ids.NodeIDSet{}
				sender.SendPushQueryF = func(inVdrs ids.NodeIDSet, requestID uint32, blkBytes []byte) {
					switch {
					case *pushQuerySent:
						t.Fatal("Asked multiple times")
					case !bytes.Equal(blkBytes, blk1.Bytes()):
						t.Fatal("got unexpected block bytes instead of blk1")
					}
					*pushQuerySent = true
					*pushQueryReqID = requestID
					pushQueriedVdrs.Union(inVdrs)
				}

				// Give the engine blk1. It should insert it into consensus and send a mixed query
				// consisting of 12 push queries and 8 pull queries.
				if err := te.Put(vdrSet.List()[0].ID(), constants.GossipMsgRequestID, blk1.Bytes()); err != nil {
					t.Fatal(err)
				}

				switch {
				case !*pullQuerySent:
					t.Fatal("expected us to send pull queries")
				case !*pushQuerySent:
					t.Fatal("expected us to send push queries")
				case *pushQueryReqID != *pullQueryReqID:
					t.Fatalf("expected equal push query (%v) and pull query (%v) req IDs", *pushQueryReqID, *pullQueryReqID)
				case pushQueriedVdrs.Len()+pullQueriedVdrs.Len() != te.Config.Params.K:
					t.Fatalf("expected num push queried (%d) + num pull queried (%d) to be %d", pushQueriedVdrs.Len(), pullQueriedVdrs.Len(), te.Config.Params.K)
				case !tt.isVdr && pushQueriedVdrs.Len() != te.Params.MixedQueryNumPushNonVdr:
					t.Fatalf("expected num push queried (%d) to be %d", pushQueriedVdrs.Len(), te.Params.MixedQueryNumPushNonVdr)
				case tt.isVdr && pushQueriedVdrs.Len() != te.Params.MixedQueryNumPushVdr:
					t.Fatalf("expected num push queried (%d) to be %d", pushQueriedVdrs.Len(), te.Params.MixedQueryNumPushVdr)
				}

				pullQueriedVdrs.Union(pushQueriedVdrs) // Now this holds all queried validators (push and pull)
				for vdr := range pullQueriedVdrs {
					if !vdrs.Contains(vdr) {
						t.Fatalf("got unexpected vdr %v", vdr)
					}
				}
			})
	}
}

func TestEngineBuildBlockWithCachedNonVerifiedParent(t *testing.T) {
	require := require.New(t)
	vdr, _, sender, vm, te, gBlk := setupDefaultConfig(t)

	sender.Default(true)

	grandParentBlk := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentV: gBlk.ID(),
		HeightV: 1,
		BytesV:  []byte{1},
	}

	parentBlkA := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentV: grandParentBlk.ID(),
		HeightV: 2,
		VerifyV: errors.New(""), // Reports as invalid
		BytesV:  []byte{2},
	}

	// Note that [parentBlkB] has the same [ID()] as [parentBlkA];
	// it's a different instantiation of the same block.
	parentBlkB := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     parentBlkA.IDV,
			StatusV: choices.Processing,
		},
		ParentV: parentBlkA.ParentV,
		HeightV: parentBlkA.HeightV,
		BytesV:  parentBlkA.BytesV,
	}

	// Child of [parentBlkA]/[parentBlkB]
	childBlk := &snowman.TestBlock{
		TestDecidable: choices.TestDecidable{
			IDV:     ids.GenerateTestID(),
			StatusV: choices.Processing,
		},
		ParentV: parentBlkA.ID(),
		HeightV: 3,
		BytesV:  []byte{3},
	}

	vm.ParseBlockF = func(b []byte) (snowman.Block, error) {
		require.Equal(grandParentBlk.BytesV, b)
		return grandParentBlk, nil
	}

	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		switch blkID {
		case gBlk.ID():
			return gBlk, nil
		case grandParentBlk.IDV:
			return grandParentBlk, nil
		default:
			return nil, errUnknownBlock
		}
	}

	queryRequestGPID := new(uint32)
	sender.SendPushQueryF = func(_ ids.NodeIDSet, requestID uint32, blkBytes []byte) {
		require.Equal(grandParentBlk.Bytes(), blkBytes)
		*queryRequestGPID = requestID
	}

	// Give the engine the grandparent
	err := te.Put(vdr, 0, grandParentBlk.BytesV)
	require.NoError(err)

	vm.ParseBlockF = func(b []byte) (snowman.Block, error) {
		require.Equal(parentBlkA.BytesV, b)
		return parentBlkA, nil
	}

	// Give the node [parentBlkA]/[parentBlkB].
	// When it's parsed we get [parentBlkA] (not [parentBlkB]).
	// [parentBlkA] fails verification and gets put into [te.nonVerifiedCache].
	err = te.Put(vdr, 0, parentBlkA.BytesV)
	require.NoError(err)

	vm.ParseBlockF = func(b []byte) (snowman.Block, error) {
		require.Equal(parentBlkB.BytesV, b)
		return parentBlkB, nil
	}

	vm.GetBlockF = func(blkID ids.ID) (snowman.Block, error) {
		switch blkID {
		case gBlk.ID():
			return gBlk, nil
		case grandParentBlk.IDV:
			return grandParentBlk, nil
		case parentBlkB.IDV:
			return parentBlkB, nil
		default:
			return nil, errUnknownBlock
		}
	}

	queryRequestAID := new(uint32)
	sender.SendPushQueryF = func(_ ids.NodeIDSet, requestID uint32, blkBytes []byte) {
		require.Equal(parentBlkA.Bytes(), blkBytes)
		*queryRequestAID = requestID
	}
	sender.CantSendPullQuery = false

	// Give the engine [parentBlkA]/[parentBlkB] again.
	// This time when we parse it we get [parentBlkB] (not [parentBlkA]).
	// When we fetch it using [GetBlockF] we get [parentBlkB].
	// Note that [parentBlkB] doesn't fail verification and is issued into consensus.
	// This evicts [parentBlkA] from [te.nonVerifiedCache].
	err = te.Put(vdr, 0, parentBlkA.BytesV)
	require.NoError(err)

	// Give 2 chits for [parentBlkA]/[parentBlkB]
	err = te.Chits(vdr, *queryRequestAID, []ids.ID{parentBlkB.IDV})
	require.NoError(err)

	err = te.Chits(vdr, *queryRequestGPID, []ids.ID{parentBlkB.IDV})
	require.NoError(err)

	// Assert that the blocks' statuses are correct.
	// The evicted [parentBlkA] shouldn't be changed.
	require.Equal(choices.Processing, parentBlkA.Status())
	require.Equal(choices.Accepted, parentBlkB.Status())

	vm.BuildBlockF = func() (snowman.Block, error) {
		return childBlk, nil
	}

	sentQuery := new(bool)
	sender.SendPushQueryF = func(ids.NodeIDSet, uint32, []byte) {
		*sentQuery = true
	}

	// Should issue a new block and send a query for it.
	err = te.Notify(common.PendingTxs)
	require.NoError(err)
	require.True(*sentQuery)
}

```

avalanchego/snow/engine/snowman/voter.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snowman

import (
	"go.uber.org/zap"

	"github.com/ava-labs/avalanchego/ids"
)

// Voter records chits received from [vdr] once its dependencies are met.
type voter struct {
	t         *Transitive
	vdr       ids.NodeID
	requestID uint32
	response  ids.ID
	deps      ids.Set
}

func (v *voter) Dependencies() ids.Set { return v.deps }

// Mark that a dependency has been met.
func (v *voter) Fulfill(id ids.ID) {
	v.deps.Remove(id)
	v.Update()
}

// Abandon this attempt to record chits.
func (v *voter) Abandon(id ids.ID) { v.Fulfill(id) }

func (v *voter) Update() {
	if v.deps.Len() != 0 || v.t.errs.Errored() {
		return
	}

	var results []ids.Bag
	if v.response == ids.Empty {
		results = v.t.polls.Drop(v.requestID, v.vdr)
	} else {
		results = v.t.polls.Vote(v.requestID, v.vdr, v.response)
	}

	if len(results) == 0 {
		return
	}

	// To prevent any potential deadlocks with un-disclosed dependencies, votes
	// must be bubbled to the nearest valid block
	for i, result := range results {
		results[i] = v.bubbleVotes(result)
	}

	for _, result := range results {
		result := result

		v.t.Ctx.Log.Debug("finishing poll",
			zap.Stringer("result", &result),
		)
		if err := v.t.Consensus.RecordPoll(result); err != nil {
			v.t.errs.Add(err)
		}
	}

	if v.t.errs.Errored() {
		return
	}

	if err := v.t.VM.SetPreference(v.t.Consensus.Preference()); err != nil {
		v.t.errs.Add(err)
		return
	}

	if v.t.Consensus.Finalized() {
		v.t.Ctx.Log.Debug("Snowman engine can quiesce")
		return
	}

	v.t.Ctx.Log.Debug("Snowman engine can't quiesce")
	v.t.repoll()
}

// bubbleVotes bubbles the [votes] a set of the number of votes for specific
// blkIDs that received votes in consensus, to their most recent ancestor that
// has been issued to consensus.
//
// Note: bubbleVotes does not bubbleVotes to all of the ancestors in consensus,
// just the most recent one. bubbling to the rest of the ancestors, which may
// also be in consensus is handled in RecordPoll.
func (v *voter) bubbleVotes(votes ids.Bag) ids.Bag {
	bubbledVotes := ids.Bag{}

votesLoop:
	for _, vote := range votes.List() {
		count := votes.Count(vote)
		// use rootID in case of this is a non-verified block ID
		rootID := v.t.nonVerifieds.GetRoot(vote)
		v.t.Ctx.Log.Verbo("bubbling vote(s) through unverified blocks",
			zap.Int("numVotes", count),
			zap.Stringer("voteID", vote),
			zap.Stringer("parentID", rootID),
		)

		blk, err := v.t.GetBlock(rootID)
		// If we cannot retrieve the block, drop [vote]
		if err != nil {
			v.t.Ctx.Log.Debug("dropping vote(s)",
				zap.String("reason", "parent couldn't be fetched"),
				zap.Stringer("parentID", rootID),
				zap.Int("numVotes", count),
				zap.Stringer("voteID", vote),
				zap.Error(err),
			)
			continue
		}

		status := blk.Status()
		blkID := blk.ID()
		// If we have not fetched [blkID] break from the loop. We will drop the
		// vote below and move on to the next vote.
		//
		// If [blk] has already been decided, break from the loop, we will drop
		// the vote below since there is no need to count the votes for a [blk]
		// we've already finalized.
		//
		// If [blk] is currently in consensus, break from the loop, we have
		// reached the first ancestor of the original [vote] that has been
		// issued consensus. In this case, the votes will be bubbled further
		// from [blk] to any of its ancestors that are also in consensus.
		for status.Fetched() && !(v.t.Consensus.Decided(blk) || v.t.Consensus.Processing(blkID)) {
			parentID := blk.Parent()
			v.t.Ctx.Log.Verbo("pushing vote(s)",
				zap.Int("numVotes", count),
				zap.Stringer("voteID", vote),
				zap.Stringer("parentID", rootID),
			)

			blkID = parentID
			blk, err = v.t.GetBlock(blkID)
			// If we cannot retrieve the block, drop [vote]
			if err != nil {
				v.t.Ctx.Log.Debug("dropping vote(s)",
					zap.String("reason", "block couldn't be fetched"),
					zap.Stringer("blkID", blkID),
					zap.Int("numVotes", count),
					zap.Stringer("voteID", vote),
					zap.Error(err),
				)
				continue votesLoop
			}
			status = blk.Status()
		}

		// If [blkID] is currently in consensus, count the votes
		if v.t.Consensus.Processing(blkID) {
			v.t.Ctx.Log.Verbo("applying vote(s)",
				zap.Int("numVotes", count),
				zap.Stringer("blkID", blkID),
				zap.Stringer("status", status),
			)
			bubbledVotes.AddCount(blkID, count)
		} else {
			v.t.Ctx.Log.Verbo("dropping vote(s)",
				zap.Int("numVotes", count),
				zap.Stringer("blkID", blkID),
				zap.Stringer("status", status),
			)
		}
	}
	return bubbledVotes
}

```

avalanchego/snow/events/blockable.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package events

import (
	"github.com/ava-labs/avalanchego/ids"
)

// Blockable defines what an object must implement to be able to block on
// dependent events being completed.
type Blockable interface {
	// IDs that this object is blocking on
	Dependencies() ids.Set
	// Notify this object that an event has been fulfilled
	Fulfill(ids.ID)
	// Notify this object that an event has been abandoned
	Abandon(ids.ID)
	// Update the state of this object without changing the status of any events
	Update()
}

```

avalanchego/snow/events/blocker.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package events

import (
	"fmt"
	"strings"

	"github.com/ava-labs/avalanchego/ids"
)

const (
	minBlockerSize = 16
)

// Blocker tracks Blockable events.
// Blocker is used to track events that require their dependencies to be
// fulfilled before them. Once a Blockable event is registered, it will be
// notified once any of its dependencies are fulfilled or abandoned.
type Blocker map[ids.ID][]Blockable

func (b *Blocker) init() {
	if *b == nil {
		*b = make(map[ids.ID][]Blockable, minBlockerSize)
	}
}

// Returns the number of items that have dependencies waiting on
// them to be fulfilled
func (b *Blocker) Len() int {
	return len(*b)
}

// Fulfill notifies all objects blocking on the event whose ID is <id> that
// the event has happened
func (b *Blocker) Fulfill(id ids.ID) {
	b.init()

	blocking := (*b)[id]
	delete(*b, id)

	for _, pending := range blocking {
		pending.Fulfill(id)
	}
}

// Abandon notifies all objects blocking on the event whose ID is <id> that
// the event has been abandoned
func (b *Blocker) Abandon(id ids.ID) {
	b.init()

	blocking := (*b)[id]
	delete(*b, id)

	for _, pending := range blocking {
		pending.Abandon(id)
	}
}

// Register a new Blockable and its dependencies
func (b *Blocker) Register(pending Blockable) {
	b.init()

	for pendingID := range pending.Dependencies() {
		(*b)[pendingID] = append((*b)[pendingID], pending)
	}

	pending.Update()
}

// PrefixedString returns the same value as the String function, with all the
// new lines prefixed by [prefix]
func (b *Blocker) PrefixedString(prefix string) string {
	b.init()

	s := strings.Builder{}

	s.WriteString(fmt.Sprintf("Blocking on %d IDs:", len(*b)))

	for key, value := range *b {
		s.WriteString(fmt.Sprintf("\n%sID[%s]: %d",
			prefix,
			key,
			len(value)))
	}

	return strings.TrimSuffix(s.String(), "\n")
}

func (b *Blocker) String() string { return b.PrefixedString("") }

```

avalanchego/snow/events/blocker_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package events

import (
	"testing"

	"github.com/ava-labs/avalanchego/ids"
)

func TestBlocker(t *testing.T) {
	b := Blocker(nil)

	a := newTestBlockable()

	id0 := ids.GenerateTestID()
	id1 := ids.GenerateTestID()
	id2 := ids.GenerateTestID()

	calledDep := new(bool)
	a.dependencies = func() ids.Set {
		*calledDep = true

		s := ids.Set{}
		s.Add(id0, id1)
		return s
	}
	calledFill := new(bool)
	a.fulfill = func(ids.ID) {
		*calledFill = true
	}
	calledAbandon := new(bool)
	a.abandon = func(ids.ID) {
		*calledAbandon = true
	}
	calledUpdate := new(bool)
	a.update = func() {
		*calledUpdate = true
	}

	b.Register(a)

	switch {
	case !*calledDep, *calledFill, *calledAbandon, !*calledUpdate:
		t.Fatalf("Called wrong function")
	}

	b.Fulfill(id2)
	b.Abandon(id2)

	switch {
	case !*calledDep, *calledFill, *calledAbandon, !*calledUpdate:
		t.Fatalf("Called wrong function")
	}

	b.Fulfill(id0)

	switch {
	case !*calledDep, !*calledFill, *calledAbandon, !*calledUpdate:
		t.Fatalf("Called wrong function")
	}

	b.Abandon(id0)

	switch {
	case !*calledDep, !*calledFill, *calledAbandon, !*calledUpdate:
		t.Fatalf("Called wrong function")
	}

	b.Abandon(id1)

	switch {
	case !*calledDep, !*calledFill, !*calledAbandon, !*calledUpdate:
		t.Fatalf("Called wrong function")
	}
}

type testBlockable struct {
	dependencies func() ids.Set
	fulfill      func(ids.ID)
	abandon      func(ids.ID)
	update       func()
}

func newTestBlockable() *testBlockable {
	return &testBlockable{
		dependencies: func() ids.Set { return ids.Set{} },
		fulfill:      func(ids.ID) {},
		abandon:      func(ids.ID) {},
		update:       func() {},
	}
}

func (b *testBlockable) Dependencies() ids.Set { return b.dependencies() }
func (b *testBlockable) Fulfill(id ids.ID)     { b.fulfill(id) }
func (b *testBlockable) Abandon(id ids.ID)     { b.abandon(id) }
func (b *testBlockable) Update()               { b.update() }

```

avalanchego/snow/networking/benchlist/benchable.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package benchlist

import (
	"github.com/ava-labs/avalanchego/ids"
)

// Benchable is notified when a validator is benched or unbenched from a given chain
type Benchable interface {
	// Mark that [validatorID] has been benched on the given chain
	Benched(chainID ids.ID, validatorID ids.NodeID)
	// Mark that [validatorID] has been unbenched from the given chain
	Unbenched(chainID ids.ID, validatorID ids.NodeID)
}

```

avalanchego/snow/networking/benchlist/benchlist.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package benchlist

import (
	"container/heap"
	"fmt"
	"math/rand"
	"sync"
	"time"

	"github.com/prometheus/client_golang/prometheus"

	"go.uber.org/zap"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/validators"
	"github.com/ava-labs/avalanchego/utils/logging"
	"github.com/ava-labs/avalanchego/utils/timer"
	"github.com/ava-labs/avalanchego/utils/timer/mockable"

	safemath "github.com/ava-labs/avalanchego/utils/math"
)

var _ heap.Interface = &benchedQueue{}

// If a peer consistently does not respond to queries, it will
// increase latencies on the network whenever that peer is polled.
// If we cannot terminate the poll early, then the poll will wait
// the full timeout before finalizing the poll and making progress.
// This can increase network latencies to an undesirable level.

// Therefore, nodes that consistently fail are "benched" such that
// queries to that node fail immediately to avoid waiting up to
// the full network timeout for a response.
type Benchlist interface {
	// RegisterResponse registers the response to a query message
	RegisterResponse(nodeID ids.NodeID)
	// RegisterFailure registers that we didn't receive a response within the timeout
	RegisterFailure(nodeID ids.NodeID)
	// IsBenched returns true if messages to [validatorID]
	// should not be sent over the network and should immediately fail.
	IsBenched(nodeID ids.NodeID) bool
}

// Data about a validator who is benched
type benchData struct {
	benchedUntil time.Time
	nodeID       ids.NodeID
	index        int
}

// Each element is a benched validator
type benchedQueue []*benchData

func (bq benchedQueue) Len() int           { return len(bq) }
func (bq benchedQueue) Less(i, j int) bool { return bq[i].benchedUntil.Before(bq[j].benchedUntil) }
func (bq benchedQueue) Swap(i, j int) {
	bq[i], bq[j] = bq[j], bq[i]
	bq[i].index = i
	bq[j].index = j
}

// Push adds an item to this  queue. x must have type *benchData
func (bq *benchedQueue) Push(x interface{}) {
	item := x.(*benchData)
	item.index = len(*bq)
	*bq = append(*bq, item)
}

// Pop returns the validator that should leave the bench next
func (bq *benchedQueue) Pop() interface{} {
	n := len(*bq)
	item := (*bq)[n-1]
	(*bq)[n-1] = nil // make sure the item is freed from memory
	*bq = (*bq)[:n-1]
	return item
}

type failureStreak struct {
	// Time of first consecutive timeout
	firstFailure time.Time
	// Number of consecutive message timeouts
	consecutive int
}

type benchlist struct {
	lock sync.RWMutex
	// This is the benchlist for chain [chainID]
	chainID ids.ID
	log     logging.Logger
	metrics metrics

	// Fires when the next validator should leave the bench
	// Calls [update] when it fires
	timer *timer.Timer

	// Tells the time. Can be faked for testing.
	clock mockable.Clock

	// notified when a node is benched or unbenched
	benchable Benchable

	// Validator set of the network
	vdrs validators.Set

	// Validator ID --> Consecutive failure information
	// [streaklock] must be held when touching [failureStreaks]
	streaklock     sync.Mutex
	failureStreaks map[ids.NodeID]failureStreak

	// IDs of validators that are currently benched
	benchlistSet ids.NodeIDSet

	// Min heap containing benched validators and their endtimes
	// Pop() returns the next validator to leave
	benchedQueue benchedQueue

	// A validator will be benched if [threshold] messages in a row
	// to them time out and the first of those messages was more than
	// [minimumFailingDuration] ago
	threshold              int
	minimumFailingDuration time.Duration

	// A benched validator will be benched for between [duration/2] and [duration]
	duration time.Duration

	// The maximum percentage of total network stake that may be benched
	// Must be in [0,1)
	maxPortion float64
}

// NewBenchlist returns a new Benchlist
func NewBenchlist(
	chainID ids.ID,
	log logging.Logger,
	benchable Benchable,
	validators validators.Set,
	threshold int,
	minimumFailingDuration,
	duration time.Duration,
	maxPortion float64,
	registerer prometheus.Registerer,
) (Benchlist, error) {
	if maxPortion < 0 || maxPortion >= 1 {
		return nil, fmt.Errorf("max portion of benched stake must be in [0,1) but got %f", maxPortion)
	}
	benchlist := &benchlist{
		chainID:                chainID,
		log:                    log,
		failureStreaks:         make(map[ids.NodeID]failureStreak),
		benchlistSet:           ids.NodeIDSet{},
		benchable:              benchable,
		vdrs:                   validators,
		threshold:              threshold,
		minimumFailingDuration: minimumFailingDuration,
		duration:               duration,
		maxPortion:             maxPortion,
	}
	benchlist.timer = timer.NewTimer(benchlist.update)
	go benchlist.timer.Dispatch()
	return benchlist, benchlist.metrics.Initialize(registerer)
}

// Update removes benched validators whose time on the bench is over
func (b *benchlist) update() {
	b.lock.Lock()
	defer b.lock.Unlock()

	now := b.clock.Time()
	for {
		// [next] is nil when no more validators should
		// leave the bench at this time
		next := b.nextToLeave(now)
		if next == nil {
			break
		}
		b.remove(next)
	}
	// Set next time update will be called
	b.setNextLeaveTime()
}

// Remove [validator] from the benchlist
// Assumes [b.lock] is held
func (b *benchlist) remove(node *benchData) {
	// Update state
	id := node.nodeID
	b.log.Debug("removing node from benchlist",
		zap.Stringer("nodeID", id),
	)
	heap.Remove(&b.benchedQueue, node.index)
	b.benchlistSet.Remove(id)
	b.benchable.Unbenched(b.chainID, id)

	// Update metrics
	b.metrics.numBenched.Set(float64(b.benchedQueue.Len()))
	benchedStake, err := b.vdrs.SubsetWeight(b.benchlistSet)
	if err != nil {
		// This should never happen
		b.log.Error("couldn't get benched stake",
			zap.Error(err),
		)
		return
	}
	b.metrics.weightBenched.Set(float64(benchedStake))
}

// Returns the next validator that should leave
// the bench at time [now]. nil if no validator should.
// Assumes [b.lock] is held
func (b *benchlist) nextToLeave(now time.Time) *benchData {
	if b.benchedQueue.Len() == 0 {
		return nil
	}
	next := b.benchedQueue[0]
	if now.Before(next.benchedUntil) {
		return nil
	}
	return next
}

// Set [b.timer] to fire when the next validator should leave the bench
// Assumes [b.lock] is held
func (b *benchlist) setNextLeaveTime() {
	if b.benchedQueue.Len() == 0 {
		b.timer.Cancel()
		return
	}
	now := b.clock.Time()
	next := b.benchedQueue[0]
	nextLeave := next.benchedUntil.Sub(now)
	b.timer.SetTimeoutIn(nextLeave)
}

// IsBenched returns true if messages to [nodeID]
// should not be sent over the network and should immediately fail.
func (b *benchlist) IsBenched(nodeID ids.NodeID) bool {
	b.lock.RLock()
	defer b.lock.RUnlock()
	return b.isBenched(nodeID)
}

// isBenched checks if [nodeID] is currently benched
// and calls cleanup if its benching period has elapsed
// Assumes [b.lock] is held.
func (b *benchlist) isBenched(nodeID ids.NodeID) bool {
	if _, ok := b.benchlistSet[nodeID]; ok {
		return true
	}
	return false
}

// RegisterResponse notes that we received a response from validator [validatorID]
func (b *benchlist) RegisterResponse(nodeID ids.NodeID) {
	b.streaklock.Lock()
	defer b.streaklock.Unlock()
	delete(b.failureStreaks, nodeID)
}

// RegisterResponse notes that a request to validator [validatorID] timed out
func (b *benchlist) RegisterFailure(nodeID ids.NodeID) {
	b.lock.Lock()
	defer b.lock.Unlock()

	if b.benchlistSet.Contains(nodeID) {
		// This validator is benched. Ignore failures until they're not.
		return
	}

	b.streaklock.Lock()
	failureStreak := b.failureStreaks[nodeID]
	// Increment consecutive failures
	failureStreak.consecutive++
	now := b.clock.Time()
	// Update first failure time
	if failureStreak.firstFailure.IsZero() {
		// This is the first consecutive failure
		failureStreak.firstFailure = now
	}
	b.failureStreaks[nodeID] = failureStreak
	b.streaklock.Unlock()

	if failureStreak.consecutive >= b.threshold && now.After(failureStreak.firstFailure.Add(b.minimumFailingDuration)) {
		b.bench(nodeID)
	}
}

// Assumes [b.lock] is held
// Assumes [nodeID] is not already benched
func (b *benchlist) bench(nodeID ids.NodeID) {
	benchedStake, err := b.vdrs.SubsetWeight(b.benchlistSet)
	if err != nil {
		// This should never happen
		b.log.Error("couldn't get benched stake, resetting benchlist",
			zap.Error(err),
		)
		return
	}

	validatorStake, isVdr := b.vdrs.GetWeight(nodeID)
	if !isVdr {
		// We might want to bench a non-validator because they don't respond to
		// my Get requests, but we choose to only bench validators.
		return
	}

	newBenchedStake, err := safemath.Add64(benchedStake, validatorStake)
	if err != nil {
		// This should never happen
		b.log.Error("overflow calculating new benched stake",
			zap.Stringer("nodeID", nodeID),
		)
		return
	}

	totalStake := b.vdrs.Weight()
	maxBenchedStake := float64(totalStake) * b.maxPortion

	if float64(newBenchedStake) > maxBenchedStake {
		b.log.Debug("not benching node",
			zap.String("reason", "benched stake would exceed max"),
			zap.Stringer("nodeID", nodeID),
			zap.Float64("benchedStake", float64(newBenchedStake)),
			zap.Float64("maxBenchedStake", maxBenchedStake),
		)
		return
	}

	// Validator is benched for between [b.duration]/2 and [b.duration]
	now := b.clock.Time()
	minBenchDuration := b.duration / 2
	minBenchedUntil := now.Add(minBenchDuration)
	maxBenchedUntil := now.Add(b.duration)
	diff := maxBenchedUntil.Sub(minBenchedUntil)
	benchedUntil := minBenchedUntil.Add(time.Duration(rand.Float64() * float64(diff))) // #nosec G404

	// Add to benchlist times with randomized delay
	b.benchlistSet.Add(nodeID)
	b.benchable.Benched(b.chainID, nodeID)

	b.streaklock.Lock()
	delete(b.failureStreaks, nodeID)
	b.streaklock.Unlock()

	heap.Push(
		&b.benchedQueue,
		&benchData{nodeID: nodeID, benchedUntil: benchedUntil},
	)
	b.log.Debug("benching validator after consecutive failed queries",
		zap.Stringer("nodeID", nodeID),
		zap.Duration("benchDuration", benchedUntil.Sub(now)),
		zap.Int("numFailedQueries", b.threshold),
	)

	// Set [b.timer] to fire when next validator should leave bench
	b.setNextLeaveTime()

	// Update metrics
	b.metrics.numBenched.Set(float64(b.benchedQueue.Len()))
	b.metrics.weightBenched.Set(float64(newBenchedStake))
}

```

avalanchego/snow/networking/benchlist/benchlist_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package benchlist

import (
	"testing"
	"time"

	"github.com/prometheus/client_golang/prometheus"

	"github.com/stretchr/testify/require"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/validators"
	"github.com/ava-labs/avalanchego/utils/logging"
	"github.com/ava-labs/avalanchego/utils/wrappers"
)

var minimumFailingDuration = 5 * time.Minute

// Test that validators are properly added to the bench
func TestBenchlistAdd(t *testing.T) {
	vdrs := validators.NewSet()
	vdr0 := validators.GenerateRandomValidator(50)
	vdr1 := validators.GenerateRandomValidator(50)
	vdr2 := validators.GenerateRandomValidator(50)
	vdr3 := validators.GenerateRandomValidator(50)
	vdr4 := validators.GenerateRandomValidator(50)

	errs := wrappers.Errs{}
	errs.Add(
		vdrs.AddWeight(vdr0.ID(), vdr0.Weight()),
		vdrs.AddWeight(vdr1.ID(), vdr1.Weight()),
		vdrs.AddWeight(vdr2.ID(), vdr2.Weight()),
		vdrs.AddWeight(vdr3.ID(), vdr3.Weight()),
		vdrs.AddWeight(vdr4.ID(), vdr4.Weight()),
	)
	if errs.Errored() {
		t.Fatal(errs.Err)
	}

	benchable := &TestBenchable{T: t}
	benchable.Default(true)

	threshold := 3
	duration := time.Minute
	maxPortion := 0.5
	benchIntf, err := NewBenchlist(
		ids.Empty,
		logging.NoLog{},
		benchable,
		vdrs,
		threshold,
		minimumFailingDuration,
		duration,
		maxPortion,
		prometheus.NewRegistry(),
	)
	if err != nil {
		t.Fatal(err)
	}
	b := benchIntf.(*benchlist)
	defer b.timer.Stop()
	now := time.Now()
	b.clock.Set(now)

	// Nobody should be benched at the start
	b.lock.Lock()
	require.False(t, b.isBenched(vdr0.ID()))
	require.False(t, b.isBenched(vdr1.ID()))
	require.False(t, b.isBenched(vdr2.ID()))
	require.False(t, b.isBenched(vdr3.ID()))
	require.False(t, b.isBenched(vdr4.ID()))
	require.Len(t, b.failureStreaks, 0)
	require.Equal(t, b.benchedQueue.Len(), 0)
	require.Equal(t, b.benchlistSet.Len(), 0)
	b.lock.Unlock()

	// Register [threshold - 1] failures in a row for vdr0
	for i := 0; i < threshold-1; i++ {
		b.RegisterFailure(vdr0.ID())
	}

	// Still shouldn't be benched due to not enough consecutive failure
	require.False(t, b.isBenched(vdr0.ID()))
	require.Equal(t, b.benchedQueue.Len(), 0)
	require.Equal(t, b.benchlistSet.Len(), 0)
	require.Len(t, b.failureStreaks, 1)
	fs := b.failureStreaks[vdr0.ID()]
	require.Equal(t, threshold-1, fs.consecutive)
	require.True(t, fs.firstFailure.Equal(now))

	// Register another failure
	b.RegisterFailure(vdr0.ID())

	// Still shouldn't be benched because not enough time (any in this case)
	// has passed since the first failure
	b.lock.Lock()
	require.False(t, b.isBenched(vdr0.ID()))
	require.Equal(t, b.benchedQueue.Len(), 0)
	require.Equal(t, b.benchlistSet.Len(), 0)
	b.lock.Unlock()

	// Move the time up
	now = now.Add(minimumFailingDuration).Add(time.Second)
	b.lock.Lock()
	b.clock.Set(now)

	benched := false
	benchable.BenchedF = func(ids.ID, ids.NodeID) {
		benched = true
	}
	b.lock.Unlock()

	// Register another failure
	b.RegisterFailure(vdr0.ID())

	// Now this validator should be benched
	b.lock.Lock()
	require.True(t, b.isBenched(vdr0.ID()))
	require.Equal(t, b.benchedQueue.Len(), 1)
	require.Equal(t, b.benchlistSet.Len(), 1)

	next := b.benchedQueue[0]
	require.Equal(t, vdr0.ID(), next.nodeID)
	require.True(t, !next.benchedUntil.After(now.Add(duration)))
	require.True(t, !next.benchedUntil.Before(now.Add(duration/2)))
	require.Len(t, b.failureStreaks, 0)
	require.True(t, benched)
	benchable.BenchedF = nil
	b.lock.Unlock()

	// Give another validator [threshold-1] failures
	for i := 0; i < threshold-1; i++ {
		b.RegisterFailure(vdr1.ID())
	}

	// Register another failure
	b.RegisterResponse(vdr1.ID())

	// vdr1 shouldn't be benched
	// The response should have cleared its consecutive failures
	b.lock.Lock()
	require.True(t, b.isBenched(vdr0.ID()))
	require.False(t, b.isBenched(vdr1.ID()))
	require.Equal(t, b.benchedQueue.Len(), 1)
	require.Equal(t, b.benchlistSet.Len(), 1)
	require.Len(t, b.failureStreaks, 0)
	b.lock.Unlock()

	// Register another failure for vdr0, who is benched
	b.RegisterFailure(vdr0.ID())

	// A failure for an already benched validator should not count against it
	b.lock.Lock()
	require.Len(t, b.failureStreaks, 0)
	b.lock.Unlock()
}

// Test that the benchlist won't bench more than the maximum portion of stake
func TestBenchlistMaxStake(t *testing.T) {
	vdrs := validators.NewSet()
	vdr0 := validators.GenerateRandomValidator(1000)
	vdr1 := validators.GenerateRandomValidator(1000)
	vdr2 := validators.GenerateRandomValidator(1000)
	vdr3 := validators.GenerateRandomValidator(2000)
	vdr4 := validators.GenerateRandomValidator(100)
	// Total weight is 5100

	errs := wrappers.Errs{}
	errs.Add(
		vdrs.AddWeight(vdr0.ID(), vdr0.Weight()),
		vdrs.AddWeight(vdr1.ID(), vdr1.Weight()),
		vdrs.AddWeight(vdr2.ID(), vdr2.Weight()),
		vdrs.AddWeight(vdr3.ID(), vdr3.Weight()),
		vdrs.AddWeight(vdr4.ID(), vdr4.Weight()),
	)
	if errs.Errored() {
		t.Fatal(errs.Err)
	}

	threshold := 3
	duration := 1 * time.Hour
	// Shouldn't bench more than 2550 (5100/2)
	maxPortion := 0.5
	benchIntf, err := NewBenchlist(
		ids.Empty,
		logging.NoLog{},
		&TestBenchable{T: t},
		vdrs,
		threshold,
		minimumFailingDuration,
		duration,
		maxPortion,
		prometheus.NewRegistry(),
	)
	if err != nil {
		t.Fatal(err)
	}
	b := benchIntf.(*benchlist)
	defer b.timer.Stop()
	now := time.Now()
	b.clock.Set(now)

	// Register [threshold-1] failures for 3 validators
	for _, vdr := range []validators.Validator{vdr0, vdr1, vdr2} {
		for i := 0; i < threshold-1; i++ {
			b.RegisterFailure(vdr.ID())
		}
	}

	// Advance the time to past the minimum failing duration
	newTime := now.Add(minimumFailingDuration).Add(time.Second)
	b.lock.Lock()
	b.clock.Set(newTime)
	b.lock.Unlock()

	// Register another failure for all three
	for _, vdr := range []validators.Validator{vdr0, vdr1, vdr2} {
		b.RegisterFailure(vdr.ID())
	}

	// Only vdr0 and vdr1 should be benched (total weight 2000)
	// Benching vdr2 (weight 1000) would cause the amount benched
	// to exceed the maximum
	b.lock.Lock()
	require.True(t, b.isBenched(vdr0.ID()))
	require.True(t, b.isBenched(vdr1.ID()))
	require.False(t, b.isBenched(vdr2.ID()))
	require.Equal(t, b.benchedQueue.Len(), 2)
	require.Equal(t, b.benchlistSet.Len(), 2)
	require.Len(t, b.failureStreaks, 1)
	fs := b.failureStreaks[vdr2.ID()]
	fs.consecutive = threshold
	fs.firstFailure = now
	b.lock.Unlock()

	// Register threshold - 1 failures for vdr4
	for i := 0; i < threshold-1; i++ {
		b.RegisterFailure(vdr4.ID())
	}

	// Advance the time past min failing duration
	newTime2 := newTime.Add(minimumFailingDuration).Add(time.Second)
	b.lock.Lock()
	b.clock.Set(newTime2)
	b.lock.Unlock()

	// Register another failure for vdr4
	b.RegisterFailure(vdr4.ID())

	// vdr4 should be benched now
	b.lock.Lock()
	require.True(t, b.isBenched(vdr0.ID()))
	require.True(t, b.isBenched(vdr1.ID()))
	require.True(t, b.isBenched(vdr4.ID()))
	require.Equal(t, 3, b.benchedQueue.Len())
	require.Equal(t, 3, b.benchlistSet.Len())
	require.Contains(t, b.benchlistSet, vdr0.ID())
	require.Contains(t, b.benchlistSet, vdr1.ID())
	require.Contains(t, b.benchlistSet, vdr4.ID())
	require.Len(t, b.failureStreaks, 1) // for vdr2
	b.lock.Unlock()

	// More failures for vdr2 shouldn't add it to the bench
	// because the max bench amount would be exceeded
	for i := 0; i < threshold-1; i++ {
		b.RegisterFailure(vdr2.ID())
	}

	b.lock.Lock()
	require.True(t, b.isBenched(vdr0.ID()))
	require.True(t, b.isBenched(vdr1.ID()))
	require.True(t, b.isBenched(vdr4.ID()))
	require.False(t, b.isBenched(vdr2.ID()))
	require.Equal(t, 3, b.benchedQueue.Len())
	require.Equal(t, 3, b.benchlistSet.Len())
	require.Len(t, b.failureStreaks, 1)
	require.Contains(t, b.failureStreaks, vdr2.ID())

	// Ensure the benched queue root has the min end time
	minEndTime := b.benchedQueue[0].benchedUntil
	benchedIDs := []ids.NodeID{vdr0.ID(), vdr1.ID(), vdr4.ID()}
	for _, benchedVdr := range b.benchedQueue {
		require.Contains(t, benchedIDs, benchedVdr.nodeID)
		require.True(t, !benchedVdr.benchedUntil.Before(minEndTime))
	}

	b.lock.Unlock()
}

// Test validators are removed from the bench correctly
func TestBenchlistRemove(t *testing.T) {
	vdrs := validators.NewSet()
	vdr0 := validators.GenerateRandomValidator(1000)
	vdr1 := validators.GenerateRandomValidator(1000)
	vdr2 := validators.GenerateRandomValidator(1000)
	vdr3 := validators.GenerateRandomValidator(1000)
	vdr4 := validators.GenerateRandomValidator(1000)
	// Total weight is 5100

	errs := wrappers.Errs{}
	errs.Add(
		vdrs.AddWeight(vdr0.ID(), vdr0.Weight()),
		vdrs.AddWeight(vdr1.ID(), vdr1.Weight()),
		vdrs.AddWeight(vdr2.ID(), vdr2.Weight()),
		vdrs.AddWeight(vdr3.ID(), vdr3.Weight()),
		vdrs.AddWeight(vdr4.ID(), vdr4.Weight()),
	)
	if errs.Errored() {
		t.Fatal(errs.Err)
	}

	count := 0
	benchable := &TestBenchable{
		T:             t,
		CantUnbenched: true,
		UnbenchedF: func(ids.ID, ids.NodeID) {
			count++
		},
	}

	threshold := 3
	duration := 2 * time.Second
	maxPortion := 0.76 // can bench 3 of the 5 validators
	benchIntf, err := NewBenchlist(
		ids.Empty,
		logging.NoLog{},
		benchable,
		vdrs,
		threshold,
		minimumFailingDuration,
		duration,
		maxPortion,
		prometheus.NewRegistry(),
	)
	if err != nil {
		t.Fatal(err)
	}
	b := benchIntf.(*benchlist)
	defer b.timer.Stop()
	now := time.Now()
	b.lock.Lock()
	b.clock.Set(now)
	b.lock.Unlock()

	// Register [threshold-1] failures for 3 validators
	for _, vdr := range []validators.Validator{vdr0, vdr1, vdr2} {
		for i := 0; i < threshold-1; i++ {
			b.RegisterFailure(vdr.ID())
		}
	}

	// Advance the time past the min failing duration and register another failure
	// for each
	now = now.Add(minimumFailingDuration).Add(time.Second)
	b.lock.Lock()
	b.clock.Set(now)
	b.lock.Unlock()
	for _, vdr := range []validators.Validator{vdr0, vdr1, vdr2} {
		b.RegisterFailure(vdr.ID())
	}

	// All 3 should be benched
	b.lock.Lock()
	require.True(t, b.isBenched(vdr0.ID()))
	require.True(t, b.isBenched(vdr1.ID()))
	require.True(t, b.isBenched(vdr2.ID()))
	require.Equal(t, 3, b.benchedQueue.Len())
	require.Equal(t, 3, b.benchlistSet.Len())
	require.Len(t, b.failureStreaks, 0)

	// Ensure the benched queue root has the min end time
	minEndTime := b.benchedQueue[0].benchedUntil
	benchedIDs := []ids.NodeID{vdr0.ID(), vdr1.ID(), vdr2.ID()}
	for _, benchedVdr := range b.benchedQueue {
		require.Contains(t, benchedIDs, benchedVdr.nodeID)
		require.True(t, !benchedVdr.benchedUntil.Before(minEndTime))
	}

	// Set the benchlist's clock past when all validators should be unbenched
	// so that when its timer fires, it can remove them
	b.clock.Set(b.clock.Time().Add(duration))
	b.lock.Unlock()

	// Make sure each validator is eventually removed
	require.Eventually(
		t,
		func() bool {
			return !b.IsBenched(vdr0.ID())
		},
		duration+time.Second, // extra time.Second as grace period
		100*time.Millisecond,
	)

	require.Eventually(
		t,
		func() bool {
			return !b.IsBenched(vdr1.ID())
		},
		duration+time.Second,
		100*time.Millisecond,
	)

	require.Eventually(
		t,
		func() bool {
			return !b.IsBenched(vdr2.ID())
		},
		duration+time.Second,
		100*time.Millisecond,
	)

	require.Equal(t, 3, count)
}

```

avalanchego/snow/networking/benchlist/manager.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package benchlist

import (
	"errors"
	"sync"
	"time"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow"
	"github.com/ava-labs/avalanchego/snow/validators"
	"github.com/ava-labs/avalanchego/utils/constants"
)

var (
	errUnknownValidators = errors.New("unknown validator set for provided chain")

	_ Manager = &manager{}
)

// Manager provides an interface for a benchlist to register whether
// queries have been successful or unsuccessful and place validators with
// consistently failing queries on a benchlist to prevent waiting up to
// the full network timeout for their responses.
type Manager interface {
	// RegisterResponse registers that we receive a request response from [nodeID]
	// regarding [chainID] within the timeout
	RegisterResponse(chainID ids.ID, nodeID ids.NodeID)
	// RegisterFailure registers that a request to [nodeID] regarding
	// [chainID] timed out
	RegisterFailure(chainID ids.ID, nodeID ids.NodeID)
	// RegisterChain registers a new chain with metrics under [namespace]
	RegisterChain(ctx *snow.ConsensusContext) error
	// IsBenched returns true if messages to [nodeID] regarding chain [chainID]
	// should not be sent over the network and should immediately fail.
	// Returns false if such messages should be sent, or if the chain is unknown.
	IsBenched(nodeID ids.NodeID, chainID ids.ID) bool
	// GetBenched returns an array of chainIDs where the specified
	// [nodeID] is benched. If called on an id.ShortID that does
	// not map to a validator, it will return an empty array.
	GetBenched(nodeID ids.NodeID) []ids.ID
}

// Config defines the configuration for a benchlist
type Config struct {
	Benchable              Benchable          `json:"-"`
	Validators             validators.Manager `json:"-"`
	StakingEnabled         bool               `json:"-"`
	Threshold              int                `json:"threshold"`
	MinimumFailingDuration time.Duration      `json:"minimumFailingDuration"`
	Duration               time.Duration      `json:"duration"`
	MaxPortion             float64            `json:"maxPortion"`
}

type manager struct {
	config *Config
	// Chain ID --> benchlist for that chain.
	// Each benchlist is safe for concurrent access.
	chainBenchlists map[ids.ID]Benchlist

	lock sync.RWMutex
}

// NewManager returns a manager for chain-specific query benchlisting
func NewManager(config *Config) Manager {
	// If the maximum portion of validators allowed to be benchlisted
	// is 0, return the no-op benchlist
	if config.MaxPortion <= 0 {
		return NewNoBenchlist()
	}
	return &manager{
		config:          config,
		chainBenchlists: make(map[ids.ID]Benchlist),
	}
}

// IsBenched returns true if messages to [nodeID] regarding [chainID]
// should not be sent over the network and should immediately fail.
func (m *manager) IsBenched(nodeID ids.NodeID, chainID ids.ID) bool {
	m.lock.RLock()
	benchlist, exists := m.chainBenchlists[chainID]
	m.lock.RUnlock()

	if !exists {
		return false
	}
	isBenched := benchlist.IsBenched(nodeID)
	return isBenched
}

// GetBenched returns an array of chainIDs where the specified
// [nodeID] is benched. If called on an id.ShortID that does
// not map to a validator, it will return an empty array.
func (m *manager) GetBenched(nodeID ids.NodeID) []ids.ID {
	m.lock.RLock()
	defer m.lock.RUnlock()

	benched := []ids.ID{}
	for chainID, benchlist := range m.chainBenchlists {
		if !benchlist.IsBenched(nodeID) {
			continue
		}
		benched = append(benched, chainID)
	}
	return benched
}

func (m *manager) RegisterChain(ctx *snow.ConsensusContext) error {
	m.lock.Lock()
	defer m.lock.Unlock()

	if _, exists := m.chainBenchlists[ctx.ChainID]; exists {
		return nil
	}

	var (
		vdrs validators.Set
		ok   bool
	)
	if m.config.StakingEnabled {
		vdrs, ok = m.config.Validators.GetValidators(ctx.SubnetID)
	} else {
		// If staking is disabled, everyone validates every chain
		vdrs, ok = m.config.Validators.GetValidators(constants.PrimaryNetworkID)
	}
	if !ok {
		return errUnknownValidators
	}

	benchlist, err := NewBenchlist(
		ctx.ChainID,
		ctx.Log,
		m.config.Benchable,
		vdrs,
		m.config.Threshold,
		m.config.MinimumFailingDuration,
		m.config.Duration,
		m.config.MaxPortion,
		ctx.Registerer,
	)
	if err != nil {
		return err
	}

	m.chainBenchlists[ctx.ChainID] = benchlist
	return nil
}

func (m *manager) RegisterResponse(chainID ids.ID, nodeID ids.NodeID) {
	m.lock.RLock()
	benchlist, exists := m.chainBenchlists[chainID]
	m.lock.RUnlock()

	if !exists {
		return
	}
	benchlist.RegisterResponse(nodeID)
}

func (m *manager) RegisterFailure(chainID ids.ID, nodeID ids.NodeID) {
	m.lock.RLock()
	benchlist, exists := m.chainBenchlists[chainID]
	m.lock.RUnlock()

	if !exists {
		return
	}
	benchlist.RegisterFailure(nodeID)
}

type noBenchlist struct{}

// NewNoBenchlist returns an empty benchlist that will never stop any queries
func NewNoBenchlist() Manager { return &noBenchlist{} }

func (noBenchlist) RegisterChain(*snow.ConsensusContext) error { return nil }
func (noBenchlist) RegisterResponse(ids.ID, ids.NodeID)        {}
func (noBenchlist) RegisterFailure(ids.ID, ids.NodeID)         {}
func (noBenchlist) IsBenched(ids.NodeID, ids.ID) bool          { return false }
func (noBenchlist) GetBenched(ids.NodeID) []ids.ID             { return []ids.ID{} }

```

avalanchego/snow/networking/benchlist/metrics.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package benchlist

import (
	"fmt"

	"github.com/prometheus/client_golang/prometheus"
)

type metrics struct {
	numBenched, weightBenched prometheus.Gauge
}

func (m *metrics) Initialize(registerer prometheus.Registerer) error {
	m.numBenched = prometheus.NewGauge(prometheus.GaugeOpts{
		Namespace: "benchlist",
		Name:      "benched_num",
		Help:      "Number of currently benched validators",
	})
	if err := registerer.Register(m.numBenched); err != nil {
		return fmt.Errorf("failed to register num benched statistics due to %w", err)
	}

	m.weightBenched = prometheus.NewGauge(prometheus.GaugeOpts{
		Namespace: "benchlist",
		Name:      "benched_weight",
		Help:      "Weight of currently benched validators",
	})
	if err := registerer.Register(m.weightBenched); err != nil {
		return fmt.Errorf("failed to register weight benched statistics due to %w", err)
	}

	return nil
}

```

avalanchego/snow/networking/benchlist/test_benchable.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package benchlist

import (
	"testing"

	"github.com/ava-labs/avalanchego/ids"
)

type TestBenchable struct {
	T *testing.T

	CantBenched, CantUnbenched bool
	BenchedF, UnbenchedF       func(chainID ids.ID, validatorID ids.NodeID)
}

// Default set the default callable value to [cant]
func (b *TestBenchable) Default(cant bool) {
	b.CantBenched = cant
	b.CantUnbenched = cant
}

func (b *TestBenchable) Benched(chainID ids.ID, validatorID ids.NodeID) {
	if b.BenchedF != nil {
		b.BenchedF(chainID, validatorID)
	} else if b.CantBenched && b.T != nil {
		b.T.Fatalf("Unexpectedly called Benched")
	}
}

func (b *TestBenchable) Unbenched(chainID ids.ID, validatorID ids.NodeID) {
	if b.UnbenchedF != nil {
		b.UnbenchedF(chainID, validatorID)
	} else if b.CantUnbenched && b.T != nil {
		b.T.Fatalf("Unexpectedly called Unbenched")
	}
}

```

avalanchego/snow/networking/handler/handler.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package handler

import (
	"fmt"
	"sync"
	"time"

	"github.com/prometheus/client_golang/prometheus"

	"go.uber.org/zap"

	"github.com/ava-labs/avalanchego/api/health"
	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/message"
	"github.com/ava-labs/avalanchego/snow"
	"github.com/ava-labs/avalanchego/snow/engine/common"
	"github.com/ava-labs/avalanchego/snow/networking/tracker"
	"github.com/ava-labs/avalanchego/snow/networking/worker"
	"github.com/ava-labs/avalanchego/snow/validators"
	"github.com/ava-labs/avalanchego/utils/constants"
	"github.com/ava-labs/avalanchego/utils/timer/mockable"
	"github.com/ava-labs/avalanchego/version"
)

const (
	threadPoolSize        = 2
	numDispatchersToClose = 3
)

var _ Handler = &handler{}

type Handler interface {
	common.Timer
	health.Checker

	Context() *snow.ConsensusContext
	IsValidator(nodeID ids.NodeID) bool

	SetStateSyncer(engine common.StateSyncer)
	StateSyncer() common.StateSyncer
	SetBootstrapper(engine common.BootstrapableEngine)
	Bootstrapper() common.BootstrapableEngine
	SetConsensus(engine common.Engine)
	Consensus() common.Engine

	SetOnStopped(onStopped func())
	Start(recoverPanic bool)
	Push(msg message.InboundMessage)
	Stop()
	StopWithError(err error)
	Stopped() chan struct{}
}

// handler passes incoming messages from the network to the consensus engine.
// (Actually, it receives the incoming messages from a ChainRouter, but same difference.)
type handler struct {
	metrics *metrics

	// Useful for faking time in tests
	clock mockable.Clock

	ctx *snow.ConsensusContext
	mc  message.InternalMsgBuilder
	// The validator set that validates this chain
	validators validators.Set
	// Receives messages from the VM
	msgFromVMChan   <-chan common.Message
	preemptTimeouts chan struct{}
	gossipFrequency time.Duration

	stateSyncer  common.StateSyncer
	bootstrapper common.BootstrapableEngine
	engine       common.Engine
	// onStopped is called in a goroutine when this handler finishes shutting
	// down. If it is nil then it is skipped.
	onStopped func()

	// Tracks cpu/disk usage caused by each peer.
	resourceTracker tracker.ResourceTracker

	// Holds messages that [engine] hasn't processed yet.
	// [unprocessedMsgsCond.L] must be held while accessing [syncMessageQueue].
	syncMessageQueue MessageQueue
	// Holds messages that [engine] hasn't processed yet.
	// [unprocessedAsyncMsgsCond.L] must be held while accessing [asyncMessageQueue].
	asyncMessageQueue MessageQueue
	// Worker pool for handling asynchronous consensus messages
	asyncMessagePool worker.Pool
	timeouts         chan struct{}

	closeOnce            sync.Once
	closingChan          chan struct{}
	numDispatchersClosed int
	// Closed when this handler and [engine] are done shutting down
	closed chan struct{}
}

// Initialize this consensus handler
// [engine] must be initialized before initializing this handler
func New(
	mc message.InternalMsgBuilder,
	ctx *snow.ConsensusContext,
	validators validators.Set,
	msgFromVMChan <-chan common.Message,
	preemptTimeouts chan struct{},
	gossipFrequency time.Duration,
	resourceTracker tracker.ResourceTracker,
) (Handler, error) {
	h := &handler{
		ctx:              ctx,
		mc:               mc,
		validators:       validators,
		msgFromVMChan:    msgFromVMChan,
		preemptTimeouts:  preemptTimeouts,
		gossipFrequency:  gossipFrequency,
		asyncMessagePool: worker.NewPool(threadPoolSize),
		timeouts:         make(chan struct{}, 1),
		closingChan:      make(chan struct{}),
		closed:           make(chan struct{}),
		resourceTracker:  resourceTracker,
	}

	var err error

	h.metrics, err = newMetrics("handler", h.ctx.Registerer)
	if err != nil {
		return nil, fmt.Errorf("initializing handler metrics errored with: %w", err)
	}
	cpuTracker := resourceTracker.CPUTracker()
	h.syncMessageQueue, err = NewMessageQueue(h.ctx.Log, h.validators, cpuTracker, "handler", h.ctx.Registerer, message.SynchronousOps)
	if err != nil {
		return nil, fmt.Errorf("initializing sync message queue errored with: %w", err)
	}
	h.asyncMessageQueue, err = NewMessageQueue(h.ctx.Log, h.validators, cpuTracker, "handler_async", h.ctx.Registerer, message.AsynchronousOps)
	if err != nil {
		return nil, fmt.Errorf("initializing async message queue errored with: %w", err)
	}
	return h, nil
}

func (h *handler) Context() *snow.ConsensusContext { return h.ctx }

func (h *handler) IsValidator(nodeID ids.NodeID) bool {
	return !h.ctx.IsValidatorOnly() ||
		nodeID == h.ctx.NodeID ||
		h.validators.Contains(nodeID)
}

func (h *handler) SetStateSyncer(engine common.StateSyncer) { h.stateSyncer = engine }
func (h *handler) StateSyncer() common.StateSyncer          { return h.stateSyncer }

func (h *handler) SetBootstrapper(engine common.BootstrapableEngine) { h.bootstrapper = engine }
func (h *handler) Bootstrapper() common.BootstrapableEngine          { return h.bootstrapper }

func (h *handler) SetConsensus(engine common.Engine) { h.engine = engine }
func (h *handler) Consensus() common.Engine          { return h.engine }

func (h *handler) SetOnStopped(onStopped func()) { h.onStopped = onStopped }

func (h *handler) selectStartingGear() (common.Engine, error) {
	if h.stateSyncer == nil {
		return h.bootstrapper, nil
	}

	stateSyncEnabled, err := h.stateSyncer.IsEnabled()
	if err != nil {
		return nil, err
	}

	if !stateSyncEnabled {
		return h.bootstrapper, nil
	}

	// drop bootstrap state from previous runs
	// before starting state sync
	return h.stateSyncer, h.bootstrapper.Clear()
}

func (h *handler) Start(recoverPanic bool) {
	h.ctx.Lock.Lock()
	defer h.ctx.Lock.Unlock()

	gear, err := h.selectStartingGear()
	if err != nil {
		h.ctx.Log.Error("chain failed to select starting gear",
			zap.Error(err),
		)
		h.shutdown()
		return
	}

	if err := gear.Start(0); err != nil {
		h.ctx.Log.Error("chain failed to start",
			zap.Error(err),
		)
		h.shutdown()
		return
	}

	if recoverPanic {
		go h.ctx.Log.RecoverAndExit(h.dispatchSync, func() {
			h.ctx.Log.Error("chain was shutdown due to a panic in the sync dispatcher")
		})
		go h.ctx.Log.RecoverAndExit(h.dispatchAsync, func() {
			h.ctx.Log.Error("chain was shutdown due to a panic in the async dispatcher")
		})
		go h.ctx.Log.RecoverAndExit(h.dispatchChans, func() {
			h.ctx.Log.Error("chain was shutdown due to a panic in the chan dispatcher")
		})
	} else {
		go h.ctx.Log.RecoverAndPanic(h.dispatchSync)
		go h.ctx.Log.RecoverAndPanic(h.dispatchAsync)
		go h.ctx.Log.RecoverAndPanic(h.dispatchChans)
	}
}

func (h *handler) HealthCheck() (interface{}, error) {
	h.ctx.Lock.Lock()
	defer h.ctx.Lock.Unlock()

	engine, err := h.getEngine()
	if err != nil {
		return nil, err
	}
	return engine.HealthCheck()
}

// Push the message onto the handler's queue
func (h *handler) Push(msg message.InboundMessage) {
	switch msg.Op() {
	case message.AppRequest, message.AppGossip, message.AppRequestFailed, message.AppResponse:
		h.asyncMessageQueue.Push(msg)
	default:
		h.syncMessageQueue.Push(msg)
	}
}

func (h *handler) RegisterTimeout(d time.Duration) {
	go func() {
		timer := time.NewTimer(d)
		defer timer.Stop()

		select {
		case <-timer.C:
		case <-h.preemptTimeouts:
		}

		// If there is already a timeout ready to fire - just drop the
		// additional timeout. This ensures that all goroutines that are spawned
		// here are able to close if the chain is shutdown.
		select {
		case h.timeouts <- struct{}{}:
		default:
		}
	}()
}

func (h *handler) Stop() {
	h.closeOnce.Do(func() {
		// Must hold the locks here to ensure there's no race condition in where
		// we check the value of [h.closing] after the call to [Signal].
		h.syncMessageQueue.Shutdown()
		h.asyncMessageQueue.Shutdown()
		close(h.closingChan)

		// TODO: switch this to use a [context.Context] with a cancel function.
		//
		// Don't process any more bootstrap messages. If a dispatcher is
		// processing a bootstrap message, stop. We do this because if we
		// didn't, and the engine was in the middle of executing state
		// transitions during bootstrapping, we wouldn't be able to grab
		// [h.ctx.Lock] until the engine finished executing state transitions,
		// which may take a long time. As a result, the router would time out on
		// shutting down this chain.
		h.bootstrapper.Halt()
	})
}

func (h *handler) StopWithError(err error) {
	h.ctx.Log.Fatal("shutting down chain",
		zap.String("reason", "received an unexpected error"),
		zap.Error(err),
	)
	h.Stop()
}

func (h *handler) Stopped() chan struct{} { return h.closed }

func (h *handler) dispatchSync() {
	defer h.closeDispatcher()

	// Handle sync messages from the router
	for {
		// Get the next message we should process. If the handler is shutting
		// down, we may fail to pop a message.
		msg, ok := h.popUnexpiredMsg(h.syncMessageQueue, h.metrics.expired)
		if !ok {
			return
		}

		// If there is an error handling the message, shut down the chain
		if err := h.handleSyncMsg(msg); err != nil {
			h.StopWithError(fmt.Errorf(
				"%w while processing sync message: %s",
				err,
				msg,
			))
			return
		}
	}
}

func (h *handler) dispatchAsync() {
	defer func() {
		h.asyncMessagePool.Shutdown()
		h.closeDispatcher()
	}()

	// Handle async messages from the router
	for {
		// Get the next message we should process. If the handler is shutting
		// down, we may fail to pop a message.
		msg, ok := h.popUnexpiredMsg(h.asyncMessageQueue, h.metrics.asyncExpired)
		if !ok {
			return
		}

		h.handleAsyncMsg(msg)
	}
}

func (h *handler) dispatchChans() {
	gossiper := time.NewTicker(h.gossipFrequency)
	defer func() {
		gossiper.Stop()
		h.closeDispatcher()
	}()

	// Handle messages generated by the handler and the VM
	for {
		var msg message.InboundMessage
		select {
		case <-h.closingChan:
			return

		case vmMSG := <-h.msgFromVMChan:
			msg = h.mc.InternalVMMessage(h.ctx.NodeID, uint32(vmMSG))

		case <-gossiper.C:
			msg = h.mc.InternalGossipRequest(h.ctx.NodeID)

		case <-h.timeouts:
			msg = h.mc.InternalTimeout(h.ctx.NodeID)
		}

		if err := h.handleChanMsg(msg); err != nil {
			h.StopWithError(fmt.Errorf(
				"%w while processing async message: %s",
				err,
				msg,
			))
			return
		}
	}
}

// Any returned error is treated as fatal
func (h *handler) handleSyncMsg(msg message.InboundMessage) error {
	h.ctx.Log.Debug("forwarding sync message to consensus",
		zap.Stringer("messageString", msg),
	)

	var (
		nodeID    = msg.NodeID()
		op        = msg.Op()
		startTime = h.clock.Time()
	)
	h.resourceTracker.StartProcessing(nodeID, startTime)
	h.ctx.Lock.Lock()
	defer func() {
		h.ctx.Lock.Unlock()

		var (
			endTime   = h.clock.Time()
			histogram = h.metrics.messages[op]
		)
		h.resourceTracker.StopProcessing(nodeID, endTime)
		histogram.Observe(float64(endTime.Sub(startTime)))
		msg.OnFinishedHandling()
		h.ctx.Log.Debug("finished handling sync message",
			zap.Stringer("messageOp", op),
		)
	}()

	engine, err := h.getEngine()
	if err != nil {
		return err
	}

	// Invariant: msg.Get(message.RequestID) must never error. The [ChainRouter]
	//            should have already successfully called this function.
	// Invariant: Response messages can never be dropped here. This is because
	//            the timeout has already been cleared. This means the engine
	//            should be invoked with a failure message if parsing of the
	//            response fails.
	switch op {
	case message.GetStateSummaryFrontier:
		requestIDIntf, err := msg.Get(message.RequestID)
		if err != nil {
			return err
		}
		requestID := requestIDIntf.(uint32)

		return engine.GetStateSummaryFrontier(nodeID, requestID)

	case message.StateSummaryFrontier:
		requestIDIntf, err := msg.Get(message.RequestID)
		if err != nil {
			return err
		}
		requestID := requestIDIntf.(uint32)

		summaryIntf, err := msg.Get(message.SummaryBytes)
		if err != nil {
			h.ctx.Log.Debug("message with invalid field",
				zap.Stringer("nodeID", nodeID),
				zap.Stringer("messageOp", op),
				zap.Uint32("requestID", requestID),
				zap.Stringer("field", message.SummaryBytes),
				zap.Error(err),
			)
			return engine.GetStateSummaryFrontierFailed(nodeID, requestID)
		}
		summary := summaryIntf.([]byte)

		return engine.StateSummaryFrontier(nodeID, requestID, summary)

	case message.GetStateSummaryFrontierFailed:
		requestIDIntf, err := msg.Get(message.RequestID)
		if err != nil {
			return err
		}
		requestID := requestIDIntf.(uint32)

		return engine.GetStateSummaryFrontierFailed(nodeID, requestID)

	case message.GetAcceptedStateSummary:
		requestIDIntf, err := msg.Get(message.RequestID)
		if err != nil {
			return err
		}
		requestID := requestIDIntf.(uint32)

		summaryHeights, err := getSummaryHeights(msg)
		if err != nil {
			h.ctx.Log.Debug("dropping message with invalid field",
				zap.Stringer("nodeID", nodeID),
				zap.Stringer("messageOp", op),
				zap.Uint32("requestID", requestID),
				zap.Stringer("field", message.SummaryHeights),
				zap.Error(err),
			)
			return nil
		}

		return engine.GetAcceptedStateSummary(nodeID, requestID, summaryHeights)

	case message.AcceptedStateSummary:
		requestIDIntf, err := msg.Get(message.RequestID)
		if err != nil {
			return err
		}
		requestID := requestIDIntf.(uint32)

		summaryIDs, err := getIDs(message.SummaryIDs, msg)
		if err != nil {
			h.ctx.Log.Debug("message with invalid field",
				zap.Stringer("nodeID", nodeID),
				zap.Stringer("messageOp", op),
				zap.Uint32("requestID", requestID),
				zap.Stringer("field", message.SummaryIDs),
				zap.Error(err),
			)
			return engine.GetAcceptedStateSummaryFailed(nodeID, requestID)
		}

		return engine.AcceptedStateSummary(nodeID, requestID, summaryIDs)

	case message.GetAcceptedStateSummaryFailed:
		requestIDIntf, err := msg.Get(message.RequestID)
		if err != nil {
			return err
		}
		requestID := requestIDIntf.(uint32)

		return engine.GetAcceptedStateSummaryFailed(nodeID, requestID)

	case message.GetAcceptedFrontier:
		requestIDIntf, err := msg.Get(message.RequestID)
		if err != nil {
			return err
		}
		requestID := requestIDIntf.(uint32)

		return engine.GetAcceptedFrontier(nodeID, requestID)

	case message.AcceptedFrontier:
		requestIDIntf, err := msg.Get(message.RequestID)
		if err != nil {
			return err
		}
		requestID := requestIDIntf.(uint32)

		containerIDs, err := getIDs(message.ContainerIDs, msg)
		if err != nil {
			h.ctx.Log.Debug("message with invalid field",
				zap.Stringer("nodeID", nodeID),
				zap.Stringer("messageOp", op),
				zap.Uint32("requestID", requestID),
				zap.Stringer("field", message.ContainerIDs),
				zap.Error(err),
			)
			return engine.GetAcceptedFrontierFailed(nodeID, requestID)
		}

		return engine.AcceptedFrontier(nodeID, requestID, containerIDs)

	case message.GetAcceptedFrontierFailed:
		requestIDIntf, err := msg.Get(message.RequestID)
		if err != nil {
			return err
		}
		requestID := requestIDIntf.(uint32)

		return engine.GetAcceptedFrontierFailed(nodeID, requestID)

	case message.GetAccepted:
		requestIDIntf, err := msg.Get(message.RequestID)
		if err != nil {
			return err
		}
		requestID := requestIDIntf.(uint32)

		containerIDs, err := getIDs(message.ContainerIDs, msg)
		if err != nil {
			h.ctx.Log.Debug("dropping message with invalid field",
				zap.Stringer("nodeID", nodeID),
				zap.Stringer("messageOp", op),
				zap.Uint32("requestID", requestID),
				zap.Stringer("field", message.ContainerIDs),
				zap.Error(err),
			)
			return nil
		}

		return engine.GetAccepted(nodeID, requestID, containerIDs)

	case message.Accepted:
		requestIDIntf, err := msg.Get(message.RequestID)
		if err != nil {
			return err
		}
		requestID := requestIDIntf.(uint32)

		containerIDs, err := getIDs(message.ContainerIDs, msg)
		if err != nil {
			h.ctx.Log.Debug("message with invalid field",
				zap.Stringer("nodeID", nodeID),
				zap.Stringer("messageOp", op),
				zap.Uint32("requestID", requestID),
				zap.Stringer("field", message.ContainerIDs),
				zap.Error(err),
			)
			return engine.GetAcceptedFailed(nodeID, requestID)
		}

		return engine.Accepted(nodeID, requestID, containerIDs)

	case message.GetAcceptedFailed:
		requestIDIntf, err := msg.Get(message.RequestID)
		if err != nil {
			return err
		}
		requestID := requestIDIntf.(uint32)

		return engine.GetAcceptedFailed(nodeID, requestID)

	case message.GetAncestors:
		requestIDIntf, err := msg.Get(message.RequestID)
		if err != nil {
			return err
		}
		requestID := requestIDIntf.(uint32)

		containerIDIntf, err := msg.Get(message.ContainerID)
		if err != nil {
			h.ctx.Log.Debug("dropping message with invalid field",
				zap.Stringer("nodeID", nodeID),
				zap.Stringer("messageOp", op),
				zap.Uint32("requestID", requestID),
				zap.Stringer("field", message.ContainerID),
				zap.Error(err),
			)
			return nil
		}
		containerIDBytes := containerIDIntf.([]byte)
		containerID, err := ids.ToID(containerIDBytes)
		if err != nil {
			h.ctx.Log.Debug("dropping message with invalid field",
				zap.Stringer("nodeID", nodeID),
				zap.Stringer("messageOp", op),
				zap.Uint32("requestID", requestID),
				zap.Stringer("field", message.ContainerID),
				zap.Error(err),
			)
			return nil
		}

		return engine.GetAncestors(nodeID, requestID, containerID)

	case message.GetAncestorsFailed:
		requestIDIntf, err := msg.Get(message.RequestID)
		if err != nil {
			return err
		}
		requestID := requestIDIntf.(uint32)

		return engine.GetAncestorsFailed(nodeID, requestID)

	case message.Ancestors:
		requestIDIntf, err := msg.Get(message.RequestID)
		if err != nil {
			return err
		}
		requestID := requestIDIntf.(uint32)

		containersIntf, err := msg.Get(message.MultiContainerBytes)
		if err != nil {
			h.ctx.Log.Debug("message with invalid field",
				zap.Stringer("nodeID", nodeID),
				zap.Stringer("messageOp", op),
				zap.Uint32("requestID", requestID),
				zap.Stringer("field", message.MultiContainerBytes),
				zap.Error(err),
			)
			return engine.GetAncestorsFailed(nodeID, requestID)
		}
		containers := containersIntf.([][]byte)

		return engine.Ancestors(nodeID, requestID, containers)

	case message.Get:
		requestIDIntf, err := msg.Get(message.RequestID)
		if err != nil {
			return err
		}
		requestID := requestIDIntf.(uint32)

		containerIDIntf, err := msg.Get(message.ContainerID)
		if err != nil {
			h.ctx.Log.Debug("dropping message with invalid field",
				zap.Stringer("nodeID", nodeID),
				zap.Stringer("messageOp", op),
				zap.Uint32("requestID", requestID),
				zap.Stringer("field", message.ContainerID),
				zap.Error(err),
			)
			return nil
		}
		containerIDBytes := containerIDIntf.([]byte)
		containerID, err := ids.ToID(containerIDBytes)
		if err != nil {
			h.ctx.Log.Debug("dropping message with invalid field",
				zap.Stringer("nodeID", nodeID),
				zap.Stringer("messageOp", op),
				zap.Uint32("requestID", requestID),
				zap.Stringer("field", message.ContainerID),
				zap.Error(err),
			)
			return nil
		}

		return engine.Get(nodeID, requestID, containerID)

	case message.GetFailed:
		requestIDIntf, err := msg.Get(message.RequestID)
		if err != nil {
			return err
		}
		requestID := requestIDIntf.(uint32)

		return engine.GetFailed(nodeID, requestID)

	case message.Put:
		requestIDIntf, err := msg.Get(message.RequestID)
		if err != nil {
			return err
		}
		requestID := requestIDIntf.(uint32)

		containerIntf, err := msg.Get(message.ContainerBytes)
		if err != nil {
			// TODO: [requestID] can overflow, which means a timeout on the
			//       request before the overflow may not be handled properly.
			if requestID == constants.GossipMsgRequestID {
				h.ctx.Log.Debug("dropping message with invalid field",
					zap.Stringer("nodeID", nodeID),
					zap.Stringer("messageOp", op),
					zap.Uint32("requestID", requestID),
					zap.Stringer("field", message.ContainerBytes),
					zap.Error(err),
				)
				return nil
			}

			h.ctx.Log.Debug("message with invalid field",
				zap.Stringer("nodeID", nodeID),
				zap.Stringer("messageOp", op),
				zap.Uint32("requestID", requestID),
				zap.Stringer("field", message.ContainerBytes),
				zap.Error(err),
			)
			return engine.GetFailed(nodeID, requestID)
		}
		container := containerIntf.([]byte)

		return engine.Put(nodeID, requestID, container)

	case message.PushQuery:
		requestIDIntf, err := msg.Get(message.RequestID)
		if err != nil {
			return err
		}
		requestID := requestIDIntf.(uint32)

		containerIntf, err := msg.Get(message.ContainerBytes)
		if err != nil {
			h.ctx.Log.Debug("dropping message with invalid field",
				zap.Stringer("nodeID", nodeID),
				zap.Stringer("messageOp", op),
				zap.Uint32("requestID", requestID),
				zap.Stringer("field", message.ContainerBytes),
				zap.Error(err),
			)
			return nil
		}
		container := containerIntf.([]byte)

		return engine.PushQuery(nodeID, requestID, container)

	case message.PullQuery:
		requestIDIntf, err := msg.Get(message.RequestID)
		if err != nil {
			return err
		}
		requestID := requestIDIntf.(uint32)

		containerIDIntf, err := msg.Get(message.ContainerID)
		if err != nil {
			h.ctx.Log.Debug("dropping message with invalid field",
				zap.Stringer("nodeID", nodeID),
				zap.Stringer("messageOp", op),
				zap.Uint32("requestID", requestID),
				zap.Stringer("field", message.ContainerID),
				zap.Error(err),
			)
			return nil
		}
		containerIDBytes := containerIDIntf.([]byte)
		containerID, err := ids.ToID(containerIDBytes)
		if err != nil {
			h.ctx.Log.Debug("dropping message with invalid field",
				zap.Stringer("nodeID", nodeID),
				zap.Stringer("messageOp", op),
				zap.Uint32("requestID", requestID),
				zap.Stringer("field", message.ContainerID),
				zap.Error(err),
			)
			return nil
		}

		return engine.PullQuery(nodeID, requestID, containerID)

	case message.Chits:
		requestIDIntf, err := msg.Get(message.RequestID)
		if err != nil {
			return err
		}
		requestID := requestIDIntf.(uint32)

		votes, err := getIDs(message.ContainerIDs, msg)
		if err != nil {
			h.ctx.Log.Debug("message with invalid field",
				zap.Stringer("nodeID", nodeID),
				zap.Stringer("messageOp", op),
				zap.Uint32("requestID", requestID),
				zap.Stringer("field", message.ContainerIDs),
				zap.Error(err),
			)
			return engine.QueryFailed(nodeID, requestID)
		}

		return engine.Chits(nodeID, requestID, votes)

	case message.QueryFailed:
		requestIDIntf, err := msg.Get(message.RequestID)
		if err != nil {
			return err
		}
		requestID := requestIDIntf.(uint32)

		return engine.QueryFailed(nodeID, requestID)

	case message.Connected:
		peerVersionIntf, err := msg.Get(message.VersionStruct)
		if err != nil {
			return err
		}
		peerVersion := peerVersionIntf.(*version.Application)

		return engine.Connected(nodeID, peerVersion)

	case message.Disconnected:
		return engine.Disconnected(nodeID)

	default:
		return fmt.Errorf(
			"attempt to submit unhandled sync msg %s from %s",
			op, nodeID,
		)
	}
}

func (h *handler) handleAsyncMsg(msg message.InboundMessage) {
	h.asyncMessagePool.Send(func() {
		if err := h.executeAsyncMsg(msg); err != nil {
			h.StopWithError(fmt.Errorf(
				"%w while processing async message: %s",
				err,
				msg,
			))
		}
	})
}

// Any returned error is treated as fatal
func (h *handler) executeAsyncMsg(msg message.InboundMessage) error {
	h.ctx.Log.Debug("forwarding async message to consensus",
		zap.Stringer("messageString", msg),
	)

	var (
		nodeID    = msg.NodeID()
		op        = msg.Op()
		startTime = h.clock.Time()
	)
	h.resourceTracker.StartProcessing(nodeID, startTime)
	defer func() {
		var (
			endTime   = h.clock.Time()
			histogram = h.metrics.messages[op]
		)
		h.resourceTracker.StopProcessing(nodeID, endTime)
		histogram.Observe(float64(endTime.Sub(startTime)))
		msg.OnFinishedHandling()
		h.ctx.Log.Debug("finished handling async message",
			zap.Stringer("messageOp", op),
		)
	}()

	engine, err := h.getEngine()
	if err != nil {
		return err
	}

	switch op {
	case message.AppRequest:
		requestIDIntf, err := msg.Get(message.RequestID)
		if err != nil {
			return err
		}
		requestID := requestIDIntf.(uint32)

		appBytesIntf, err := msg.Get(message.AppBytes)
		if err != nil {
			h.ctx.Log.Debug("dropping message with invalid field",
				zap.Stringer("nodeID", nodeID),
				zap.Stringer("messageOp", op),
				zap.Uint32("requestID", requestID),
				zap.Stringer("field", message.AppBytes),
				zap.Error(err),
			)
			return nil
		}
		appBytes := appBytesIntf.([]byte)

		return engine.AppRequest(nodeID, requestID, msg.ExpirationTime(), appBytes)

	case message.AppResponse:
		requestIDIntf, err := msg.Get(message.RequestID)
		if err != nil {
			return err
		}
		requestID := requestIDIntf.(uint32)

		appBytesIntf, err := msg.Get(message.AppBytes)
		if err != nil {
			h.ctx.Log.Debug("message with invalid field",
				zap.Stringer("nodeID", nodeID),
				zap.Stringer("messageOp", op),
				zap.Uint32("requestID", requestID),
				zap.Stringer("field", message.AppBytes),
				zap.Error(err),
			)
			return engine.AppRequestFailed(nodeID, requestID)
		}
		appBytes := appBytesIntf.([]byte)

		return engine.AppResponse(nodeID, requestID, appBytes)

	case message.AppRequestFailed:
		requestIDIntf, err := msg.Get(message.RequestID)
		if err != nil {
			return err
		}
		requestID := requestIDIntf.(uint32)

		return engine.AppRequestFailed(nodeID, requestID)

	case message.AppGossip:
		appBytesIntf, err := msg.Get(message.AppBytes)
		if err != nil {
			h.ctx.Log.Debug("dropping message with invalid field",
				zap.Stringer("nodeID", nodeID),
				zap.Stringer("messageOp", op),
				zap.Stringer("field", message.AppBytes),
				zap.Error(err),
			)
			return nil
		}
		appBytes := appBytesIntf.([]byte)

		return engine.AppGossip(nodeID, appBytes)

	default:
		return fmt.Errorf(
			"attempt to submit unhandled async msg %s from %s",
			op, nodeID,
		)
	}
}

// Any returned error is treated as fatal
func (h *handler) handleChanMsg(msg message.InboundMessage) error {
	h.ctx.Log.Debug("forwarding chan message to consensus",
		zap.Stringer("messageString", msg),
	)

	var (
		op        = msg.Op()
		startTime = h.clock.Time()
	)
	h.ctx.Lock.Lock()
	defer func() {
		h.ctx.Lock.Unlock()

		var (
			endTime   = h.clock.Time()
			histogram = h.metrics.messages[op]
		)
		histogram.Observe(float64(endTime.Sub(startTime)))
		msg.OnFinishedHandling()
		h.ctx.Log.Debug("finished handling chan message",
			zap.Stringer("messageOp", op),
		)
	}()

	engine, err := h.getEngine()
	if err != nil {
		return err
	}

	switch op := msg.Op(); op {
	case message.Notify:
		vmMsgIntf, err := msg.Get(message.VMMessage)
		if err != nil {
			return err
		}
		vmMsg := vmMsgIntf.(uint32)

		return engine.Notify(common.Message(vmMsg))

	case message.GossipRequest:
		return engine.Gossip()

	case message.Timeout:
		return engine.Timeout()

	default:
		return fmt.Errorf(
			"attempt to submit unhandled chan msg %s",
			op,
		)
	}
}

func (h *handler) getEngine() (common.Engine, error) {
	state := h.ctx.GetState()
	switch state {
	case snow.StateSyncing:
		return h.stateSyncer, nil
	case snow.Bootstrapping:
		return h.bootstrapper, nil
	case snow.NormalOp:
		return h.engine, nil
	default:
		return nil, fmt.Errorf("unknown handler for state %s", state)
	}
}

func (h *handler) popUnexpiredMsg(queue MessageQueue, expired prometheus.Counter) (message.InboundMessage, bool) {
	for {
		// Get the next message we should process. If the handler is shutting
		// down, we may fail to pop a message.
		msg, ok := queue.Pop()
		if !ok {
			return nil, false
		}

		// If this message's deadline has passed, don't process it.
		if expirationTime := msg.ExpirationTime(); !expirationTime.IsZero() && h.clock.Time().After(expirationTime) {
			h.ctx.Log.Verbo("dropping message",
				zap.String("reason", "timeout"),
				zap.Stringer("nodeID", msg.NodeID()),
				zap.Stringer("messageString", msg),
			)
			expired.Inc()
			msg.OnFinishedHandling()
			continue
		}

		return msg, true
	}
}

func (h *handler) closeDispatcher() {
	h.ctx.Lock.Lock()
	defer h.ctx.Lock.Unlock()

	h.numDispatchersClosed++
	if h.numDispatchersClosed < numDispatchersToClose {
		return
	}

	h.shutdown()
}

func (h *handler) shutdown() {
	defer func() {
		if h.onStopped != nil {
			go h.onStopped()
		}
		close(h.closed)
	}()

	currentEngine, err := h.getEngine()
	if err != nil {
		h.ctx.Log.Error("failed fetching current engine during shutdown",
			zap.Error(err),
		)
		return
	}

	if err := currentEngine.Shutdown(); err != nil {
		h.ctx.Log.Error("failed while shutting down the chain",
			zap.Error(err),
		)
	}
}

```

avalanchego/snow/networking/handler/handler_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package handler

import (
	"errors"
	"testing"
	"time"

	"github.com/prometheus/client_golang/prometheus"

	"github.com/stretchr/testify/require"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/message"
	"github.com/ava-labs/avalanchego/snow"
	"github.com/ava-labs/avalanchego/snow/engine/common"
	"github.com/ava-labs/avalanchego/snow/networking/tracker"
	"github.com/ava-labs/avalanchego/snow/validators"
	"github.com/ava-labs/avalanchego/utils/math/meter"
	"github.com/ava-labs/avalanchego/utils/resource"
)

func TestHandlerDropsTimedOutMessages(t *testing.T) {
	called := make(chan struct{})

	metrics := prometheus.NewRegistry()
	mc, err := message.NewCreator(metrics, "dummyNamespace", true, 10*time.Second)
	require.NoError(t, err)

	ctx := snow.DefaultConsensusContextTest()

	vdrs := validators.NewSet()
	vdr0 := ids.GenerateTestNodeID()
	err = vdrs.AddWeight(vdr0, 1)
	require.NoError(t, err)

	resourceTracker, err := tracker.NewResourceTracker(prometheus.NewRegistry(), resource.NoUsage, meter.ContinuousFactory{}, time.Second)
	require.NoError(t, err)
	handlerIntf, err := New(
		mc,
		ctx,
		vdrs,
		nil,
		nil,
		time.Second,
		resourceTracker,
	)
	require.NoError(t, err)
	handler := handlerIntf.(*handler)

	bootstrapper := &common.BootstrapperTest{
		BootstrapableTest: common.BootstrapableTest{
			T: t,
		},
		EngineTest: common.EngineTest{
			T: t,
		},
	}
	bootstrapper.Default(false)
	bootstrapper.ContextF = func() *snow.ConsensusContext { return ctx }
	bootstrapper.GetAcceptedFrontierF = func(nodeID ids.NodeID, requestID uint32) error {
		t.Fatalf("GetAcceptedFrontier message should have timed out")
		return nil
	}
	bootstrapper.GetAcceptedF = func(nodeID ids.NodeID, requestID uint32, containerIDs []ids.ID) error {
		called <- struct{}{}
		return nil
	}
	handler.SetBootstrapper(bootstrapper)
	ctx.SetState(snow.Bootstrapping) // assumed bootstrapping is ongoing

	pastTime := time.Now()
	mc.SetTime(pastTime)
	handler.clock.Set(pastTime)

	nodeID := ids.EmptyNodeID
	reqID := uint32(1)
	deadline := time.Nanosecond
	chainID := ids.ID{}
	msg := mc.InboundGetAcceptedFrontier(chainID, reqID, deadline, nodeID)
	handler.Push(msg)

	currentTime := time.Now().Add(time.Second)
	mc.SetTime(currentTime)
	handler.clock.Set(currentTime)

	reqID++
	msg = mc.InboundGetAccepted(chainID, reqID, deadline, nil, nodeID)
	handler.Push(msg)

	bootstrapper.StartF = func(startReqID uint32) error { return nil }

	handler.Start(false)

	ticker := time.NewTicker(time.Second)
	defer ticker.Stop()
	select {
	case <-ticker.C:
		t.Fatalf("Calling engine function timed out")
	case <-called:
	}
}

func TestHandlerClosesOnError(t *testing.T) {
	closed := make(chan struct{}, 1)
	ctx := snow.DefaultConsensusContextTest()

	vdrs := validators.NewSet()
	err := vdrs.AddWeight(ids.GenerateTestNodeID(), 1)
	require.NoError(t, err)
	metrics := prometheus.NewRegistry()
	mc, err := message.NewCreator(metrics, "dummyNamespace", true, 10*time.Second)
	require.NoError(t, err)

	resourceTracker, err := tracker.NewResourceTracker(prometheus.NewRegistry(), resource.NoUsage, meter.ContinuousFactory{}, time.Second)
	require.NoError(t, err)
	handlerIntf, err := New(
		mc,
		ctx,
		vdrs,
		nil,
		nil,
		time.Second,
		resourceTracker,
	)
	require.NoError(t, err)
	handler := handlerIntf.(*handler)

	handler.clock.Set(time.Now())
	handler.SetOnStopped(func() {
		closed <- struct{}{}
	})

	bootstrapper := &common.BootstrapperTest{
		BootstrapableTest: common.BootstrapableTest{
			T: t,
		},
		EngineTest: common.EngineTest{
			T: t,
		},
	}
	bootstrapper.Default(false)
	bootstrapper.ContextF = func() *snow.ConsensusContext { return ctx }
	bootstrapper.GetAcceptedFrontierF = func(nodeID ids.NodeID, requestID uint32) error {
		return errors.New("Engine error should cause handler to close")
	}
	handler.SetBootstrapper(bootstrapper)

	engine := &common.EngineTest{T: t}
	engine.Default(false)
	engine.ContextF = func() *snow.ConsensusContext { return ctx }
	handler.SetConsensus(engine)

	// assume bootstrapping is ongoing so that InboundGetAcceptedFrontier
	// should normally be handled
	ctx.SetState(snow.Bootstrapping)

	bootstrapper.StartF = func(startReqID uint32) error { return nil }

	handler.Start(false)

	nodeID := ids.EmptyNodeID
	reqID := uint32(1)
	deadline := time.Nanosecond
	msg := mc.InboundGetAcceptedFrontier(ids.ID{}, reqID, deadline, nodeID)
	handler.Push(msg)

	ticker := time.NewTicker(time.Second)
	select {
	case <-ticker.C:
		t.Fatalf("Handler shutdown timed out before calling toClose")
	case <-closed:
	}
}

func TestHandlerDropsGossipDuringBootstrapping(t *testing.T) {
	closed := make(chan struct{}, 1)
	ctx := snow.DefaultConsensusContextTest()
	vdrs := validators.NewSet()
	err := vdrs.AddWeight(ids.GenerateTestNodeID(), 1)
	require.NoError(t, err)

	mc := message.NewInternalBuilder()
	resourceTracker, err := tracker.NewResourceTracker(prometheus.NewRegistry(), resource.NoUsage, meter.ContinuousFactory{}, time.Second)
	require.NoError(t, err)
	handlerIntf, err := New(
		mc,
		ctx,
		vdrs,
		nil,
		nil,
		1,
		resourceTracker,
	)
	require.NoError(t, err)
	handler := handlerIntf.(*handler)

	handler.clock.Set(time.Now())

	bootstrapper := &common.BootstrapperTest{
		BootstrapableTest: common.BootstrapableTest{
			T: t,
		},
		EngineTest: common.EngineTest{
			T: t,
		},
	}
	bootstrapper.Default(false)
	bootstrapper.ContextF = func() *snow.ConsensusContext { return ctx }
	bootstrapper.GetFailedF = func(nodeID ids.NodeID, requestID uint32) error {
		closed <- struct{}{}
		return nil
	}
	handler.SetBootstrapper(bootstrapper)
	ctx.SetState(snow.Bootstrapping) // assumed bootstrapping is ongoing

	bootstrapper.StartF = func(startReqID uint32) error { return nil }

	handler.Start(false)

	nodeID := ids.EmptyNodeID
	chainID := ids.Empty
	reqID := uint32(1)
	inMsg := mc.InternalFailedRequest(message.GetFailed, nodeID, chainID, reqID)
	handler.Push(inMsg)

	ticker := time.NewTicker(time.Second)
	select {
	case <-ticker.C:
		t.Fatalf("Handler shutdown timed out before calling toClose")
	case <-closed:
	}
}

// Test that messages from the VM are handled
func TestHandlerDispatchInternal(t *testing.T) {
	calledNotify := make(chan struct{}, 1)
	ctx := snow.DefaultConsensusContextTest()
	msgFromVMChan := make(chan common.Message)
	vdrs := validators.NewSet()
	err := vdrs.AddWeight(ids.GenerateTestNodeID(), 1)
	require.NoError(t, err)

	mc := message.NewInternalBuilder()
	resourceTracker, err := tracker.NewResourceTracker(prometheus.NewRegistry(), resource.NoUsage, meter.ContinuousFactory{}, time.Second)
	require.NoError(t, err)
	handler, err := New(
		mc,
		ctx,
		vdrs,
		msgFromVMChan,
		nil,
		time.Second,
		resourceTracker,
	)
	require.NoError(t, err)

	bootstrapper := &common.BootstrapperTest{
		BootstrapableTest: common.BootstrapableTest{
			T: t,
		},
		EngineTest: common.EngineTest{
			T: t,
		},
	}
	bootstrapper.Default(false)
	handler.SetBootstrapper(bootstrapper)

	engine := &common.EngineTest{T: t}
	engine.Default(false)
	engine.ContextF = func() *snow.ConsensusContext { return ctx }
	engine.NotifyF = func(common.Message) error {
		calledNotify <- struct{}{}
		return nil
	}
	handler.SetConsensus(engine)
	ctx.SetState(snow.NormalOp) // assumed bootstrapping is done

	bootstrapper.StartF = func(startReqID uint32) error { return nil }

	handler.Start(false)
	msgFromVMChan <- 0

	select {
	case <-time.After(20 * time.Millisecond):
		t.Fatalf("should have called notify")
	case <-calledNotify:
	}
}

```

avalanchego/snow/networking/handler/message_queue.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package handler

import (
	"sync"

	"github.com/prometheus/client_golang/prometheus"

	"go.uber.org/zap"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/message"
	"github.com/ava-labs/avalanchego/snow/networking/tracker"
	"github.com/ava-labs/avalanchego/snow/validators"
	"github.com/ava-labs/avalanchego/utils/logging"
	"github.com/ava-labs/avalanchego/utils/timer/mockable"
)

var _ MessageQueue = &messageQueue{}

type MessageQueue interface {
	// Add a message.
	//
	// If called after [Shutdown], the message will immediately be marked as
	// having been handled.
	Push(message.InboundMessage)

	// Get and remove a message.
	//
	// If there are no available messages, this function will block until a
	// message becomes available or the queue is [Shutdown].
	Pop() (message.InboundMessage, bool)

	// Returns the number of messages currently on the queue
	Len() int

	// Shutdown and empty the queue.
	Shutdown()
}

// TODO: Use a better data structure for this.
// We can do something better than pushing to the back of a queue. A multi-level
// queue?
type messageQueue struct {
	// Useful for faking time in tests
	clock   mockable.Clock
	metrics messageQueueMetrics

	log logging.Logger
	// Validator set for the chain associated with this
	vdrs validators.Set
	// Tracks CPU utilization of each node
	cpuTracker tracker.Tracker

	cond   *sync.Cond
	closed bool
	// Node ID --> Messages this node has in [msgs]
	nodeToUnprocessedMsgs map[ids.NodeID]int
	// Unprocessed messages
	msgs []message.InboundMessage
}

func NewMessageQueue(
	log logging.Logger,
	vdrs validators.Set,
	cpuTracker tracker.Tracker,
	metricsNamespace string,
	metricsRegisterer prometheus.Registerer,
	ops []message.Op,
) (MessageQueue, error) {
	m := &messageQueue{
		log:                   log,
		vdrs:                  vdrs,
		cpuTracker:            cpuTracker,
		cond:                  sync.NewCond(&sync.Mutex{}),
		nodeToUnprocessedMsgs: make(map[ids.NodeID]int),
	}
	return m, m.metrics.initialize(metricsNamespace, metricsRegisterer, ops)
}

func (m *messageQueue) Push(msg message.InboundMessage) {
	m.cond.L.Lock()
	defer m.cond.L.Unlock()

	if m.closed {
		msg.OnFinishedHandling()
		return
	}

	// Add the message to the queue
	m.msgs = append(m.msgs, msg)
	m.nodeToUnprocessedMsgs[msg.NodeID()]++

	// Update metrics
	m.metrics.nodesWithMessages.Set(float64(len(m.nodeToUnprocessedMsgs)))
	m.metrics.len.Inc()
	m.metrics.ops[msg.Op()].Inc()

	// Signal a waiting thread
	m.cond.Signal()
}

// FIFO, but skip over messages whose senders whose messages have caused us to
// use excessive CPU recently.
func (m *messageQueue) Pop() (message.InboundMessage, bool) {
	m.cond.L.Lock()
	defer m.cond.L.Unlock()

	for {
		if m.closed {
			return nil, false
		}
		if len(m.msgs) != 0 {
			break
		}
		m.cond.Wait()
	}

	n := len(m.msgs)
	i := 0
	for {
		if i == n {
			m.log.Debug("canPop is false for all unprocessed messages",
				zap.Int("numMessages", n),
			)
		}
		msg := m.msgs[0]
		m.msgs[0] = nil
		nodeID := msg.NodeID()
		// See if it's OK to process [msg] next
		if m.canPop(msg) || i == n { // i should never == n but handle anyway as a fail-safe
			if cap(m.msgs) == 1 {
				m.msgs = nil // Give back memory if possible
			} else {
				m.msgs = m.msgs[1:]
			}
			m.nodeToUnprocessedMsgs[nodeID]--
			if m.nodeToUnprocessedMsgs[nodeID] == 0 {
				delete(m.nodeToUnprocessedMsgs, nodeID)
			}
			m.metrics.nodesWithMessages.Set(float64(len(m.nodeToUnprocessedMsgs)))
			m.metrics.len.Dec()
			m.metrics.ops[msg.Op()].Dec()
			return msg, true
		}
		// [msg.nodeID] is causing excessive CPU usage.
		// Push [msg] to back of [m.msgs] and handle it later.
		m.msgs = append(m.msgs, msg)
		m.msgs = m.msgs[1:]
		i++
		m.metrics.numExcessiveCPU.Inc()
	}
}

func (m *messageQueue) Len() int {
	m.cond.L.Lock()
	defer m.cond.L.Unlock()

	return len(m.msgs)
}

func (m *messageQueue) Shutdown() {
	m.cond.L.Lock()
	defer m.cond.L.Unlock()

	// Remove all the current messages from the queue
	for _, msg := range m.msgs {
		msg.OnFinishedHandling()
	}
	m.msgs = nil
	m.nodeToUnprocessedMsgs = nil

	// Update metrics
	m.metrics.nodesWithMessages.Set(0)
	m.metrics.len.Set(0)

	// Mark the queue as closed
	m.closed = true
	m.cond.Broadcast()
}

// canPop will return true for at least one message in [m.msgs]
func (m *messageQueue) canPop(msg message.InboundMessage) bool {
	// Always pop connected and disconnected messages.
	if op := msg.Op(); op == message.Connected || op == message.Disconnected {
		return true
	}

	// If the deadline to handle [msg] has passed, always pop it.
	// It will be dropped immediately.
	if expirationTime := msg.ExpirationTime(); !expirationTime.IsZero() && m.clock.Time().After(expirationTime) {
		return true
	}
	// Every node has some allowed CPU allocation depending on
	// the number of nodes with unprocessed messages.
	baseMaxCPU := 1 / float64(len(m.nodeToUnprocessedMsgs))
	nodeID := msg.NodeID()
	weight, isVdr := m.vdrs.GetWeight(nodeID)
	if !isVdr {
		weight = 0
	}
	// The sum of validator weights should never be 0, but handle
	// that case for completeness here to avoid divide by 0.
	portionWeight := float64(0)
	totalVdrsWeight := m.vdrs.Weight()
	if totalVdrsWeight != 0 {
		portionWeight = float64(weight) / float64(totalVdrsWeight)
	}
	// Validators are allowed to use more CPU. More weight --> more CPU use allowed.
	recentCPUUsage := m.cpuTracker.Usage(nodeID, m.clock.Time())
	maxCPU := baseMaxCPU + (1.0-baseMaxCPU)*portionWeight
	return recentCPUUsage <= maxCPU
}

```

avalanchego/snow/networking/handler/message_queue_metrics.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package handler

import (
	"fmt"

	"github.com/prometheus/client_golang/prometheus"

	"github.com/ava-labs/avalanchego/message"
	"github.com/ava-labs/avalanchego/utils/wrappers"
)

type messageQueueMetrics struct {
	ops               map[message.Op]prometheus.Gauge
	len               prometheus.Gauge
	nodesWithMessages prometheus.Gauge
	numExcessiveCPU   prometheus.Counter
}

func (m *messageQueueMetrics) initialize(
	metricsNamespace string,
	metricsRegisterer prometheus.Registerer,
	ops []message.Op,
) error {
	namespace := fmt.Sprintf("%s_%s", metricsNamespace, "unprocessed_msgs")
	m.len = prometheus.NewGauge(prometheus.GaugeOpts{
		Namespace: namespace,
		Name:      "len",
		Help:      "Messages ready to be processed",
	})
	m.nodesWithMessages = prometheus.NewGauge(prometheus.GaugeOpts{
		Namespace: namespace,
		Name:      "nodes",
		Help:      "Nodes from which there are at least 1 message ready to be processed",
	})
	m.numExcessiveCPU = prometheus.NewCounter(prometheus.CounterOpts{
		Namespace: namespace,
		Name:      "excessive_cpu",
		Help:      "Times we deferred handling a message from a node because the node was using excessive CPU",
	})

	errs := wrappers.Errs{}
	m.ops = make(map[message.Op]prometheus.Gauge, len(ops))

	for _, op := range ops {
		opStr := op.String()
		opMetric := prometheus.NewGauge(prometheus.GaugeOpts{
			Namespace: namespace,
			Name:      fmt.Sprintf("%s_count", opStr),
			Help:      fmt.Sprintf("Number of of %s messages in the message queue.", opStr),
		})
		m.ops[op] = opMetric
		errs.Add(metricsRegisterer.Register(opMetric))
	}

	errs.Add(
		metricsRegisterer.Register(m.len),
		metricsRegisterer.Register(m.nodesWithMessages),
		metricsRegisterer.Register(m.numExcessiveCPU),
	)
	return errs.Err
}

```

avalanchego/snow/networking/handler/message_queue_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package handler

import (
	"fmt"
	"testing"
	"time"

	"github.com/golang/mock/gomock"

	"github.com/prometheus/client_golang/prometheus"

	"github.com/stretchr/testify/require"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/message"
	"github.com/ava-labs/avalanchego/snow/networking/tracker"
	"github.com/ava-labs/avalanchego/snow/validators"
	"github.com/ava-labs/avalanchego/utils/logging"
)

func TestQueue(t *testing.T) {
	for _, useProto := range []bool{false, true} {
		t.Run(fmt.Sprintf("use proto buf message creator %v", useProto), func(tt *testing.T) {
			ctrl := gomock.NewController(tt)
			defer ctrl.Finish()

			require := require.New(tt)
			cpuTracker := tracker.NewMockTracker(ctrl)
			vdrs := validators.NewSet()
			vdr1ID, vdr2ID := ids.GenerateTestNodeID(), ids.GenerateTestNodeID()
			require.NoError(vdrs.AddWeight(vdr1ID, 1))
			require.NoError(vdrs.AddWeight(vdr2ID, 1))
			mIntf, err := NewMessageQueue(logging.NoLog{}, vdrs, cpuTracker, "", prometheus.NewRegistry(), message.SynchronousOps)
			require.NoError(err)
			u := mIntf.(*messageQueue)
			currentTime := time.Now()
			u.clock.Set(currentTime)

			metrics := prometheus.NewRegistry()
			var mc message.Creator
			if !useProto {
				mc, err = message.NewCreator(metrics, "dummyNamespace", true, 10*time.Second)
			} else {
				mc, err = message.NewCreatorWithProto(metrics, "dummyNamespace", true, 10*time.Second)
			}
			require.NoError(err)

			mc.SetTime(currentTime)
			msg1 := mc.InboundPut(
				ids.Empty,
				0,
				nil,
				vdr1ID,
			)

			// Push then pop should work regardless of usage when there are no other
			// messages on [u.msgs]
			cpuTracker.EXPECT().Usage(vdr1ID, gomock.Any()).Return(0.1).Times(1)
			u.Push(msg1)
			require.EqualValues(1, u.nodeToUnprocessedMsgs[vdr1ID])
			require.EqualValues(1, u.Len())
			gotMsg1, ok := u.Pop()
			require.True(ok)
			require.Len(u.nodeToUnprocessedMsgs, 0)
			require.EqualValues(0, u.Len())
			require.EqualValues(msg1, gotMsg1)

			cpuTracker.EXPECT().Usage(vdr1ID, gomock.Any()).Return(0.0).Times(1)
			u.Push(msg1)
			require.EqualValues(1, u.nodeToUnprocessedMsgs[vdr1ID])
			require.EqualValues(1, u.Len())
			gotMsg1, ok = u.Pop()
			require.True(ok)
			require.Len(u.nodeToUnprocessedMsgs, 0)
			require.EqualValues(0, u.Len())
			require.EqualValues(msg1, gotMsg1)

			cpuTracker.EXPECT().Usage(vdr1ID, gomock.Any()).Return(1.0).Times(1)
			u.Push(msg1)
			require.EqualValues(1, u.nodeToUnprocessedMsgs[vdr1ID])
			require.EqualValues(1, u.Len())
			gotMsg1, ok = u.Pop()
			require.True(ok)
			require.Len(u.nodeToUnprocessedMsgs, 0)
			require.EqualValues(0, u.Len())
			require.EqualValues(msg1, gotMsg1)

			cpuTracker.EXPECT().Usage(vdr1ID, gomock.Any()).Return(0.0).Times(1)
			u.Push(msg1)
			require.EqualValues(1, u.nodeToUnprocessedMsgs[vdr1ID])
			require.EqualValues(1, u.Len())
			gotMsg1, ok = u.Pop()
			require.True(ok)
			require.Len(u.nodeToUnprocessedMsgs, 0)
			require.EqualValues(0, u.Len())
			require.EqualValues(msg1, gotMsg1)

			// Push msg1 from vdr1ID
			u.Push(msg1)
			require.EqualValues(1, u.nodeToUnprocessedMsgs[vdr1ID])
			require.EqualValues(1, u.Len())

			msg2 := mc.InboundGet(ids.Empty, 0, 0, ids.Empty, vdr2ID)

			// Push msg2 from vdr2ID
			u.Push(msg2)
			require.EqualValues(2, u.Len())
			require.EqualValues(1, u.nodeToUnprocessedMsgs[vdr2ID])
			// Set vdr1's usage to 99% and vdr2's to .01
			cpuTracker.EXPECT().Usage(vdr1ID, gomock.Any()).Return(.99).Times(2)
			cpuTracker.EXPECT().Usage(vdr2ID, gomock.Any()).Return(.01).Times(1)
			// Pop should return msg2 first because vdr1 has exceeded it's portion of CPU time
			gotMsg2, ok := u.Pop()
			require.True(ok)
			require.EqualValues(1, u.Len())
			require.EqualValues(msg2, gotMsg2)
			gotMsg1, ok = u.Pop()
			require.True(ok)
			require.EqualValues(msg1, gotMsg1)
			require.Len(u.nodeToUnprocessedMsgs, 0)
			require.EqualValues(0, u.Len())

			// u is now empty
			// Non-validators should be able to put messages onto [u]
			nonVdrNodeID1, nonVdrNodeID2 := ids.GenerateTestNodeID(), ids.GenerateTestNodeID()
			msg3 := mc.InboundPullQuery(ids.Empty, 0, 0, ids.Empty, nonVdrNodeID1)
			msg4 := mc.InboundPushQuery(ids.Empty, 0, 0, nil, nonVdrNodeID2)
			u.Push(msg3)
			u.Push(msg4)
			u.Push(msg1)
			require.EqualValues(3, u.Len())

			// msg1 should get popped first because nonVdrNodeID1 and nonVdrNodeID2
			// exceeded their limit
			cpuTracker.EXPECT().Usage(nonVdrNodeID1, gomock.Any()).Return(.34).Times(1)
			cpuTracker.EXPECT().Usage(nonVdrNodeID2, gomock.Any()).Return(.34).Times(2)
			cpuTracker.EXPECT().Usage(vdr1ID, gomock.Any()).Return(0.0).Times(1)

			// u.msgs is [msg3, msg4, msg1]
			gotMsg1, ok = u.Pop()
			require.True(ok)
			require.EqualValues(msg1, gotMsg1)
			// u.msgs is [msg3, msg4]
			cpuTracker.EXPECT().Usage(nonVdrNodeID1, gomock.Any()).Return(.51).Times(2)
			gotMsg4, ok := u.Pop()
			require.True(ok)
			require.EqualValues(msg4, gotMsg4)
			// u.msgs is [msg3]
			gotMsg3, ok := u.Pop()
			require.True(ok)
			require.EqualValues(msg3, gotMsg3)
			require.EqualValues(0, u.Len())
		})
	}
}

```

avalanchego/snow/networking/handler/metrics.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package handler

import (
	"fmt"

	"github.com/prometheus/client_golang/prometheus"

	"github.com/ava-labs/avalanchego/message"
	"github.com/ava-labs/avalanchego/utils/metric"
	"github.com/ava-labs/avalanchego/utils/wrappers"
)

type metrics struct {
	expired      prometheus.Counter
	asyncExpired prometheus.Counter
	messages     map[message.Op]metric.Averager
}

func newMetrics(namespace string, reg prometheus.Registerer) (*metrics, error) {
	errs := wrappers.Errs{}

	expired := prometheus.NewCounter(prometheus.CounterOpts{
		Namespace: namespace,
		Name:      "expired",
		Help:      "Incoming sync messages dropped because the message deadline expired",
	})
	asyncExpired := prometheus.NewCounter(prometheus.CounterOpts{
		Namespace: namespace,
		Name:      "async_expired",
		Help:      "Incoming async messages dropped because the message deadline expired",
	})
	errs.Add(
		reg.Register(expired),
		reg.Register(asyncExpired),
	)

	messages := make(map[message.Op]metric.Averager, len(message.ConsensusOps))
	for _, op := range message.ConsensusOps {
		opStr := op.String()
		messages[op] = metric.NewAveragerWithErrs(
			namespace,
			opStr,
			fmt.Sprintf("time (in ns) of processing a %s", opStr),
			reg,
			&errs,
		)
	}

	return &metrics{
		expired:      expired,
		asyncExpired: asyncExpired,
		messages:     messages,
	}, errs.Err
}

```

avalanchego/snow/networking/handler/parser.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package handler

import (
	"errors"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/message"
)

var (
	errDuplicatedID     = errors.New("inbound message contains duplicated ID")
	errDuplicatedHeight = errors.New("inbound message contains duplicated height")
)

func getIDs(field message.Field, msg message.InboundMessage) ([]ids.ID, error) {
	idsBytesIntf, err := msg.Get(field)
	if err != nil {
		return nil, err
	}
	idsBytes := idsBytesIntf.([][]byte)

	res := make([]ids.ID, len(idsBytes))
	idSet := ids.NewSet(len(idsBytes))
	for i, bytes := range idsBytes {
		id, err := ids.ToID(bytes)
		if err != nil {
			return nil, err
		}
		if idSet.Contains(id) {
			return nil, errDuplicatedID
		}
		res[i] = id
		idSet.Add(id)
	}
	return res, nil
}

func getSummaryHeights(msg message.InboundMessage) ([]uint64, error) {
	heightsIntf, err := msg.Get(message.SummaryHeights)
	if err != nil {
		return nil, err
	}
	heights := heightsIntf.([]uint64)

	heightsSet := make(map[uint64]struct{}, len(heights))
	for _, height := range heights {
		if _, found := heightsSet[height]; found {
			return nil, errDuplicatedHeight
		}
		heightsSet[height] = struct{}{}
	}
	return heights, nil
}

```

avalanchego/snow/networking/router/chain_router.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package router

import (
	"encoding/binary"
	"errors"
	"fmt"
	"strings"
	"sync"
	"time"

	"github.com/prometheus/client_golang/prometheus"

	"go.uber.org/zap"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/message"
	"github.com/ava-labs/avalanchego/snow/networking/benchlist"
	"github.com/ava-labs/avalanchego/snow/networking/handler"
	"github.com/ava-labs/avalanchego/snow/networking/timeout"
	"github.com/ava-labs/avalanchego/utils/constants"
	"github.com/ava-labs/avalanchego/utils/hashing"
	"github.com/ava-labs/avalanchego/utils/linkedhashmap"
	"github.com/ava-labs/avalanchego/utils/logging"
	"github.com/ava-labs/avalanchego/utils/timer/mockable"
	"github.com/ava-labs/avalanchego/utils/wrappers"
	"github.com/ava-labs/avalanchego/version"
)

var (
	errUnknownChain = errors.New("received message for unknown chain")

	_ Router              = &ChainRouter{}
	_ benchlist.Benchable = &ChainRouter{}
)

type requestEntry struct {
	// When this request was registered
	time time.Time
	// The type of request that was made
	op message.Op
}

type peer struct {
	version        *version.Application
	trackedSubnets ids.Set
}

// ChainRouter routes incoming messages from the validator network
// to the consensus engines that the messages are intended for.
// Note that consensus engines are uniquely identified by the ID of the chain
// that they are working on.
type ChainRouter struct {
	clock      mockable.Clock
	log        logging.Logger
	msgCreator message.InternalMsgBuilder
	lock       sync.Mutex
	chains     map[ids.ID]handler.Handler

	// It is only safe to call [RegisterResponse] with the router lock held. Any
	// other calls to the timeout manager with the router lock held could cause
	// a deadlock because the timeout manager will call Benched and Unbenched.
	timeoutManager timeout.Manager

	closeTimeout time.Duration
	peers        map[ids.NodeID]*peer
	// node ID --> chains that node is benched on
	// invariant: if a node is benched on any chain, it is treated as disconnected on all chains
	benched        map[ids.NodeID]ids.Set
	criticalChains ids.Set
	onFatal        func(exitCode int)
	metrics        *routerMetrics
	// Parameters for doing health checks
	healthConfig HealthConfig
	// aggregator of requests based on their time
	timedRequests linkedhashmap.LinkedHashmap[ids.ID, requestEntry]
	// Must only be accessed in method [createRequestID].
	// [lock] must be held when [requestIDBytes] is accessed.
	requestIDBytes []byte
}

// Initialize the router.
//
// When this router receives an incoming message, it cancels the timeout in
// [timeouts] associated with the request that caused the incoming message, if
// applicable.
func (cr *ChainRouter) Initialize(
	nodeID ids.NodeID,
	log logging.Logger,
	msgCreator message.InternalMsgBuilder,
	timeoutManager timeout.Manager,
	closeTimeout time.Duration,
	criticalChains ids.Set,
	whitelistedSubnets ids.Set,
	onFatal func(exitCode int),
	healthConfig HealthConfig,
	metricsNamespace string,
	metricsRegisterer prometheus.Registerer,
) error {
	cr.log = log
	cr.msgCreator = msgCreator
	cr.chains = make(map[ids.ID]handler.Handler)
	cr.timeoutManager = timeoutManager
	cr.closeTimeout = closeTimeout
	cr.benched = make(map[ids.NodeID]ids.Set)
	cr.criticalChains = criticalChains
	cr.onFatal = onFatal
	cr.timedRequests = linkedhashmap.New[ids.ID, requestEntry]()
	cr.peers = make(map[ids.NodeID]*peer)
	cr.healthConfig = healthConfig
	cr.requestIDBytes = make([]byte, hashing.AddrLen+hashing.HashLen+wrappers.IntLen+wrappers.ByteLen) // Validator ID, Chain ID, Request ID, Msg Type

	// Mark myself as connected
	myself := &peer{
		version: version.CurrentApp,
	}
	myself.trackedSubnets.Union(whitelistedSubnets)
	myself.trackedSubnets.Add(constants.PrimaryNetworkID)
	cr.peers[nodeID] = myself

	// Register metrics
	rMetrics, err := newRouterMetrics(metricsNamespace, metricsRegisterer)
	if err != nil {
		return err
	}
	cr.metrics = rMetrics
	return nil
}

// RegisterRequest marks that we should expect to receive a reply from the given
// validator regarding the given chain and the reply should have the given
// requestID.
// The type of message we expect is [op].
// Every registered request must be cleared either by receiving a valid reply
// and passing it to the appropriate chain or by a timeout.
// This method registers a timeout that calls such methods if we don't get a
// reply in time.
func (cr *ChainRouter) RegisterRequest(
	nodeID ids.NodeID,
	chainID ids.ID,
	requestID uint32,
	op message.Op,
) {
	cr.lock.Lock()
	// When we receive a response message type (Chits, Put, Accepted, etc.)
	// we validate that we actually sent the corresponding request.
	// Give this request a unique ID so we can do that validation.
	uniqueRequestID := cr.createRequestID(nodeID, chainID, requestID, op)
	// Add to the set of unfulfilled requests
	cr.timedRequests.Put(uniqueRequestID, requestEntry{
		time: cr.clock.Time(),
		op:   op,
	})
	cr.metrics.outstandingRequests.Set(float64(cr.timedRequests.Len()))
	cr.lock.Unlock()

	failedOp, exists := message.ResponseToFailedOps[op]
	if !exists {
		// This should never happen
		cr.log.Error("failed to convert message operation",
			zap.Stringer("messageOp", op),
		)
		return
	}

	// Register a timeout to fire if we don't get a reply in time.
	cr.timeoutManager.RegisterRequest(nodeID, chainID, op, uniqueRequestID, func() {
		msg := cr.msgCreator.InternalFailedRequest(failedOp, nodeID, chainID, requestID)
		cr.HandleInbound(msg)
	})
}

func (cr *ChainRouter) HandleInbound(msg message.InboundMessage) {
	nodeID := msg.NodeID()
	op := msg.Op()

	chainIDIntf, err := msg.Get(message.ChainID)
	if err != nil {
		cr.log.Debug("dropping message with invalid field",
			zap.Stringer("nodeID", nodeID),
			zap.Stringer("messageOp", op),
			zap.Stringer("field", message.ChainID),
			zap.Error(err),
		)

		msg.OnFinishedHandling()
		return
	}
	chainIDBytes := chainIDIntf.([]byte)
	chainID, err := ids.ToID(chainIDBytes)
	if err != nil {
		cr.log.Debug("dropping message with invalid field",
			zap.Stringer("nodeID", nodeID),
			zap.Stringer("messageOp", op),
			zap.Stringer("field", message.ChainID),
			zap.Error(err),
		)

		msg.OnFinishedHandling()
		return
	}

	// AppGossip is the only message currently not containing a requestID
	// Here we assign the requestID already in use for gossiped containers
	// to allow a uniform handling of all messages
	var requestID uint32
	if op == message.AppGossip {
		requestID = constants.GossipMsgRequestID
	} else {
		// Invariant: Getting a [RequestID] must never error in the handler. Any
		//            verification performed by the message is done here.
		requestIDIntf, err := msg.Get(message.RequestID)
		if err != nil {
			cr.log.Debug("dropping message with invalid field",
				zap.Stringer("nodeID", nodeID),
				zap.Stringer("messageOp", op),
				zap.Stringer("field", message.RequestID),
				zap.Error(err),
			)

			msg.OnFinishedHandling()
			return
		}
		requestID = requestIDIntf.(uint32)
	}

	cr.lock.Lock()
	defer cr.lock.Unlock()

	// Get the chain, if it exists
	chain, exists := cr.chains[chainID]
	if !exists || !chain.IsValidator(nodeID) {
		cr.log.Debug("dropping message",
			zap.Stringer("messageOp", op),
			zap.Stringer("nodeID", nodeID),
			zap.Stringer("chainID", chainID),
			zap.Error(errUnknownChain),
		)

		msg.OnFinishedHandling()
		return
	}

	ctx := chain.Context()

	// TODO: [requestID] can overflow, which means a timeout on the request
	//       before the overflow may not be handled properly.
	if _, notRequested := message.UnrequestedOps[op]; notRequested ||
		(op == message.Put && requestID == constants.GossipMsgRequestID) {
		if ctx.IsExecuting() {
			cr.log.Debug("dropping message and skipping queue",
				zap.String("reason", "the chain is currently executing"),
				zap.Stringer("messageOp", op),
			)
			cr.metrics.droppedRequests.Inc()

			msg.OnFinishedHandling()
			return
		}
		chain.Push(msg)
		return
	}

	if expectedResponse, isFailed := message.FailedToResponseOps[op]; isFailed {
		// Create the request ID of the request we sent that this message is in
		// response to.
		uniqueRequestID, req := cr.clearRequest(expectedResponse, nodeID, chainID, requestID)
		if req == nil {
			// This was a duplicated response.
			msg.OnFinishedHandling()
			return
		}

		// Tell the timeout manager we are no longer expecting a response
		cr.timeoutManager.RemoveRequest(uniqueRequestID)

		// Pass the failure to the chain
		chain.Push(msg)
		return
	}

	if ctx.IsExecuting() {
		cr.log.Debug("dropping message and skipping queue",
			zap.String("reason", "the chain is currently executing"),
			zap.Stringer("messageOp", op),
		)
		cr.metrics.droppedRequests.Inc()

		msg.OnFinishedHandling()
		return
	}

	uniqueRequestID, req := cr.clearRequest(op, nodeID, chainID, requestID)
	if req == nil {
		// We didn't request this message.
		msg.OnFinishedHandling()
		return
	}

	// Calculate how long it took [nodeID] to reply
	latency := cr.clock.Time().Sub(req.time)

	// Tell the timeout manager we got a response
	cr.timeoutManager.RegisterResponse(nodeID, chainID, uniqueRequestID, req.op, latency)

	// Pass the response to the chain
	chain.Push(msg)
}

// Shutdown shuts down this router
func (cr *ChainRouter) Shutdown() {
	cr.log.Info("shutting down chain router")
	cr.lock.Lock()
	prevChains := cr.chains
	cr.chains = map[ids.ID]handler.Handler{}
	cr.lock.Unlock()

	for _, chain := range prevChains {
		chain.Stop()
	}

	ticker := time.NewTicker(cr.closeTimeout)
	defer ticker.Stop()

	for _, chain := range prevChains {
		select {
		case <-chain.Stopped():
		case <-ticker.C:
			cr.log.Warn("timed out while shutting down the chains")
			return
		}
	}
}

// AddChain registers the specified chain so that incoming
// messages can be routed to it
func (cr *ChainRouter) AddChain(chain handler.Handler) {
	cr.lock.Lock()
	defer cr.lock.Unlock()

	chainID := chain.Context().ChainID
	cr.log.Debug("registering chain with chain router",
		zap.Stringer("chainID", chainID),
	)
	chain.SetOnStopped(func() {
		cr.removeChain(chainID)
	})
	cr.chains[chainID] = chain

	// Notify connected validators
	subnetID := chain.Context().SubnetID
	for validatorID, peer := range cr.peers {
		// If this validator is benched on any chain, treat them as disconnected on all chains
		if _, benched := cr.benched[validatorID]; !benched && peer.trackedSubnets.Contains(subnetID) {
			msg := cr.msgCreator.InternalConnected(validatorID, peer.version)
			chain.Push(msg)
		}
	}
}

// Connected routes an incoming notification that a validator was just connected
func (cr *ChainRouter) Connected(nodeID ids.NodeID, nodeVersion *version.Application, subnetID ids.ID) {
	cr.lock.Lock()
	defer cr.lock.Unlock()

	connectedPeer, exists := cr.peers[nodeID]
	if !exists {
		connectedPeer = &peer{
			version: nodeVersion,
		}
		cr.peers[nodeID] = connectedPeer
	}
	connectedPeer.trackedSubnets.Add(subnetID)

	// If this validator is benched on any chain, treat them as disconnected on all chains
	if _, benched := cr.benched[nodeID]; benched {
		return
	}

	msg := cr.msgCreator.InternalConnected(nodeID, nodeVersion)

	// TODO: fire up an event when validator state changes i.e when they leave set, disconnect.
	// we cannot put a subnet-only validator check here since Disconnected would not be handled properly.
	for _, chain := range cr.chains {
		if subnetID == chain.Context().SubnetID {
			chain.Push(msg)
		}
	}
}

// Disconnected routes an incoming notification that a validator was connected
func (cr *ChainRouter) Disconnected(nodeID ids.NodeID) {
	cr.lock.Lock()
	defer cr.lock.Unlock()

	peer := cr.peers[nodeID]
	delete(cr.peers, nodeID)
	if _, benched := cr.benched[nodeID]; benched {
		return
	}

	msg := cr.msgCreator.InternalDisconnected(nodeID)

	// TODO: fire up an event when validator state changes i.e when they leave set, disconnect.
	// we cannot put a subnet-only validator check here since if a validator connects then it leaves validator-set, it would not be disconnected properly.
	for _, chain := range cr.chains {
		if peer.trackedSubnets.Contains(chain.Context().SubnetID) {
			chain.Push(msg)
		}
	}
}

// Benched routes an incoming notification that a validator was benched
func (cr *ChainRouter) Benched(chainID ids.ID, nodeID ids.NodeID) {
	cr.lock.Lock()
	defer cr.lock.Unlock()

	benchedChains, exists := cr.benched[nodeID]
	benchedChains.Add(chainID)
	cr.benched[nodeID] = benchedChains
	peer, hasPeer := cr.peers[nodeID]
	if exists || !hasPeer {
		// If the set already existed, then the node was previously benched.
		return
	}

	msg := cr.msgCreator.InternalDisconnected(nodeID)

	for _, chain := range cr.chains {
		if peer.trackedSubnets.Contains(chain.Context().SubnetID) {
			chain.Push(msg)
		}
	}
}

// Unbenched routes an incoming notification that a validator was just unbenched
func (cr *ChainRouter) Unbenched(chainID ids.ID, nodeID ids.NodeID) {
	cr.lock.Lock()
	defer cr.lock.Unlock()

	benchedChains := cr.benched[nodeID]
	benchedChains.Remove(chainID)
	if benchedChains.Len() == 0 {
		delete(cr.benched, nodeID)
	} else {
		cr.benched[nodeID] = benchedChains
		return // This node is still benched
	}

	peer, found := cr.peers[nodeID]
	if !found {
		return
	}

	msg := cr.msgCreator.InternalConnected(nodeID, peer.version)

	for _, chain := range cr.chains {
		if peer.trackedSubnets.Contains(chain.Context().SubnetID) {
			chain.Push(msg)
		}
	}
}

// HealthCheck returns results of router health checks. Returns:
// 1) Information about health check results
// 2) An error if the health check reports unhealthy
func (cr *ChainRouter) HealthCheck() (interface{}, error) {
	cr.lock.Lock()
	defer cr.lock.Unlock()

	numOutstandingReqs := cr.timedRequests.Len()
	isOutstandingReqs := numOutstandingReqs <= cr.healthConfig.MaxOutstandingRequests
	healthy := isOutstandingReqs
	details := map[string]interface{}{
		"outstandingRequests": numOutstandingReqs,
	}

	// check for long running requests
	now := cr.clock.Time()
	processingRequest := now
	if _, longestRunning, exists := cr.timedRequests.Oldest(); exists {
		processingRequest = longestRunning.time
	}
	timeReqRunning := now.Sub(processingRequest)
	isOutstanding := timeReqRunning <= cr.healthConfig.MaxOutstandingDuration
	healthy = healthy && isOutstanding
	details["longestRunningRequest"] = timeReqRunning.String()
	cr.metrics.longestRunningRequest.Set(float64(timeReqRunning))

	if !healthy {
		var errorReasons []string
		if !isOutstandingReqs {
			errorReasons = append(errorReasons, fmt.Sprintf("number of outstanding requests %d > %d", numOutstandingReqs, cr.healthConfig.MaxOutstandingRequests))
		}
		if !isOutstanding {
			errorReasons = append(errorReasons, fmt.Sprintf("time for outstanding requests %s > %s", timeReqRunning, cr.healthConfig.MaxOutstandingDuration))
		}
		// The router is not healthy
		return details, fmt.Errorf("the router is not healthy reason: %s", strings.Join(errorReasons, ", "))
	}
	return details, nil
}

// RemoveChain removes the specified chain so that incoming
// messages can't be routed to it
func (cr *ChainRouter) removeChain(chainID ids.ID) {
	cr.lock.Lock()
	chain, exists := cr.chains[chainID]
	if !exists {
		cr.log.Debug("can't remove unknown chain",
			zap.Stringer("chainID", chainID),
		)
		cr.lock.Unlock()
		return
	}
	delete(cr.chains, chainID)
	cr.lock.Unlock()

	chain.Stop()

	ticker := time.NewTicker(cr.closeTimeout)
	defer ticker.Stop()
	select {
	case <-chain.Stopped():
	case <-ticker.C:
		chain.Context().Log.Warn("timed out while shutting down")
	}

	if cr.onFatal != nil && cr.criticalChains.Contains(chainID) {
		go cr.onFatal(1)
	}
}

func (cr *ChainRouter) clearRequest(
	op message.Op,
	nodeID ids.NodeID,
	chainID ids.ID,
	requestID uint32,
) (ids.ID, *requestEntry) {
	// Create the request ID of the request we sent that this message is (allegedly) in response to.
	uniqueRequestID := cr.createRequestID(nodeID, chainID, requestID, op)
	// Mark that an outstanding request has been fulfilled
	request, exists := cr.timedRequests.Get(uniqueRequestID)
	if !exists {
		return uniqueRequestID, nil
	}

	cr.timedRequests.Delete(uniqueRequestID)
	cr.metrics.outstandingRequests.Set(float64(cr.timedRequests.Len()))
	return uniqueRequestID, &request
}

// Assumes [cr.lock] is held.
// Assumes [message.Op] is an alias of byte.
func (cr *ChainRouter) createRequestID(nodeID ids.NodeID, chainID ids.ID, requestID uint32, op message.Op) ids.ID {
	copy(cr.requestIDBytes, nodeID[:])
	copy(cr.requestIDBytes[hashing.AddrLen:], chainID[:])
	binary.BigEndian.PutUint32(cr.requestIDBytes[hashing.AddrLen+hashing.HashLen:], requestID)
	cr.requestIDBytes[hashing.AddrLen+hashing.HashLen+wrappers.IntLen] = byte(op)
	return hashing.ComputeHash256Array(cr.requestIDBytes)
}

```

avalanchego/snow/networking/router/chain_router_metrics.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package router

import (
	"github.com/prometheus/client_golang/prometheus"

	"github.com/ava-labs/avalanchego/utils/wrappers"
)

// routerMetrics about router messages
type routerMetrics struct {
	outstandingRequests   prometheus.Gauge
	longestRunningRequest prometheus.Gauge
	droppedRequests       prometheus.Counter
}

func newRouterMetrics(namespace string, registerer prometheus.Registerer) (*routerMetrics, error) {
	rMetrics := &routerMetrics{}
	rMetrics.outstandingRequests = prometheus.NewGauge(
		prometheus.GaugeOpts{
			Namespace: namespace,
			Name:      "outstanding",
			Help:      "Number of outstanding requests (all types)",
		},
	)
	rMetrics.longestRunningRequest = prometheus.NewGauge(
		prometheus.GaugeOpts{
			Namespace: namespace,
			Name:      "longest_running",
			Help:      "Time (in ns) the longest request took",
		},
	)
	rMetrics.droppedRequests = prometheus.NewCounter(
		prometheus.CounterOpts{
			Namespace: namespace,
			Name:      "dropped",
			Help:      "Number of dropped requests (all types)",
		},
	)

	errs := wrappers.Errs{}
	errs.Add(
		registerer.Register(rMetrics.outstandingRequests),
		registerer.Register(rMetrics.longestRunningRequest),
		registerer.Register(rMetrics.droppedRequests),
	)
	return rMetrics, errs.Err
}

```

avalanchego/snow/networking/router/chain_router_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package router

import (
	"sync"
	"testing"
	"time"

	"github.com/prometheus/client_golang/prometheus"

	"github.com/stretchr/testify/require"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/message"
	"github.com/ava-labs/avalanchego/snow"
	"github.com/ava-labs/avalanchego/snow/engine/common"
	"github.com/ava-labs/avalanchego/snow/networking/benchlist"
	"github.com/ava-labs/avalanchego/snow/networking/handler"
	"github.com/ava-labs/avalanchego/snow/networking/timeout"
	"github.com/ava-labs/avalanchego/snow/networking/tracker"
	"github.com/ava-labs/avalanchego/snow/validators"
	"github.com/ava-labs/avalanchego/utils/logging"
	"github.com/ava-labs/avalanchego/utils/math/meter"
	"github.com/ava-labs/avalanchego/utils/resource"
	"github.com/ava-labs/avalanchego/utils/timer"
	"github.com/ava-labs/avalanchego/version"
)

func TestShutdown(t *testing.T) {
	vdrs := validators.NewSet()
	err := vdrs.AddWeight(ids.GenerateTestNodeID(), 1)
	require.NoError(t, err)
	benchlist := benchlist.NewNoBenchlist()
	tm, err := timeout.NewManager(
		&timer.AdaptiveTimeoutConfig{
			InitialTimeout:     time.Millisecond,
			MinimumTimeout:     time.Millisecond,
			MaximumTimeout:     10 * time.Second,
			TimeoutCoefficient: 1.25,
			TimeoutHalflife:    5 * time.Minute,
		},
		benchlist,
		"",
		prometheus.NewRegistry(),
	)
	require.NoError(t, err)
	go tm.Dispatch()

	chainRouter := ChainRouter{}

	mc := message.NewInternalBuilder()
	err = chainRouter.Initialize(ids.EmptyNodeID, logging.NoLog{}, mc, tm, time.Second, ids.Set{}, ids.Set{}, nil, HealthConfig{}, "", prometheus.NewRegistry())
	require.NoError(t, err)

	shutdownCalled := make(chan struct{}, 1)

	ctx := snow.DefaultConsensusContextTest()
	resourceTracker, err := tracker.NewResourceTracker(prometheus.NewRegistry(), resource.NoUsage, meter.ContinuousFactory{}, time.Second)
	require.NoError(t, err)
	handler, err := handler.New(
		mc,
		ctx,
		vdrs,
		nil,
		nil,
		time.Second,
		resourceTracker,
	)
	require.NoError(t, err)

	bootstrapper := &common.BootstrapperTest{
		BootstrapableTest: common.BootstrapableTest{
			T: t,
		},
		EngineTest: common.EngineTest{
			T: t,
		},
	}
	bootstrapper.Default(true)
	bootstrapper.CantGossip = false
	bootstrapper.ContextF = func() *snow.ConsensusContext { return ctx }
	bootstrapper.ShutdownF = func() error { shutdownCalled <- struct{}{}; return nil }
	bootstrapper.ConnectedF = func(nodeID ids.NodeID, nodeVersion *version.Application) error { return nil }
	bootstrapper.HaltF = func() {}
	handler.SetBootstrapper(bootstrapper)

	engine := &common.EngineTest{T: t}
	engine.Default(true)
	engine.CantGossip = false
	engine.ContextF = func() *snow.ConsensusContext { return ctx }
	engine.ShutdownF = func() error { shutdownCalled <- struct{}{}; return nil }
	engine.ConnectedF = func(nodeID ids.NodeID, nodeVersion *version.Application) error { return nil }
	engine.HaltF = func() {}
	handler.SetConsensus(engine)
	ctx.SetState(snow.NormalOp) // assumed bootstrap is done

	chainRouter.AddChain(handler)

	bootstrapper.StartF = func(startReqID uint32) error { return nil }
	handler.Start(false)

	chainRouter.Shutdown()

	ticker := time.NewTicker(250 * time.Millisecond)
	select {
	case <-ticker.C:
		t.Fatalf("Handler shutdown was not called or timed out after 250ms during chainRouter shutdown")
	case <-shutdownCalled:
	}

	select {
	case <-handler.Stopped():
	default:
		t.Fatal("handler shutdown but never closed its closing channel")
	}
}

func TestShutdownTimesOut(t *testing.T) {
	nodeID := ids.EmptyNodeID
	vdrs := validators.NewSet()
	err := vdrs.AddWeight(ids.GenerateTestNodeID(), 1)
	require.NoError(t, err)
	benchlist := benchlist.NewNoBenchlist()
	metrics := prometheus.NewRegistry()
	// Ensure that the Ancestors request does not timeout
	tm, err := timeout.NewManager(
		&timer.AdaptiveTimeoutConfig{
			InitialTimeout:     time.Second,
			MinimumTimeout:     500 * time.Millisecond,
			MaximumTimeout:     10 * time.Second,
			TimeoutCoefficient: 1.25,
			TimeoutHalflife:    5 * time.Minute,
		},
		benchlist,
		"",
		metrics,
	)
	require.NoError(t, err)
	go tm.Dispatch()

	chainRouter := ChainRouter{}

	mc, err := message.NewCreator(metrics, "dummyNamespace", true, 10*time.Second)
	require.NoError(t, err)

	err = chainRouter.Initialize(ids.EmptyNodeID,
		logging.NoLog{},
		mc,
		tm,
		time.Millisecond,
		ids.Set{},
		ids.Set{},
		nil,
		HealthConfig{},
		"",
		metrics,
	)
	require.NoError(t, err)

	ctx := snow.DefaultConsensusContextTest()
	resourceTracker, err := tracker.NewResourceTracker(prometheus.NewRegistry(), resource.NoUsage, meter.ContinuousFactory{}, time.Second)
	require.NoError(t, err)
	handler, err := handler.New(
		mc,
		ctx,
		vdrs,
		nil,
		nil,
		time.Second,
		resourceTracker,
	)
	require.NoError(t, err)

	bootstrapFinished := make(chan struct{}, 1)
	bootstrapper := &common.BootstrapperTest{
		BootstrapableTest: common.BootstrapableTest{
			T: t,
		},
		EngineTest: common.EngineTest{
			T: t,
		},
	}
	bootstrapper.Default(true)
	bootstrapper.CantGossip = false
	bootstrapper.ContextF = func() *snow.ConsensusContext { return ctx }
	bootstrapper.ConnectedF = func(nodeID ids.NodeID, nodeVersion *version.Application) error { return nil }
	bootstrapper.HaltF = func() {}
	bootstrapper.AncestorsF = func(nodeID ids.NodeID, requestID uint32, containers [][]byte) error {
		// Ancestors blocks for two seconds
		time.Sleep(2 * time.Second)
		bootstrapFinished <- struct{}{}
		return nil
	}
	handler.SetBootstrapper(bootstrapper)

	engine := &common.EngineTest{T: t}
	engine.Default(false)
	engine.ContextF = func() *snow.ConsensusContext { return ctx }
	closed := new(int)
	engine.ShutdownF = func() error { *closed++; return nil }
	handler.SetConsensus(engine)
	ctx.SetState(snow.NormalOp) // assumed bootstrapping is done

	chainRouter.AddChain(handler)

	bootstrapper.StartF = func(startReqID uint32) error { return nil }
	handler.Start(false)

	shutdownFinished := make(chan struct{}, 1)

	go func() {
		chainID := ids.ID{}
		msg := mc.InboundAncestors(chainID, 1, nil, nodeID)
		handler.Push(msg)

		time.Sleep(50 * time.Millisecond) // Pause to ensure message gets processed

		chainRouter.Shutdown()
		shutdownFinished <- struct{}{}
	}()

	select {
	case <-bootstrapFinished:
		t.Fatalf("Shutdown should have finished in one millisecond before timing out instead of waiting for engine to finish shutting down.")
	case <-shutdownFinished:
	}
}

// Ensure that a timeout fires if we don't get a response to a request
func TestRouterTimeout(t *testing.T) {
	// Create a timeout manager
	maxTimeout := 25 * time.Millisecond
	tm, err := timeout.NewManager(
		&timer.AdaptiveTimeoutConfig{
			InitialTimeout:     10 * time.Millisecond,
			MinimumTimeout:     10 * time.Millisecond,
			MaximumTimeout:     maxTimeout,
			TimeoutCoefficient: 1,
			TimeoutHalflife:    5 * time.Minute,
		},
		benchlist.NewNoBenchlist(),
		"",
		prometheus.NewRegistry(),
	)
	require.NoError(t, err)
	go tm.Dispatch()

	// Create a router
	chainRouter := ChainRouter{}

	mc := message.NewInternalBuilder()
	err = chainRouter.Initialize(ids.EmptyNodeID, logging.NoLog{}, mc, tm, time.Millisecond, ids.Set{}, ids.Set{}, nil, HealthConfig{}, "", prometheus.NewRegistry())
	require.NoError(t, err)

	// Create bootstrapper, engine and handler
	var (
		calledGetFailed, calledGetAncestorsFailed,
		calledQueryFailed, calledQueryFailed2,
		calledGetAcceptedFailed, calledGetAcceptedFrontierFailed bool

		wg = sync.WaitGroup{}
	)

	ctx := snow.DefaultConsensusContextTest()
	vdrs := validators.NewSet()
	err = vdrs.AddWeight(ids.GenerateTestNodeID(), 1)
	require.NoError(t, err)

	resourceTracker, err := tracker.NewResourceTracker(prometheus.NewRegistry(), resource.NoUsage, meter.ContinuousFactory{}, time.Second)
	require.NoError(t, err)
	handler, err := handler.New(
		mc,
		ctx,
		vdrs,
		nil,
		nil,
		time.Second,
		resourceTracker,
	)
	require.NoError(t, err)

	bootstrapper := &common.BootstrapperTest{
		BootstrapableTest: common.BootstrapableTest{
			T: t,
		},
		EngineTest: common.EngineTest{
			T: t,
		},
	}
	bootstrapper.Default(true)
	bootstrapper.CantGossip = false
	bootstrapper.ContextF = func() *snow.ConsensusContext { return ctx }
	bootstrapper.ConnectedF = func(nodeID ids.NodeID, nodeVersion *version.Application) error { return nil }
	bootstrapper.HaltF = func() {}
	bootstrapper.GetFailedF = func(nodeID ids.NodeID, requestID uint32) error { wg.Done(); calledGetFailed = true; return nil }
	bootstrapper.GetAncestorsFailedF = func(nodeID ids.NodeID, requestID uint32) error {
		defer wg.Done()
		calledGetAncestorsFailed = true
		return nil
	}
	bootstrapper.QueryFailedF = func(nodeID ids.NodeID, requestID uint32) error {
		defer wg.Done()
		if !calledQueryFailed {
			calledQueryFailed = true
			return nil
		}
		calledQueryFailed2 = true
		return nil
	}
	bootstrapper.GetAcceptedFailedF = func(nodeID ids.NodeID, requestID uint32) error {
		defer wg.Done()
		calledGetAcceptedFailed = true
		return nil
	}
	bootstrapper.GetAcceptedFrontierFailedF = func(nodeID ids.NodeID, requestID uint32) error {
		defer wg.Done()
		calledGetAcceptedFrontierFailed = true
		return nil
	}
	handler.SetBootstrapper(bootstrapper)
	ctx.SetState(snow.Bootstrapping) // assumed bootstrapping is ongoing

	chainRouter.AddChain(handler)

	bootstrapper.StartF = func(startReqID uint32) error { return nil }
	handler.Start(false)

	// Register requests for each request type
	msgs := []message.Op{
		message.Put,
		message.Ancestors,
		message.Chits,
		message.Chits,
		message.Accepted,
		message.AcceptedFrontier,
	}

	wg.Add(len(msgs))

	for i, msg := range msgs {
		chainRouter.RegisterRequest(ids.GenerateTestNodeID(), ctx.ChainID, uint32(i), msg)
	}

	wg.Wait()

	chainRouter.lock.Lock()
	defer chainRouter.lock.Unlock()
	require.True(t, calledGetFailed && calledGetAncestorsFailed && calledQueryFailed2 && calledGetAcceptedFailed && calledGetAcceptedFrontierFailed)
}

func TestRouterClearTimeouts(t *testing.T) {
	// Create a timeout manager
	tm, err := timeout.NewManager(
		&timer.AdaptiveTimeoutConfig{
			InitialTimeout:     3 * time.Second,
			MinimumTimeout:     3 * time.Second,
			MaximumTimeout:     5 * time.Minute,
			TimeoutCoefficient: 1,
			TimeoutHalflife:    5 * time.Minute,
		},
		benchlist.NewNoBenchlist(),
		"",
		prometheus.NewRegistry(),
	)
	require.NoError(t, err)
	go tm.Dispatch()

	// Create a router
	chainRouter := ChainRouter{}

	metrics := prometheus.NewRegistry()
	mc, err := message.NewCreator(metrics, "dummyNamespace", true, 10*time.Second)
	require.NoError(t, err)

	err = chainRouter.Initialize(ids.EmptyNodeID, logging.NoLog{}, mc, tm, time.Millisecond, ids.Set{}, ids.Set{}, nil, HealthConfig{}, "", prometheus.NewRegistry())
	require.NoError(t, err)

	// Create bootstrapper, engine and handler
	ctx := snow.DefaultConsensusContextTest()
	vdrs := validators.NewSet()
	err = vdrs.AddWeight(ids.GenerateTestNodeID(), 1)
	require.NoError(t, err)

	resourceTracker, err := tracker.NewResourceTracker(prometheus.NewRegistry(), resource.NoUsage, meter.ContinuousFactory{}, time.Second)
	require.NoError(t, err)
	handler, err := handler.New(
		mc,
		ctx,
		vdrs,
		nil,
		nil,
		time.Second,
		resourceTracker,
	)
	require.NoError(t, err)

	bootstrapper := &common.BootstrapperTest{
		BootstrapableTest: common.BootstrapableTest{
			T: t,
		},
		EngineTest: common.EngineTest{
			T: t,
		},
	}
	bootstrapper.Default(false)
	bootstrapper.ContextF = func() *snow.ConsensusContext { return ctx }
	handler.SetBootstrapper(bootstrapper)

	engine := &common.EngineTest{T: t}
	engine.Default(false)
	engine.ContextF = func() *snow.ConsensusContext { return ctx }
	handler.SetConsensus(engine)
	ctx.SetState(snow.NormalOp) // assumed bootstrapping is done

	chainRouter.AddChain(handler)

	bootstrapper.StartF = func(startReqID uint32) error { return nil }
	handler.Start(false)

	// Register requests for each request type
	ops := []message.Op{
		message.Put,
		message.Ancestors,
		message.Chits,
		message.Accepted,
		message.AcceptedFrontier,
	}

	vID := ids.GenerateTestNodeID()
	for i, op := range ops {
		chainRouter.RegisterRequest(vID, ctx.ChainID, uint32(i), op)
	}

	// Clear each timeout by simulating responses to the queries
	// Note: Depends on the ordering of [msgs]
	var inMsg message.InboundMessage

	// Put
	inMsg = mc.InboundPut(ctx.ChainID, 0, nil, vID)
	chainRouter.HandleInbound(inMsg)

	// Ancestors
	inMsg = mc.InboundAncestors(ctx.ChainID, 1, nil, vID)
	chainRouter.HandleInbound(inMsg)

	// Chits
	inMsg = mc.InboundChits(ctx.ChainID, 2, nil, vID)
	chainRouter.HandleInbound(inMsg)

	// Accepted
	inMsg = mc.InboundAccepted(ctx.ChainID, 3, nil, vID)
	chainRouter.HandleInbound(inMsg)

	// Accepted Frontier
	inMsg = mc.InboundAcceptedFrontier(ctx.ChainID, 4, nil, vID)
	chainRouter.HandleInbound(inMsg)

	require.Equal(t, chainRouter.timedRequests.Len(), 0)
}

func TestValidatorOnlyMessageDrops(t *testing.T) {
	// Create a timeout manager
	maxTimeout := 25 * time.Millisecond
	tm, err := timeout.NewManager(
		&timer.AdaptiveTimeoutConfig{
			InitialTimeout:     10 * time.Millisecond,
			MinimumTimeout:     10 * time.Millisecond,
			MaximumTimeout:     maxTimeout,
			TimeoutCoefficient: 1,
			TimeoutHalflife:    5 * time.Minute,
		},
		benchlist.NewNoBenchlist(),
		"",
		prometheus.NewRegistry(),
	)
	require.NoError(t, err)
	go tm.Dispatch()

	// Create a router
	chainRouter := ChainRouter{}

	metrics := prometheus.NewRegistry()
	mc, err := message.NewCreator(metrics, "dummyNamespace", true, 10*time.Second)
	require.NoError(t, err)

	err = chainRouter.Initialize(ids.EmptyNodeID, logging.NoLog{}, mc, tm, time.Millisecond, ids.Set{}, ids.Set{}, nil, HealthConfig{}, "", prometheus.NewRegistry())
	require.NoError(t, err)

	// Create bootstrapper, engine and handler
	calledF := false
	wg := sync.WaitGroup{}

	ctx := snow.DefaultConsensusContextTest()
	ctx.SetValidatorOnly()
	vdrs := validators.NewSet()
	vID := ids.GenerateTestNodeID()
	err = vdrs.AddWeight(vID, 1)
	require.NoError(t, err)
	resourceTracker, err := tracker.NewResourceTracker(prometheus.NewRegistry(), resource.NoUsage, meter.ContinuousFactory{}, time.Second)
	require.NoError(t, err)
	handler, err := handler.New(
		mc,
		ctx,
		vdrs,
		nil,
		nil,
		time.Second,
		resourceTracker,
	)
	require.NoError(t, err)

	bootstrapper := &common.BootstrapperTest{
		BootstrapableTest: common.BootstrapableTest{
			T: t,
		},
		EngineTest: common.EngineTest{
			T: t,
		},
	}
	bootstrapper.Default(false)
	bootstrapper.ContextF = func() *snow.ConsensusContext { return ctx }
	bootstrapper.PullQueryF = func(nodeID ids.NodeID, requestID uint32, containerID ids.ID) error {
		defer wg.Done()
		calledF = true
		return nil
	}
	handler.SetBootstrapper(bootstrapper)
	ctx.SetState(snow.Bootstrapping) // assumed bootstrapping is ongoing

	engine := &common.EngineTest{T: t}
	engine.ContextF = func() *snow.ConsensusContext { return ctx }
	engine.Default(false)
	handler.SetConsensus(engine)

	chainRouter.AddChain(handler)

	bootstrapper.StartF = func(startReqID uint32) error { return nil }
	handler.Start(false)

	var inMsg message.InboundMessage
	dummyContainerID := ids.GenerateTestID()
	reqID := uint32(0)

	// Non-validator case
	nID := ids.GenerateTestNodeID()

	calledF = false
	inMsg = mc.InboundPullQuery(ctx.ChainID, reqID, time.Hour, dummyContainerID,
		nID,
	)
	chainRouter.HandleInbound(inMsg)

	require.False(t, calledF) // should not be called

	// Validator case
	calledF = false
	reqID++
	inMsg = mc.InboundPullQuery(ctx.ChainID, reqID, time.Hour, dummyContainerID,
		vID,
	)
	wg.Add(1)
	chainRouter.HandleInbound(inMsg)

	wg.Wait()
	require.True(t, calledF) // should be called since this is a validator request

	// register a validator request
	reqID++
	chainRouter.RegisterRequest(vID, ctx.ChainID, reqID, message.Get)
	require.Equal(t, 1, chainRouter.timedRequests.Len())

	// remove it from validators
	err = vdrs.Set(validators.NewSet().List())
	require.NoError(t, err)

	inMsg = mc.InboundPut(ctx.ChainID, reqID, nil, nID)
	chainRouter.HandleInbound(inMsg)

	// shouldn't clear out timed request, as the request should be cleared when
	// the GetFailed message is sent
	require.Equal(t, 1, chainRouter.timedRequests.Len())
}

```

avalanchego/snow/networking/router/health.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package router

import "time"

// HealthConfig describes parameters for router health checks.
type HealthConfig struct {
	// Reports unhealthy if we drop more than [MaxDropRate] of messages
	MaxDropRate float64 `json:"maxDropRate"`

	// Halflife of averager used to calculate the message drop rate
	// Must be > 0.
	// Larger value --> Drop rate affected less by recent messages
	MaxDropRateHalflife time.Duration `json:"maxDropRateHalflife"`

	// Reports unhealthy if more than this number of requests are outstanding.
	// Must be > 0
	MaxOutstandingRequests int `json:"maxOutstandingRequests"`

	// Reports unhealthy if there is a request outstanding for longer than this
	MaxOutstandingDuration time.Duration `json:"maxOutstandingDuration"`

	// Reports unhealthy if there is at least 1 outstanding not processed
	// before this mark
	MaxRunTimeRequests time.Duration `json:"maxRunTimeRequests"`
}

```

avalanchego/snow/networking/router/inbound_handler.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package router

import (
	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/message"
	"github.com/ava-labs/avalanchego/version"
)

var _ InboundHandler = InboundHandlerFunc(nil)

// InboundHandler handles inbound messages
type InboundHandler interface {
	HandleInbound(msg message.InboundMessage)
}

// The ExternalRouterFunc type is an adapter to allow the use of ordinary
// functions as ExternalRouters. If f is a function with the appropriate
// signature, ExternalRouterFunc(f) is an ExternalRouter that calls f.
type InboundHandlerFunc func(msg message.InboundMessage)

func (f InboundHandlerFunc) HandleInbound(msg message.InboundMessage) {
	f(msg)
}

// ExternalHandler handles messages from external parties
type ExternalHandler interface {
	InboundHandler

	Connected(nodeID ids.NodeID, nodeVersion *version.Application, subnetID ids.ID)
	Disconnected(nodeID ids.NodeID)
}

```

avalanchego/snow/networking/router/router.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package router

import (
	"time"

	"github.com/prometheus/client_golang/prometheus"

	"github.com/ava-labs/avalanchego/api/health"
	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/message"
	"github.com/ava-labs/avalanchego/snow/networking/benchlist"
	"github.com/ava-labs/avalanchego/snow/networking/handler"
	"github.com/ava-labs/avalanchego/snow/networking/timeout"
	"github.com/ava-labs/avalanchego/utils/logging"
)

// Router routes consensus messages to the Handler of the consensus
// engine that the messages are intended for
type Router interface {
	ExternalHandler
	InternalHandler

	Initialize(
		nodeID ids.NodeID,
		log logging.Logger,
		msgCreator message.InternalMsgBuilder,
		timeouts timeout.Manager,
		shutdownTimeout time.Duration,
		criticalChains ids.Set,
		whiteListedSubnets ids.Set,
		onFatal func(exitCode int),
		healthConfig HealthConfig,
		metricsNamespace string,
		metricsRegisterer prometheus.Registerer,
	) error
	Shutdown()
	AddChain(chain handler.Handler)
	health.Checker
}

// InternalHandler deals with messages internal to this node
type InternalHandler interface {
	benchlist.Benchable

	RegisterRequest(
		nodeID ids.NodeID,
		chainID ids.ID,
		requestID uint32,
		op message.Op,
	)
}

```

avalanchego/snow/networking/sender/external_sender.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package sender

import (
	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/message"
)

// ExternalSender sends consensus messages to other validators
// Right now this is implemented in the networking package
type ExternalSender interface {
	// Send a message to a specific set of nodes
	Send(
		msg message.OutboundMessage,
		nodeIDs ids.NodeIDSet,
		subnetID ids.ID,
		validatorOnly bool,
	) ids.NodeIDSet

	// Send a message to a random group of nodes in a subnet.
	// Nodes are sampled based on their validator status.
	Gossip(
		msg message.OutboundMessage,
		subnetID ids.ID,
		validatorOnly bool,
		numValidatorsToSend int,
		numNonValidatorsToSend int,
		numPeersToSend int,
	) ids.NodeIDSet
}

```

avalanchego/snow/networking/sender/sender.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package sender

import (
	"fmt"
	"time"

	"github.com/prometheus/client_golang/prometheus"

	"go.uber.org/zap"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/message"
	"github.com/ava-labs/avalanchego/snow"
	"github.com/ava-labs/avalanchego/snow/engine/common"
	"github.com/ava-labs/avalanchego/snow/networking/router"
	"github.com/ava-labs/avalanchego/snow/networking/timeout"
	"github.com/ava-labs/avalanchego/utils/constants"
	"github.com/ava-labs/avalanchego/utils/timer/mockable"
)

var _ common.Sender = &sender{}

type GossipConfig struct {
	AcceptedFrontierValidatorSize    uint `json:"gossipAcceptedFrontierValidatorSize" yaml:"gossipAcceptedFrontierValidatorSize"`
	AcceptedFrontierNonValidatorSize uint `json:"gossipAcceptedFrontierNonValidatorSize" yaml:"gossipAcceptedFrontierNonValidatorSize"`
	AcceptedFrontierPeerSize         uint `json:"gossipAcceptedFrontierPeerSize" yaml:"gossipAcceptedFrontierPeerSize"`
	OnAcceptValidatorSize            uint `json:"gossipOnAcceptValidatorSize" yaml:"gossipOnAcceptValidatorSize"`
	OnAcceptNonValidatorSize         uint `json:"gossipOnAcceptNonValidatorSize" yaml:"gossipOnAcceptNonValidatorSize"`
	OnAcceptPeerSize                 uint `json:"gossipOnAcceptPeerSize" yaml:"gossipOnAcceptPeerSize"`
	AppGossipValidatorSize           uint `json:"appGossipValidatorSize" yaml:"appGossipValidatorSize"`
	AppGossipNonValidatorSize        uint `json:"appGossipNonValidatorSize" yaml:"appGossipNonValidatorSize"`
	AppGossipPeerSize                uint `json:"appGossipPeerSize" yaml:"appGossipPeerSize"`
}

// sender is a wrapper around an ExternalSender.
// Messages to this node are put directly into [router] rather than
// being sent over the network via the wrapped ExternalSender.
// sender registers outbound requests with [router] so that [router]
// fires a timeout if we don't get a response to the request.
type sender struct {
	ctx                 *snow.ConsensusContext
	msgCreator          message.Creator
	msgCreatorWithProto message.Creator

	// TODO: remove this once we complete banff migration
	banffTime time.Time

	clock mockable.Clock

	sender   ExternalSender // Actually does the sending over the network
	router   router.Router
	timeouts timeout.Manager

	gossipConfig GossipConfig

	// Request message type --> Counts how many of that request
	// have failed because the node was benched
	failedDueToBench map[message.Op]prometheus.Counter
}

func New(
	ctx *snow.ConsensusContext,
	msgCreator message.Creator,
	msgCreatorWithProto message.Creator,
	banffTime time.Time,
	externalSender ExternalSender,
	router router.Router,
	timeouts timeout.Manager,
	gossipConfig GossipConfig,
) (common.Sender, error) {
	s := &sender{
		ctx:                 ctx,
		msgCreator:          msgCreator,
		msgCreatorWithProto: msgCreatorWithProto,
		banffTime:           banffTime,
		sender:              externalSender,
		router:              router,
		timeouts:            timeouts,
		gossipConfig:        gossipConfig,
		failedDueToBench:    make(map[message.Op]prometheus.Counter, len(message.ConsensusRequestOps)),
	}

	for _, op := range message.ConsensusRequestOps {
		counter := prometheus.NewCounter(
			prometheus.CounterOpts{
				Name: fmt.Sprintf("%s_failed_benched", op),
				Help: fmt.Sprintf("# of times a %s request was not sent because the node was benched", op),
			},
		)
		if err := ctx.Registerer.Register(counter); err != nil {
			return nil, fmt.Errorf("couldn't register metric for %s: %w", op, err)
		}
		s.failedDueToBench[op] = counter
	}
	return s, nil
}

func (s *sender) getMsgCreator() message.Creator {
	now := s.clock.Time()
	if now.Before(s.banffTime) {
		return s.msgCreator
	}
	return s.msgCreatorWithProto
}

func (s *sender) SendGetStateSummaryFrontier(nodeIDs ids.NodeIDSet, requestID uint32) {
	// Note that this timeout duration won't exactly match the one that gets
	// registered. That's OK.
	deadline := s.timeouts.TimeoutDuration()

	// Tell the router to expect a response message or a message notifying
	// that we won't get a response from each of these nodes.
	// We register timeouts for all nodes, regardless of whether we fail
	// to send them a message, to avoid busy looping when disconnected from
	// the internet.
	for nodeID := range nodeIDs {
		s.router.RegisterRequest(nodeID, s.ctx.ChainID, requestID, message.StateSummaryFrontier)
	}

	msgCreator := s.getMsgCreator()

	// Sending a message to myself. No need to send it over the network.
	// Just put it right into the router. Asynchronously to avoid deadlock.
	if nodeIDs.Contains(s.ctx.NodeID) {
		nodeIDs.Remove(s.ctx.NodeID)
		inMsg := msgCreator.InboundGetStateSummaryFrontier(s.ctx.ChainID, requestID, deadline, s.ctx.NodeID)
		go s.router.HandleInbound(inMsg)
	}

	// Create the outbound message.
	outMsg, err := msgCreator.GetStateSummaryFrontier(s.ctx.ChainID, requestID, deadline)

	// Send the message over the network.
	var sentTo ids.NodeIDSet
	if err == nil {
		sentTo = s.sender.Send(outMsg, nodeIDs, s.ctx.SubnetID, s.ctx.IsValidatorOnly())
	} else {
		s.ctx.Log.Error("failed to build message",
			zap.Stringer("messageOp", message.GetStateSummaryFrontier),
			zap.Stringer("chainID", s.ctx.ChainID),
			zap.Uint32("requestID", requestID),
			zap.Duration("deadline", deadline),
			zap.Error(err),
		)
	}

	for nodeID := range nodeIDs {
		if !sentTo.Contains(nodeID) {
			s.ctx.Log.Debug("failed to send message",
				zap.Stringer("messageOp", message.GetStateSummaryFrontier),
				zap.Stringer("nodeID", nodeID),
				zap.Stringer("chainID", s.ctx.ChainID),
				zap.Uint32("requestID", requestID),
			)
		}
	}
}

func (s *sender) SendStateSummaryFrontier(nodeID ids.NodeID, requestID uint32, summary []byte) {
	msgCreator := s.getMsgCreator()

	// Sending this message to myself.
	if nodeID == s.ctx.NodeID {
		inMsg := msgCreator.InboundStateSummaryFrontier(s.ctx.ChainID, requestID, summary, nodeID)
		go s.router.HandleInbound(inMsg)
		return
	}

	// Create the outbound message.
	outMsg, err := msgCreator.StateSummaryFrontier(s.ctx.ChainID, requestID, summary)
	if err != nil {
		s.ctx.Log.Error("failed to build message",
			zap.Stringer("messageOp", message.StateSummaryFrontier),
			zap.Stringer("chainID", s.ctx.ChainID),
			zap.Uint32("requestID", requestID),
			zap.Binary("summaryBytes", summary),
			zap.Error(err),
		)
		return
	}

	// Send the message over the network.
	nodeIDs := ids.NewNodeIDSet(1)
	nodeIDs.Add(nodeID)
	if sentTo := s.sender.Send(outMsg, nodeIDs, s.ctx.SubnetID, s.ctx.IsValidatorOnly()); sentTo.Len() == 0 {
		s.ctx.Log.Debug("failed to send message",
			zap.Stringer("messageOp", message.StateSummaryFrontier),
			zap.Stringer("nodeID", nodeID),
			zap.Stringer("chainID", s.ctx.ChainID),
			zap.Uint32("requestID", requestID),
		)
		s.ctx.Log.Verbo("failed to send message",
			zap.Stringer("messageOp", message.StateSummaryFrontier),
			zap.Stringer("nodeID", nodeID),
			zap.Stringer("chainID", s.ctx.ChainID),
			zap.Uint32("requestID", requestID),
			zap.Binary("summary", summary),
		)
	}
}

func (s *sender) SendGetAcceptedStateSummary(nodeIDs ids.NodeIDSet, requestID uint32, heights []uint64) {
	// Note that this timeout duration won't exactly match the one that gets
	// registered. That's OK.
	deadline := s.timeouts.TimeoutDuration()

	// Tell the router to expect a response message or a message notifying
	// that we won't get a response from each of these nodes.
	// We register timeouts for all nodes, regardless of whether we fail
	// to send them a message, to avoid busy looping when disconnected from
	// the internet.
	for nodeID := range nodeIDs {
		s.router.RegisterRequest(nodeID, s.ctx.ChainID, requestID, message.AcceptedStateSummary)
	}

	msgCreator := s.getMsgCreator()

	// Sending a message to myself. No need to send it over the network.
	// Just put it right into the router. Asynchronously to avoid deadlock.
	if nodeIDs.Contains(s.ctx.NodeID) {
		nodeIDs.Remove(s.ctx.NodeID)
		inMsg := msgCreator.InboundGetAcceptedStateSummary(s.ctx.ChainID, requestID, heights, deadline, s.ctx.NodeID)
		go s.router.HandleInbound(inMsg)
	}

	// Create the outbound message.
	outMsg, err := msgCreator.GetAcceptedStateSummary(s.ctx.ChainID, requestID, deadline, heights)

	// Send the message over the network.
	var sentTo ids.NodeIDSet
	if err == nil {
		sentTo = s.sender.Send(outMsg, nodeIDs, s.ctx.SubnetID, s.ctx.IsValidatorOnly())
	} else {
		s.ctx.Log.Error("failed to build message",
			zap.Stringer("messageOp", message.GetAcceptedStateSummary),
			zap.Stringer("chainID", s.ctx.ChainID),
			zap.Uint32("requestID", requestID),
			zap.Uint64s("heights", heights),
			zap.Error(err),
		)
	}

	for nodeID := range nodeIDs {
		if !sentTo.Contains(nodeID) {
			s.ctx.Log.Debug("failed to send message",
				zap.Stringer("messageOp", message.GetAcceptedStateSummary),
				zap.Stringer("nodeID", nodeID),
				zap.Stringer("chainID", s.ctx.ChainID),
				zap.Uint32("requestID", requestID),
				zap.Uint64s("heights", heights),
			)
		}
	}
}

func (s *sender) SendAcceptedStateSummary(nodeID ids.NodeID, requestID uint32, summaryIDs []ids.ID) {
	msgCreator := s.getMsgCreator()

	if nodeID == s.ctx.NodeID {
		inMsg := msgCreator.InboundAcceptedStateSummary(s.ctx.ChainID, requestID, summaryIDs, nodeID)
		go s.router.HandleInbound(inMsg)
		return
	}

	// Create the outbound message.
	outMsg, err := msgCreator.AcceptedStateSummary(s.ctx.ChainID, requestID, summaryIDs)
	if err != nil {
		s.ctx.Log.Error("failed to build message",
			zap.Stringer("messageOp", message.AcceptedStateSummary),
			zap.Stringer("chainID", s.ctx.ChainID),
			zap.Uint32("requestID", requestID),
			zap.Stringer("summaryIDs", ids.SliceStringer(summaryIDs)),
			zap.Error(err),
		)
		return
	}

	// Send the message over the network.
	nodeIDs := ids.NewNodeIDSet(1)
	nodeIDs.Add(nodeID)
	if sentTo := s.sender.Send(outMsg, nodeIDs, s.ctx.SubnetID, s.ctx.IsValidatorOnly()); sentTo.Len() == 0 {
		s.ctx.Log.Debug("failed to send message",
			zap.Stringer("messageOp", message.AcceptedStateSummary),
			zap.Stringer("nodeID", nodeID),
			zap.Stringer("chainID", s.ctx.ChainID),
			zap.Uint32("requestID", requestID),
			zap.Stringer("summaryIDs", ids.SliceStringer(summaryIDs)),
		)
	}
}

func (s *sender) SendGetAcceptedFrontier(nodeIDs ids.NodeIDSet, requestID uint32) {
	// Note that this timeout duration won't exactly match the one that gets
	// registered. That's OK.
	deadline := s.timeouts.TimeoutDuration()

	// Tell the router to expect a response message or a message notifying
	// that we won't get a response from each of these nodes.
	// We register timeouts for all nodes, regardless of whether we fail
	// to send them a message, to avoid busy looping when disconnected from
	// the internet.
	for nodeID := range nodeIDs {
		s.router.RegisterRequest(nodeID, s.ctx.ChainID, requestID, message.AcceptedFrontier)
	}

	msgCreator := s.getMsgCreator()

	// Sending a message to myself. No need to send it over the network.
	// Just put it right into the router. Asynchronously to avoid deadlock.
	if nodeIDs.Contains(s.ctx.NodeID) {
		nodeIDs.Remove(s.ctx.NodeID)
		inMsg := msgCreator.InboundGetAcceptedFrontier(s.ctx.ChainID, requestID, deadline, s.ctx.NodeID)
		go s.router.HandleInbound(inMsg)
	}

	// Create the outbound message.
	outMsg, err := msgCreator.GetAcceptedFrontier(s.ctx.ChainID, requestID, deadline)

	// Send the message over the network.
	var sentTo ids.NodeIDSet
	if err == nil {
		sentTo = s.sender.Send(outMsg, nodeIDs, s.ctx.SubnetID, s.ctx.IsValidatorOnly())
	} else {
		s.ctx.Log.Error("failed to build message",
			zap.Stringer("messageOp", message.GetAcceptedFrontier),
			zap.Stringer("chainID", s.ctx.ChainID),
			zap.Uint32("requestID", requestID),
			zap.Duration("deadline", deadline),
			zap.Error(err),
		)
	}

	for nodeID := range nodeIDs {
		if !sentTo.Contains(nodeID) {
			s.ctx.Log.Debug("failed to send message",
				zap.Stringer("messageOp", message.GetAcceptedFrontier),
				zap.Stringer("nodeID", nodeID),
				zap.Stringer("chainID", s.ctx.ChainID),
				zap.Uint32("requestID", requestID),
			)
		}
	}
}

func (s *sender) SendAcceptedFrontier(nodeID ids.NodeID, requestID uint32, containerIDs []ids.ID) {
	msgCreator := s.getMsgCreator()

	// Sending this message to myself.
	if nodeID == s.ctx.NodeID {
		inMsg := msgCreator.InboundAcceptedFrontier(s.ctx.ChainID, requestID, containerIDs, nodeID)
		go s.router.HandleInbound(inMsg)
		return
	}

	// Create the outbound message.
	outMsg, err := msgCreator.AcceptedFrontier(s.ctx.ChainID, requestID, containerIDs)
	if err != nil {
		s.ctx.Log.Error("failed to build message",
			zap.Stringer("messageOp", message.AcceptedFrontier),
			zap.Stringer("chainID", s.ctx.ChainID),
			zap.Uint32("requestID", requestID),
			zap.Stringer("containerIDs", ids.SliceStringer(containerIDs)),
			zap.Error(err),
		)
		return
	}

	// Send the message over the network.
	nodeIDs := ids.NewNodeIDSet(1)
	nodeIDs.Add(nodeID)
	if sentTo := s.sender.Send(outMsg, nodeIDs, s.ctx.SubnetID, s.ctx.IsValidatorOnly()); sentTo.Len() == 0 {
		s.ctx.Log.Debug("failed to send message",
			zap.Stringer("messageOp", message.AcceptedFrontier),
			zap.Stringer("nodeID", nodeID),
			zap.Stringer("chainID", s.ctx.ChainID),
			zap.Uint32("requestID", requestID),
			zap.Stringer("containerIDs", ids.SliceStringer(containerIDs)),
		)
	}
}

func (s *sender) SendGetAccepted(nodeIDs ids.NodeIDSet, requestID uint32, containerIDs []ids.ID) {
	// Note that this timeout duration won't exactly match the one that gets
	// registered. That's OK.
	deadline := s.timeouts.TimeoutDuration()

	// Tell the router to expect a response message or a message notifying
	// that we won't get a response from each of these nodes.
	// We register timeouts for all nodes, regardless of whether we fail
	// to send them a message, to avoid busy looping when disconnected from
	// the internet.
	for nodeID := range nodeIDs {
		s.router.RegisterRequest(nodeID, s.ctx.ChainID, requestID, message.Accepted)
	}

	msgCreator := s.getMsgCreator()

	// Sending a message to myself. No need to send it over the network.
	// Just put it right into the router. Asynchronously to avoid deadlock.
	if nodeIDs.Contains(s.ctx.NodeID) {
		nodeIDs.Remove(s.ctx.NodeID)
		inMsg := msgCreator.InboundGetAccepted(s.ctx.ChainID, requestID, deadline, containerIDs, s.ctx.NodeID)
		go s.router.HandleInbound(inMsg)
	}

	// Create the outbound message.
	outMsg, err := msgCreator.GetAccepted(s.ctx.ChainID, requestID, deadline, containerIDs)

	// Send the message over the network.
	var sentTo ids.NodeIDSet
	if err == nil {
		sentTo = s.sender.Send(outMsg, nodeIDs, s.ctx.SubnetID, s.ctx.IsValidatorOnly())
	} else {
		s.ctx.Log.Error("failed to build message",
			zap.Stringer("messageOp", message.GetAccepted),
			zap.Stringer("chainID", s.ctx.ChainID),
			zap.Uint32("requestID", requestID),
			zap.Stringer("containerIDs", ids.SliceStringer(containerIDs)),
			zap.Error(err),
		)
	}

	for nodeID := range nodeIDs {
		if !sentTo.Contains(nodeID) {
			s.ctx.Log.Debug("failed to send message",
				zap.Stringer("messageOp", message.GetAccepted),
				zap.Stringer("nodeID", nodeID),
				zap.Stringer("chainID", s.ctx.ChainID),
				zap.Uint32("requestID", requestID),
				zap.Stringer("containerIDs", ids.SliceStringer(containerIDs)),
			)
		}
	}
}

func (s *sender) SendAccepted(nodeID ids.NodeID, requestID uint32, containerIDs []ids.ID) {
	msgCreator := s.getMsgCreator()

	if nodeID == s.ctx.NodeID {
		inMsg := msgCreator.InboundAccepted(s.ctx.ChainID, requestID, containerIDs, nodeID)
		go s.router.HandleInbound(inMsg)
		return
	}

	// Create the outbound message.
	outMsg, err := msgCreator.Accepted(s.ctx.ChainID, requestID, containerIDs)
	if err != nil {
		s.ctx.Log.Error("failed to build message",
			zap.Stringer("messageOp", message.Accepted),
			zap.Stringer("chainID", s.ctx.ChainID),
			zap.Uint32("requestID", requestID),
			zap.Stringer("containerIDs", ids.SliceStringer(containerIDs)),
			zap.Error(err),
		)
		return
	}

	// Send the message over the network.
	nodeIDs := ids.NewNodeIDSet(1)
	nodeIDs.Add(nodeID)
	if sentTo := s.sender.Send(outMsg, nodeIDs, s.ctx.SubnetID, s.ctx.IsValidatorOnly()); sentTo.Len() == 0 {
		s.ctx.Log.Debug("failed to send message",
			zap.Stringer("messageOp", message.Accepted),
			zap.Stringer("nodeID", nodeID),
			zap.Stringer("chainID", s.ctx.ChainID),
			zap.Uint32("requestID", requestID),
			zap.Stringer("containerIDs", ids.SliceStringer(containerIDs)),
		)
	}
}

func (s *sender) SendGetAncestors(nodeID ids.NodeID, requestID uint32, containerID ids.ID) {
	// Tell the router to expect a response message or a message notifying
	// that we won't get a response from this node.
	s.router.RegisterRequest(nodeID, s.ctx.ChainID, requestID, message.Ancestors)

	msgCreator := s.getMsgCreator()

	// Sending a GetAncestors to myself always fails.
	if nodeID == s.ctx.NodeID {
		inMsg := msgCreator.InternalFailedRequest(message.GetAncestorsFailed, nodeID, s.ctx.ChainID, requestID)
		go s.router.HandleInbound(inMsg)
		return
	}

	// [nodeID] may be benched. That is, they've been unresponsive
	// so we don't even bother sending requests to them. We just have them immediately fail.
	if s.timeouts.IsBenched(nodeID, s.ctx.ChainID) {
		s.failedDueToBench[message.GetAncestors].Inc() // update metric
		s.timeouts.RegisterRequestToUnreachableValidator()
		inMsg := msgCreator.InternalFailedRequest(message.GetAncestorsFailed, nodeID, s.ctx.ChainID, requestID)
		go s.router.HandleInbound(inMsg)
		return
	}

	// Note that this timeout duration won't exactly match the one that gets
	// registered. That's OK.
	deadline := s.timeouts.TimeoutDuration()
	// Create the outbound message.
	outMsg, err := msgCreator.GetAncestors(s.ctx.ChainID, requestID, deadline, containerID)
	if err != nil {
		s.ctx.Log.Error("failed to build message",
			zap.Stringer("messageOp", message.GetAncestors),
			zap.Stringer("chainID", s.ctx.ChainID),
			zap.Uint32("requestID", requestID),
			zap.Stringer("containerID", containerID),
			zap.Error(err),
		)

		inMsg := msgCreator.InternalFailedRequest(message.GetAncestorsFailed, nodeID, s.ctx.ChainID, requestID)
		go s.router.HandleInbound(inMsg)
		return
	}

	// Send the message over the network.
	nodeIDs := ids.NewNodeIDSet(1)
	nodeIDs.Add(nodeID)
	if sentTo := s.sender.Send(outMsg, nodeIDs, s.ctx.SubnetID, s.ctx.IsValidatorOnly()); sentTo.Len() == 0 {
		s.ctx.Log.Debug("failed to send message",
			zap.Stringer("messageOp", message.GetAncestors),
			zap.Stringer("nodeID", nodeID),
			zap.Stringer("chainID", s.ctx.ChainID),
			zap.Uint32("requestID", requestID),
			zap.Stringer("containerID", containerID),
		)

		s.timeouts.RegisterRequestToUnreachableValidator()
		inMsg := msgCreator.InternalFailedRequest(message.GetAncestorsFailed, nodeID, s.ctx.ChainID, requestID)
		go s.router.HandleInbound(inMsg)
	}
}

// SendAncestors sends an Ancestors message to the consensus engine running on the specified chain
// on the specified node.
// The Ancestors message gives the recipient the contents of several containers.
func (s *sender) SendAncestors(nodeID ids.NodeID, requestID uint32, containers [][]byte) {
	msgCreator := s.getMsgCreator()

	// Create the outbound message.
	outMsg, err := msgCreator.Ancestors(s.ctx.ChainID, requestID, containers)
	if err != nil {
		s.ctx.Log.Error("failed to build message",
			zap.Stringer("messageOp", message.Ancestors),
			zap.Stringer("chainID", s.ctx.ChainID),
			zap.Uint32("requestID", requestID),
			zap.Int("numContainers", len(containers)),
			zap.Error(err),
		)
		return
	}

	// Send the message over the network.
	nodeIDs := ids.NewNodeIDSet(1)
	nodeIDs.Add(nodeID)
	if sentTo := s.sender.Send(outMsg, nodeIDs, s.ctx.SubnetID, s.ctx.IsValidatorOnly()); sentTo.Len() == 0 {
		s.ctx.Log.Debug("failed to send message",
			zap.Stringer("messageOp", message.Ancestors),
			zap.Stringer("nodeID", nodeID),
			zap.Stringer("chainID", s.ctx.ChainID),
			zap.Uint32("requestID", requestID),
			zap.Int("numContainers", len(containers)),
		)
	}
}

// SendGet sends a Get message to the consensus engine running on the specified
// chain to the specified node. The Get message signifies that this
// consensus engine would like the recipient to send this consensus engine the
// specified container.
func (s *sender) SendGet(nodeID ids.NodeID, requestID uint32, containerID ids.ID) {
	// Tell the router to expect a response message or a message notifying
	// that we won't get a response from this node.
	s.router.RegisterRequest(nodeID, s.ctx.ChainID, requestID, message.Put)

	msgCreator := s.getMsgCreator()

	// Sending a Get to myself always fails.
	if nodeID == s.ctx.NodeID {
		inMsg := msgCreator.InternalFailedRequest(message.GetFailed, nodeID, s.ctx.ChainID, requestID)
		go s.router.HandleInbound(inMsg)
		return
	}

	// [nodeID] may be benched. That is, they've been unresponsive
	// so we don't even bother sending requests to them. We just have them immediately fail.
	if s.timeouts.IsBenched(nodeID, s.ctx.ChainID) {
		s.failedDueToBench[message.Get].Inc() // update metric
		s.timeouts.RegisterRequestToUnreachableValidator()
		inMsg := msgCreator.InternalFailedRequest(message.GetFailed, nodeID, s.ctx.ChainID, requestID)
		go s.router.HandleInbound(inMsg)
		return
	}

	// Note that this timeout duration won't exactly match the one that gets
	// registered. That's OK.
	deadline := s.timeouts.TimeoutDuration()
	// Create the outbound message.
	outMsg, err := msgCreator.Get(s.ctx.ChainID, requestID, deadline, containerID)

	// Send the message over the network.
	var sentTo ids.NodeIDSet
	if err == nil {
		nodeIDs := ids.NewNodeIDSet(1)
		nodeIDs.Add(nodeID)
		sentTo = s.sender.Send(outMsg, nodeIDs, s.ctx.SubnetID, s.ctx.IsValidatorOnly())
	} else {
		s.ctx.Log.Error("failed to build message",
			zap.Stringer("messageOp", message.Get),
			zap.Stringer("chainID", s.ctx.ChainID),
			zap.Uint32("requestID", requestID),
			zap.Duration("deadline", deadline),
			zap.Stringer("containerID", containerID),
			zap.Error(err),
		)
	}

	if sentTo.Len() == 0 {
		s.ctx.Log.Debug("failed to send message",
			zap.Stringer("messageOp", message.Get),
			zap.Stringer("nodeID", nodeID),
			zap.Stringer("chainID", s.ctx.ChainID),
			zap.Uint32("requestID", requestID),
			zap.Stringer("containerID", containerID),
		)

		s.timeouts.RegisterRequestToUnreachableValidator()
		inMsg := msgCreator.InternalFailedRequest(message.GetFailed, nodeID, s.ctx.ChainID, requestID)
		go s.router.HandleInbound(inMsg)
	}
}

// SendPut sends a Put message to the consensus engine running on the specified chain
// on the specified node.
// The Put message signifies that this consensus engine is giving to the recipient
// the contents of the specified container.
func (s *sender) SendPut(nodeID ids.NodeID, requestID uint32, container []byte) {
	msgCreator := s.getMsgCreator()

	// Create the outbound message.
	outMsg, err := msgCreator.Put(s.ctx.ChainID, requestID, container)
	if err != nil {
		s.ctx.Log.Error("failed to build message",
			zap.Stringer("messageOp", message.Put),
			zap.Stringer("chainID", s.ctx.ChainID),
			zap.Uint32("requestID", requestID),
			zap.Binary("container", container),
			zap.Error(err),
		)
		return
	}

	// Send the message over the network.
	nodeIDs := ids.NewNodeIDSet(1)
	nodeIDs.Add(nodeID)
	if sentTo := s.sender.Send(outMsg, nodeIDs, s.ctx.SubnetID, s.ctx.IsValidatorOnly()); sentTo.Len() == 0 {
		s.ctx.Log.Debug("failed to send message",
			zap.Stringer("messageOp", message.Put),
			zap.Stringer("nodeID", nodeID),
			zap.Stringer("chainID", s.ctx.ChainID),
			zap.Uint32("requestID", requestID),
		)
		s.ctx.Log.Verbo("failed to send message",
			zap.Stringer("messageOp", message.Put),
			zap.Stringer("nodeID", nodeID),
			zap.Stringer("chainID", s.ctx.ChainID),
			zap.Uint32("requestID", requestID),
			zap.Binary("container", container),
		)
	}
}

// SendPushQuery sends a PushQuery message to the consensus engines running on the specified chains
// on the specified nodes.
// The PushQuery message signifies that this consensus engine would like each node to send
// their preferred frontier given the existence of the specified container.
func (s *sender) SendPushQuery(nodeIDs ids.NodeIDSet, requestID uint32, container []byte) {
	// Tell the router to expect a response message or a message notifying
	// that we won't get a response from each of these nodes.
	// We register timeouts for all nodes, regardless of whether we fail
	// to send them a message, to avoid busy looping when disconnected from
	// the internet.
	for nodeID := range nodeIDs {
		s.router.RegisterRequest(nodeID, s.ctx.ChainID, requestID, message.Chits)
	}

	// Note that this timeout duration won't exactly match the one that gets
	// registered. That's OK.
	deadline := s.timeouts.TimeoutDuration()

	msgCreator := s.getMsgCreator()

	// Sending a message to myself. No need to send it over the network.
	// Just put it right into the router. Do so asynchronously to avoid deadlock.
	if nodeIDs.Contains(s.ctx.NodeID) {
		nodeIDs.Remove(s.ctx.NodeID)
		inMsg := msgCreator.InboundPushQuery(s.ctx.ChainID, requestID, deadline, container, s.ctx.NodeID)
		go s.router.HandleInbound(inMsg)
	}

	// Some of [nodeIDs] may be benched. That is, they've been unresponsive
	// so we don't even bother sending messages to them. We just have them immediately fail.
	for nodeID := range nodeIDs {
		if s.timeouts.IsBenched(nodeID, s.ctx.ChainID) {
			s.failedDueToBench[message.PushQuery].Inc() // update metric
			nodeIDs.Remove(nodeID)
			s.timeouts.RegisterRequestToUnreachableValidator()

			// Immediately register a failure. Do so asynchronously to avoid deadlock.
			inMsg := msgCreator.InternalFailedRequest(message.QueryFailed, nodeID, s.ctx.ChainID, requestID)
			go s.router.HandleInbound(inMsg)
		}
	}

	// Create the outbound message.
	// [sentTo] are the IDs of validators who may receive the message.
	outMsg, err := msgCreator.PushQuery(s.ctx.ChainID, requestID, deadline, container)

	// Send the message over the network.
	var sentTo ids.NodeIDSet
	if err == nil {
		sentTo = s.sender.Send(outMsg, nodeIDs, s.ctx.SubnetID, s.ctx.IsValidatorOnly())
	} else {
		s.ctx.Log.Error("failed to build message",
			zap.Stringer("messageOp", message.PushQuery),
			zap.Stringer("chainID", s.ctx.ChainID),
			zap.Uint32("requestID", requestID),
			zap.Binary("container", container),
			zap.Error(err),
		)
	}

	for nodeID := range nodeIDs {
		if !sentTo.Contains(nodeID) {
			s.ctx.Log.Debug("failed to send message",
				zap.Stringer("messageOp", message.PushQuery),
				zap.Stringer("nodeID", nodeID),
				zap.Stringer("chainID", s.ctx.ChainID),
				zap.Uint32("requestID", requestID),
			)
			s.ctx.Log.Verbo("failed to send message",
				zap.Stringer("messageOp", message.PushQuery),
				zap.Stringer("nodeID", nodeID),
				zap.Stringer("chainID", s.ctx.ChainID),
				zap.Uint32("requestID", requestID),
				zap.Binary("container", container),
			)

			// Register failures for nodes we didn't send a request to.
			s.timeouts.RegisterRequestToUnreachableValidator()
			inMsg := msgCreator.InternalFailedRequest(message.QueryFailed, nodeID, s.ctx.ChainID, requestID)
			go s.router.HandleInbound(inMsg)
		}
	}
}

// SendPullQuery sends a PullQuery message to the consensus engines running on the specified chains
// on the specified nodes.
// The PullQuery message signifies that this consensus engine would like each node to send
// their preferred frontier.
func (s *sender) SendPullQuery(nodeIDs ids.NodeIDSet, requestID uint32, containerID ids.ID) {
	// Tell the router to expect a response message or a message notifying
	// that we won't get a response from each of these nodes.
	// We register timeouts for all nodes, regardless of whether we fail
	// to send them a message, to avoid busy looping when disconnected from
	// the internet.
	for nodeID := range nodeIDs {
		s.router.RegisterRequest(nodeID, s.ctx.ChainID, requestID, message.Chits)
	}

	// Note that this timeout duration won't exactly match the one that gets
	// registered. That's OK.
	deadline := s.timeouts.TimeoutDuration()

	msgCreator := s.getMsgCreator()

	// Sending a message to myself. No need to send it over the network.
	// Just put it right into the router. Do so asynchronously to avoid deadlock.
	if nodeIDs.Contains(s.ctx.NodeID) {
		nodeIDs.Remove(s.ctx.NodeID)
		inMsg := msgCreator.InboundPullQuery(s.ctx.ChainID, requestID, deadline, containerID, s.ctx.NodeID)
		go s.router.HandleInbound(inMsg)
	}

	// Some of the nodes in [nodeIDs] may be benched. That is, they've been unresponsive
	// so we don't even bother sending messages to them. We just have them immediately fail.
	for nodeID := range nodeIDs {
		if s.timeouts.IsBenched(nodeID, s.ctx.ChainID) {
			s.failedDueToBench[message.PullQuery].Inc() // update metric
			nodeIDs.Remove(nodeID)
			s.timeouts.RegisterRequestToUnreachableValidator()
			// Immediately register a failure. Do so asynchronously to avoid deadlock.
			inMsg := msgCreator.InternalFailedRequest(message.QueryFailed, nodeID, s.ctx.ChainID, requestID)
			go s.router.HandleInbound(inMsg)
		}
	}

	// Create the outbound message.
	outMsg, err := msgCreator.PullQuery(s.ctx.ChainID, requestID, deadline, containerID)

	// Send the message over the network.
	var sentTo ids.NodeIDSet
	if err == nil {
		sentTo = s.sender.Send(outMsg, nodeIDs, s.ctx.SubnetID, s.ctx.IsValidatorOnly())
	} else {
		s.ctx.Log.Error("failed to build message",
			zap.Stringer("messageOp", message.PullQuery),
			zap.Stringer("chainID", s.ctx.ChainID),
			zap.Uint32("requestID", requestID),
			zap.Duration("deadline", deadline),
			zap.Stringer("containerID", containerID),
			zap.Error(err),
		)
	}

	for nodeID := range nodeIDs {
		if !sentTo.Contains(nodeID) {
			s.ctx.Log.Debug("failed to send message",
				zap.Stringer("messageOp", message.PullQuery),
				zap.Stringer("nodeID", nodeID),
				zap.Stringer("chainID", s.ctx.ChainID),
				zap.Uint32("requestID", requestID),
				zap.Stringer("containerID", containerID),
			)

			// Register failures for nodes we didn't send a request to.
			s.timeouts.RegisterRequestToUnreachableValidator()
			inMsg := msgCreator.InternalFailedRequest(message.QueryFailed, nodeID, s.ctx.ChainID, requestID)
			go s.router.HandleInbound(inMsg)
		}
	}
}

// SendChits sends chits
func (s *sender) SendChits(nodeID ids.NodeID, requestID uint32, votes []ids.ID) {
	msgCreator := s.getMsgCreator()

	// If [nodeID] is myself, send this message directly
	// to my own router rather than sending it over the network
	if nodeID == s.ctx.NodeID {
		inMsg := msgCreator.InboundChits(s.ctx.ChainID, requestID, votes, nodeID)
		go s.router.HandleInbound(inMsg)
		return
	}

	// Create the outbound message.
	outMsg, err := msgCreator.Chits(s.ctx.ChainID, requestID, votes)
	if err != nil {
		s.ctx.Log.Error("failed to build message",
			zap.Stringer("messageOp", message.Chits),
			zap.Stringer("chainID", s.ctx.ChainID),
			zap.Uint32("requestID", requestID),
			zap.Stringer("containerIDs", ids.SliceStringer(votes)),
			zap.Error(err),
		)
		return
	}

	// Send the message over the network.
	nodeIDs := ids.NewNodeIDSet(1)
	nodeIDs.Add(nodeID)
	if sentTo := s.sender.Send(outMsg, nodeIDs, s.ctx.SubnetID, s.ctx.IsValidatorOnly()); sentTo.Len() == 0 {
		s.ctx.Log.Debug("failed to send message",
			zap.Stringer("messageOp", message.Chits),
			zap.Stringer("nodeID", nodeID),
			zap.Stringer("chainID", s.ctx.ChainID),
			zap.Uint32("requestID", requestID),
			zap.Stringer("containerIDs", ids.SliceStringer(votes)),
		)
	}
}

// SendAppRequest sends an application-level request to the given nodes.
// The meaning of this request, and how it should be handled, is defined by the VM.
func (s *sender) SendAppRequest(nodeIDs ids.NodeIDSet, requestID uint32, appRequestBytes []byte) error {
	// Tell the router to expect a response message or a message notifying
	// that we won't get a response from each of these nodes.
	// We register timeouts for all nodes, regardless of whether we fail
	// to send them a message, to avoid busy looping when disconnected from
	// the internet.
	for nodeID := range nodeIDs {
		s.router.RegisterRequest(nodeID, s.ctx.ChainID, requestID, message.AppResponse)
	}

	// Note that this timeout duration won't exactly match the one that gets
	// registered. That's OK.
	deadline := s.timeouts.TimeoutDuration()

	msgCreator := s.getMsgCreator()

	// Sending a message to myself. No need to send it over the network.
	// Just put it right into the router. Do so asynchronously to avoid deadlock.
	if nodeIDs.Contains(s.ctx.NodeID) {
		nodeIDs.Remove(s.ctx.NodeID)
		inMsg := msgCreator.InboundAppRequest(s.ctx.ChainID, requestID, deadline, appRequestBytes, s.ctx.NodeID)
		go s.router.HandleInbound(inMsg)
	}

	// Some of the nodes in [nodeIDs] may be benched. That is, they've been unresponsive
	// so we don't even bother sending messages to them. We just have them immediately fail.
	for nodeID := range nodeIDs {
		if s.timeouts.IsBenched(nodeID, s.ctx.ChainID) {
			s.failedDueToBench[message.AppRequest].Inc() // update metric
			nodeIDs.Remove(nodeID)
			s.timeouts.RegisterRequestToUnreachableValidator()

			// Immediately register a failure. Do so asynchronously to avoid deadlock.
			inMsg := msgCreator.InternalFailedRequest(message.AppRequestFailed, nodeID, s.ctx.ChainID, requestID)
			go s.router.HandleInbound(inMsg)
		}
	}

	// Create the outbound message.
	// [sentTo] are the IDs of nodes who may receive the message.
	outMsg, err := msgCreator.AppRequest(s.ctx.ChainID, requestID, deadline, appRequestBytes)

	// Send the message over the network.
	var sentTo ids.NodeIDSet
	if err == nil {
		sentTo = s.sender.Send(outMsg, nodeIDs, s.ctx.SubnetID, s.ctx.IsValidatorOnly())
	} else {
		s.ctx.Log.Error("failed to build message",
			zap.Stringer("messageOp", message.AppRequest),
			zap.Stringer("chainID", s.ctx.ChainID),
			zap.Uint32("requestID", requestID),
			zap.Binary("payload", appRequestBytes),
			zap.Error(err),
		)
	}

	for nodeID := range nodeIDs {
		if !sentTo.Contains(nodeID) {
			s.ctx.Log.Debug("failed to send message",
				zap.Stringer("messageOp", message.AppRequest),
				zap.Stringer("nodeID", nodeID),
				zap.Stringer("chainID", s.ctx.ChainID),
				zap.Uint32("requestID", requestID),
			)
			s.ctx.Log.Verbo("failed to send message",
				zap.Stringer("messageOp", message.AppRequest),
				zap.Stringer("nodeID", nodeID),
				zap.Stringer("chainID", s.ctx.ChainID),
				zap.Uint32("requestID", requestID),
				zap.Binary("payload", appRequestBytes),
			)

			// Register failures for nodes we didn't send a request to.
			s.timeouts.RegisterRequestToUnreachableValidator()
			inMsg := msgCreator.InternalFailedRequest(message.AppRequestFailed, nodeID, s.ctx.ChainID, requestID)
			go s.router.HandleInbound(inMsg)
		}
	}
	return nil
}

// SendAppResponse sends a response to an application-level request from the
// given node
func (s *sender) SendAppResponse(nodeID ids.NodeID, requestID uint32, appResponseBytes []byte) error {
	msgCreator := s.getMsgCreator()

	if nodeID == s.ctx.NodeID {
		inMsg := msgCreator.InboundAppResponse(s.ctx.ChainID, requestID, appResponseBytes, nodeID)
		go s.router.HandleInbound(inMsg)
		return nil
	}

	// Create the outbound message.
	outMsg, err := msgCreator.AppResponse(s.ctx.ChainID, requestID, appResponseBytes)
	if err != nil {
		s.ctx.Log.Error("failed to build message",
			zap.Stringer("messageOp", message.AppResponse),
			zap.Stringer("chainID", s.ctx.ChainID),
			zap.Uint32("requestID", requestID),
			zap.Binary("payload", appResponseBytes),
			zap.Error(err),
		)
		return nil
	}

	// Send the message over the network.
	nodeIDs := ids.NewNodeIDSet(1)
	nodeIDs.Add(nodeID)
	if sentTo := s.sender.Send(outMsg, nodeIDs, s.ctx.SubnetID, s.ctx.IsValidatorOnly()); sentTo.Len() == 0 {
		s.ctx.Log.Debug("failed to send message",
			zap.Stringer("messageOp", message.AppResponse),
			zap.Stringer("nodeID", nodeID),
			zap.Stringer("chainID", s.ctx.ChainID),
			zap.Uint32("requestID", requestID),
		)
		s.ctx.Log.Verbo("failed to send message",
			zap.Stringer("messageOp", message.AppResponse),
			zap.Stringer("nodeID", nodeID),
			zap.Stringer("chainID", s.ctx.ChainID),
			zap.Uint32("requestID", requestID),
			zap.Binary("payload", appResponseBytes),
		)
	}
	return nil
}

func (s *sender) SendAppGossipSpecific(nodeIDs ids.NodeIDSet, appGossipBytes []byte) error {
	msgCreator := s.getMsgCreator()

	// Create the outbound message.
	outMsg, err := msgCreator.AppGossip(s.ctx.ChainID, appGossipBytes)
	if err != nil {
		s.ctx.Log.Error("failed to build message",
			zap.Stringer("messageOp", message.AppGossip),
			zap.Stringer("chainID", s.ctx.ChainID),
			zap.Binary("payload", appGossipBytes),
			zap.Error(err),
		)
		return nil
	}

	// Send the message over the network.
	if sentTo := s.sender.Send(outMsg, nodeIDs, s.ctx.SubnetID, s.ctx.IsValidatorOnly()); sentTo.Len() == 0 {
		for nodeID := range nodeIDs {
			if !sentTo.Contains(nodeID) {
				s.ctx.Log.Debug("failed to send message",
					zap.Stringer("messageOp", message.AppGossip),
					zap.Stringer("nodeID", nodeID),
					zap.Stringer("chainID", s.ctx.ChainID),
				)
				s.ctx.Log.Verbo("failed to send message",
					zap.Stringer("messageOp", message.AppGossip),
					zap.Stringer("nodeID", nodeID),
					zap.Stringer("chainID", s.ctx.ChainID),
					zap.Binary("payload", appGossipBytes),
				)
			}
		}
	}
	return nil
}

// SendAppGossip sends an application-level gossip message.
func (s *sender) SendAppGossip(appGossipBytes []byte) error {
	msgCreator := s.getMsgCreator()

	// Create the outbound message.
	outMsg, err := msgCreator.AppGossip(s.ctx.ChainID, appGossipBytes)
	if err != nil {
		s.ctx.Log.Error("failed to build message",
			zap.Stringer("messageOp", message.AppGossip),
			zap.Stringer("chainID", s.ctx.ChainID),
			zap.Binary("payload", appGossipBytes),
			zap.Error(err),
		)
		return nil
	}

	validatorSize := int(s.gossipConfig.AppGossipValidatorSize)
	nonValidatorSize := int(s.gossipConfig.AppGossipNonValidatorSize)
	peerSize := int(s.gossipConfig.AppGossipPeerSize)

	sentTo := s.sender.Gossip(outMsg, s.ctx.SubnetID, s.ctx.IsValidatorOnly(), validatorSize, nonValidatorSize, peerSize)
	if sentTo.Len() == 0 {
		s.ctx.Log.Debug("failed to send message",
			zap.Stringer("messageOp", message.AppGossip),
			zap.Stringer("chainID", s.ctx.ChainID),
		)
		s.ctx.Log.Verbo("failed to send message",
			zap.Stringer("messageOp", message.AppGossip),
			zap.Stringer("chainID", s.ctx.ChainID),
			zap.Binary("payload", appGossipBytes),
		)
	}
	return nil
}

// SendGossip gossips the provided container
func (s *sender) SendGossip(container []byte) {
	msgCreator := s.getMsgCreator()

	// Create the outbound message.
	outMsg, err := msgCreator.Put(s.ctx.ChainID, constants.GossipMsgRequestID, container)
	if err != nil {
		s.ctx.Log.Error("failed to build message",
			zap.Stringer("messageOp", message.Put),
			zap.Stringer("chainID", s.ctx.ChainID),
			zap.Binary("container", container),
			zap.Error(err),
		)
		return
	}

	sentTo := s.sender.Gossip(
		outMsg,
		s.ctx.SubnetID,
		s.ctx.IsValidatorOnly(),
		int(s.gossipConfig.AcceptedFrontierValidatorSize),
		int(s.gossipConfig.AcceptedFrontierNonValidatorSize),
		int(s.gossipConfig.AcceptedFrontierPeerSize),
	)
	if sentTo.Len() == 0 {
		s.ctx.Log.Debug("failed to send message",
			zap.Stringer("messageOp", message.Put),
			zap.Stringer("chainID", s.ctx.ChainID),
		)
		s.ctx.Log.Verbo("failed to send message",
			zap.Stringer("messageOp", message.Put),
			zap.Stringer("chainID", s.ctx.ChainID),
			zap.Binary("container", container),
		)
	}
}

// Accept is called after every consensus decision
func (s *sender) Accept(ctx *snow.ConsensusContext, _ ids.ID, container []byte) error {
	if ctx.GetState() != snow.NormalOp {
		// don't gossip during bootstrapping
		return nil
	}

	msgCreator := s.getMsgCreator()

	// Create the outbound message.
	outMsg, err := msgCreator.Put(s.ctx.ChainID, constants.GossipMsgRequestID, container)
	if err != nil {
		s.ctx.Log.Error("failed to build message",
			zap.Stringer("messageOp", message.Put),
			zap.Stringer("chainID", s.ctx.ChainID),
			zap.Binary("container", container),
			zap.Error(err),
		)
		return nil
	}

	sentTo := s.sender.Gossip(
		outMsg,
		s.ctx.SubnetID,
		s.ctx.IsValidatorOnly(),
		int(s.gossipConfig.OnAcceptValidatorSize),
		int(s.gossipConfig.OnAcceptNonValidatorSize),
		int(s.gossipConfig.OnAcceptPeerSize),
	)
	if sentTo.Len() == 0 {
		s.ctx.Log.Debug("failed to send message",
			zap.Stringer("messageOp", message.Put),
			zap.Stringer("chainID", s.ctx.ChainID),
		)
		s.ctx.Log.Verbo("failed to send message",
			zap.Stringer("messageOp", message.Put),
			zap.Stringer("chainID", s.ctx.ChainID),
			zap.Binary("container", container),
		)
	}
	return nil
}

```

avalanchego/snow/networking/sender/sender_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package sender

import (
	"math/rand"
	"sync"
	"testing"
	"time"

	"github.com/prometheus/client_golang/prometheus"

	"github.com/stretchr/testify/require"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/message"
	"github.com/ava-labs/avalanchego/snow"
	"github.com/ava-labs/avalanchego/snow/engine/common"
	"github.com/ava-labs/avalanchego/snow/networking/benchlist"
	"github.com/ava-labs/avalanchego/snow/networking/handler"
	"github.com/ava-labs/avalanchego/snow/networking/router"
	"github.com/ava-labs/avalanchego/snow/networking/timeout"
	"github.com/ava-labs/avalanchego/snow/networking/tracker"
	"github.com/ava-labs/avalanchego/snow/validators"
	"github.com/ava-labs/avalanchego/utils/logging"
	"github.com/ava-labs/avalanchego/utils/math/meter"
	"github.com/ava-labs/avalanchego/utils/resource"
	"github.com/ava-labs/avalanchego/utils/timer"
	"github.com/ava-labs/avalanchego/version"
)

var defaultGossipConfig = GossipConfig{
	AcceptedFrontierPeerSize:  2,
	OnAcceptPeerSize:          2,
	AppGossipValidatorSize:    2,
	AppGossipNonValidatorSize: 2,
}

func TestTimeout(t *testing.T) {
	vdrs := validators.NewSet()
	err := vdrs.AddWeight(ids.GenerateTestNodeID(), 1)
	require.NoError(t, err)
	benchlist := benchlist.NewNoBenchlist()
	tm, err := timeout.NewManager(
		&timer.AdaptiveTimeoutConfig{
			InitialTimeout:     time.Millisecond,
			MinimumTimeout:     time.Millisecond,
			MaximumTimeout:     10 * time.Second,
			TimeoutHalflife:    5 * time.Minute,
			TimeoutCoefficient: 1.25,
		},
		benchlist,
		"",
		prometheus.NewRegistry(),
	)
	require.NoError(t, err)
	go tm.Dispatch()

	chainRouter := router.ChainRouter{}

	metrics := prometheus.NewRegistry()
	mc, err := message.NewCreator(metrics, "dummyNamespace", true, 10*time.Second)
	require.NoError(t, err)
	mcProto, err := message.NewCreatorWithProto(metrics, "dummyNamespace", true, 10*time.Second)
	require.NoError(t, err)

	err = chainRouter.Initialize(ids.EmptyNodeID, logging.NoLog{}, mc, tm, time.Second, ids.Set{}, ids.Set{}, nil, router.HealthConfig{}, "", prometheus.NewRegistry())
	require.NoError(t, err)

	context := snow.DefaultConsensusContextTest()
	externalSender := &ExternalSenderTest{TB: t}
	externalSender.Default(false)

	sender, err := New(context, mc, mcProto, time.Now().Add(time.Hour) /* TODO: test with banff accepted */, externalSender, &chainRouter, tm, defaultGossipConfig)
	require.NoError(t, err)

	wg := sync.WaitGroup{}
	wg.Add(2)
	failedVDRs := ids.NodeIDSet{}
	ctx := snow.DefaultConsensusContextTest()
	resourceTracker, err := tracker.NewResourceTracker(prometheus.NewRegistry(), resource.NoUsage, meter.ContinuousFactory{}, time.Second)
	require.NoError(t, err)
	handler, err := handler.New(
		mc,
		ctx,
		vdrs,
		nil,
		nil,
		time.Hour,
		resourceTracker,
	)
	require.NoError(t, err)

	bootstrapper := &common.BootstrapperTest{
		BootstrapableTest: common.BootstrapableTest{
			T: t,
		},
		EngineTest: common.EngineTest{
			T: t,
		},
	}
	bootstrapper.Default(true)
	bootstrapper.CantGossip = false
	bootstrapper.ContextF = func() *snow.ConsensusContext { return ctx }
	bootstrapper.ConnectedF = func(nodeID ids.NodeID, nodeVersion *version.Application) error { return nil }
	bootstrapper.QueryFailedF = func(nodeID ids.NodeID, _ uint32) error {
		failedVDRs.Add(nodeID)
		wg.Done()
		return nil
	}
	handler.SetBootstrapper(bootstrapper)
	ctx.SetState(snow.Bootstrapping) // assumed bootstrap is ongoing

	chainRouter.AddChain(handler)

	bootstrapper.StartF = func(startReqID uint32) error { return nil }
	handler.Start(false)

	vdrIDs := ids.NodeIDSet{}
	vdrIDs.Add(ids.NodeID{255})
	vdrIDs.Add(ids.NodeID{254})

	sender.SendPullQuery(vdrIDs, 0, ids.Empty)

	wg.Wait()

	if !failedVDRs.Equals(vdrIDs) {
		t.Fatalf("Timeouts should have fired")
	}
}

func TestReliableMessages(t *testing.T) {
	vdrs := validators.NewSet()
	err := vdrs.AddWeight(ids.NodeID{1}, 1)
	require.NoError(t, err)
	benchlist := benchlist.NewNoBenchlist()
	tm, err := timeout.NewManager(
		&timer.AdaptiveTimeoutConfig{
			InitialTimeout:     time.Millisecond,
			MinimumTimeout:     time.Millisecond,
			MaximumTimeout:     time.Millisecond,
			TimeoutHalflife:    5 * time.Minute,
			TimeoutCoefficient: 1.25,
		},
		benchlist,
		"",
		prometheus.NewRegistry(),
	)
	require.NoError(t, err)

	go tm.Dispatch()

	chainRouter := router.ChainRouter{}

	metrics := prometheus.NewRegistry()
	mc, err := message.NewCreator(metrics, "dummyNamespace", true, 10*time.Second)
	require.NoError(t, err)
	mcProto, err := message.NewCreatorWithProto(metrics, "dummyNamespace", true, 10*time.Second)
	require.NoError(t, err)

	err = chainRouter.Initialize(ids.EmptyNodeID, logging.NoLog{}, mc, tm, time.Second, ids.Set{}, ids.Set{}, nil, router.HealthConfig{}, "", prometheus.NewRegistry())
	require.NoError(t, err)

	context := snow.DefaultConsensusContextTest()

	externalSender := &ExternalSenderTest{TB: t}
	externalSender.Default(false)

	sender, err := New(context, mc, mcProto, time.Now().Add(time.Hour) /* TODO: test with banff accepted */, externalSender, &chainRouter, tm, defaultGossipConfig)
	require.NoError(t, err)

	ctx := snow.DefaultConsensusContextTest()
	resourceTracker, err := tracker.NewResourceTracker(prometheus.NewRegistry(), resource.NoUsage, meter.ContinuousFactory{}, time.Second)
	require.NoError(t, err)
	handler, err := handler.New(
		mc,
		ctx,
		vdrs,
		nil,
		nil,
		1,
		resourceTracker,
	)
	require.NoError(t, err)

	bootstrapper := &common.BootstrapperTest{
		BootstrapableTest: common.BootstrapableTest{
			T: t,
		},
		EngineTest: common.EngineTest{
			T: t,
		},
	}
	bootstrapper.Default(true)
	bootstrapper.CantGossip = false
	bootstrapper.ContextF = func() *snow.ConsensusContext { return ctx }
	bootstrapper.ConnectedF = func(nodeID ids.NodeID, nodeVersion *version.Application) error { return nil }
	queriesToSend := 1000
	awaiting := make([]chan struct{}, queriesToSend)
	for i := 0; i < queriesToSend; i++ {
		awaiting[i] = make(chan struct{}, 1)
	}
	bootstrapper.QueryFailedF = func(nodeID ids.NodeID, reqID uint32) error {
		close(awaiting[int(reqID)])
		return nil
	}
	bootstrapper.CantGossip = false
	handler.SetBootstrapper(bootstrapper)
	ctx.SetState(snow.Bootstrapping) // assumed bootstrap is ongoing

	chainRouter.AddChain(handler)

	bootstrapper.StartF = func(startReqID uint32) error { return nil }
	handler.Start(false)

	go func() {
		for i := 0; i < queriesToSend; i++ {
			vdrIDs := ids.NodeIDSet{}
			vdrIDs.Add(ids.NodeID{1})

			sender.SendPullQuery(vdrIDs, uint32(i), ids.Empty)
			time.Sleep(time.Duration(rand.Float64() * float64(time.Microsecond))) // #nosec G404
		}
	}()

	for _, await := range awaiting {
		<-await
	}
}

func TestReliableMessagesToMyself(t *testing.T) {
	benchlist := benchlist.NewNoBenchlist()
	vdrs := validators.NewSet()
	err := vdrs.AddWeight(ids.GenerateTestNodeID(), 1)
	require.NoError(t, err)
	tm, err := timeout.NewManager(
		&timer.AdaptiveTimeoutConfig{
			InitialTimeout:     10 * time.Millisecond,
			MinimumTimeout:     10 * time.Millisecond,
			MaximumTimeout:     10 * time.Millisecond, // Timeout fires immediately
			TimeoutHalflife:    5 * time.Minute,
			TimeoutCoefficient: 1.25,
		},
		benchlist,
		"",
		prometheus.NewRegistry(),
	)
	require.NoError(t, err)

	go tm.Dispatch()

	chainRouter := router.ChainRouter{}

	metrics := prometheus.NewRegistry()
	mc, err := message.NewCreator(metrics, "dummyNamespace", true, 10*time.Second)
	require.NoError(t, err)
	mcProto, err := message.NewCreatorWithProto(metrics, "dummyNamespace", true, 10*time.Second)
	require.NoError(t, err)

	err = chainRouter.Initialize(ids.EmptyNodeID, logging.NoLog{}, mc, tm, time.Second, ids.Set{}, ids.Set{}, nil, router.HealthConfig{}, "", prometheus.NewRegistry())
	require.NoError(t, err)

	context := snow.DefaultConsensusContextTest()

	externalSender := &ExternalSenderTest{TB: t}
	externalSender.Default(false)

	sender, err := New(context, mc, mcProto, time.Now().Add(time.Hour) /* TODO: test with banff accepted */, externalSender, &chainRouter, tm, defaultGossipConfig)
	require.NoError(t, err)

	ctx := snow.DefaultConsensusContextTest()
	resourceTracker, err := tracker.NewResourceTracker(prometheus.NewRegistry(), resource.NoUsage, meter.ContinuousFactory{}, time.Second)
	require.NoError(t, err)
	handler, err := handler.New(
		mc,
		ctx,
		vdrs,
		nil,
		nil,
		time.Second,
		resourceTracker,
	)
	require.NoError(t, err)

	bootstrapper := &common.BootstrapperTest{
		BootstrapableTest: common.BootstrapableTest{
			T: t,
		},
		EngineTest: common.EngineTest{
			T: t,
		},
	}
	bootstrapper.Default(true)
	bootstrapper.CantGossip = false
	bootstrapper.ContextF = func() *snow.ConsensusContext { return ctx }
	bootstrapper.ConnectedF = func(nodeID ids.NodeID, nodeVersion *version.Application) error { return nil }
	queriesToSend := 2
	awaiting := make([]chan struct{}, queriesToSend)
	for i := 0; i < queriesToSend; i++ {
		awaiting[i] = make(chan struct{}, 1)
	}
	bootstrapper.QueryFailedF = func(nodeID ids.NodeID, reqID uint32) error {
		close(awaiting[int(reqID)])
		return nil
	}
	handler.SetBootstrapper(bootstrapper)
	ctx.SetState(snow.Bootstrapping) // assumed bootstrap is ongoing

	chainRouter.AddChain(handler)

	bootstrapper.StartF = func(startReqID uint32) error { return nil }
	handler.Start(false)

	go func() {
		for i := 0; i < queriesToSend; i++ {
			// Send a pull query to some random peer that won't respond
			// because they don't exist. This will almost immediately trigger
			// a query failed message
			vdrIDs := ids.NodeIDSet{}
			vdrIDs.Add(ids.GenerateTestNodeID())
			sender.SendPullQuery(vdrIDs, uint32(i), ids.Empty)
		}
	}()

	for _, await := range awaiting {
		<-await
	}
}

```

avalanchego/snow/networking/sender/test_external_sender.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package sender

import (
	"errors"
	"testing"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/message"
)

var (
	errSend   = errors.New("unexpectedly called Send")
	errGossip = errors.New("unexpectedly called Gossip")
)

// ExternalSenderTest is a test sender
type ExternalSenderTest struct {
	TB testing.TB

	CantSend, CantGossip bool

	SendF   func(msg message.OutboundMessage, nodeIDs ids.NodeIDSet, subnetID ids.ID, validatorOnly bool) ids.NodeIDSet
	GossipF func(msg message.OutboundMessage, subnetID ids.ID, validatorOnly bool, numValidatorsToSend, numNonValidatorsToSend, numPeersToSend int) ids.NodeIDSet
}

// Default set the default callable value to [cant]
func (s *ExternalSenderTest) Default(cant bool) {
	s.CantSend = cant
	s.CantGossip = cant
}

func (s *ExternalSenderTest) Send(
	msg message.OutboundMessage,
	nodeIDs ids.NodeIDSet,
	subnetID ids.ID,
	validatorOnly bool,
) ids.NodeIDSet {
	if s.SendF != nil {
		return s.SendF(msg, nodeIDs, subnetID, validatorOnly)
	}
	if s.CantSend {
		if s.TB != nil {
			s.TB.Helper()
			s.TB.Fatal(errSend)
		}
	}
	return nil
}

// Given a msg type, the corresponding mock function is called if it was initialized.
// If it wasn't initialized and this function shouldn't be called and testing was
// initialized, then testing will fail.
func (s *ExternalSenderTest) Gossip(
	msg message.OutboundMessage,
	subnetID ids.ID,
	validatorOnly bool,
	numValidatorsToSend int,
	numNonValidatorsToSend int,
	numPeersToSend int,
) ids.NodeIDSet {
	if s.GossipF != nil {
		return s.GossipF(msg, subnetID, validatorOnly, numValidatorsToSend, numNonValidatorsToSend, numPeersToSend)
	}
	if s.CantGossip {
		if s.TB != nil {
			s.TB.Helper()
			s.TB.Fatal(errGossip)
		}
	}
	return nil
}

```

avalanchego/snow/networking/timeout/manager.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package timeout

import (
	"fmt"
	"time"

	"github.com/prometheus/client_golang/prometheus"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/message"
	"github.com/ava-labs/avalanchego/snow"
	"github.com/ava-labs/avalanchego/snow/networking/benchlist"
	"github.com/ava-labs/avalanchego/utils/timer"
)

var _ Manager = &manager{}

// Manages timeouts for requests sent to peers.
type Manager interface {
	// Start the manager. Must be called before any other method.
	// Should be called in a goroutine.
	Dispatch()
	// TimeoutDuration returns the current timeout duration.
	TimeoutDuration() time.Duration
	// IsBenched returns true if messages to [nodeID] regarding [chainID]
	// should not be sent over the network and should immediately fail.
	IsBenched(nodeID ids.NodeID, chainID ids.ID) bool
	// Register the existence of the given chain.
	// Must be called before any method calls that use the
	// ID of the chain.
	RegisterChain(ctx *snow.ConsensusContext) error
	// RegisterRequest notes that we expect a response of type [op] from
	// [nodeID] for chain [chainID]. If we don't receive a response in
	// time, [timeoutHandler] is executed.
	RegisterRequest(
		nodeID ids.NodeID,
		chainID ids.ID,
		op message.Op,
		requestID ids.ID,
		timeoutHandler func(),
	)
	// Registers that we would have sent a request to a validator but they
	// are unreachable because they are benched or because of network conditions
	// (e.g. we're not connected), so we didn't send the query. For the sake
	// of calculating the average latency and network timeout, we act as
	// though we sent the validator a request and it timed out.
	RegisterRequestToUnreachableValidator()
	// Registers that [nodeID] sent us a response of type [op]
	// for the given chain. The response corresponds to the given
	// requestID we sent them. [latency] is the time between us
	// sending them the request and receiving their response.
	RegisterResponse(
		nodeID ids.NodeID,
		chainID ids.ID,
		requestID ids.ID,
		op message.Op,
		latency time.Duration,
	)
	// Mark that we no longer expect a response to this request we sent.
	// Does not modify the timeout.
	RemoveRequest(requestID ids.ID)
}

func NewManager(
	timeoutConfig *timer.AdaptiveTimeoutConfig,
	benchlistMgr benchlist.Manager,
	metricsNamespace string,
	metricsRegister prometheus.Registerer,
) (Manager, error) {
	tm, err := timer.NewAdaptiveTimeoutManager(
		timeoutConfig,
		metricsNamespace,
		metricsRegister,
	)
	if err != nil {
		return nil, fmt.Errorf("couldn't create timeout manager: %w", err)
	}
	return &manager{
		benchlistMgr: benchlistMgr,
		tm:           tm,
	}, nil
}

type manager struct {
	tm           timer.AdaptiveTimeoutManager
	benchlistMgr benchlist.Manager
	metrics      metrics
}

func (m *manager) Dispatch() {
	m.tm.Dispatch()
}

func (m *manager) TimeoutDuration() time.Duration {
	return m.tm.TimeoutDuration()
}

// IsBenched returns true if messages to [nodeID] regarding [chainID]
// should not be sent over the network and should immediately fail.
func (m *manager) IsBenched(nodeID ids.NodeID, chainID ids.ID) bool {
	return m.benchlistMgr.IsBenched(nodeID, chainID)
}

func (m *manager) RegisterChain(ctx *snow.ConsensusContext) error {
	if err := m.metrics.RegisterChain(ctx); err != nil {
		return fmt.Errorf("couldn't register timeout metrics for chain %s: %w", ctx.ChainID, err)
	}
	if err := m.benchlistMgr.RegisterChain(ctx); err != nil {
		return fmt.Errorf("couldn't register chain %s with benchlist manager: %w", ctx.ChainID, err)
	}
	return nil
}

// RegisterRequest notes that we expect a response of type [op] from
// [nodeID] regarding chain [chainID]. If we don't receive a response in
// time, [timeoutHandler]  is executed.
func (m *manager) RegisterRequest(
	nodeID ids.NodeID,
	chainID ids.ID,
	op message.Op,
	requestID ids.ID,
	timeoutHandler func(),
) {
	newTimeoutHandler := func() {
		// If this request timed out, tell the benchlist manager
		m.benchlistMgr.RegisterFailure(chainID, nodeID)
		timeoutHandler()
	}
	m.tm.Put(requestID, op, newTimeoutHandler)
}

// RegisterResponse registers that we received a response from [nodeID]
// regarding the given request ID and chain.
func (m *manager) RegisterResponse(
	nodeID ids.NodeID,
	chainID ids.ID,
	requestID ids.ID,
	op message.Op,
	latency time.Duration,
) {
	m.metrics.Observe(nodeID, chainID, op, latency)
	m.benchlistMgr.RegisterResponse(chainID, nodeID)
	m.tm.Remove(requestID)
}

func (m *manager) RemoveRequest(requestID ids.ID) {
	m.tm.Remove(requestID)
}

func (m *manager) RegisterRequestToUnreachableValidator() {
	m.tm.ObserveLatency(m.TimeoutDuration())
}

```

avalanchego/snow/networking/timeout/manager_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package timeout

import (
	"sync"
	"testing"
	"time"

	"github.com/prometheus/client_golang/prometheus"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/message"
	"github.com/ava-labs/avalanchego/snow/networking/benchlist"
	"github.com/ava-labs/avalanchego/utils/timer"
)

func TestManagerFire(t *testing.T) {
	benchlist := benchlist.NewNoBenchlist()
	manager, err := NewManager(
		&timer.AdaptiveTimeoutConfig{
			InitialTimeout:     time.Millisecond,
			MinimumTimeout:     time.Millisecond,
			MaximumTimeout:     10 * time.Second,
			TimeoutCoefficient: 1.25,
			TimeoutHalflife:    5 * time.Minute,
		},
		benchlist,
		"",
		prometheus.NewRegistry(),
	)
	if err != nil {
		t.Fatal(err)
	}
	go manager.Dispatch()

	wg := sync.WaitGroup{}
	wg.Add(1)

	manager.RegisterRequest(ids.NodeID{}, ids.ID{}, message.PullQuery, ids.GenerateTestID(), wg.Done)

	wg.Wait()
}

func TestManagerCancel(t *testing.T) {
	benchlist := benchlist.NewNoBenchlist()
	manager, err := NewManager(
		&timer.AdaptiveTimeoutConfig{
			InitialTimeout:     time.Millisecond,
			MinimumTimeout:     time.Millisecond,
			MaximumTimeout:     10 * time.Second,
			TimeoutCoefficient: 1.25,
			TimeoutHalflife:    5 * time.Minute,
		},
		benchlist,
		"",
		prometheus.NewRegistry(),
	)
	if err != nil {
		t.Fatal(err)
	}
	go manager.Dispatch()

	wg := sync.WaitGroup{}
	wg.Add(1)

	fired := new(bool)

	id := ids.GenerateTestID()
	manager.RegisterRequest(ids.NodeID{}, ids.ID{}, message.PullQuery, id, func() { *fired = true })

	manager.RegisterResponse(ids.NodeID{}, ids.ID{}, id, message.Get, 1*time.Second)

	manager.RegisterRequest(ids.NodeID{}, ids.ID{}, message.PullQuery, ids.GenerateTestID(), wg.Done)

	wg.Wait()

	if *fired {
		t.Fatalf("Should have cancelled the function")
	}
}

```

avalanchego/snow/networking/timeout/metrics.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package timeout

import (
	"fmt"
	"sync"
	"time"

	"github.com/prometheus/client_golang/prometheus"

	"go.uber.org/zap"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/message"
	"github.com/ava-labs/avalanchego/snow"
	"github.com/ava-labs/avalanchego/utils/metric"
	"github.com/ava-labs/avalanchego/utils/wrappers"
)

const (
	defaultRequestHelpMsg = "time (in ns) spent waiting for a response to this message"
	validatorIDLabel      = "validatorID"
)

type metrics struct {
	lock           sync.Mutex
	chainToMetrics map[ids.ID]*chainMetrics
}

func (m *metrics) RegisterChain(ctx *snow.ConsensusContext) error {
	m.lock.Lock()
	defer m.lock.Unlock()

	if m.chainToMetrics == nil {
		m.chainToMetrics = map[ids.ID]*chainMetrics{}
	}
	if _, exists := m.chainToMetrics[ctx.ChainID]; exists {
		return fmt.Errorf("chain %s has already been registered", ctx.ChainID)
	}
	cm, err := newChainMetrics(ctx, false)
	if err != nil {
		return fmt.Errorf("couldn't create metrics for chain %s: %w", ctx.ChainID, err)
	}
	m.chainToMetrics[ctx.ChainID] = cm
	return nil
}

// Record that a response of type [op] took [latency]
func (m *metrics) Observe(nodeID ids.NodeID, chainID ids.ID, op message.Op, latency time.Duration) {
	m.lock.Lock()
	defer m.lock.Unlock()

	cm, exists := m.chainToMetrics[chainID]
	if !exists {
		// TODO should this log an error?
		return
	}
	cm.observe(nodeID, op, latency)
}

// chainMetrics contains message response time metrics for a chain
type chainMetrics struct {
	ctx *snow.ConsensusContext

	messageLatencies map[message.Op]metric.Averager

	summaryEnabled   bool
	messageSummaries map[message.Op]*prometheus.SummaryVec
}

func newChainMetrics(ctx *snow.ConsensusContext, summaryEnabled bool) (*chainMetrics, error) {
	cm := &chainMetrics{
		ctx: ctx,

		messageLatencies: make(map[message.Op]metric.Averager, len(message.ConsensusResponseOps)),

		summaryEnabled:   summaryEnabled,
		messageSummaries: make(map[message.Op]*prometheus.SummaryVec, len(message.ConsensusResponseOps)),
	}

	errs := wrappers.Errs{}
	for _, op := range message.ConsensusResponseOps {
		cm.messageLatencies[op] = metric.NewAveragerWithErrs(
			"lat",
			op.String(),
			defaultRequestHelpMsg,
			ctx.Registerer,
			&errs,
		)

		if !summaryEnabled {
			continue
		}

		summaryName := fmt.Sprintf("%s_peer", op)
		summary := prometheus.NewSummaryVec(
			prometheus.SummaryOpts{
				Namespace: "lat",
				Name:      summaryName,
				Help:      defaultRequestHelpMsg,
			},
			[]string{validatorIDLabel},
		)
		cm.messageSummaries[op] = summary

		if err := ctx.Registerer.Register(summary); err != nil {
			errs.Add(fmt.Errorf("failed to register %s statistics: %w", summaryName, err))
		}
	}
	return cm, errs.Err
}

func (cm *chainMetrics) observe(nodeID ids.NodeID, op message.Op, latency time.Duration) {
	lat := float64(latency)
	if msg, exists := cm.messageLatencies[op]; exists {
		msg.Observe(lat)
	}

	if !cm.summaryEnabled {
		return
	}

	labels := prometheus.Labels{
		validatorIDLabel: nodeID.String(),
	}

	msg, exists := cm.messageSummaries[op]
	if !exists {
		return
	}

	observer, err := msg.GetMetricWith(labels)
	if err != nil {
		cm.ctx.Log.Warn("failed to get observer with validatorID",
			zap.Error(err),
		)
		return
	}
	observer.Observe(lat)
}

```

avalanchego/snow/networking/tracker/mock_resource_tracker.go:
```
// Code generated by MockGen. DO NOT EDIT.
// Source: snow/networking/tracker/resource_tracker.go

// Package tracker is a generated GoMock package.
package tracker

import (
	reflect "reflect"
	time "time"

	ids "github.com/ava-labs/avalanchego/ids"
	gomock "github.com/golang/mock/gomock"
)

// MockTracker is a mock of Tracker interface.
type MockTracker struct {
	ctrl     *gomock.Controller
	recorder *MockTrackerMockRecorder
}

// MockTrackerMockRecorder is the mock recorder for MockTracker.
type MockTrackerMockRecorder struct {
	mock *MockTracker
}

// NewMockTracker creates a new mock instance.
func NewMockTracker(ctrl *gomock.Controller) *MockTracker {
	mock := &MockTracker{ctrl: ctrl}
	mock.recorder = &MockTrackerMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockTracker) EXPECT() *MockTrackerMockRecorder {
	return m.recorder
}

// TimeUntilUsage mocks base method.
func (m *MockTracker) TimeUntilUsage(nodeID ids.NodeID, now time.Time, value float64) time.Duration {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "TimeUntilUsage", nodeID, now, value)
	ret0, _ := ret[0].(time.Duration)
	return ret0
}

// TimeUntilUsage indicates an expected call of TimeUntilUsage.
func (mr *MockTrackerMockRecorder) TimeUntilUsage(nodeID, now, value interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "TimeUntilUsage", reflect.TypeOf((*MockTracker)(nil).TimeUntilUsage), nodeID, now, value)
}

// TotalUsage mocks base method.
func (m *MockTracker) TotalUsage() float64 {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "TotalUsage")
	ret0, _ := ret[0].(float64)
	return ret0
}

// TotalUsage indicates an expected call of TotalUsage.
func (mr *MockTrackerMockRecorder) TotalUsage() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "TotalUsage", reflect.TypeOf((*MockTracker)(nil).TotalUsage))
}

// Usage mocks base method.
func (m *MockTracker) Usage(nodeID ids.NodeID, now time.Time) float64 {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "Usage", nodeID, now)
	ret0, _ := ret[0].(float64)
	return ret0
}

// Usage indicates an expected call of Usage.
func (mr *MockTrackerMockRecorder) Usage(nodeID, now interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Usage", reflect.TypeOf((*MockTracker)(nil).Usage), nodeID, now)
}

// MockResourceTracker is a mock of ResourceTracker interface.
type MockResourceTracker struct {
	ctrl     *gomock.Controller
	recorder *MockResourceTrackerMockRecorder
}

// MockResourceTrackerMockRecorder is the mock recorder for MockResourceTracker.
type MockResourceTrackerMockRecorder struct {
	mock *MockResourceTracker
}

// NewMockResourceTracker creates a new mock instance.
func NewMockResourceTracker(ctrl *gomock.Controller) *MockResourceTracker {
	mock := &MockResourceTracker{ctrl: ctrl}
	mock.recorder = &MockResourceTrackerMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockResourceTracker) EXPECT() *MockResourceTrackerMockRecorder {
	return m.recorder
}

// CPUTracker mocks base method.
func (m *MockResourceTracker) CPUTracker() Tracker {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "CPUTracker")
	ret0, _ := ret[0].(Tracker)
	return ret0
}

// CPUTracker indicates an expected call of CPUTracker.
func (mr *MockResourceTrackerMockRecorder) CPUTracker() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "CPUTracker", reflect.TypeOf((*MockResourceTracker)(nil).CPUTracker))
}

// DiskTracker mocks base method.
func (m *MockResourceTracker) DiskTracker() Tracker {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "DiskTracker")
	ret0, _ := ret[0].(Tracker)
	return ret0
}

// DiskTracker indicates an expected call of DiskTracker.
func (mr *MockResourceTrackerMockRecorder) DiskTracker() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "DiskTracker", reflect.TypeOf((*MockResourceTracker)(nil).DiskTracker))
}

// StartProcessing mocks base method.
func (m *MockResourceTracker) StartProcessing(arg0 ids.NodeID, arg1 time.Time) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "StartProcessing", arg0, arg1)
}

// StartProcessing indicates an expected call of StartProcessing.
func (mr *MockResourceTrackerMockRecorder) StartProcessing(arg0, arg1 interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "StartProcessing", reflect.TypeOf((*MockResourceTracker)(nil).StartProcessing), arg0, arg1)
}

// StopProcessing mocks base method.
func (m *MockResourceTracker) StopProcessing(arg0 ids.NodeID, arg1 time.Time) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "StopProcessing", arg0, arg1)
}

// StopProcessing indicates an expected call of StopProcessing.
func (mr *MockResourceTrackerMockRecorder) StopProcessing(arg0, arg1 interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "StopProcessing", reflect.TypeOf((*MockResourceTracker)(nil).StopProcessing), arg0, arg1)
}

```

avalanchego/snow/networking/tracker/mock_targeter.go:
```
// Code generated by MockGen. DO NOT EDIT.
// Source: snow/networking/tracker/targeter.go

// Package tracker is a generated GoMock package.
package tracker

import (
	reflect "reflect"

	ids "github.com/ava-labs/avalanchego/ids"
	gomock "github.com/golang/mock/gomock"
)

// MockTargeter is a mock of Targeter interface.
type MockTargeter struct {
	ctrl     *gomock.Controller
	recorder *MockTargeterMockRecorder
}

// MockTargeterMockRecorder is the mock recorder for MockTargeter.
type MockTargeterMockRecorder struct {
	mock *MockTargeter
}

// NewMockTargeter creates a new mock instance.
func NewMockTargeter(ctrl *gomock.Controller) *MockTargeter {
	mock := &MockTargeter{ctrl: ctrl}
	mock.recorder = &MockTargeterMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockTargeter) EXPECT() *MockTargeterMockRecorder {
	return m.recorder
}

// TargetUsage mocks base method.
func (m *MockTargeter) TargetUsage(nodeID ids.NodeID) float64 {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "TargetUsage", nodeID)
	ret0, _ := ret[0].(float64)
	return ret0
}

// TargetUsage indicates an expected call of TargetUsage.
func (mr *MockTargeterMockRecorder) TargetUsage(nodeID interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "TargetUsage", reflect.TypeOf((*MockTargeter)(nil).TargetUsage), nodeID)
}

```

avalanchego/snow/networking/tracker/resource_tracker.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package tracker

import (
	"fmt"
	"sync"
	"time"

	"github.com/prometheus/client_golang/prometheus"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/utils/linkedhashmap"
	"github.com/ava-labs/avalanchego/utils/math/meter"
	"github.com/ava-labs/avalanchego/utils/resource"
	"github.com/ava-labs/avalanchego/utils/wrappers"
)

const epsilon = 1e-9

var _ ResourceTracker = &resourceTracker{}

type Tracker interface {
	// Returns the current usage for the given node.
	Usage(nodeID ids.NodeID, now time.Time) float64
	// Returns the current usage by all nodes.
	TotalUsage() float64
	// Returns the duration between [now] and when the usage of [nodeID] reaches
	// [value], assuming that the node uses no more resources.
	// If the node's usage isn't known, or is already <= [value], returns the
	// zero duration.
	TimeUntilUsage(nodeID ids.NodeID, now time.Time, value float64) time.Duration
}

type DiskTracker interface {
	Tracker
	AvailableDiskBytes() uint64
}

// ResourceTracker is an interface for tracking peers' usage of resources
type ResourceTracker interface {
	CPUTracker() Tracker
	DiskTracker() DiskTracker
	// Registers that the given node started processing at the given time.
	StartProcessing(ids.NodeID, time.Time)
	// Registers that the given node stopped processing at the given time.
	StopProcessing(ids.NodeID, time.Time)
}

type cpuResourceTracker struct {
	t *resourceTracker
}

func (t *cpuResourceTracker) Usage(nodeID ids.NodeID, now time.Time) float64 {
	rt := t.t
	rt.lock.Lock()
	defer rt.lock.Unlock()

	realCPUUsage := rt.resources.CPUUsage()
	rt.metrics.cpuMetric.Set(realCPUUsage)

	measuredProcessingTime := rt.processingMeter.Read(now)
	rt.metrics.processingTimeMetric.Set(measuredProcessingTime)

	if measuredProcessingTime == 0 {
		return 0
	}

	m, exists := rt.meters.Get(nodeID)
	if !exists {
		return 0
	}

	portionUsageByNode := m.Read(now) / measuredProcessingTime
	return realCPUUsage * portionUsageByNode
}

func (t *cpuResourceTracker) TotalUsage() float64 {
	return t.t.resources.CPUUsage()
}

func (t *cpuResourceTracker) TimeUntilUsage(nodeID ids.NodeID, now time.Time, value float64) time.Duration {
	rt := t.t
	rt.lock.Lock()
	defer rt.lock.Unlock()

	rt.prune(now)

	m, exists := rt.meters.Get(nodeID)
	if !exists {
		return 0
	}

	measuredProcessingTime := rt.processingMeter.Read(now)
	rt.metrics.processingTimeMetric.Set(measuredProcessingTime)

	if measuredProcessingTime == 0 {
		return 0
	}

	realCPUUsage := rt.resources.CPUUsage()
	rt.metrics.cpuMetric.Set(realCPUUsage)

	if realCPUUsage == 0 {
		return 0
	}

	scale := realCPUUsage / measuredProcessingTime
	return m.TimeUntil(now, value/scale)
}

type diskResourceTracker struct {
	t *resourceTracker
}

func (t *diskResourceTracker) Usage(nodeID ids.NodeID, now time.Time) float64 {
	rt := t.t
	rt.lock.Lock()
	defer rt.lock.Unlock()

	// [realWriteUsage] is only used for metrics.
	realReadUsage, realWriteUsage := rt.resources.DiskUsage()
	rt.metrics.diskReadsMetric.Set(realReadUsage)
	rt.metrics.diskWritesMetric.Set(realWriteUsage)

	measuredProcessingTime := rt.processingMeter.Read(now)
	rt.metrics.processingTimeMetric.Set(measuredProcessingTime)

	if measuredProcessingTime == 0 {
		return 0
	}

	m, exists := rt.meters.Get(nodeID)
	if !exists {
		return 0
	}

	portionUsageByNode := m.Read(now) / measuredProcessingTime
	return realReadUsage * portionUsageByNode
}

func (t *diskResourceTracker) AvailableDiskBytes() uint64 {
	rt := t.t
	rt.lock.Lock()
	defer rt.lock.Unlock()

	bytesAvailable := rt.resources.AvailableDiskBytes()
	rt.metrics.diskSpaceAvailable.Set(float64(bytesAvailable))
	return bytesAvailable
}

func (t *diskResourceTracker) TotalUsage() float64 {
	realReadUsage, _ := t.t.resources.DiskUsage()
	return realReadUsage
}

func (t *diskResourceTracker) TimeUntilUsage(nodeID ids.NodeID, now time.Time, value float64) time.Duration {
	rt := t.t
	rt.lock.Lock()
	defer rt.lock.Unlock()

	rt.prune(now)

	m, exists := rt.meters.Get(nodeID)
	if !exists {
		return 0
	}

	measuredProcessingTime := rt.processingMeter.Read(now)
	rt.metrics.processingTimeMetric.Set(measuredProcessingTime)

	if measuredProcessingTime == 0 {
		return 0
	}

	// [realWriteUsage] is only used for metrics.
	realReadUsage, realWriteUsage := rt.resources.DiskUsage()
	rt.metrics.diskReadsMetric.Set(realReadUsage)
	rt.metrics.diskWritesMetric.Set(realWriteUsage)

	if realReadUsage == 0 {
		return 0
	}

	scale := realReadUsage / measuredProcessingTime
	return m.TimeUntil(now, value/scale)
}

type resourceTracker struct {
	lock sync.RWMutex

	resources resource.User
	factory   meter.Factory
	// Tracks total number of current processing requests by all nodes.
	processingMeter meter.Meter
	halflife        time.Duration
	// Each element is a meter that tracks the number of current processing
	// requests by a node. [meters] is ordered by the last time that a meter was
	// utilized. This doesn't necessarily result in the meters being sorted
	// based on their usage. However, in practice the nodes that are not being
	// utilized will move towards the oldest elements where they can be deleted.
	meters  linkedhashmap.LinkedHashmap[ids.NodeID, meter.Meter]
	metrics *trackerMetrics
}

func NewResourceTracker(
	reg prometheus.Registerer,
	resources resource.User,
	factory meter.Factory,
	halflife time.Duration,
) (ResourceTracker, error) {
	t := &resourceTracker{
		factory:         factory,
		resources:       resources,
		processingMeter: factory.New(halflife),
		halflife:        halflife,
		meters:          linkedhashmap.New[ids.NodeID, meter.Meter](),
	}
	var err error
	t.metrics, err = newCPUTrackerMetrics("resource_tracker", reg)
	if err != nil {
		return nil, fmt.Errorf("initializing resourceTracker metrics errored with: %w", err)
	}
	return t, nil
}

func (rt *resourceTracker) CPUTracker() Tracker {
	return &cpuResourceTracker{t: rt}
}

func (rt *resourceTracker) DiskTracker() DiskTracker {
	return &diskResourceTracker{t: rt}
}

func (rt *resourceTracker) StartProcessing(nodeID ids.NodeID, now time.Time) {
	rt.lock.Lock()
	defer rt.lock.Unlock()

	meter := rt.getMeter(nodeID)
	meter.Inc(now, 1)
	rt.processingMeter.Inc(now, 1)
}

func (rt *resourceTracker) StopProcessing(nodeID ids.NodeID, now time.Time) {
	rt.lock.Lock()
	defer rt.lock.Unlock()

	meter := rt.getMeter(nodeID)
	meter.Dec(now, 1)
	rt.processingMeter.Dec(now, 1)
}

// getMeter returns the meter used to measure CPU time spent processing
// messages from [nodeID].
// assumes [rt.lock] is held.
func (rt *resourceTracker) getMeter(nodeID ids.NodeID) meter.Meter {
	m, exists := rt.meters.Get(nodeID)
	if exists {
		return m
	}

	newMeter := rt.factory.New(rt.halflife)
	rt.meters.Put(nodeID, newMeter)
	return newMeter
}

// prune attempts to remove meters that currently show a value less than
// [epsilon].
//
// Because [rt.meters] isn't guaranteed to be sorted by their values, this
// doesn't guarantee that all meters showing less than [epsilon] are removed.
func (rt *resourceTracker) prune(now time.Time) {
	for {
		oldest, meter, exists := rt.meters.Oldest()
		if !exists {
			return
		}

		if meter.Read(now) > epsilon {
			return
		}

		rt.meters.Delete(oldest)
	}
}

type trackerMetrics struct {
	processingTimeMetric prometheus.Gauge
	cpuMetric            prometheus.Gauge
	diskReadsMetric      prometheus.Gauge
	diskWritesMetric     prometheus.Gauge
	diskSpaceAvailable   prometheus.Gauge
}

func newCPUTrackerMetrics(namespace string, reg prometheus.Registerer) (*trackerMetrics, error) {
	m := &trackerMetrics{
		processingTimeMetric: prometheus.NewGauge(prometheus.GaugeOpts{
			Namespace: namespace,
			Name:      "processing_time",
			Help:      "Tracked processing time over all nodes. Value expected to be in [0, number of CPU cores], but can go higher due to IO bound processes and thread multiplexing",
		}),
		cpuMetric: prometheus.NewGauge(prometheus.GaugeOpts{
			Namespace: namespace,
			Name:      "cpu_usage",
			Help:      "CPU usage tracked by the resource manager. Value should be in [0, number of CPU cores]",
		}),
		diskReadsMetric: prometheus.NewGauge(prometheus.GaugeOpts{
			Namespace: namespace,
			Name:      "disk_reads",
			Help:      "Disk reads (bytes/sec) tracked by the resource manager",
		}),
		diskWritesMetric: prometheus.NewGauge(prometheus.GaugeOpts{
			Namespace: namespace,
			Name:      "disk_writes",
			Help:      "Disk writes (bytes/sec) tracked by the resource manager",
		}),
		diskSpaceAvailable: prometheus.NewGauge(prometheus.GaugeOpts{
			Namespace: namespace,
			Name:      "disk_available_space",
			Help:      "Available space remaining (bytes) on the database volume",
		}),
	}
	errs := wrappers.Errs{}
	errs.Add(
		reg.Register(m.processingTimeMetric),
		reg.Register(m.cpuMetric),
		reg.Register(m.diskReadsMetric),
		reg.Register(m.diskWritesMetric),
		reg.Register(m.diskSpaceAvailable),
	)
	return m, errs.Err
}

```

avalanchego/snow/networking/tracker/resource_tracker_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package tracker

import (
	"testing"
	"time"

	"github.com/golang/mock/gomock"

	"github.com/prometheus/client_golang/prometheus"

	"github.com/stretchr/testify/require"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/utils/math/meter"
	"github.com/ava-labs/avalanchego/utils/resource"
)

func TestNewCPUTracker(t *testing.T) {
	require := require.New(t)

	reg := prometheus.NewRegistry()
	halflife := 5 * time.Second
	factory := &meter.ContinuousFactory{}

	trackerIntf, err := NewResourceTracker(reg, resource.NoUsage, factory, halflife)
	require.NoError(err)
	tracker, ok := trackerIntf.(*resourceTracker)
	require.True(ok)
	require.Equal(factory, tracker.factory)
	require.NotNil(tracker.processingMeter)
	require.Equal(halflife, tracker.halflife)
	require.NotNil(tracker.meters)
	require.NotNil(tracker.metrics)
}

func TestCPUTracker(t *testing.T) {
	halflife := 5 * time.Second

	ctrl := gomock.NewController(t)
	mockUser := resource.NewMockUser(ctrl)
	mockUser.EXPECT().CPUUsage().Return(1.0).Times(3)

	tracker, err := NewResourceTracker(prometheus.NewRegistry(), mockUser, meter.ContinuousFactory{}, time.Second)
	require.NoError(t, err)

	node1 := ids.NodeID{1}
	node2 := ids.NodeID{2}

	// Note that all the durations between start and end are [halflife].
	startTime1 := time.Now()
	endTime1 := startTime1.Add(halflife)
	// Note that all CPU usage is attributed to at-large allocation.
	tracker.StartProcessing(node1, startTime1)
	tracker.StopProcessing(node1, endTime1)

	startTime2 := endTime1
	endTime2 := startTime2.Add(halflife)
	// Note that all CPU usage is attributed to at-large allocation.
	tracker.StartProcessing(node2, startTime2)
	tracker.StopProcessing(node2, endTime2)

	cpuTracker := tracker.CPUTracker()

	node1Utilization := cpuTracker.Usage(node1, endTime2)
	node2Utilization := cpuTracker.Usage(node2, endTime2)
	if node1Utilization >= node2Utilization {
		t.Fatalf("Utilization should have been higher for the more recent spender")
	}

	cumulative := cpuTracker.TotalUsage()
	sum := node1Utilization + node2Utilization
	if cumulative != sum {
		t.Fatalf("Cumulative utilization: %f should have been equal to the sum of the spenders: %f", cumulative, sum)
	}

	mockUser.EXPECT().CPUUsage().Return(.5).Times(3)

	startTime3 := endTime2
	endTime3 := startTime3.Add(halflife)
	newNode1Utilization := cpuTracker.Usage(node1, endTime3)
	if newNode1Utilization >= node1Utilization {
		t.Fatalf("node CPU utilization should decrease over time")
	}
	newCumulative := cpuTracker.TotalUsage()
	if newCumulative >= cumulative {
		t.Fatal("at-large CPU utilization should decrease over time ")
	}

	startTime4 := endTime3
	endTime4 := startTime4.Add(halflife)
	// Note that only half of CPU usage is attributed to at-large allocation.
	tracker.StartProcessing(node1, startTime4)
	tracker.StopProcessing(node1, endTime4)

	cumulative = cpuTracker.TotalUsage()
	sum = node1Utilization + node2Utilization
	if cumulative >= sum {
		t.Fatal("Sum of CPU usage should exceed cumulative at-large utilization")
	}
}

func TestCPUTrackerTimeUntilCPUUtilization(t *testing.T) {
	halflife := 5 * time.Second
	tracker, err := NewResourceTracker(prometheus.NewRegistry(), resource.NoUsage, meter.ContinuousFactory{}, halflife)
	require.NoError(t, err)
	now := time.Now()
	nodeID := ids.GenerateTestNodeID()
	// Start the meter
	tracker.StartProcessing(nodeID, now)
	// One halflife passes; stop the meter
	now = now.Add(halflife)
	tracker.StopProcessing(nodeID, now)
	cpuTracker := tracker.CPUTracker()
	// Read the current value
	currentVal := cpuTracker.Usage(nodeID, now)
	// Suppose we want to wait for the value to be
	// a third of its current value
	desiredVal := currentVal / 3
	// See when that should happen
	timeUntilDesiredVal := cpuTracker.TimeUntilUsage(nodeID, now, desiredVal)
	// Get the actual value at that time
	now = now.Add(timeUntilDesiredVal)
	actualVal := cpuTracker.Usage(nodeID, now)
	// Make sure the actual/expected are close
	require.InDelta(t, desiredVal, actualVal, .00001)
	// Make sure TimeUntilUsage returns the zero duration if
	// the value provided >= the current value
	require.Zero(t, cpuTracker.TimeUntilUsage(nodeID, now, actualVal))
	require.Zero(t, cpuTracker.TimeUntilUsage(nodeID, now, actualVal+.1))
	// Make sure it returns the zero duration if the node isn't known
	require.Zero(t, cpuTracker.TimeUntilUsage(ids.GenerateTestNodeID(), now, 0.0001))
}

```

avalanchego/snow/networking/tracker/targeter.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package tracker

import (
	"math"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/validators"
)

var _ Targeter = &targeter{}

type Targeter interface {
	// Returns the target usage of the given node.
	TargetUsage(nodeID ids.NodeID) float64
}

type TargeterConfig struct {
	// VdrAlloc is the amount of the resource to split over validators, weighted
	// by stake.
	VdrAlloc float64 `json:"vdrAlloc"`

	// MaxNonVdrUsage is the amount of the resource which, if utilized, will
	// result in allocations being based only on the stake weighted allocation.
	MaxNonVdrUsage float64 `json:"maxNonVdrUsage"`

	// MaxNonVdrNodeUsage is the amount of the resource to allocate to a node
	// before adding the stake weighted allocation.
	MaxNonVdrNodeUsage float64 `json:"maxNonVdrNodeUsage"`
}

func NewTargeter(
	config *TargeterConfig,
	vdrs validators.Set,
	tracker Tracker,
) Targeter {
	return &targeter{
		vdrs:               vdrs,
		tracker:            tracker,
		vdrAlloc:           config.VdrAlloc,
		maxNonVdrUsage:     config.MaxNonVdrUsage,
		maxNonVdrNodeUsage: config.MaxNonVdrNodeUsage,
	}
}

type targeter struct {
	vdrs               validators.Set
	tracker            Tracker
	vdrAlloc           float64
	maxNonVdrUsage     float64
	maxNonVdrNodeUsage float64
}

func (t *targeter) TargetUsage(nodeID ids.NodeID) float64 {
	// This node's at-large allocation is min([remaining at large], [max at large for a given peer])
	usage := t.tracker.TotalUsage()
	baseAlloc := math.Max(0, t.maxNonVdrUsage-usage)
	baseAlloc = math.Min(baseAlloc, t.maxNonVdrNodeUsage)

	// This node gets a stake-weighted portion of the validator allocation.
	weight, _ := t.vdrs.GetWeight(nodeID)
	vdrAlloc := t.vdrAlloc * float64(weight) / float64(t.vdrs.Weight())
	return vdrAlloc + baseAlloc
}

```

avalanchego/snow/networking/tracker/targeter_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package tracker

import (
	"testing"

	"github.com/golang/mock/gomock"

	"github.com/stretchr/testify/require"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/validators"
)

// Assert fields are set correctly.
func TestNewTargeter(t *testing.T) {
	require := require.New(t)
	ctrl := gomock.NewController(t)
	defer ctrl.Finish()

	config := &TargeterConfig{
		VdrAlloc:           10,
		MaxNonVdrUsage:     10,
		MaxNonVdrNodeUsage: 10,
	}
	vdrs := validators.NewSet()
	tracker := NewMockTracker(ctrl)

	targeterIntf := NewTargeter(
		config,
		vdrs,
		tracker,
	)
	targeter, ok := targeterIntf.(*targeter)
	require.True(ok)
	require.Equal(vdrs, targeter.vdrs)
	require.Equal(tracker, targeter.tracker)
	require.Equal(config.MaxNonVdrUsage, targeter.maxNonVdrUsage)
	require.Equal(config.MaxNonVdrNodeUsage, targeter.maxNonVdrNodeUsage)
}

func TestTarget(t *testing.T) {
	ctrl := gomock.NewController(t)
	defer ctrl.Finish()

	vdr := ids.NodeID{1}
	vdrWeight := uint64(1)
	totalVdrWeight := uint64(10)
	nonVdr := ids.NodeID{2}
	vdrs := validators.NewSet()
	if err := vdrs.AddWeight(vdr, 1); err != nil {
		t.Fatal(err)
	}
	if err := vdrs.AddWeight(ids.GenerateTestNodeID(), totalVdrWeight-vdrWeight); err != nil {
		t.Fatal(err)
	}

	tracker := NewMockTracker(ctrl)
	config := &TargeterConfig{
		VdrAlloc:           20,
		MaxNonVdrUsage:     10,
		MaxNonVdrNodeUsage: 5,
	}

	targeter := NewTargeter(
		config,
		vdrs,
		tracker,
	)

	type test struct {
		name           string
		setup          func()
		nodeID         ids.NodeID
		expectedTarget float64
	}
	tests := []test{
		{
			name: "Vdr alloc and at-large alloc",
			setup: func() {
				// At large utilization is less than max
				tracker.EXPECT().TotalUsage().Return(config.MaxNonVdrUsage - 1).Times(1)
			},
			nodeID:         vdr,
			expectedTarget: 2 + 1, // 20 * (1/10) + min(max(0,10-9),5)
		},
		{
			name: "no vdr alloc and at-large alloc",
			setup: func() {
				// At large utilization is less than max
				tracker.EXPECT().TotalUsage().Return(config.MaxNonVdrUsage - 1).Times(1)
			},
			nodeID:         nonVdr,
			expectedTarget: 0 + 1, // 0 * (1/10) + min(max(0,10-9), 5)
		},
		{
			name: "at-large alloc maxed",
			setup: func() {
				tracker.EXPECT().TotalUsage().Return(float64(0)).Times(1)
			},
			nodeID:         nonVdr,
			expectedTarget: 0 + 5, // 0 * (1/10) + min(max(0,10-0), 5)
		},
		{
			name: "at-large alloc completely used",
			setup: func() {
				tracker.EXPECT().TotalUsage().Return(config.MaxNonVdrUsage).Times(1)
			},
			nodeID:         nonVdr,
			expectedTarget: 0 + 0, // 0 * (1/10) + min(max(0,10-10), 5)
		},
		{
			name: "at-large alloc exceeded used",
			setup: func() {
				tracker.EXPECT().TotalUsage().Return(config.MaxNonVdrUsage + 1).Times(1)
			},
			nodeID:         nonVdr,
			expectedTarget: 0 + 0, // 0 * (1/10) + min(max(0,10-11), 5)
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			tt.setup()
			target := targeter.TargetUsage(tt.nodeID)
			require.Equal(t, tt.expectedTarget, target)
		})
	}
}

```

avalanchego/snow/networking/worker/mock_pool.go:
```
// Code generated by mockery v2.9.4. DO NOT EDIT.

package worker

import (
	mock "github.com/stretchr/testify/mock"
)

// Pool is an autogenerated mock type for the Pool type
type MockPool struct {
	mock.Mock
}

// Send provides a mock function with given fields: _a0
func (_m *MockPool) Send(_a0 Request) {
	_m.Called(_a0)
}

// Shutdown provides a mock function with given fields:
func (_m *MockPool) Shutdown() {
	_m.Called()
}

```

avalanchego/snow/networking/worker/pool.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package worker

import (
	"sync"
)

var _ Pool = &pool{}

type Request func()

type Pool interface {
	// Send the request to the worker pool.
	//
	// Send should never be called after [Shutdown] is called.
	Send(Request)

	// Shutdown the worker pool.
	//
	// This method will block until all workers have finished their current
	// tasks.
	//
	// It is safe to call shutdown multiple times.
	Shutdown()
}

type pool struct {
	requests chan Request

	shutdownOnce sync.Once
	shutdownWG   sync.WaitGroup
}

func NewPool(size int) Pool {
	p := &pool{
		requests: make(chan Request),
	}
	p.shutdownWG.Add(size)
	for w := 0; w < size; w++ {
		go p.runWorker()
	}
	return p
}

func (p *pool) runWorker() {
	defer p.shutdownWG.Done()

	for request := range p.requests {
		request()
	}
}

func (p *pool) Shutdown() {
	p.shutdownOnce.Do(func() {
		close(p.requests)
	})
	p.shutdownWG.Wait()
}

func (p *pool) Send(msg Request) {
	p.requests <- msg
}

```

avalanchego/snow/state.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package snow

import "errors"

const (
	Initializing = iota
	StateSyncing
	Bootstrapping
	NormalOp
)

var ErrUnknownState = errors.New("unknown state")

type State uint8

func (st State) String() string {
	switch st {
	case Initializing:
		return "Initializing state"
	case StateSyncing:
		return "State syncing state"
	case Bootstrapping:
		return "Bootstrapping state"
	case NormalOp:
		return "Normal operations state"
	default:
		return "Unknown state"
	}
}

```

avalanchego/snow/uptime/locked_calculator.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package uptime

import (
	"errors"
	"sync"
	"time"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/utils"
)

var (
	errNotReady = errors.New("should not be called")

	_ LockedCalculator = &lockedCalculator{}
)

type LockedCalculator interface {
	Calculator

	SetCalculator(isBootstrapped *utils.AtomicBool, lock sync.Locker, newC Calculator)
}

type lockedCalculator struct {
	lock           sync.RWMutex
	isBootstrapped *utils.AtomicBool
	calculatorLock sync.Locker
	c              Calculator
}

func NewLockedCalculator() LockedCalculator {
	return &lockedCalculator{}
}

func (c *lockedCalculator) CalculateUptime(nodeID ids.NodeID) (time.Duration, time.Time, error) {
	c.lock.RLock()
	defer c.lock.RUnlock()

	if c.isBootstrapped == nil || !c.isBootstrapped.GetValue() {
		return 0, time.Time{}, errNotReady
	}

	c.calculatorLock.Lock()
	defer c.calculatorLock.Unlock()

	return c.c.CalculateUptime(nodeID)
}

func (c *lockedCalculator) CalculateUptimePercent(nodeID ids.NodeID) (float64, error) {
	c.lock.RLock()
	defer c.lock.RUnlock()

	if c.isBootstrapped == nil || !c.isBootstrapped.GetValue() {
		return 0, errNotReady
	}

	c.calculatorLock.Lock()
	defer c.calculatorLock.Unlock()

	return c.c.CalculateUptimePercent(nodeID)
}

func (c *lockedCalculator) CalculateUptimePercentFrom(nodeID ids.NodeID, startTime time.Time) (float64, error) {
	c.lock.RLock()
	defer c.lock.RUnlock()

	if c.isBootstrapped == nil || !c.isBootstrapped.GetValue() {
		return 0, errNotReady
	}

	c.calculatorLock.Lock()
	defer c.calculatorLock.Unlock()

	return c.c.CalculateUptimePercentFrom(nodeID, startTime)
}

func (c *lockedCalculator) SetCalculator(isBootstrapped *utils.AtomicBool, lock sync.Locker, newC Calculator) {
	c.lock.Lock()
	defer c.lock.Unlock()

	c.isBootstrapped = isBootstrapped
	c.calculatorLock = lock
	c.c = newC
}

```

avalanchego/snow/uptime/locked_calculator_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package uptime

import (
	"errors"
	"sync"
	"testing"
	"time"

	"github.com/stretchr/testify/mock"
	"github.com/stretchr/testify/require"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/snow/uptime/mocks"
	"github.com/ava-labs/avalanchego/utils"
)

func TestLockedCalculator(t *testing.T) {
	require := require.New(t)
	lc := NewLockedCalculator()
	require.NotNil(t)

	// Should still error because ctx is nil
	nodeID := ids.GenerateTestNodeID()
	_, _, err := lc.CalculateUptime(nodeID)
	require.EqualValues(errNotReady, err)
	_, err = lc.CalculateUptimePercent(nodeID)
	require.EqualValues(errNotReady, err)
	_, err = lc.CalculateUptimePercentFrom(nodeID, time.Now())
	require.EqualValues(errNotReady, err)

	var isBootstrapped utils.AtomicBool
	mockCalc := &mocks.Calculator{}

	// Should still error because ctx is not bootstrapped
	lc.SetCalculator(&isBootstrapped, &sync.Mutex{}, mockCalc)
	_, _, err = lc.CalculateUptime(nodeID)
	require.EqualValues(errNotReady, err)
	_, err = lc.CalculateUptimePercent(nodeID)
	require.EqualValues(errNotReady, err)
	_, err = lc.CalculateUptimePercentFrom(nodeID, time.Now())
	require.EqualValues(errNotReady, err)

	isBootstrapped.SetValue(true)

	// Should return the value from the mocked inner calculator
	mockErr := errors.New("mock error")
	mockCalc.On("CalculateUptime", mock.Anything).Return(time.Duration(0), time.Time{}, mockErr)
	_, _, err = lc.CalculateUptime(nodeID)
	require.EqualValues(mockErr, err)
	mockCalc.On("CalculateUptimePercent", mock.Anything).Return(float64(0), mockErr)
	_, err = lc.CalculateUptimePercent(nodeID)
	require.EqualValues(mockErr, err)
	mockCalc.On("CalculateUptimePercentFrom", mock.Anything, mock.Anything).Return(float64(0), mockErr)
	_, err = lc.CalculateUptimePercentFrom(nodeID, time.Now())
	require.EqualValues(mockErr, err)
}

```

avalanchego/snow/uptime/manager.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package uptime

import (
	"time"

	"github.com/ava-labs/avalanchego/database"
	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/utils/timer/mockable"
)

var _ TestManager = &manager{}

type Manager interface {
	Tracker
	Calculator
}

type Tracker interface {
	// Should only be called once
	StartTracking(nodeIDs []ids.NodeID) error

	// Should only be called once
	Shutdown(nodeIDs []ids.NodeID) error

	Connect(nodeID ids.NodeID) error
	IsConnected(nodeID ids.NodeID) bool
	Disconnect(nodeID ids.NodeID) error
}

type Calculator interface {
	CalculateUptime(nodeID ids.NodeID) (time.Duration, time.Time, error)
	CalculateUptimePercent(nodeID ids.NodeID) (float64, error)
	// CalculateUptimePercentFrom expects [startTime] to be truncated (floored) to the nearest second
	CalculateUptimePercentFrom(nodeID ids.NodeID, startTime time.Time) (float64, error)
}

type TestManager interface {
	Manager
	SetTime(time.Time)
}

type manager struct {
	// Used to get time. Useful for faking time during tests.
	clock mockable.Clock

	state           State
	connections     map[ids.NodeID]time.Time
	startedTracking bool
}

func NewManager(state State) Manager {
	return &manager{
		state:       state,
		connections: make(map[ids.NodeID]time.Time),
	}
}

func (m *manager) StartTracking(nodeIDs []ids.NodeID) error {
	now := m.clock.UnixTime()
	for _, nodeID := range nodeIDs {
		upDuration, lastUpdated, err := m.state.GetUptime(nodeID)
		if err != nil {
			return err
		}

		// If we are in a weird reality where time has moved backwards, then we
		// shouldn't modify the validator's uptime.
		if now.Before(lastUpdated) {
			continue
		}

		durationOffline := now.Sub(lastUpdated)
		newUpDuration := upDuration + durationOffline
		if err := m.state.SetUptime(nodeID, newUpDuration, now); err != nil {
			return err
		}
	}
	m.startedTracking = true
	return nil
}

func (m *manager) Shutdown(nodeIDs []ids.NodeID) error {
	now := m.clock.UnixTime()
	for _, nodeID := range nodeIDs {
		if _, connected := m.connections[nodeID]; connected {
			if err := m.Disconnect(nodeID); err != nil {
				return err
			}
			continue
		}

		upDuration, lastUpdated, err := m.state.GetUptime(nodeID)
		if err != nil {
			return err
		}

		// If we are in a weird reality where time has moved backwards, then we
		// shouldn't modify the validator's uptime.
		if now.Before(lastUpdated) {
			continue
		}

		if err := m.state.SetUptime(nodeID, upDuration, now); err != nil {
			return err
		}
	}
	return nil
}

func (m *manager) Connect(nodeID ids.NodeID) error {
	m.connections[nodeID] = m.clock.UnixTime()
	return nil
}

func (m *manager) IsConnected(nodeID ids.NodeID) bool {
	_, connected := m.connections[nodeID]
	return connected
}

func (m *manager) Disconnect(nodeID ids.NodeID) error {
	if !m.startedTracking {
		delete(m.connections, nodeID)
		return nil
	}

	newDuration, newLastUpdated, err := m.CalculateUptime(nodeID)
	delete(m.connections, nodeID)
	if err == database.ErrNotFound {
		// If a non-validator disconnects, we don't care
		return nil
	}
	if err != nil {
		return err
	}
	return m.state.SetUptime(nodeID, newDuration, newLastUpdated)
}

func (m *manager) CalculateUptime(nodeID ids.NodeID) (time.Duration, time.Time, error) {
	upDuration, lastUpdated, err := m.state.GetUptime(nodeID)
	if err != nil {
		return 0, time.Time{}, err
	}

	now := m.clock.UnixTime()
	// If we are in a weird reality where time has gone backwards, make sure
	// that we don't double count or delete any uptime.
	if now.Before(lastUpdated) {
		return upDuration, lastUpdated, nil
	}

	timeConnected, isConnected := m.connections[nodeID]
	if !isConnected {
		return upDuration, now, nil
	}

	// The time the peer connected needs to be adjusted to ensure no time period
	// is double counted.
	if timeConnected.Before(lastUpdated) {
		timeConnected = lastUpdated
	}

	// If we are in a weird reality where time has gone backwards, make sure
	// that we don't double count or delete any uptime.
	if now.Before(timeConnected) {
		return upDuration, now, nil
	}

	// Increase the uptimes by the amount of time this node has been running
	// since the last time it's uptime was written to disk.
	durationConnected := now.Sub(timeConnected)
	newUpDuration := upDuration + durationConnected
	return newUpDuration, now, nil
}

func (m *manager) CalculateUptimePercent(nodeID ids.NodeID) (float64, error) {
	startTime, err := m.state.GetStartTime(nodeID)
	if err != nil {
		return 0, err
	}
	return m.CalculateUptimePercentFrom(nodeID, startTime)
}

func (m *manager) CalculateUptimePercentFrom(nodeID ids.NodeID, startTime time.Time) (float64, error) {
	upDuration, now, err := m.CalculateUptime(nodeID)
	if err != nil {
		return 0, err
	}
	bestPossibleUpDuration := now.Sub(startTime)
	if bestPossibleUpDuration == 0 {
		return 1, nil
	}
	uptime := float64(upDuration) / float64(bestPossibleUpDuration)
	return uptime, nil
}

func (m *manager) SetTime(newTime time.Time) {
	m.clock.Set(newTime)
}

```

avalanchego/snow/uptime/manager_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package uptime

import (
	"errors"
	"testing"
	"time"

	"github.com/stretchr/testify/require"

	"github.com/ava-labs/avalanchego/ids"
)

func TestStartTracking(t *testing.T) {
	require := require.New(t)

	nodeID0 := ids.GenerateTestNodeID()
	startTime := time.Now()

	s := NewTestState()
	s.AddNode(nodeID0, startTime)

	up := NewManager(s).(*manager)

	currentTime := startTime.Add(time.Second)
	up.clock.Set(currentTime)

	err := up.StartTracking([]ids.NodeID{nodeID0})
	require.NoError(err)

	duration, lastUpdated, err := up.CalculateUptime(nodeID0)
	require.NoError(err)
	require.Equal(time.Second, duration)
	require.Equal(up.clock.UnixTime(), lastUpdated)
}

func TestStartTrackingDBError(t *testing.T) {
	require := require.New(t)

	nodeID0 := ids.GenerateTestNodeID()
	startTime := time.Now()

	s := NewTestState()
	s.dbWriteError = errors.New("err")
	s.AddNode(nodeID0, startTime)

	up := NewManager(s).(*manager)

	currentTime := startTime.Add(time.Second)
	up.clock.Set(currentTime)

	err := up.StartTracking([]ids.NodeID{nodeID0})
	require.Error(err)
}

func TestStartTrackingNonValidator(t *testing.T) {
	require := require.New(t)

	s := NewTestState()
	up := NewManager(s).(*manager)

	nodeID0 := ids.GenerateTestNodeID()
	err := up.StartTracking([]ids.NodeID{nodeID0})
	require.Error(err)
}

func TestStartTrackingInThePast(t *testing.T) {
	require := require.New(t)

	nodeID0 := ids.GenerateTestNodeID()
	startTime := time.Now()

	s := NewTestState()
	s.AddNode(nodeID0, startTime)

	up := NewManager(s).(*manager)

	currentTime := startTime.Add(-time.Second)
	up.clock.Set(currentTime)

	err := up.StartTracking([]ids.NodeID{nodeID0})
	require.NoError(err)

	duration, lastUpdated, err := up.CalculateUptime(nodeID0)
	require.NoError(err)
	require.Equal(time.Duration(0), duration)
	require.Equal(startTime.Truncate(time.Second), lastUpdated)
}

func TestShutdownDecreasesUptime(t *testing.T) {
	require := require.New(t)

	nodeID0 := ids.GenerateTestNodeID()
	currentTime := time.Now()
	startTime := currentTime

	s := NewTestState()
	s.AddNode(nodeID0, startTime)

	up := NewManager(s).(*manager)
	up.clock.Set(currentTime)

	err := up.StartTracking([]ids.NodeID{nodeID0})
	require.NoError(err)

	currentTime = startTime.Add(time.Second)
	up.clock.Set(currentTime)

	err = up.Shutdown([]ids.NodeID{nodeID0})
	require.NoError(err)

	up = NewManager(s).(*manager)
	up.clock.Set(currentTime)

	err = up.StartTracking([]ids.NodeID{nodeID0})
	require.NoError(err)

	duration, lastUpdated, err := up.CalculateUptime(nodeID0)
	require.NoError(err)
	require.Equal(time.Duration(0), duration)
	require.Equal(up.clock.UnixTime(), lastUpdated)
}

func TestShutdownIncreasesUptime(t *testing.T) {
	require := require.New(t)

	nodeID0 := ids.GenerateTestNodeID()
	currentTime := time.Now()
	startTime := currentTime

	s := NewTestState()
	s.AddNode(nodeID0, startTime)

	up := NewManager(s).(*manager)
	up.clock.Set(currentTime)

	err := up.StartTracking([]ids.NodeID{nodeID0})
	require.NoError(err)

	err = up.Connect(nodeID0)
	require.NoError(err)

	currentTime = startTime.Add(time.Second)
	up.clock.Set(currentTime)

	err = up.Shutdown([]ids.NodeID{nodeID0})
	require.NoError(err)

	up = NewManager(s).(*manager)
	up.clock.Set(currentTime)

	err = up.StartTracking([]ids.NodeID{nodeID0})
	require.NoError(err)

	duration, lastUpdated, err := up.CalculateUptime(nodeID0)
	require.NoError(err)
	require.Equal(time.Second, duration)
	require.Equal(up.clock.UnixTime(), lastUpdated)
}

func TestShutdownDisconnectedNonValidator(t *testing.T) {
	require := require.New(t)

	nodeID0 := ids.GenerateTestNodeID()

	s := NewTestState()
	up := NewManager(s).(*manager)

	err := up.StartTracking(nil)
	require.NoError(err)

	err = up.Shutdown([]ids.NodeID{nodeID0})
	require.Error(err)
}

func TestShutdownConnectedDBError(t *testing.T) {
	require := require.New(t)

	nodeID0 := ids.GenerateTestNodeID()
	startTime := time.Now()

	s := NewTestState()
	s.AddNode(nodeID0, startTime)
	up := NewManager(s).(*manager)

	err := up.StartTracking(nil)
	require.NoError(err)

	err = up.Connect(nodeID0)
	require.NoError(err)

	s.dbReadError = errors.New("err")
	err = up.Shutdown([]ids.NodeID{nodeID0})
	require.Error(err)
}

func TestShutdownNonConnectedPast(t *testing.T) {
	require := require.New(t)

	nodeID0 := ids.GenerateTestNodeID()
	currentTime := time.Now()
	startTime := currentTime

	s := NewTestState()
	s.AddNode(nodeID0, startTime)
	up := NewManager(s).(*manager)
	up.clock.Set(currentTime)

	err := up.StartTracking([]ids.NodeID{nodeID0})
	require.NoError(err)

	currentTime = currentTime.Add(-time.Second)
	up.clock.Set(currentTime)

	err = up.Shutdown([]ids.NodeID{nodeID0})
	require.NoError(err)

	duration, lastUpdated, err := s.GetUptime(nodeID0)
	require.NoError(err)
	require.Equal(time.Duration(0), duration)
	require.Equal(startTime.Truncate(time.Second), lastUpdated)
}

func TestShutdownNonConnectedDBError(t *testing.T) {
	require := require.New(t)

	nodeID0 := ids.GenerateTestNodeID()
	currentTime := time.Now()
	startTime := currentTime

	s := NewTestState()
	s.AddNode(nodeID0, startTime)
	up := NewManager(s).(*manager)
	up.clock.Set(currentTime)

	err := up.StartTracking([]ids.NodeID{nodeID0})
	require.NoError(err)

	currentTime = currentTime.Add(time.Second)
	up.clock.Set(currentTime)

	s.dbWriteError = errors.New("err")
	err = up.Shutdown([]ids.NodeID{nodeID0})
	require.Error(err)
}

func TestConnectAndDisconnect(t *testing.T) {
	require := require.New(t)

	nodeID0 := ids.GenerateTestNodeID()
	currentTime := time.Now()
	startTime := currentTime

	s := NewTestState()
	s.AddNode(nodeID0, startTime)

	up := NewManager(s).(*manager)
	up.clock.Set(currentTime)

	connected := up.IsConnected(nodeID0)
	require.False(connected)

	err := up.StartTracking([]ids.NodeID{nodeID0})
	require.NoError(err)

	connected = up.IsConnected(nodeID0)
	require.False(connected)

	duration, lastUpdated, err := up.CalculateUptime(nodeID0)
	require.NoError(err)
	require.Equal(time.Duration(0), duration)
	require.Equal(up.clock.UnixTime(), lastUpdated)

	err = up.Connect(nodeID0)
	require.NoError(err)

	connected = up.IsConnected(nodeID0)
	require.True(connected)

	currentTime = currentTime.Add(time.Second)
	up.clock.Set(currentTime)

	duration, lastUpdated, err = up.CalculateUptime(nodeID0)
	require.NoError(err)
	require.Equal(time.Second, duration)
	require.Equal(up.clock.UnixTime(), lastUpdated)

	err = up.Disconnect(nodeID0)
	require.NoError(err)

	connected = up.IsConnected(nodeID0)
	require.False(connected)

	currentTime = currentTime.Add(time.Second)
	up.clock.Set(currentTime)

	duration, lastUpdated, err = up.CalculateUptime(nodeID0)
	require.NoError(err)
	require.Equal(time.Second, duration)
	require.Equal(up.clock.UnixTime(), lastUpdated)
}

func TestConnectAndDisconnectBeforeTracking(t *testing.T) {
	require := require.New(t)

	nodeID0 := ids.GenerateTestNodeID()
	currentTime := time.Now()
	startTime := currentTime

	s := NewTestState()
	s.AddNode(nodeID0, startTime)

	up := NewManager(s).(*manager)
	currentTime = currentTime.Add(time.Second)
	up.clock.Set(currentTime)

	err := up.Connect(nodeID0)
	require.NoError(err)

	currentTime = currentTime.Add(time.Second)
	up.clock.Set(currentTime)

	err = up.Disconnect(nodeID0)
	require.NoError(err)

	err = up.StartTracking([]ids.NodeID{nodeID0})
	require.NoError(err)

	duration, lastUpdated, err := up.CalculateUptime(nodeID0)
	require.NoError(err)
	require.Equal(2*time.Second, duration)
	require.Equal(up.clock.UnixTime(), lastUpdated)
}

func TestUnrelatedNodeDisconnect(t *testing.T) {
	require := require.New(t)

	nodeID0 := ids.GenerateTestNodeID()
	nodeID1 := ids.GenerateTestNodeID()
	currentTime := time.Now()
	startTime := currentTime

	s := NewTestState()
	s.AddNode(nodeID0, startTime)

	up := NewManager(s).(*manager)
	up.clock.Set(currentTime)

	err := up.StartTracking([]ids.NodeID{nodeID0})
	require.NoError(err)

	duration, lastUpdated, err := up.CalculateUptime(nodeID0)
	require.NoError(err)
	require.Equal(time.Duration(0), duration)
	require.Equal(up.clock.UnixTime(), lastUpdated)

	err = up.Connect(nodeID0)
	require.NoError(err)

	err = up.Connect(nodeID1)
	require.NoError(err)

	currentTime = currentTime.Add(time.Second)
	up.clock.Set(currentTime)

	duration, lastUpdated, err = up.CalculateUptime(nodeID0)
	require.NoError(err)
	require.Equal(time.Second, duration)
	require.Equal(up.clock.UnixTime(), lastUpdated)

	err = up.Disconnect(nodeID1)
	require.NoError(err)

	currentTime = currentTime.Add(time.Second)
	up.clock.Set(currentTime)

	duration, lastUpdated, err = up.CalculateUptime(nodeID0)
	require.NoError(err)
	require.Equal(2*time.Second, duration)
	require.Equal(up.clock.UnixTime(), lastUpdated)
}

func TestCalculateUptimeWhenNeverConnected(t *testing.T) {
	require := require.New(t)

	nodeID0 := ids.GenerateTestNodeID()
	startTime := time.Now()

	s := NewTestState()
	s.AddNode(nodeID0, startTime)

	up := NewManager(s).(*manager)

	currentTime := startTime.Add(time.Second)
	up.clock.Set(currentTime)

	duration, lastUpdated, err := up.CalculateUptime(nodeID0)
	require.NoError(err)
	require.Equal(time.Duration(0), duration)
	require.Equal(up.clock.UnixTime(), lastUpdated)

	uptime, err := up.CalculateUptimePercentFrom(nodeID0, startTime)
	require.NoError(err)
	require.Equal(0., uptime)
}

func TestCalculateUptimeWhenConnectedBeforeTracking(t *testing.T) {
	require := require.New(t)

	nodeID0 := ids.GenerateTestNodeID()
	currentTime := time.Now()
	startTime := currentTime

	s := NewTestState()
	s.AddNode(nodeID0, startTime)

	up := NewManager(s).(*manager)
	up.clock.Set(currentTime)

	err := up.Connect(nodeID0)
	require.NoError(err)

	currentTime = currentTime.Add(time.Second)
	up.clock.Set(currentTime)

	err = up.StartTracking([]ids.NodeID{nodeID0})
	require.NoError(err)

	currentTime = currentTime.Add(time.Second)
	up.clock.Set(currentTime)

	duration, lastUpdated, err := up.CalculateUptime(nodeID0)
	require.NoError(err)
	require.Equal(2*time.Second, duration)
	require.Equal(up.clock.UnixTime(), lastUpdated)
}

func TestCalculateUptimeWhenConnectedInFuture(t *testing.T) {
	require := require.New(t)

	nodeID0 := ids.GenerateTestNodeID()
	currentTime := time.Now()
	startTime := currentTime

	s := NewTestState()
	s.AddNode(nodeID0, startTime)

	up := NewManager(s).(*manager)
	up.clock.Set(currentTime)

	err := up.StartTracking([]ids.NodeID{nodeID0})
	require.NoError(err)

	currentTime = currentTime.Add(2 * time.Second)
	up.clock.Set(currentTime)

	err = up.Connect(nodeID0)
	require.NoError(err)

	currentTime = currentTime.Add(-time.Second)
	up.clock.Set(currentTime)

	duration, lastUpdated, err := up.CalculateUptime(nodeID0)
	require.NoError(err)
	require.Equal(time.Duration(0), duration)
	require.Equal(up.clock.UnixTime(), lastUpdated)
}

func TestCalculateUptimeNonValidator(t *testing.T) {
	require := require.New(t)

	nodeID0 := ids.GenerateTestNodeID()
	startTime := time.Now()

	s := NewTestState()

	up := NewManager(s).(*manager)

	_, err := up.CalculateUptimePercentFrom(nodeID0, startTime)
	require.Error(err)
}

func TestCalculateUptimePercentageDivBy0(t *testing.T) {
	require := require.New(t)

	nodeID0 := ids.GenerateTestNodeID()
	currentTime := time.Now()
	startTime := currentTime

	s := NewTestState()
	s.AddNode(nodeID0, startTime)

	up := NewManager(s).(*manager)
	up.clock.Set(currentTime)

	uptime, err := up.CalculateUptimePercentFrom(nodeID0, startTime.Truncate(time.Second))
	require.NoError(err)
	require.Equal(float64(1), uptime)
}

func TestCalculateUptimePercentage(t *testing.T) {
	require := require.New(t)

	nodeID0 := ids.GenerateTestNodeID()
	currentTime := time.Now()
	startTime := currentTime

	s := NewTestState()
	s.AddNode(nodeID0, startTime)

	up := NewManager(s).(*manager)

	currentTime = currentTime.Add(time.Second)
	up.clock.Set(currentTime)

	uptime, err := up.CalculateUptimePercentFrom(nodeID0, startTime)
	require.NoError(err)
	require.Equal(float64(0), uptime)
}

func TestShutdownUnixTimeRegression(t *testing.T) {
	require := require.New(t)

	nodeID0 := ids.GenerateTestNodeID()
	currentTime := time.Now()
	startTime := currentTime

	s := NewTestState()
	s.AddNode(nodeID0, startTime)

	up := NewManager(s).(*manager)
	up.clock.Set(currentTime)

	err := up.StartTracking([]ids.NodeID{nodeID0})
	require.NoError(err)

	err = up.Connect(nodeID0)
	require.NoError(err)

	currentTime = startTime.Add(time.Second)
	up.clock.Set(currentTime)

	err = up.Shutdown([]ids.NodeID{nodeID0})
	require.NoError(err)

	currentTime = startTime.Add(time.Second)
	up.clock.Set(currentTime)

	up = NewManager(s).(*manager)

	currentTime = startTime.Add(time.Second)
	up.clock.Set(currentTime)

	err = up.StartTracking([]ids.NodeID{nodeID0})
	require.NoError(err)

	err = up.Connect(nodeID0)
	require.NoError(err)

	currentTime = startTime.Add(time.Second)
	up.clock.Set(currentTime)

	perc, err := up.CalculateUptimePercent(nodeID0)
	require.NoError(err)
	require.GreaterOrEqual(float64(1), perc)
}

```

avalanchego/snow/uptime/mocks/calculator.go:
```
// Code generated by mockery v2.9.4. DO NOT EDIT.

package mocks

import (
	ids "github.com/ava-labs/avalanchego/ids"
	mock "github.com/stretchr/testify/mock"

	time "time"
)

// Calculator is an autogenerated mock type for the Calculator type
type Calculator struct {
	mock.Mock
}

// CalculateUptime provides a mock function with given fields: nodeID
func (_m *Calculator) CalculateUptime(nodeID ids.NodeID) (time.Duration, time.Time, error) {
	ret := _m.Called(nodeID)

	var r0 time.Duration
	if rf, ok := ret.Get(0).(func(ids.NodeID) time.Duration); ok {
		r0 = rf(nodeID)
	} else {
		r0 = ret.Get(0).(time.Duration)
	}

	var r1 time.Time
	if rf, ok := ret.Get(1).(func(ids.NodeID) time.Time); ok {
		r1 = rf(nodeID)
	} else {
		r1 = ret.Get(1).(time.Time)
	}

	var r2 error
	if rf, ok := ret.Get(2).(func(ids.NodeID) error); ok {
		r2 = rf(nodeID)
	} else {
		r2 = ret.Error(2)
	}

	return r0, r1, r2
}

// CalculateUptimePercent provides a mock function with given fields: nodeID
func (_m *Calculator) CalculateUptimePercent(nodeID ids.NodeID) (float64, error) {
	ret := _m.Called(nodeID)

	var r0 float64
	if rf, ok := ret.Get(0).(func(ids.NodeID) float64); ok {
		r0 = rf(nodeID)
	} else {
		r0 = ret.Get(0).(float64)
	}

	var r1 error
	if rf, ok := ret.Get(1).(func(ids.NodeID) error); ok {
		r1 = rf(nodeID)
	} else {
		r1 = ret.Error(1)
	}

	return r0, r1
}

// CalculateUptimePercentFrom provides a mock function with given fields: nodeID, startTime
func (_m *Calculator) CalculateUptimePercentFrom(nodeID ids.NodeID, startTime time.Time) (float64, error) {
	ret := _m.Called(nodeID, startTime)

	var r0 float64
	if rf, ok := ret.Get(0).(func(ids.NodeID, time.Time) float64); ok {
		r0 = rf(nodeID, startTime)
	} else {
		r0 = ret.Get(0).(float64)
	}

	var r1 error
	if rf, ok := ret.Get(1).(func(ids.NodeID, time.Time) error); ok {
		r1 = rf(nodeID, startTime)
	} else {
		r1 = ret.Error(1)
	}

	return r0, r1
}

```

avalanchego/snow/uptime/state.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package uptime

import (
	"time"

	"github.com/ava-labs/avalanchego/ids"
)

type State interface {
	// GetUptime returns [upDuration] and [lastUpdated] to be truncated (floored) to the nearest second
	GetUptime(nodeID ids.NodeID) (upDuration time.Duration, lastUpdated time.Time, err error)
	// SetUptime expects [upDuration] and [lastUpdated] to be truncated (floored) to the nearest second
	SetUptime(nodeID ids.NodeID, upDuration time.Duration, lastUpdated time.Time) error
	// GetStartTime returns [startTime] truncated (floored) to the nearest second
	GetStartTime(nodeID ids.NodeID) (startTime time.Time, err error)
}

```

avalanchego/snow/uptime/test_state.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package uptime

import (
	"time"

	"github.com/ava-labs/avalanchego/database"
	"github.com/ava-labs/avalanchego/ids"
)

var _ State = &TestState{}

type uptime struct {
	upDuration  time.Duration
	lastUpdated time.Time
	startTime   time.Time
}

type TestState struct {
	dbReadError  error
	dbWriteError error
	nodes        map[ids.NodeID]*uptime
}

func NewTestState() *TestState {
	return &TestState{
		nodes: make(map[ids.NodeID]*uptime),
	}
}

func (s *TestState) AddNode(nodeID ids.NodeID, startTime time.Time) {
	st := time.Unix(startTime.Unix(), 0)
	s.nodes[nodeID] = &uptime{
		lastUpdated: st,
		startTime:   st,
	}
}

func (s *TestState) GetUptime(nodeID ids.NodeID) (time.Duration, time.Time, error) {
	up, exists := s.nodes[nodeID]
	if !exists {
		return 0, time.Time{}, database.ErrNotFound
	}
	return up.upDuration, up.lastUpdated, s.dbReadError
}

func (s *TestState) SetUptime(nodeID ids.NodeID, upDuration time.Duration, lastUpdated time.Time) error {
	up, exists := s.nodes[nodeID]
	if !exists {
		return database.ErrNotFound
	}
	up.upDuration = upDuration
	up.lastUpdated = time.Unix(lastUpdated.Unix(), 0)
	return s.dbWriteError
}

func (s *TestState) GetStartTime(nodeID ids.NodeID) (time.Time, error) {
	up, exists := s.nodes[nodeID]
	if !exists {
		return time.Time{}, database.ErrNotFound
	}
	return up.startTime, s.dbReadError
}

```

avalanchego/snow/validators/connector.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package validators

import (
	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/version"
)

// Connector represents a handler that is called when a connection is marked as
// connected or disconnected
type Connector interface {
	Connected(id ids.NodeID, nodeVersion *version.Application) error
	Disconnected(id ids.NodeID) error
}

```

avalanchego/snow/validators/custom.go:
```
package validators

import (
	"errors"
	"fmt"
	"os"
	"strings"
	"time"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/utils/constants"
)

const (
	songbirdValidatorWeight = 50_000
	costonValidatorWeight   = 200_000
	customValidatorWeight   = 200_000
	customValidatorEnv      = "CUSTOM_VALIDATORS"
	customValidatorExpEnv   = "CUSTOM_VALIDATORS_EXPIRATION"
)

var (
	// Set dates before release
	songbirdValidatorsExpTime = time.Date(10000, time.January, 1, 0, 0, 0, 0, time.UTC)
	costonValidatorsExpTime   = time.Date(10000, time.January, 1, 0, 0, 0, 0, time.UTC)
	customValidatorsExpTime   = time.Date(10000, time.January, 1, 0, 0, 0, 0, time.UTC)
)

var (
	defaultValidators = defaultValidatorSet{}
	errNotInitialized = errors.New("default validator set not initialized")
)

func DefaultValidatorList() []Validator {
	return defaultValidators.list()
}

func IsDefaultValidator(vdrID ids.NodeID) bool {
	return defaultValidators.isValidator(vdrID)
}

func InitializeDefaultValidators(networkID uint32, timestamp time.Time) {
	defaultValidators.initialize(networkID, timestamp)
}

func ExpiredDefaultValidators(networkID uint32, timestamp time.Time) []Validator {
	return defaultValidators.expiredValidators(networkID, timestamp)
}

type defaultValidatorSet struct {
	initialized bool
	vdrMap      map[ids.NodeID]Validator
}

func (dvs *defaultValidatorSet) initialize(networkID uint32, timestamp time.Time) {
	if dvs.initialized {
		return
	}

	var vdrs []Validator
	switch networkID {
	case constants.LocalID:
		vdrs = loadCustomValidators(timestamp)
	case constants.SongbirdID:
		vdrs = loadSongbirdValidators(timestamp)
	case constants.CostonID:
		vdrs = loadCostonValidators(timestamp)
	}
	dvs.vdrMap = make(map[ids.NodeID]Validator)
	for _, vdr := range vdrs {
		dvs.vdrMap[vdr.ID()] = vdr
	}
	dvs.initialized = true
}

func (dvs *defaultValidatorSet) expiredValidators(networkID uint32, timestamp time.Time) []Validator {
	if !dvs.initialized {
		panic(errNotInitialized)
	}

	switch networkID {
	case constants.LocalID:
		if !timestamp.Before(customValidatorsExpTime) {
			return dvs.list()
		}
	case constants.SongbirdID:
		if !timestamp.Before(songbirdValidatorsExpTime) {
			return dvs.list()
		}
	case constants.CostonID:
		if !timestamp.Before(costonValidatorsExpTime) {
			return dvs.list()
		}
	}
	return nil
}

func (dvs *defaultValidatorSet) list() []Validator {
	if !dvs.initialized {
		panic(errNotInitialized)
	}
	vdrs := make([]Validator, 0, len(dvs.vdrMap))
	for _, vdr := range dvs.vdrMap {
		vdrs = append(vdrs, vdr)
	}
	return vdrs
}

func (dvs *defaultValidatorSet) isValidator(vdrID ids.NodeID) bool {
	if !dvs.initialized {
		panic(errNotInitialized)
	}
	_, ok := dvs.vdrMap[vdrID]
	return ok
}

func loadCustomValidators(timestamp time.Time) []Validator {
	customValidatorList := os.Getenv(customValidatorEnv)
	customValidatorExpString := os.Getenv(customValidatorExpEnv)
	if len(customValidatorExpString) > 0 {
		if t, err := time.Parse(time.RFC3339, customValidatorExpString); err == nil {
			customValidatorsExpTime = t
		}
		// Ignore if error occurs, use default expiration time
	}
	if !timestamp.Before(customValidatorsExpTime) {
		return nil
	}
	nodeIDs := strings.Split(customValidatorList, ",")
	return createValidators(nodeIDs, uint64(customValidatorWeight))
}

func loadCostonValidators(timestamp time.Time) []Validator {
	if !timestamp.Before(costonValidatorsExpTime) {
		return nil
	}
	nodeIDs := []string{
		"NodeID-5dDZXn99LCkDoEi6t9gTitZuQmhokxQTc",
		"NodeID-EkH8wyEshzEQBToAdR7Fexxcj9rrmEEHZ",
		"NodeID-FPAwqHjs8Mw8Cuki5bkm3vSVisZr8t2Lu",
		"NodeID-AQghDJTU3zuQj73itPtfTZz6CxsTQVD3R",
		"NodeID-HaZ4HpanjndqSuN252chFsTysmdND5meA",
	}
	return createValidators(nodeIDs, uint64(costonValidatorWeight))
}

func loadSongbirdValidators(timestamp time.Time) []Validator {
	if !timestamp.Before(songbirdValidatorsExpTime) {
		return nil
	}
	nodeIDs := []string{
		"NodeID-3M9KVT6ixi4gVMisbm5TnPXYXgFN5LHuv",
		"NodeID-NnX4fajAmyvpL9RLfheNdc47FKKDuQW8i",
		"NodeID-AzdF8JNU468uwZYGquHt7bhDrsggZpK67",
		"NodeID-FqeGcnLAXbDTthd382aP9uyu1i47paRRh",
		"NodeID-B9HuZ5hDkRodyRRsiMEHWgMmmMF7xSKbj",
		"NodeID-Jx3E1F7mfkseZmqnFgDUFV3eusMxVdT6Z",
		"NodeID-FnvWuwvJGezs4uaBLujkfeM8U3gmAUY3Z",
		"NodeID-LhVs6hzHjBcEkzA1Eu8Qxb9nEQAk1Qbgf",
		"NodeID-9SqDo3MxpvEDN4bE4rLTyM7HkkKAw4h96",
		"NodeID-4tStYRTi3KDxFmv1YHTZAQxbzeyMA7z52",
		"NodeID-8XnMh17zo6pB8Pa2zptRBi9TbbMZgij2t",
		"NodeID-Cn9P5wgg7d9RNLqm4dFLCUV2diCxpkj7f",
		"NodeID-PEDdah7g7Efiii1xw8ex2dH58oMfByzjb",
		"NodeID-QCt9AxMPt5nn445CQGoA3yktqkChnKmPY",
		"NodeID-9bWz6J61B8WbQtzeSyA1jsXosyVbuUJd1",
		"NodeID-DLMnewsEwtSH8Qk7p9RGzUVyZAaZVMKsk",
		"NodeID-7meEpyjmGbL577th58dm4nvvtVZiJusFp",
		"NodeID-JeYnnrUkuArAAe2Sjo47Z3X5yfeF7cw43",
		"NodeID-Fdwp9Wtjh5rxzuTCF9z4zrSM31y7ZzBQS",
		"NodeID-JdEBRLS98PansyFKQUzFKqk4xqrVZ41nC",
	}
	return createValidators(nodeIDs, uint64(songbirdValidatorWeight))
}

func createValidators(nodeIDs []string, weight uint64) (vdrs []Validator) {
	for _, nodeID := range nodeIDs {
		if nodeID == "" {
			continue
		}

		shortID, err := ids.ShortFromPrefixedString(nodeID, ids.NodeIDPrefix)
		if err != nil {
			panic(fmt.Sprintf("invalid validator node ID: %s", nodeID))
		}
		vdrs = append(vdrs, &validator{
			nodeID: ids.NodeID(shortID),
			weight: weight,
		})
	}
	return
}

```

avalanchego/snow/validators/custom_test.go:
```
package validators

import (
	"testing"
	"time"

	"github.com/ava-labs/avalanchego/utils/constants"
	"github.com/stretchr/testify/require"
)

func TestValidatorsBeforeExpiration(t *testing.T) {
	songbirdValidatorsExpTime = time.Date(2024, time.February, 1, 0, 0, 0, 0, time.UTC)

	vs := defaultValidatorSet{}
	vs.initialize(constants.SongbirdID, time.Date(2024, time.January, 1, 0, 0, 0, 0, time.UTC))

	vds := vs.list()
	require.Len(t, vds, 20)

	expVdrs := vs.expiredValidators(constants.SongbirdID, time.Date(2024, time.February, 2, 0, 0, 0, 0, time.UTC))
	require.Len(t, expVdrs, 20)
}

func TestValidatorsAfterExpiration(t *testing.T) {
	songbirdValidatorsExpTime = time.Date(2024, time.February, 1, 0, 0, 0, 0, time.UTC)

	vs := defaultValidatorSet{}
	vs.initialize(constants.SongbirdID, time.Date(2024, time.March, 1, 0, 0, 0, 0, time.UTC))

	vds := vs.list()
	require.Len(t, vds, 0)
}

```

avalanchego/snow/validators/manager.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package validators

import (
	"fmt"
	"strings"
	"sync"

	"github.com/ava-labs/avalanchego/ids"
)

var _ Manager = &manager{}

// Manager holds the validator set of each subnet
type Manager interface {
	fmt.Stringer

	// Set a subnet's validator set
	Set(ids.ID, Set) error

	// AddWeight adds weight to a given validator on the given subnet
	AddWeight(ids.ID, ids.NodeID, uint64) error

	// RemoveWeight removes weight from a given validator on a given subnet
	RemoveWeight(ids.ID, ids.NodeID, uint64) error

	// GetValidators returns the validator set for the given subnet
	// Returns false if the subnet doesn't exist
	GetValidators(ids.ID) (Set, bool)

	// MaskValidator hides the named validator from future samplings
	MaskValidator(ids.NodeID) error

	// RevealValidator ensures the named validator is not hidden from future
	// samplings
	RevealValidator(ids.NodeID) error

	// Contains returns true if there is a validator with the specified ID
	// currently in the set.
	Contains(ids.ID, ids.NodeID) bool
}

// NewManager returns a new, empty manager
func NewManager() Manager {
	return &manager{
		subnetToVdrs: make(map[ids.ID]Set),
	}
}

type manager struct {
	lock sync.RWMutex

	// Key: Subnet ID
	// Value: The validators that validate the subnet
	subnetToVdrs map[ids.ID]Set

	maskedVdrs ids.NodeIDSet
}

func (m *manager) Set(subnetID ids.ID, newSet Set) error {
	m.lock.Lock()
	defer m.lock.Unlock()

	oldSet, exists := m.subnetToVdrs[subnetID]
	if !exists {
		m.subnetToVdrs[subnetID] = newSet
		return nil
	}
	return oldSet.Set(newSet.List())
}

func (m *manager) AddWeight(subnetID ids.ID, vdrID ids.NodeID, weight uint64) error {
	m.lock.Lock()
	defer m.lock.Unlock()

	vdrs, ok := m.subnetToVdrs[subnetID]
	if !ok {
		vdrs = NewSet()
		for _, maskedVdrID := range m.maskedVdrs.List() {
			if err := vdrs.MaskValidator(maskedVdrID); err != nil {
				return err
			}
		}
		m.subnetToVdrs[subnetID] = vdrs
	}
	return vdrs.AddWeight(vdrID, weight)
}

func (m *manager) RemoveWeight(subnetID ids.ID, vdrID ids.NodeID, weight uint64) error {
	m.lock.Lock()
	defer m.lock.Unlock()

	if vdrs, ok := m.subnetToVdrs[subnetID]; ok {
		return vdrs.RemoveWeight(vdrID, weight)
	}
	return nil
}

func (m *manager) GetValidators(subnetID ids.ID) (Set, bool) {
	m.lock.RLock()
	defer m.lock.RUnlock()

	vdrs, ok := m.subnetToVdrs[subnetID]
	return vdrs, ok
}

func (m *manager) MaskValidator(vdrID ids.NodeID) error {
	m.lock.Lock()
	defer m.lock.Unlock()

	if m.maskedVdrs.Contains(vdrID) {
		return nil
	}
	m.maskedVdrs.Add(vdrID)

	for _, vdrs := range m.subnetToVdrs {
		if err := vdrs.MaskValidator(vdrID); err != nil {
			return err
		}
	}
	return nil
}

func (m *manager) RevealValidator(vdrID ids.NodeID) error {
	m.lock.Lock()
	defer m.lock.Unlock()

	if !m.maskedVdrs.Contains(vdrID) {
		return nil
	}
	m.maskedVdrs.Remove(vdrID)

	for _, vdrs := range m.subnetToVdrs {
		if err := vdrs.RevealValidator(vdrID); err != nil {
			return err
		}
	}
	return nil
}

func (m *manager) Contains(subnetID ids.ID, vdrID ids.NodeID) bool {
	m.lock.RLock()
	defer m.lock.RUnlock()

	vdrs, ok := m.subnetToVdrs[subnetID]
	if ok {
		return vdrs.Contains(vdrID)
	}
	return false
}

func (m *manager) String() string {
	m.lock.RLock()
	defer m.lock.RUnlock()

	subnets := make([]ids.ID, 0, len(m.subnetToVdrs))
	for subnetID := range m.subnetToVdrs {
		subnets = append(subnets, subnetID)
	}
	ids.SortIDs(subnets)

	sb := strings.Builder{}

	sb.WriteString(fmt.Sprintf("Validator Manager: (Size = %d)",
		len(subnets),
	))
	for _, subnetID := range subnets {
		vdrs := m.subnetToVdrs[subnetID]
		sb.WriteString(fmt.Sprintf(
			"\n    Subnet[%s]: %s",
			subnetID,
			vdrs.PrefixedString("    "),
		))
	}

	return sb.String()
}

```

avalanchego/snow/validators/set.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package validators

import (
	"fmt"
	"strings"
	"sync"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/utils/formatting"
	"github.com/ava-labs/avalanchego/utils/sampler"

	safemath "github.com/ava-labs/avalanchego/utils/math"
)

const (
	// If, when the validator set is reset, cap(set)/len(set) > MaxExcessCapacityFactor,
	// the underlying arrays' capacities will be reduced by a factor of capacityReductionFactor.
	// Higher value for maxExcessCapacityFactor --> less aggressive array downsizing --> less memory allocations
	// but more unnecessary data in the underlying array that can't be garbage collected.
	// Higher value for capacityReductionFactor --> more aggressive array downsizing --> more memory allocations
	// but less unnecessary data in the underlying array that can't be garbage collected.
	maxExcessCapacityFactor = 4
	capacityReductionFactor = 2
)

var _ Set = &set{}

// Set of validators that can be sampled
type Set interface {
	fmt.Stringer
	PrefixedString(string) string

	// Set removes all the current validators and adds all the provided
	// validators to the set.
	Set([]Validator) error

	// AddWeight to a staker.
	AddWeight(ids.NodeID, uint64) error

	// GetWeight retrieves the validator weight from the set.
	GetWeight(ids.NodeID) (uint64, bool)

	// SubsetWeight returns the sum of the weights of the validators.
	SubsetWeight(ids.NodeIDSet) (uint64, error)

	// RemoveWeight from a staker.
	RemoveWeight(ids.NodeID, uint64) error

	// Contains returns true if there is a validator with the specified ID
	// currently in the set.
	Contains(ids.NodeID) bool

	// Len returns the number of validators currently in the set.
	Len() int

	// List all the validators in this group
	List() []Validator

	// Weight returns the cumulative weight of all validators in the set.
	Weight() uint64

	// Sample returns a collection of validators, potentially with duplicates.
	// If sampling the requested size isn't possible, an error will be returned.
	Sample(size int) ([]Validator, error)

	// MaskValidator hides the named validator from future samplings
	MaskValidator(ids.NodeID) error

	// When a validator's weight changes, or a validator is added/removed,
	// this listener is called.
	RegisterCallbackListener(SetCallbackListener)

	RevealValidator(ids.NodeID) error
}

type SetCallbackListener interface {
	OnValidatorAdded(validatorID ids.NodeID, weight uint64)
	OnValidatorRemoved(validatorID ids.NodeID, weight uint64)
	OnValidatorWeightChanged(validatorID ids.NodeID, oldWeight, newWeight uint64)
}

// NewSet returns a new, empty set of validators.
func NewSet() Set {
	return &set{
		vdrMap:  make(map[ids.NodeID]int),
		sampler: sampler.NewWeightedWithoutReplacement(),
	}
}

// NewBestSet returns a new, empty set of validators.
func NewBestSet(expectedSampleSize int) Set {
	return &set{
		vdrMap:  make(map[ids.NodeID]int),
		sampler: sampler.NewBestWeightedWithoutReplacement(expectedSampleSize),
	}
}

// set of validators. Validator function results are cached. Therefore, to
// update a validators weight, one should ensure to call add with the updated
// validator.
type set struct {
	initialized       bool
	lock              sync.RWMutex
	vdrMap            map[ids.NodeID]int
	vdrSlice          []*validator
	vdrWeights        []uint64
	vdrMaskedWeights  []uint64
	sampler           sampler.WeightedWithoutReplacement
	totalWeight       uint64
	maskedVdrs        ids.NodeIDSet
	callbackListeners []SetCallbackListener
}

func (s *set) Set(vdrs []Validator) error {
	s.lock.Lock()
	defer s.lock.Unlock()

	return s.set(vdrs)
}

func (s *set) set(vdrs []Validator) error {
	// find all the nodes that are going to be added or have their weight changed
	nodesInResultSet := ids.NewNodeIDSet(len(vdrs))
	for _, vdr := range vdrs {
		vdrID := vdr.ID()
		if nodesInResultSet.Contains(vdrID) {
			continue
		}
		nodesInResultSet.Add(vdrID)

		newWeight := vdr.Weight()
		index, contains := s.vdrMap[vdrID]
		if !contains {
			s.callValidatorAddedCallbacks(vdrID, newWeight)
			continue
		}

		existingWeight := s.vdrWeights[index]
		if existingWeight != newWeight {
			s.callWeightChangeCallbacks(vdrID, existingWeight, newWeight)
		}
	}

	// find all nodes that are going to be removed
	for nodeID, index := range s.vdrMap {
		if !nodesInResultSet.Contains(nodeID) {
			s.callValidatorRemovedCallbacks(nodeID, s.vdrWeights[index])
		}
	}

	lenVdrs := len(vdrs)
	// If the underlying arrays are much larger than necessary, resize them to
	// allow garbage collection of unused memory
	if cap(s.vdrSlice) > len(s.vdrSlice)*maxExcessCapacityFactor {
		newCap := cap(s.vdrSlice) / capacityReductionFactor
		if newCap < lenVdrs {
			newCap = lenVdrs
		}
		s.vdrSlice = make([]*validator, 0, newCap)
		s.vdrWeights = make([]uint64, 0, newCap)
		s.vdrMaskedWeights = make([]uint64, 0, newCap)
	} else {
		s.vdrSlice = s.vdrSlice[:0]
		s.vdrWeights = s.vdrWeights[:0]
		s.vdrMaskedWeights = s.vdrMaskedWeights[:0]
	}
	s.vdrMap = make(map[ids.NodeID]int, lenVdrs)
	s.totalWeight = 0
	s.initialized = false

	for _, vdr := range vdrs {
		vdrID := vdr.ID()
		if s.contains(vdrID) {
			continue
		}
		w := vdr.Weight()
		if w == 0 {
			continue // This validator would never be sampled anyway
		}

		i := len(s.vdrSlice)
		s.vdrMap[vdrID] = i
		s.vdrSlice = append(s.vdrSlice, &validator{
			nodeID: vdr.ID(),
			weight: vdr.Weight(),
		})
		s.vdrWeights = append(s.vdrWeights, w)
		s.vdrMaskedWeights = append(s.vdrMaskedWeights, 0)

		if s.maskedVdrs.Contains(vdrID) {
			continue
		}
		s.vdrMaskedWeights[len(s.vdrMaskedWeights)-1] = w

		newTotalWeight, err := safemath.Add64(s.totalWeight, w)
		if err != nil {
			return err
		}
		s.totalWeight = newTotalWeight
	}
	return nil
}

func (s *set) AddWeight(vdrID ids.NodeID, weight uint64) error {
	if weight == 0 {
		return nil // This validator would never be sampled anyway
	}
	s.lock.Lock()
	defer s.lock.Unlock()

	return s.addWeight(vdrID, weight)
}

func (s *set) addWeight(vdrID ids.NodeID, weight uint64) error {
	var vdr *validator
	i, nodeExists := s.vdrMap[vdrID]
	if !nodeExists {
		vdr = &validator{
			nodeID: vdrID,
		}
		i = len(s.vdrSlice)
		s.vdrSlice = append(s.vdrSlice, vdr)
		s.vdrWeights = append(s.vdrWeights, 0)
		s.vdrMaskedWeights = append(s.vdrMaskedWeights, 0)
		s.vdrMap[vdrID] = i
		s.callValidatorAddedCallbacks(vdrID, weight)
	} else {
		vdr = s.vdrSlice[i]
	}

	oldWeight := s.vdrWeights[i]
	s.vdrWeights[i] += weight
	vdr.addWeight(weight)

	if nodeExists {
		s.callWeightChangeCallbacks(vdrID, oldWeight, vdr.weight)
	}

	if s.maskedVdrs.Contains(vdrID) {
		return nil
	}
	s.vdrMaskedWeights[i] += weight

	newTotalWeight, err := safemath.Add64(s.totalWeight, weight)
	if err != nil {
		return nil
	}
	s.totalWeight = newTotalWeight
	s.initialized = false
	return nil
}

func (s *set) GetWeight(vdrID ids.NodeID) (uint64, bool) {
	s.lock.RLock()
	defer s.lock.RUnlock()

	return s.getWeight(vdrID)
}

func (s *set) getWeight(vdrID ids.NodeID) (uint64, bool) {
	if index, ok := s.vdrMap[vdrID]; ok {
		return s.vdrMaskedWeights[index], true
	}
	return 0, false
}

func (s *set) SubsetWeight(subset ids.NodeIDSet) (uint64, error) {
	s.lock.RLock()
	defer s.lock.RUnlock()

	totalWeight := uint64(0)
	for vdrID := range subset {
		weight, ok := s.getWeight(vdrID)
		if !ok {
			continue
		}
		newWeight, err := safemath.Add64(totalWeight, weight)
		if err != nil {
			return 0, err
		}
		totalWeight = newWeight
	}
	return totalWeight, nil
}

func (s *set) RemoveWeight(vdrID ids.NodeID, weight uint64) error {
	if weight == 0 {
		return nil
	}
	s.lock.Lock()
	defer s.lock.Unlock()

	return s.removeWeight(vdrID, weight)
}

func (s *set) removeWeight(vdrID ids.NodeID, weight uint64) error {
	i, ok := s.vdrMap[vdrID]
	if !ok {
		return nil
	}

	// Validator exists
	vdr := s.vdrSlice[i]

	oldWeight := s.vdrWeights[i]
	weight = safemath.Min64(oldWeight, weight)
	s.vdrWeights[i] -= weight
	vdr.removeWeight(weight)
	if !s.maskedVdrs.Contains(vdrID) {
		s.totalWeight -= weight
		s.vdrMaskedWeights[i] -= weight
	}

	if vdr.Weight() == 0 {
		s.callValidatorRemovedCallbacks(vdrID, oldWeight)
		if err := s.remove(vdrID); err != nil {
			return err
		}
	} else {
		s.callWeightChangeCallbacks(vdrID, oldWeight, vdr.weight)
	}
	s.initialized = false
	return nil
}

func (s *set) Get(vdrID ids.NodeID) (Validator, bool) {
	s.lock.RLock()
	defer s.lock.RUnlock()

	return s.get(vdrID)
}

func (s *set) get(vdrID ids.NodeID) (Validator, bool) {
	index, ok := s.vdrMap[vdrID]
	if !ok {
		return nil, false
	}
	return s.vdrSlice[index], true
}

func (s *set) remove(vdrID ids.NodeID) error {
	// Get the element to remove
	i, contains := s.vdrMap[vdrID]
	if !contains {
		return nil
	}

	// Get the last element
	e := len(s.vdrSlice) - 1
	eVdr := s.vdrSlice[e]

	// Move e -> i
	iElem := s.vdrSlice[i]
	s.vdrMap[eVdr.ID()] = i
	s.vdrSlice[i] = eVdr
	s.vdrWeights[i] = s.vdrWeights[e]
	s.vdrMaskedWeights[i] = s.vdrMaskedWeights[e]

	// Remove i
	delete(s.vdrMap, vdrID)
	s.vdrSlice[e] = nil
	s.vdrSlice = s.vdrSlice[:e]
	s.vdrWeights = s.vdrWeights[:e]
	s.vdrMaskedWeights = s.vdrMaskedWeights[:e]

	if !s.maskedVdrs.Contains(vdrID) {
		newTotalWeight, err := safemath.Sub64(s.totalWeight, iElem.Weight())
		if err != nil {
			return err
		}
		s.totalWeight = newTotalWeight
	}
	s.initialized = false
	return nil
}

func (s *set) Contains(vdrID ids.NodeID) bool {
	s.lock.RLock()
	defer s.lock.RUnlock()

	return s.contains(vdrID)
}

func (s *set) contains(vdrID ids.NodeID) bool {
	_, contains := s.vdrMap[vdrID]
	return contains
}

func (s *set) Len() int {
	s.lock.RLock()
	defer s.lock.RUnlock()

	return s.len()
}

func (s *set) len() int { return len(s.vdrSlice) }

func (s *set) List() []Validator {
	s.lock.RLock()
	defer s.lock.RUnlock()

	return s.list()
}

func (s *set) list() []Validator {
	list := make([]Validator, len(s.vdrSlice))
	for i, vdr := range s.vdrSlice {
		list[i] = vdr
	}
	return list
}

func (s *set) Sample(size int) ([]Validator, error) {
	if size == 0 {
		return nil, nil
	}
	s.lock.Lock()
	defer s.lock.Unlock()

	return s.sample(size)
}

func (s *set) sample(size int) ([]Validator, error) {
	if !s.initialized {
		if err := s.sampler.Initialize(s.vdrMaskedWeights); err != nil {
			return nil, err
		}
		s.initialized = true
	}
	indices, err := s.sampler.Sample(size)
	if err != nil {
		return nil, err
	}

	list := make([]Validator, size)
	for i, index := range indices {
		list[i] = s.vdrSlice[index]
	}
	return list, nil
}

func (s *set) Weight() uint64 {
	s.lock.RLock()
	defer s.lock.RUnlock()

	return s.totalWeight
}

func (s *set) String() string {
	return s.PrefixedString("")
}

func (s *set) PrefixedString(prefix string) string {
	s.lock.RLock()
	defer s.lock.RUnlock()

	return s.prefixedString(prefix)
}

func (s *set) prefixedString(prefix string) string {
	sb := strings.Builder{}

	totalWeight := uint64(0)
	for _, weight := range s.vdrWeights {
		totalWeight += weight
	}

	sb.WriteString(fmt.Sprintf("Validator Set: (Size = %d, SampleableWeight = %d, Weight = %d)",
		len(s.vdrSlice),
		s.totalWeight,
		totalWeight,
	))
	format := fmt.Sprintf("\n%s    Validator[%s]: %%33s, %%d/%%d", prefix, formatting.IntFormat(len(s.vdrSlice)-1))
	for i, vdr := range s.vdrSlice {
		sb.WriteString(fmt.Sprintf(format,
			i,
			vdr.ID(),
			s.vdrMaskedWeights[i],
			vdr.Weight()))
	}

	return sb.String()
}

func (s *set) MaskValidator(vdrID ids.NodeID) error {
	s.lock.Lock()
	defer s.lock.Unlock()

	return s.maskValidator(vdrID)
}

func (s *set) maskValidator(vdrID ids.NodeID) error {
	if s.maskedVdrs.Contains(vdrID) {
		return nil
	}

	s.maskedVdrs.Add(vdrID)

	// Get the element to mask
	i, contains := s.vdrMap[vdrID]
	if !contains {
		return nil
	}

	s.vdrMaskedWeights[i] = 0
	s.totalWeight -= s.vdrWeights[i]
	s.initialized = false

	return nil
}

func (s *set) RevealValidator(vdrID ids.NodeID) error {
	s.lock.Lock()
	defer s.lock.Unlock()

	return s.revealValidator(vdrID)
}

func (s *set) revealValidator(vdrID ids.NodeID) error {
	if !s.maskedVdrs.Contains(vdrID) {
		return nil
	}

	s.maskedVdrs.Remove(vdrID)

	// Get the element to reveal
	i, contains := s.vdrMap[vdrID]
	if !contains {
		return nil
	}

	weight := s.vdrWeights[i]
	s.vdrMaskedWeights[i] = weight
	newTotalWeight, err := safemath.Add64(s.totalWeight, weight)
	if err != nil {
		return err
	}
	s.totalWeight = newTotalWeight
	s.initialized = false

	return nil
}

func (s *set) RegisterCallbackListener(callbackListener SetCallbackListener) {
	s.lock.Lock()
	defer s.lock.Unlock()

	s.callbackListeners = append(s.callbackListeners, callbackListener)
	for node, index := range s.vdrMap {
		callbackListener.OnValidatorAdded(node, s.vdrWeights[index])
	}
}

// Assumes [s.lock] is held
func (s *set) callWeightChangeCallbacks(node ids.NodeID, oldWeight, newWeight uint64) {
	for _, callbackListener := range s.callbackListeners {
		callbackListener.OnValidatorWeightChanged(node, oldWeight, newWeight)
	}
}

// Assumes [s.lock] is held
func (s *set) callValidatorAddedCallbacks(node ids.NodeID, weight uint64) {
	for _, callbackListener := range s.callbackListeners {
		callbackListener.OnValidatorAdded(node, weight)
	}
}

// Assumes [s.lock] is held
func (s *set) callValidatorRemovedCallbacks(node ids.NodeID, weight uint64) {
	for _, callbackListener := range s.callbackListeners {
		callbackListener.OnValidatorRemoved(node, weight)
	}
}

```

avalanchego/snow/validators/set_test.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package validators

import (
	"math"
	"testing"
	"time"

	"github.com/stretchr/testify/require"

	"github.com/ava-labs/avalanchego/ids"
	"github.com/ava-labs/avalanchego/utils/constants"
)

func TestSetSet(t *testing.T) {
	vdr0 := NewValidator(ids.EmptyNodeID, 1)
	vdr1 := NewValidator(ids.NodeID{0xFF}, math.MaxInt64-1)
	// Should be discarded, because it has a weight of 0
	vdr2 := NewValidator(ids.NodeID{0xAA}, 0)

	s := NewSet()
	err := s.Set([]Validator{vdr0, vdr1, vdr2})
	require.NoError(t, err)

	length := s.Len()
	require.Equal(t, 2, length, "should have two validators")

	contains := s.Contains(vdr0.ID())
	require.True(t, contains, "should have contained vdr0")

	contains = s.Contains(vdr1.ID())
	require.True(t, contains, "should have contained vdr1")

	sampled, err := s.Sample(1)
	require.NoError(t, err)
	require.Len(t, sampled, 1, "should have only sampled one validator")
	require.Equal(t, vdr1.ID(), sampled[0].ID(), "should have sampled vdr1")
}

func TestSamplerSample(t *testing.T) {
	vdr0 := ids.GenerateTestNodeID()
	vdr1 := ids.GenerateTestNodeID()

	s := NewSet()
	err := s.AddWeight(vdr0, 1)
	require.NoError(t, err)

	sampled, err := s.Sample(1)
	require.NoError(t, err)
	require.Len(t, sampled, 1, "should have only sampled one validator")
	require.Equal(t, vdr0, sampled[0].ID(), "should have sampled vdr0")

	_, err = s.Sample(2)
	require.Error(t, err, "should have errored during sampling")

	err = s.AddWeight(vdr1, math.MaxInt64-1)
	require.NoError(t, err)

	sampled, err = s.Sample(1)
	require.NoError(t, err)
	require.Len(t, sampled, 1, "should have only sampled one validator")
	require.Equal(t, vdr1, sampled[0].ID(), "should have sampled vdr1")

	sampled, err = s.Sample(2)
	require.NoError(t, err)
	require.Len(t, sampled, 2, "should have sampled two validators")
	require.Equal(t, vdr1, sampled[0].ID(), "should have sampled vdr1")
	require.Equal(t, vdr1, sampled[1].ID(), "should have sampled vdr1")

	sampled, err = s.Sample(3)
	require.NoError(t, err)
	require.Len(t, sampled, 3, "should have sampled three validators")
	require.Equal(t, vdr1, sampled[0].ID(), "should have sampled vdr1")
	require.Equal(t, vdr1, sampled[1].ID(), "should have sampled vdr1")
	require.Equal(t, vdr1, sampled[2].ID(), "should have sampled vdr1")
}

func TestSamplerDuplicate(t *testing.T) {
	vdr0 := ids.GenerateTestNodeID()
	vdr1 := ids.GenerateTestNodeID()

	s := NewSet()
	err := s.AddWeight(vdr0, 1)
	require.NoError(t, err)

	err = s.AddWeight(vdr1, 1)
	require.NoError(t, err)

	err = s.AddWeight(vdr1, math.MaxInt64-2)
	require.NoError(t, err)

	sampled, err := s.Sample(1)
	require.NoError(t, err)
	require.Len(t, sampled, 1, "should have only sampled one validator")
	require.Equal(t, vdr1, sampled[0].ID(), "should have sampled vdr1")
}

func TestSamplerContains(t *testing.T) {
	vdr := ids.GenerateTestNodeID()

	s := NewSet()
	err := s.AddWeight(vdr, 1)
	require.NoError(t, err)

	contains := s.Contains(vdr)
	require.True(t, contains, "should have contained validator")

	err = s.RemoveWeight(vdr, 1)
	require.NoError(t, err)

	contains = s.Contains(vdr)
	require.False(t, contains, "shouldn't have contained validator")
}

func TestSamplerString(t *testing.T) {
	vdr0 := ids.EmptyNodeID
	vdr1 := ids.NodeID{
		0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,
		0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,
	}

	s := NewSet()
	err := s.AddWeight(vdr0, 1)
	require.NoError(t, err)

	err = s.AddWeight(vdr1, math.MaxInt64-1)
	require.NoError(t, err)

	expected := "Validator Set: (Size = 2, SampleableWeight = 9223372036854775807, Weight = 9223372036854775807)\n" +
		"    Validator[0]: NodeID-111111111111111111116DBWJs, 1/1\n" +
		"    Validator[1]: NodeID-QLbz7JHiBTspS962RLKV8GndWFwdYhk6V, 9223372036854775806/9223372036854775806"
	result := s.String()
	require.Equal(t, expected, result, "wrong string returned")
}

func TestSetWeight(t *testing.T) {
	vdr0 := ids.NodeID{1}
	weight0 := uint64(93)
	vdr1 := ids.NodeID{2}
	weight1 := uint64(123)

	s := NewSet()
	err := s.AddWeight(vdr0, weight0)
	require.NoError(t, err)

	err = s.AddWeight(vdr1, weight1)
	require.NoError(t, err)

	setWeight := s.Weight()
	expectedWeight := weight0 + weight1
	require.Equal(t, expectedWeight, setWeight, "wrong set weight")
}

func TestSetSubsetWeight(t *testing.T) {
	vdr0 := ids.NodeID{1}
	weight0 := uint64(93)
	vdr1 := ids.NodeID{2}
	weight1 := uint64(123)
	vdr2 := ids.NodeID{3}
	weight2 := uint64(810)
	subset := ids.NodeIDSet{}
	subset.Add(vdr0)
	subset.Add(vdr1)

	s := NewSet()
	err := s.AddWeight(vdr0, weight0)
	require.NoError(t, err)

	err = s.AddWeight(vdr1, weight1)
	require.NoError(t, err)
	err = s.AddWeight(vdr2, weight2)
	require.NoError(t, err)

	subsetWeight, err := s.SubsetWeight(subset)
	if err != nil {
		t.Fatal(err)
	}
	expectedWeight := weight0 + weight1
	require.Equal(t, expectedWeight, subsetWeight, "wrong subset weight")
}

func TestSamplerMasked(t *testing.T) {
	vdr0 := ids.EmptyNodeID
	vdr1 := ids.NodeID{
		0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,
		0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,
	}

	s := NewSet()
	err := s.AddWeight(vdr0, 1)
	require.NoError(t, err)

	err = s.MaskValidator(vdr1)
	require.NoError(t, err)

	{
		expected := "Validator Set: (Size = 1, SampleableWeight = 1, Weight = 1)\n" +
			"    Validator[0]: NodeID-111111111111111111116DBWJs, 1/1"
		result := s.String()
		require.Equal(t, expected, result, "wrong string returned")
	}

	err = s.AddWeight(vdr1, math.MaxInt64-1)
	require.NoError(t, err)

	{
		expected := "Validator Set: (Size = 2, SampleableWeight = 1, Weight = 9223372036854775807)\n" +
			"    Validator[0]: NodeID-111111111111111111116DBWJs, 1/1\n" +
			"    Validator[1]: NodeID-QLbz7JHiBTspS962RLKV8GndWFwdYhk6V, 0/9223372036854775806"
		result := s.String()
		require.Equal(t, expected, result, "wrong string returned")
	}

	err = s.RevealValidator(vdr1)
	require.NoError(t, err)

	{
		expected := "Validator Set: (Size = 2, SampleableWeight = 9223372036854775807, Weight = 9223372036854775807)\n" +
			"    Validator[0]: NodeID-111111111111111111116DBWJs, 1/1\n" +
			"    Validator[1]: NodeID-QLbz7JHiBTspS962RLKV8GndWFwdYhk6V, 9223372036854775806/9223372036854775806"
		result := s.String()
		require.Equal(t, expected, result, "wrong string returned")
	}

	err = s.MaskValidator(vdr1)
	require.NoError(t, err)

	{
		expected := "Validator Set: (Size = 2, SampleableWeight = 1, Weight = 9223372036854775807)\n" +
			"    Validator[0]: NodeID-111111111111111111116DBWJs, 1/1\n" +
			"    Validator[1]: NodeID-QLbz7JHiBTspS962RLKV8GndWFwdYhk6V, 0/9223372036854775806"
		result := s.String()
		require.Equal(t, expected, result, "wrong string returned")
	}

	err = s.RevealValidator(vdr1)
	require.NoError(t, err)

	{
		expected := "Validator Set: (Size = 2, SampleableWeight = 9223372036854775807, Weight = 9223372036854775807)\n" +
			"    Validator[0]: NodeID-111111111111111111116DBWJs, 1/1\n" +
			"    Validator[1]: NodeID-QLbz7JHiBTspS962RLKV8GndWFwdYhk6V, 9223372036854775806/9223372036854775806"
		result := s.String()
		require.Equal(t, expected, result, "wrong string returned")
	}

	err = s.RevealValidator(vdr1)
	require.NoError(t, err)

	{
		expected := "Validator Set: (Size = 2, SampleableWeight = 9223372036854775807, Weight = 9223372036854775807)\n" +
			"    Validator[0]: NodeID-111111111111111111116DBWJs, 1/1\n" +
			"    Validator[1]: NodeID-QLbz7JHiBTspS962RLKV8GndWFwdYhk6V, 9223372036854775806/9223372036854775806"
		result := s.String()
		require.Equal(t, expected, result, "wrong string returned")
	}
}

var _ SetCallbackListener = &callbackListener{}

type callbackListener struct {
	t         *testing.T
	onWeight  func(ids.NodeID, uint64, uint64)
	onAdd     func(ids.NodeID, uint64)
	onRemoved func(ids.NodeID, uint64)
}

func (c *callbackListener) OnValidatorAdded(nodeID ids.NodeID, weight uint64) {
	if c.onAdd != nil {
		c.onAdd(nodeID, weight)
	} else {
		c.t.Fail()
	}
}

func (c *callbackListener) OnValidatorRemoved(nodeID ids.NodeID, weight uint64) {
	if c.onRemoved != nil {
		c.onRemoved(nodeID, weight)
	} else {
		c.t.Fail()
	}
}

func (c *callbackListener) OnValidatorWeightChanged(nodeID ids.NodeID, oldWeight, newWeight uint64) {
	if c.onWeight != nil {
		c.onWeight(nodeID, oldWeight, newWeight)
	} else {
		c.t.Fail()
	}
}

func TestSetAddWeightCallback(t *testing.T) {
	vdr0 := ids.NodeID{1}
	weight0 := uint64(1)
	weight1 := uint64(93)

	s := NewSet()
	err := s.AddWeight(vdr0, weight0)
	require.NoError(t, err)
	callcount := 0
	s.RegisterCallbackListener(&callbackListener{
		t: t,
		onAdd: func(nodeID ids.NodeID, weight uint64) {
			if nodeID == vdr0 {
				require.Equal(t, weight0, weight)
			}
			callcount++
		},
		onWeight: func(nodeID ids.NodeID, oldWeight, newWeight uint64) {
			require.Equal(t, vdr0, nodeID)
			require.Equal(t, weight0, oldWeight)
			require.Equal(t, weight0+weight1, newWeight)
			callcount++
		},
	})
	err = s.AddWeight(vdr0, weight1)
	require.NoError(t, err)
	require.Equal(t, 2, callcount)
}

func TestSetRemoveWeightCallback(t *testing.T) {
	vdr0 := ids.NodeID{1}
	weight0 := uint64(93)
	weight1 := uint64(92)

	s := NewSet()
	callcount := 0
	err := s.AddWeight(vdr0, weight0)
	require.NoError(t, err)
	s.RegisterCallbackListener(&callbackListener{
		t: t,
		onAdd: func(nodeID ids.NodeID, weight uint64) {
			if nodeID == vdr0 {
				require.Equal(t, weight0, weight)
			}
			callcount++
		},
		onWeight: func(nodeID ids.NodeID, oldWeight, newWeight uint64) {
			require.Equal(t, vdr0, nodeID)
			require.Equal(t, weight0, oldWeight)
			require.Equal(t, weight0-weight1, newWeight)
			callcount++
		},
	})
	err = s.RemoveWeight(vdr0, weight1)
	require.NoError(t, err)
	require.Equal(t, 2, callcount)
}

func TestSetValidatorRemovedCallback(t *testing.T) {
	vdr0 := ids.NodeID{1}
	weight0 := uint64(93)

	s := NewSet()
	callcount := 0
	err := s.AddWeight(vdr0, weight0)
	require.NoError(t, err)

	s.RegisterCallbackListener(&callbackListener{
		t: t,
		onAdd: func(nodeID ids.NodeID, weight uint64) {
			if nodeID == vdr0 {
				require.Equal(t, weight0, weight)
			}
			callcount++
		},
		onRemoved: func(nodeID ids.NodeID, weight uint64) {
			require.Equal(t, vdr0, nodeID)
			require.Equal(t, weight0, weight)
			callcount++
		},
	})
	err = s.RemoveWeight(vdr0, weight0)
	require.NoError(t, err)
	require.Equal(t, 2, callcount)
}

func TestSetValidatorSetCallback(t *testing.T) {
	vdr0 := ids.NodeID{1}
	weight0 := uint64(93)
	vdr1 := ids.NodeID{2}
	weight1 := uint64(94)
	vdr2 := ids.NodeID{3}
	weight2 := uint64(95)

	s := NewSet()
	err := s.AddWeight(vdr0, weight0)
	require.NoError(t, err)
	err = s.AddWeight(vdr1, weight1)
	require.NoError(t, err)

	newValidators := []Validator{&validator{nodeID: vdr0, weight: weight0}, &validator{nodeID: vdr2, weight: weight2}}
	callcount := 0
	s.RegisterCallbackListener(&callbackListener{
		t: t,
		onAdd: func(nodeID ids.NodeID, weight uint64) {
			if nodeID == vdr0 {
				require.Equal(t, weight0, weight)
			}
			if nodeID == vdr1 {
				require.Equal(t, weight1, weight)
			}
			if nodeID == vdr2 {
				require.Equal(t, weight2, weight)
			}
			callcount++
		},
		onRemoved: func(nodeID ids.NodeID, weight uint64) {
			require.Equal(t, vdr1, nodeID)
			require.Equal(t, weight1, weight)
			callcount++
		},
	})

	err = s.Set(newValidators)
	require.NoError(t, err)
	require.Equal(t, 4, callcount)
}

func TestDefaultValidatorExpiration(t *testing.T) {
	set := NewSet()

	t.Setenv("CUSTOM_VALIDATORS", "NodeID-5dDZXn99LCkDoEi6t9gTitZuQmhokxQTc,NodeID-AQghDJTU3zuQj73itPtfTZz6CxsTQVD3R")
	t.Setenv("CUSTOM_VALIDATORS_EXPIRATION", "2024-02-01T00:00:00Z")

	vs := defaultValidatorSet{}
	vs.initialize(constants.LocalID, time.Date(2024, time.January, 1, 0, 0, 0, 0, time.UTC))

	require.Len(t, vs.list(), 2)

	for _, v := range vs.list() {
		require.NoError(t, set.AddWeight(v.ID(), v.Weight()))
	}

	require.Equal(t, 2, set.Len())

	// Get expired validators
	expVdrs := vs.expiredValidators(constants.LocalID, time.Date(2024, time.February, 2, 0, 0, 0, 0, time.UTC))
	require.Len(t, expVdrs, 2)

	// Remove expired validators
	for _, v := range expVdrs {
		require.NoError(t, set.RemoveWeight(v.ID(), v.Weight()))
	}
	require.Equal(t, 0, set.Len())

	// Removing expired validators again should not error
	for _, v := range expVdrs {
		require.NoError(t, set.RemoveWeight(v.ID(), v.Weight()))
	}
}

```

avalanchego/snow/validators/state.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package validators

import (
	"sync"

	"github.com/ava-labs/avalanchego/ids"
)

var _ State = &lockedState{}

// State allows the lookup of validator sets on specified subnets at the
// requested P-chain height.
type State interface {
	// GetMinimumHeight returns the minimum height of the block still in the
	// proposal window.
	GetMinimumHeight() (uint64, error)
	// GetCurrentHeight returns the current height of the P-chain.
	GetCurrentHeight() (uint64, error)

	// GetValidatorSet returns the weights of the nodeIDs for the provided
	// subnet at the requested P-chain height.
	// The returned map should not be modified.
	GetValidatorSet(height uint64, subnetID ids.ID) (map[ids.NodeID]uint64, error)
}

type lockedState struct {
	lock sync.Locker
	s    State
}

func NewLockedState(lock sync.Locker, s State) State {
	return &lockedState{
		lock: lock,
		s:    s,
	}
}

func (s *lockedState) GetMinimumHeight() (uint64, error) {
	s.lock.Lock()
	defer s.lock.Unlock()

	return s.s.GetMinimumHeight()
}

func (s *lockedState) GetCurrentHeight() (uint64, error) {
	s.lock.Lock()
	defer s.lock.Unlock()

	return s.s.GetCurrentHeight()
}

func (s *lockedState) GetValidatorSet(height uint64, subnetID ids.ID) (map[ids.NodeID]uint64, error) {
	s.lock.Lock()
	defer s.lock.Unlock()

	return s.s.GetValidatorSet(height, subnetID)
}

type noValidators struct {
	State
}

func NewNoValidatorsState(state State) State {
	return &noValidators{
		State: state,
	}
}

func (*noValidators) GetValidatorSet(uint64, ids.ID) (map[ids.NodeID]uint64, error) {
	return nil, nil
}

```

avalanchego/snow/validators/test_state.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package validators

import (
	"errors"
	"testing"

	"github.com/ava-labs/avalanchego/ids"
)

var (
	errMinimumHeight   = errors.New("unexpectedly called GetMinimumHeight")
	errCurrentHeight   = errors.New("unexpectedly called GetCurrentHeight")
	errGetValidatorSet = errors.New("unexpectedly called GetValidatorSet")
)

var _ State = &TestState{}

type TestState struct {
	T *testing.T

	CantGetMinimumHeight,
	CantGetCurrentHeight,
	CantGetValidatorSet bool

	GetMinimumHeightF func() (uint64, error)
	GetCurrentHeightF func() (uint64, error)
	GetValidatorSetF  func(height uint64, subnetID ids.ID) (map[ids.NodeID]uint64, error)
}

func (vm *TestState) GetMinimumHeight() (uint64, error) {
	if vm.GetMinimumHeightF != nil {
		return vm.GetMinimumHeightF()
	}
	if vm.CantGetMinimumHeight && vm.T != nil {
		vm.T.Fatal(errMinimumHeight)
	}
	return 0, errMinimumHeight
}

func (vm *TestState) GetCurrentHeight() (uint64, error) {
	if vm.GetCurrentHeightF != nil {
		return vm.GetCurrentHeightF()
	}
	if vm.CantGetCurrentHeight && vm.T != nil {
		vm.T.Fatal(errCurrentHeight)
	}
	return 0, errCurrentHeight
}

func (vm *TestState) GetValidatorSet(height uint64, subnetID ids.ID) (map[ids.NodeID]uint64, error) {
	if vm.GetValidatorSetF != nil {
		return vm.GetValidatorSetF(height, subnetID)
	}
	if vm.CantGetValidatorSet && vm.T != nil {
		vm.T.Fatal(errGetValidatorSet)
	}
	return nil, errGetValidatorSet
}

```

avalanchego/snow/validators/validator.go:
```
// Copyright (C) 2019-2022, Ava Labs, Inc. All rights reserved.
// See the file LICENSE for licensing terms.

package validators

import (
	"math"

	"github.com/ava-labs/avalanchego/ids"

	safemath "github.com/ava-labs/avalanchego/utils/math"
)

var _ Validator = &validator{}

// Validator is the minimal description of someone that can be sampled.
type Validator interface {
	// ID returns the node ID of this validator
	ID() ids.NodeID

	// Weight that can be used for weighted sampling. If this validator is
	// validating the primary network, returns the amount of AVAX staked.
	Weight() uint64
}

// validator is a struct that contains the base values required by the validator
// interface.
type validator struct {
	nodeID ids.NodeID
	weight uint64
}

func (v *validator) ID() ids.NodeID { return v.nodeID }
func (v *validator) Weight() uint64 { return v.weight }

func (v *validator) addWeight(weight uint64) {
	newTotalWeight, err := safemath.Add64(weight, v.weight)
	if err != nil {
		newTotalWeight = math.MaxUint64
	}
	v.weight = newTotalWeight
}

func (v *validator) removeWeight(weight uint64) {
	newTotalWeight, err := safemath.Sub64(v.weight, weight)
	if err != nil {
		newTotalWeight = 0
	}
	v.weight = newTotalWeight
}

// NewValidator returns a validator object that implements the Validator
// interface
func NewValidator(
	nodeID ids.NodeID,
	weight uint64,
) Validator {
	return &validator{
		nodeID: nodeID,
		weight: weight,
	}
}

// GenerateRandomValidator creates a random validator with the provided weight
func GenerateRandomValidator(weight uint64) Validator {
	return NewValidator(
		ids.GenerateTestNodeID(),
		weight,
	)
}

```

